{"snippet": "print(classification_report(Y,Y_pred))\n", "intent": "But the model is blindly always predicting \"No default\".\n"}
{"snippet": "print(classification_report(Y_bal,Y_bal_pred))\n", "intent": "Balancing classes did not solve the problem of always predicting no-default.\n"}
{"snippet": "print(classification_report(Y,Y_pred))\n", "intent": "The model is no longer predicting all cases to be one class\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nnames = []; results = []\nprint('MODEL: ACCURACY (STD)\\n')\nfor name, model in models.items():\n    cv_results = cross_val_score(model, X_train, y_train, cv = KFold(10), scoring = 'accuracy')\n    names.append(name)\n    results.append(cv_results)\n    print('{}: {:.2f} ({:.2f})'.format(name, cv_results.mean(), cv_results.std()))\n", "intent": "Everything is set up. We will now evalute our models for accuracy and plot the results.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        gram_mix_imag = gram_matrix(feats[layer])\n        loss = tf.reduce_sum(tf.pow(gram_mix_imag - style_targets[i], 2))\n        style_loss += style_weights[i] * loss    \n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nprint(mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "y_pred = linreg_fit.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test, y_pred)\nprint(MSE)\n", "intent": "We now use the ${\\tt predict()}$ function to estimate the response for the test\nobservations, and we use ${\\tt sklearn}$ to caclulate the MSE.\n"}
{"snippet": "class KNN(KNN):\n    @abstractmethod\n    def score(self, X_test, y_test):\n        y_pred = self.predict(X_test)\n        match_counter = 0\n        for y_pred_i, y_i in zip(y_pred, y_test):\n            if y_pred_i == y_i:\n                match_counter += 1\n        return match_counter / len(y_test)\n", "intent": "And a simple scoring method:\n"}
{"snippet": "split = 0.75\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\nx_a = np.array([4.1, -0.1, 2.2])\nx_b = np.array([6.1, 0.4, 1.3])\ny_pred = predict(X_train, y_train, [x_a,x_b], k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "split = 0.75\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\nx_a = np.array([4.1,-0.1,2.2])\nx_b = np.array([6.1,0.4,1.3])\ny_pred = predict(X_train, y_train, [x_a,x_b], k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(best_rf, X_train_std, y_train, cv=10, scoring='accuracy')\nprint('CV Accuracy {}, Average Accuracy {}'.format(scores, scores.mean()))\n", "intent": "Average Cross Val Accuracy Score: .98\n"}
{"snippet": "scores_f1 = cross_val_score(best_rf, X_train_std, y_train, cv=10, scoring='f1_macro')\nprint('CV F1 Score {}, Average F1 Score {}'.format(scores_f1, scores_f1.mean()))\n", "intent": "Average Cross Val F1 Score: .92\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\npred = output.predict(X_test)\nscore1 = accuracy_score(y_test,pred)\nprint score1\nscore2 = f1_score(y_test,pred)\nprint score2\n", "intent": "**Prediction Scores**\n    * Accuracy - .994\n    * F1       - .96\n"}
{"snippet": "scores_Acc_1 = cross_val_score(best_params, X_train_std2, y_train2, cv=10, scoring='accuracy')\nprint('CV Accuracy {}, Average Accuracy Score {}'.format(scores_Acc_1, scores_Acc_1.mean()))\n", "intent": "Average Cross Val Accuracy Score: .97\n"}
{"snippet": "scores_F1_1 = cross_val_score(best_params, X_train_std2, y_train2, cv=10, scoring='f1_macro')\nprint('CV F1 Score {}, Average F1 Score {}'.format(scores_F1_1, scores_F1_1.mean()))\n", "intent": "Average Cross Val F1 Score: .85\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\npred2 = output2.predict(X_test_std2)\nscore3 = accuracy_score(y_test2,pred2)\nprint score3\nscore4 = f1_score(y_test2,pred2)\nprint score4\n", "intent": "**Prediction Scores**\n    * Accuracy - .98\n    * F1       - .86\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\npred_log = output3.predict(X_test_std2)\nscore_Acc_2 = accuracy_score(y_test2,pred_log)\nprint score_Acc_2\nscore_F1_2 = f1_score(y_test2,pred_log)\nprint score_F1_2\n", "intent": "**Prediction Scores**\n    * Accuracy - .978\n    * F1       - .8\n"}
{"snippet": "df2['sklearn_admit'] = logreg.predict_proba(X)[:, 1].round()\ndf2.head(20)\n", "intent": "**Examining Accuracy**\n"}
{"snippet": "R_squared = np.sum((lm.predict(X) - np.mean(bos.PRICE))** 2) / (np.sum((lm.predict(X) - np.mean(bos.PRICE))** 2)  + \n                                                                     np.sum((bos.PRICE - lm.predict(X)) ** 2))\nR_squared                                                              \n", "intent": "The result can be negative which indicates a poor model.\n"}
{"snippet": "cv_score(clf, Xtestlr, ytestlr)\n", "intent": "The GridSearchCV gave the same best value of C = 0.1\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The test data accuracy score is lower but the smaller difference between the training and test scores means the model generalizes to new data better.\n"}
{"snippet": "ab_tuned = clf_ab\npred_ab_tuned = ab_tuned.predict(X_test)\nclassificationReport(y_test, pred_ab_tuned)\n", "intent": "Now lets see how this new model performs:\n"}
{"snippet": "def calc_ms_error(weights, distances):\n    ms_error = 0.0\n    for record in distances:\n        x_1 = Normalized_Examples[record[1]]\n        x_2 = Normalized_Examples[record[2]]\n        dist = weighted_euclidean_distance(x_1, x_2, weights)\n        ms_error += np.power(dist - record[3], 2)\n    return np.divide(ms_error, 150)\n", "intent": "Let's see how far we are from values in Available_Distances\n"}
{"snippet": "X_new=[[3,4,5,2],[5,4,3,2]]\nknn.predict(X_new)\n", "intent": "- Returns Ndarray \n- Can predict multiple observation at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.mean_absolute_error(true,pred))\n", "intent": "** Mean Absolute Error**(MAE) is the mean absolute value of the erros\n$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$\n"}
{"snippet": " print(metrics.mean_squared_error(true,pred))\n", "intent": "**Mean Squared Error**(MSE) is the mean of the squared erros:\n$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$\n"}
{"snippet": "from numpy import sqrt\nprint(sqrt(metrics.mean_squared_error(true,pred)))\n", "intent": "**Root mean Squared error**(RMSE) is the sqare root of the mean squared errors:\n$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$\n"}
{"snippet": "Resnet50_dog_predictions = [np.argmax(Resnet50_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50_dog]\ntest_accuracy = 100*np.sum(np.array(Resnet50_dog_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_dog_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def make_predictions(df):\n    t_labels = get_labels(\"labels_pca\")\n    df = clean_data(df)\n    df = engineer_features(df)\n    with open(\"model.pkl\",\"r\") as mdl:\n        model = pickle.load(mdl)\n        mdl.close()\n    predictions = model.predict(df[t_labels])\n    return predictions\n", "intent": "This function provides the user a fast way to make predictions over a dataset\n**Example**\n"}
{"snippet": "Y_pred_ar1 = arima_predict(Y_train[-500:],Y_test[0:100000],p = 1,q = 0)\n", "intent": "The result shows that AR(1) is the better one. So the following I'll use the AR(1) to predict 100000 Y_pred\n"}
{"snippet": "y_hat = mdl_knn.predict(X_new)\n", "intent": "* New observations are called \"out-of-sample\" data\n* Uses the information it learned during the model training process\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    residual = tf.abs(predictions - labels)\n    condition = tf.less(residual, delta)\n    small_res = 0.5 * tf.square(residual)\n    large_res = delta * residual - 0.5 * tf.square(delta)\n    return tf.where(condition, small_res, large_res)\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "def compute_loss(X, y, w):\n    pass\ndef compute_grad(X, y, w):\n    pass\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "train_ng_df['nb_predict'] = MultinomialNB_ng.predict(np.stack(train_ng_df['vect'], axis=0))\nprint(\"Training score:\")\nprint(MultinomialNB_ng.score(np.stack(train_ng_df['vect'], axis=0), train_ng_df['category']))\n", "intent": "...and save predictions to the dataframe:\n"}
{"snippet": "print(sklearn.metrics.precision_score(test_ng_df['nb_predict'], test_ng_df['category'], average = 'weighted')) \nprint(sklearn.metrics.recall_score(test_ng_df['nb_predict'], test_ng_df['category'], average = 'weighted')) \nprint(sklearn.metrics.f1_score(test_ng_df['nb_predict'], test_ng_df['category'], average = 'weighted')) \n", "intent": "Let's calculate the precision, recall, and F-measures.\n"}
{"snippet": "sklearn.metrics.accuracy_score(df_exampleTree_test['category'],clf_tree.predict(np.stack(df_exampleTree_test['vect'], axis = 0)))\n", "intent": "Lets look at accuracy:\n"}
{"snippet": "np.random.seed(123)\nmodel.load_weights(\"weights_/model_weights_esempio.h5\")\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"The loss on test data is: {}. \\nThe accuracy on test data is: {}\".format(score[0], score[1]))\n", "intent": "**predict and show predictions**\n"}
{"snippet": "score = model_1a.evaluate(X_test, Y_test, verbose=0)\nprint(\"The loss on test data is: {}. \\nThe accuracy on test data is: {}\".format(score[0], score[1]))\n", "intent": "** Evaluate your model: ** \n"}
{"snippet": "model_2a.load_weights(\"weights_/model_weights_2a.h5\")\nmodel_2a.evaluate(X_test, y_test, steps = 1)\n", "intent": "** If your model is correct you should be able to download the model weights **\n"}
{"snippet": "per_fold_eval_criteria = cross_val_score(estimator=clf,\n                                    X=X,\n                                    y=y,\n                                    cv=cv,\n                                    scoring=auc_roc_scorer\n                                   )\nLR_auc = per_fold_eval_criteria\n", "intent": "After calculating the costs of the model on the test folds, we also compute the area under the multiclass ROC curve.\n"}
{"snippet": "per_fold_eval_criteria = cross_val_score(estimator=clf,\n                                    X=X,\n                                    y=y,\n                                    cv=cv,\n                                    scoring=auc_roc_scorer,\n                                    n_jobs=-1\n                                    )\nSVM_auc = per_fold_eval_criteria\n", "intent": "Once again, the area under curve metric is generated for the multi-class ROC curve.\n"}
{"snippet": "per_fold_eval_criteria = cross_val_score(estimator=clf,\n                                    X=X,\n                                    y=y,\n                                    cv=cv,\n                                    scoring=auc_roc_scorer,\n                                    n_jobs=-1\n                                   )\nEC_auc = per_fold_eval_criteria\n", "intent": "Once again, the area under the ROC curve will be evaluated for the final ensemble classifier.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "feature_score_silhouette = []\nfor feature in possible_features_with_bias:\n    score, silhouette = kmean_dark_separator_score(df_players[[feature]])\n    feature_score_silhouette.append((feature, score, silhouette))\n", "intent": "Let's look at all 1-feature cases\n"}
{"snippet": "for i in range(len(ordered_features)):\n    score, silhouette = kmean_dark_separator_score(df_players[ordered_features[i:]])\n    print('Before removing {:12} score is {:.02f}  silhouette is {:.02f}'.format(ordered_features[i], score, silhouette))\n", "intent": "We remove each feature iteratively and print the resulting heuristic score and silhouette score\n"}
{"snippet": "\"Silhouette score: {:.2f}\".format(silhouette_score(X, cluster_indexes))\n", "intent": "Also, the silhouette score is good.\n"}
{"snippet": "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\nout, _ = relu_forward(x)\ncorrect_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n                        [ 0.,          0.,          0.04545455,  0.13636364,],\n                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\nprint 'Testing relu_forward function:'\nprint 'difference: ', rel_error(out, correct_out)\n", "intent": "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:\n"}
{"snippet": "predict_train = lin_reg.predict(X_train)\npredict_test = lin_reg.predict(X_test)\n", "intent": "Now that on our training data, linear regression has been done, let us perform prediction of both training data and testing data.\n"}
{"snippet": "top_model.evaluate(test_data, test_labels)\n", "intent": "Reuse the top_model with trained weights and evaluate.\n"}
{"snippet": "def predict(x, w, b):\n    w1, w2 = w\n    b1, b2 = b\n    _, result = forward_pass(x, w, b)\n    return result\n", "intent": "Prediction is a single feed forward part for each input, to generate classified output.\n"}
{"snippet": "x_test = np.array(['you son of bitch'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "2 false predictions: predicted 1, actually 2.\n"}
{"snippet": "laos_life_exp = bmi_life_model.predict(21.07931)\nprint(laos_life_exp)\n", "intent": "(1) Predict using a BMI of 21.07931 and assign it to the variable \"laos_life_exp\".\n"}
{"snippet": "df=train_data[train_data[\"Age\"].isnull()]\nX = df.iloc[:,[0,1,4,6]].values\ny = reg.predict(X)\ndf['Age']=y\ntrain_data = new_train.append(df)\n", "intent": "Now lets fill our missing Age with predicted values\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nfrom numpy import ravel\nfor nOfClusters in range(2,5):\n    clusterLabels = ravel(cut_tree(psSimi_link, nOfClusters)) \n    silhouette_avg = silhouette_score(psSimi_matrix,clusterLabels,metric='precomputed')\n    print(\"For n_clusters =\", nOfClusters, \n          \"The average silhouette_score is:\", silhouette_avg)\n", "intent": "Based on the dendogram it doesn't look like there's a great way to cluster, but we'll also check numerically:\n"}
{"snippet": "for nOfClusters in range(2,5):\n    clusterLabels = ravel(cut_tree(psSimi_link, nOfClusters)) \n    silhouette_avg = silhouette_score(psSimi_matrix,clusterLabels,metric='precomputed')\n    print(\"For n_clusters =\", nOfClusters, \n          \"The average silhouette_score is:\", silhouette_avg)\n", "intent": "Based on the dendogram it doesn't look like there's a great way to cluster, but we'll also check numerically:\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nfrom numpy import ravel\nfor nOfClusters in range(2,5):\n    clusterLabels = ravel(cut_tree(psSimi_link, nOfClusters)) \n    silhouette_avg = silhouette_score(psSimi_matrix,clusterLabels,metric='precomputed')\n    print(\"For n_clusters =\", nOfClusters, \n          \"The average silhouette_score is:\", silhouette_avg)\n", "intent": "It doesn't look like there's a great way to cluster, but we'll also check numerically:\n"}
{"snippet": "preds = model.predict(X_valid)\n", "intent": "Our first task in evaluating the model performance is to predict mortality using the hold out dataset (i.e. validation data).\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    score = r2_score(y_true, y_predict)\n    return score\n", "intent": "The performance metric used is the r2 score and the mean squared error\n"}
{"snippet": "average_cost(ytest, clfgnb.predict(Xtest), cost)\n", "intent": "YOUR TURN NOW\n>Calculate the average cost of the gnb classifier on the test set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\ncnf_matrix = confusion_matrix(y_actual, y_hat)\naccuracy = accuracy_score(y_actual, y_hat)\nprint(cnf_matrix)\nprint(accuracy)\n", "intent": "How accurate is our model?\nLets find out by looking at the confusion matrix\n"}
{"snippet": "print('predicted:', sentimental_detect_model.predict(tfidf4)[0])\nprint('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "all_predictions = sentimental_detect_model.predict(messages_tfidf)\nprint(all_predictions)\n", "intent": "Now I want to determine how well our model will do overall on the entire dataset. \n"}
{"snippet": "y_hat = pipelined_model.predict(X_test)\nRMSE = np.sqrt(mean_squared_error(y_test, y_hat))\nprint('Model: Random forest', ' | ', 'RMSE: ', RMSE)\n", "intent": "Once more, we make predictions using the test set.\n"}
{"snippet": "silhouette_score(clustered_df.loc[:,'topic_0':'topic_2'],clustered_df['lda_cluster_id'])\n", "intent": "Lets just first check how well separated the clusters are using the silhouette score. Details are in the report\n"}
{"snippet": "clustered_df[\"category_num\"] = clustered_df[\"category\"].astype('category').cat.codes\nadjusted_mutual_info_score(clustered_df[\"lda_cluster_id\"],clustered_df[\"category_num\"])\n", "intent": "Now we can compare the adjusted mutual information and adjusted RAND score\n"}
{"snippet": "dog_predictions = [np.argmax(dog_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_ResNet]\ntest_accuracy = 100*np.sum(np.array(dog_predictions)==\n                           np.argmax(test_targets, axis=1))/len(dog_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(y_test, y_pred))\n", "intent": "**MEAN ABSOLUTE ERROR**\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_test, y_pred))\n", "intent": "**MEAN SQUARED ERROR**\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt(mean_squared_error(y_test, y_pred)))\n", "intent": "**ROOT MEAN SQUARED ERROR**\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "spat_err = ml_error.ML_Error(y, X, w, \n                             name_y='home value', name_x=['income','crime'], \n                             name_w='columbus.gal', name_ds='columbus')\nprint(spat_err.summary)\n", "intent": "$$y = \\lambda W\\epsilon + \\beta X + \\epsilon$$\n"}
{"snippet": "blank_dti['dti']=result.predict((x_blank).astype(float))\n", "intent": "*Finding dti values for the dataframe with null values.*\n"}
{"snippet": "pred_random=randomsearchcvmodel.predict(x_test)\nrandom=ConfusionMatrix(np.array(y_test),np.array(pred_random))\nprint(random)\nprint(random.print_stats())\nrandom.plot()\n", "intent": "*Confusion Matrix for RandomizedSearchCV model*\n"}
{"snippet": "Result=classifier.predict(test)\nprint(Result)\nprint(len(Result))\n", "intent": "<h2>Result</h2>\nThe final result is\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "<b>Using lm.predict() to predict off the X_test set of the data.</b>\n"}
{"snippet": "metrics.explained_variance_score(y_test,predictions)\n", "intent": "<b>These four independent variables explain 88% of the variation in the number of games won.</b>\n"}
{"snippet": "predictions = predict(parameters, X)\nprint(Y.shape)\nprint(predictions.shape)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "x_test = np.array(['Andrew Ng is awesome'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "index = 0\nscores = []\nfor model in models:\n    scores.append(model.evaluate(inputs[index],outputs[index], verbose=0))\n    index += 1    \n", "intent": "Reviewing model scores:\n"}
{"snippet": "predictions = forest.predict(generatedPosts_train_data_features)\n", "intent": "Then we use the Random Forest model from step 2, to predict(classify) each generated post:\n"}
{"snippet": "resnet50_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ncorrect_predictions = np.array(resnet50_predictions)==np.argmax(test_targets, axis=1)\ntest_ResNet50_accuracy = 100*np.sum(correct_predictions)/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_ResNet50_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_classes = model.predict((X_test-av)/std)\npredicted_probabilities = np.clip(model.predict_proba((X_test-av)/std), 0.35, 0.65)\naccuracy = metrics.accuracy_score(y_test, predicted_classes)\nlog_loss = metrics.log_loss(y_test,predicted_probabilities)\ncm = metrics.confusion_matrix(y_test, predicted_classes)\nprint('accuracy: '+str(accuracy)+' logloss= '+str(log_loss))\nprint('confusionMatrix:')\nprint(cm/len(X_test))\n", "intent": "The following piece of code gives a better log-loss function, but the accuracy and confusion matrices remain the same: can you tell why?\n"}
{"snippet": "clusterLabelCount = labelsAndData.map(lambda p: (clusters.predict(p[1]), p[0])).distinct().sortByKey().collect()\nfor item in clusterLabelCount:\n    print(item)\n", "intent": "<div class=\"alert alert-warning\">\nHere we print all the attacks and the cluster they belong to.\n</div>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\",\n                        cv=10)\ntree_rmse_scores = np.sqrt(-scores)\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\ndisplay_scores(tree_rmse_scores)\n", "intent": "Cross-validation for decision tree\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, \n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Cross validation for linear model\n"}
{"snippet": "pred=list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "pred2=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,pred2))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions=logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred1=pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred1=mod1.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nauc = roc_auc_score(train_labels, log_reg.predict_proba(train)[:,1])\nprint('Area under ROC curve (AUC) for training set: %.4f'%auc)\n", "intent": "This competition has AUC as test metric.\n"}
{"snippet": "X_test = imgs[10000:20000]      \ny_test = y[10000:20000]\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Use you newly trained model to predict the class of the clothing, what's the accuracy rate? \n"}
{"snippet": "VGG19_predictions = [np.argmax(transfer_learning_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Accuracy on the train: ', accuracy_score(y_train, best_clf.predict(X_train)))\nprint('Accuracy on the test: ', accuracy_score(y_test, y_pred))\n", "intent": "on 100% of data without bad entries with regex without BAD DUPLICATES\n"}
{"snippet": "print('Accuracy on the train: ', accuracy_score(y_train, best_clf.predict(X_train)))\nprint('Accuracy on the test: ', accuracy_score(y_test, best_clf.predict(X_test)))\n", "intent": "If we use the initial data set, the resulting accuracy is much worse:\n"}
{"snippet": "def lazy_print(idx):\n    print('%r => %s, true: %s' % (X_test[idx], best_clf.predict([X_test[idx]]),y_test[idx]))\n", "intent": "And our classifier works fine:\n"}
{"snippet": "predictionDF.cache()\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\nauPRC = evaluator.evaluate(predictionDF)\nprint(\"\\nArea under precision-recall curve: = \" + str(auPRC))\nrecall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictionDF)\nprint(\"\\nrecall = \" + str(recall))\nprecision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictionDF)\nprint(\"\\nPrecision = \" + str(precision))\npredictionDF.unpersist()\n", "intent": "Calculate Precision, Recall and the Area Under the Precision / Recall curve.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0)))\n                     for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==\n                           np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "X_5k = df['5k_split'].as_matrix()\nX_5k = X_5k.reshape((len(X_5k), 1))\ncv_score_MSE = cross_val_score(lr, X=X_5k, y=y, scoring='neg_mean_squared_error', cv=5)\nprint(\"MSE Cross Validation Results\\nMean: {:.2E}\\nStd. Dev.: {:.2E}\\n\".format(-Decimal(np.mean(cv_score_MSE)), \\\n                                                                               Decimal(np.std(cv_score_MSE))))\ncv_score_R2 = cross_val_score(lr, X=X_5k, y=y, cv=5)\nprint(\"R^2 Cross Validation Results\\nMean: {:.3}\\nStd. Dev.: {:.3}\".format(np.mean(cv_score_R2), \\\n                                                                             np.std(cv_score_R2)))\n", "intent": "Let's use one last baseline, namely 5k_split. So we'll only look at where people are at the 5k mark.\n"}
{"snippet": "X_L2_ratio = df['L2_speed_diff_ratio'].as_matrix()\nX_L2_ratio = X_L2_ratio.reshape((len(X_L2_ratio), 1)) \ncv_score_MSE = cross_val_score(lr, X=X_L2_ratio, y=y, scoring='neg_mean_squared_error', cv=5)\nprint(\"MSE Cross Validation Results\\nMean: {:.2E}\\nStd. Dev.: {:.2E}\\n\".format(-Decimal(np.mean(cv_score_MSE)), \\\n                                                                               Decimal(np.std(cv_score_MSE))))\ncv_score_R2 = cross_val_score(lr, X=X_L2_ratio, y=y, cv=5)\nprint(\"R^2 Cross Validation Results\\nMean: {:.3}\\nStd. Dev.: {:.3}\".format(np.mean(cv_score_R2), \\\n                                                                             np.std(cv_score_R2)))\n", "intent": "Again we look at the CV scores. It's not fantastic predictor, but it's something.\n"}
{"snippet": "X_combined = np.concatenate((X_L2_ratio, X_5k_minus_40k_ratio, X_30k_minus_35k_ratio), axis=1)\ncv_score_MSE = cross_val_score(lr, X=X_combined, y=y, scoring='neg_mean_squared_error', cv=5)\nprint(\"MSE Cross Validation Results\\nMean: {:.2E}\\nStd. Dev.: {:.2E}\\n\".format(-Decimal(np.mean(cv_score_MSE)), \\\n                                                                               Decimal(np.std(cv_score_MSE))))\ncv_score_R2 = cross_val_score(lr, X=X_combined, y=y, cv=5)\nprint(\"R^2 Cross Validation Results\\nMean: {:.3}\\nStd. Dev.: {:.3}\".format(np.mean(cv_score_R2), \\\n                                                                             np.std(cv_score_R2)))\n", "intent": "Now, let's combine all of these engineered features and use them together.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        gram_source = style_targets[i]\n        style_loss += tf.reduce_sum((gram-gram_source)**2)*style_weights[i]\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "first_alg = r2_score(houses.price, predicted)\nprint(first_alg)\n", "intent": "We can see how acurate is our algorithm by finding the ** coefficient of determination ** - r2_score(y_true, y_pred) \n"}
{"snippet": "model.predict(3)\n", "intent": "We can see the predic output for value 3\n"}
{"snippet": "model.predict([[0], [1], [2]])\n", "intent": "To predict many values\n"}
{"snippet": "cross_val_score(tree, features, labels, cv = 10)\n", "intent": "We will preapare 10 data separate sets and to see if the algorithm is stable\n"}
{"snippet": "predicted = logistic_regression.predict(X_test)\nconfusion_matrix(y_test, predicted)\n", "intent": "We need to see confusion matrix\n"}
{"snippet": "p_score = precision_score(y_test, predicted, average=None)\n", "intent": "We see unbalanced data, therefore accuracy is not a good measure. We need precision score and recall score\n"}
{"snippet": "predicted = random_forest.predict(X_test)\nconfusion_matrix(y_test, predicted)\n", "intent": "Again, we need to check the confusion matrix\n"}
{"snippet": "first_alg = r2_score(drills_sample.longitude, predicted5)\n", "intent": "We can see how acurate is our algorithm by finding the ** coefficient of determination ** - r2_score(y_true, y_pred) \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    for i_ in range(len(style_layers)):\n        gram_ = gram_matrix[feats[style_layers[i_]]]\n        style_loss += style_weights[i_] * tf.reduce_sum((gram_-style_targets[i_])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "model.load_weights(\"cnn_model\")\npred = model.predict(X_test)\nY_pred=np.copy(pred)\nY_pred=np.where(Y_pred>0.5,1,0)\nprint(classification_report(Y_test, Y_pred, target_names=mlb.classes_))\n", "intent": "- Use X_test as input to CNN model to make prediction to obtain Y_pred\n- Compare Y_pred and Y_test\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_test, y_pred)\n", "intent": "We can see the classifier only used 3 features (in order of importance): glucose, bmi and age.\n"}
{"snippet": "accuracy_score(y_validation, y_rf_pred)\n", "intent": "The validation accuracy is obtained by comparing with the true class values stored as y_validation:\n"}
{"snippet": "x=prepare_for_prediction(\"   1'' Drive Impact Sockets SUNEXpress Warranty Program Hammers & Needle Scalers 3/8'' Drive Impact Socket Sets Under Hoist Transmission Jacks Drive Tools 1/2'' Drive Impact Socket Sets 3/8'' Drive Impact Sockets \nmodel.predict(x)\n", "intent": "The example to predict\n"}
{"snippet": "docs_new = ['God is love', 'OpenGL on the GPU is fast']\nX_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredicted = clf.predict(X_new_tfidf)\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, twenty_train.target_names[category]))\n", "intent": "Now testing data below\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n    target_names=twenty_test.target_names))\n", "intent": "For performance analysis of classifiers, using metrics provided by sk-learn\n"}
{"snippet": "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n", "intent": "the result of calling fit on GridSearchCV object is a classifier on which we can then use the predict function \n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint (roc_auc_score(Y_test,y_pred))\n", "intent": "<b> the accuracy of our model comes out to be 90% </b>\n"}
{"snippet": "yPred = clf.predict(xTest)\naccuracy = accuracy_score(yTest, yPred)\nprint(\"Accuracy score:\", accuracy)\n", "intent": "Calculate accuracy score for predicted values.\n"}
{"snippet": "acc  = sklearn.cross_validation.cross_val_score(model, random_features, training_labels, cv=5)\n", "intent": "Perform 5-fold cross validation:\n"}
{"snippet": "model.predict(test_features)\n", "intent": "To test the classifier on a set of (test) feature vectors, use the `predict` method:\n"}
{"snippet": "test_pred = np.argmax(model.predict(test_res),axis=1)\ntest_y = np.argmax(test_targets,axis=1)\ntest_acc = np.sum(test_y==test_pred)*100/len(test_y)\nprint(\"Test accuracy is {0}%\".format(str(test_acc)[:5]))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)\n", "intent": "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:\n"}
{"snippet": "error_all = []\nfor n in xrange(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(train_data[target], predictions)\n    error_all.append(error)\n    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "preds = clstr.predict(features[['splits1', 'splits2', 'age']])\ntuf = view_clusters(df, features, preds)\n", "intent": "Let's see what these clusters look like.\n"}
{"snippet": "y_pred = dt.predict(X_test)\n", "intent": "Predicting using a decision tree classifier.\n"}
{"snippet": "unq = df.copy()\ndel unq['id']\nunq = unq.drop_duplicates()\nawl, _ = formatter.format(unq, ['obj'], 'hash')\npreds  = classifer.predict(awl['x'][0], verbose = True)\n", "intent": "Create set of all unique records (without duplicates), then project them using the model.  The projection should take tens of seconds.\n"}
{"snippet": "iris_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(iris_new)\n", "intent": "Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(iris.target, _)\n", "intent": "scikit provides the function `metrics.accuracy_score` to compute the **training accuracy**:\n"}
{"snippet": "knn.predict(data_test)\n", "intent": "First we predict the target values for the test samples:\n"}
{"snippet": "metrics.accuracy_score(target_test, _)\n", "intent": "Then we evaluate the performance of the prediction:\n"}
{"snippet": "scores = cross_val_score(logreg, features_array, target, cv=5,\n                         scoring='roc_auc')\nscores.min(), scores.mean(), scores.max(), scores.std()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "preds = model_tuberculosis.predict(X_test)\nsave_array('nih-tuberculosis-preds.bc', preds)\n", "intent": "<a id=\"section-predictions\"></a>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfinal_model = grid_search.best_estimator_   \ny_te_estimation = final_model.predict(X_te)\nfinal_mse = mean_squared_error(y_te, y_te_estimation)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance\n"}
{"snippet": "Y_mean = np.mean(Y) * np.ones(n_fake)\nmean_squared_error(Y, Y_mean)\n", "intent": " To ask how we can do better, let's consider an even more naive choice, where we choose the mean of the training data as the prediction for any data:\n"}
{"snippet": "predictions = rf.predict(test_features)\nerrors = abs(predictions-test_labels)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'minutes.')\n", "intent": "Make predictions on the dataset.\n"}
{"snippet": "def calculate_loss(model):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    z1 = X.dot(W1) + b1\n    a1 = expit(z1)\n    z2 = a1.dot(W2) + b2\n    scores = expit(z2)\n    data_loss = np.sum((scores - y)**2)\n    return 1./num_examples * data_loss\n", "intent": "First let's implement the loss function we defined above. We use this to evaluate how well our model is doing:\n"}
{"snippet": "py_x = clf.predict_proba(X[ML_feat])\ndy_x = clf.predict(X[ML_feat])\n", "intent": "We can predict on the whole set and then evaluate on the test and train separately.\n"}
{"snippet": "sklearn.metrics.accuracy_score(ytrain_car_non, dy_x)\n", "intent": "For the sake of the argument, let's look at the training error.\n"}
{"snippet": "Xtest_car_av_index = Xtest['CAR_AV']==1\nytest_car_av = ytest[Xtest_car_av_index]\nXtest_pTR = Xtest_car_av[~dy_x_test]\nytest_pTR = ytest_car_av[~dy_x_test]\nXtest_no_car = Xtest[Xtest['CAR_AV']!=1]\nytest_no_car = ytest[Xtest['CAR_AV']!=1]\nXte_stage2 = pd.concat([Xtest_pTR, Xtest_no_car])\nyte_stage2 = pd.concat([ytest_pTR, ytest_no_car])\ndy_x2_test = model2.predict(Xte_stage2[ML_feat])\n", "intent": "Then do the same breakout for test\n"}
{"snippet": "(sklearn.metrics.accuracy_score(yte_stage2, dy_x2_test)*len(yte_stage2)+sum(ytest_car_av[dy_x_test]==3))/len(ytest)\n", "intent": "Better than the training set??\n"}
{"snippet": "sklearn.metrics.confusion_matrix(ytest, model.predict(Xtest[ML_feat]))\n", "intent": "Better than predicting the dominant class.\n"}
{"snippet": "deep_model.load_weights(deep_model_weight_filepath)\ndeep_model_pred_val = deep_model.predict([X_val_cat, X_val_num, X_val_word], batch_size=2000)\ndeep_model_val_AUROC = roc_auc_score(y_val_cat, deep_model_pred_val)\nprint(\"deep_model_val_AUROC AUROC: {}\".format(deep_model_val_AUROC))\nAUROC_file = \"deep_model_val_AUROC.txt\"\nf = open(AUROC_file, \"w\")\nf.write(\"{}\".format(deep_model_val_AUROC))\nf.close()\n", "intent": "Load the best weights and make predictions on the validation set. Report the AUROC\n"}
{"snippet": "RCs = achrom.get_RCs(Seqs_train_full, y_train_full)\nRThat_train_pyt = np.array([achrom.calculate_RT(i, RCs) for i in Seqs_train_full])\nRThat_valid_pyt = np.array([achrom.calculate_RT(i, RCs) for i in Seqs_valid_full])\nprint (\"Pyteomics MSE (train): {:.2f}\".format(mean_squared_error(y_train_full, RThat_train_pyt)))\nprint (\"Pyteomics MSE (valid): {:.2f}\".format(mean_squared_error(y_valid_full, RThat_valid_pyt)))\n", "intent": "Here we compare the neural network model with a simple linear regression model (as used and implemented in pyteomics).\n"}
{"snippet": "Inception_predictions = [np.argmax(final_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy Inception: %.4f%%' % test_accuracy)\nResnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy1 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy Resnet50: %.4f%%' % test_accuracy1)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = cross_val_score(clf_ext, X, y, cv=20)\nscores_test = clf_ext.score(X_test, y_test)\nprint('Mean score= %.3f, Std deviation = %.3f'%(np.mean(scores), np.std(scores)))\nprint('Testscore = %.3f'%(scores_test))\n", "intent": "To ensure we are not overfitting random Datapoint we then check the model in cross validation.\n"}
{"snippet": "for s in stocks:\n    err = ((dfs_test[s]['mid_price_indicator'].values - reg[s].predict_proba(\n        dfs_test[s]['queue_imbalance'].values.reshape(-1, 1))[:,1]) ** 2).mean()\n    print('Error for {} is {}'.format(s, err))\n", "intent": "We calculate residual $r_i$:\n$$ r_i = \\hat{y_i} - y_i $$\nWe use mean square residual on testing set to assess the predictive power.\n"}
{"snippet": "reg_log = {}\npred_log = {}\npred_out_of_sample = {}\nfor s in stocks:\n    reg_log[s] = lob.logistic_regression(dfs[s], 0, len(dfs[s]))\n    pred_log[s] = reg_log[s].predict(dfs_test[s]['queue_imbalance'].values.reshape(-1, 1))\n", "intent": "For each stock we will present roc curves for SVMs with calculated roc areas compared to previous logistic regression results.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for ind, layer in enumerate(style_layers):\n        G = gram_matrix(feats[layer])\n        style_loss += content_loss(style_weights[ind], G, style_targets[ind])\n    return tf.reduce_sum(style_loss)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import r2_score\nmodel = CNN_A10()\nmodel.load_weights('saved_models/weights.best.from_scratch_CNN_multi_newA10.hdf5')\ny_pred = model.predict(X_test)\nprint(r2_score(y_test,y_pred))\n", "intent": "Calculate R2 score for test set\n"}
{"snippet": "model = MLP_B2()\nmodel.load_weights('saved_models/weights.best.from_scratch_MLPtype2_B2.hdf5')\nX_test_cont = np.copy(DataCollection[1:,:,:]) \ny_pred_cont = model.predict(X_test_cont)\n", "intent": "Predict for y_pred_cont and transform y_pred_cont to percentual values\n"}
{"snippet": "model = MLP_B2()\nmodel.load_weights('saved_models_historic/weights.best.from_scratch_MLPtype2_B2_Timepoints'+str(PredictionTimepoints)+'.hdf5')\nX_test_cont = np.copy(DataCollection[1:,:,:]) \ny_pred_cont = model.predict(X_test_cont)\n", "intent": "Predict for y_pred_cont and transform y_pred_cont to percentual values\n"}
{"snippet": "knn.predict([[3,5,4,2]])\n", "intent": "- Returns a NumPy array\n- 0 = setosa, 1 = versicolor, 2 = virinica\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4)\nmetrics.mean_squared_error(true, pred)\n", "intent": "- Mean of the squared errors\n- Harder to interperet than MAE\n"}
{"snippet": "print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nnp.sqrt(metrics.mean_squared_error(true, pred))\n", "intent": "- Square root of the Mean of the squared errors\n"}
{"snippet": "from sklearn.metrics import classification_report\nconvert = {2: 0, 1: 1, 0: 2}\nnew_labels = [convert[x] for x in labels]\nprint(classification_report(iris.target, new_labels))\n", "intent": "Comparing the points to the labels, we see that 2 is red, 1 is green, and 0 is blue.\n"}
{"snippet": "y_pred_log = model_log.predict(x_test) \ny_pred_knn = model_knn.predict(x_test) \ny_pred_svm = model_svm.predict(x_test) \ny_pred_dt = model_dt.predict(x_test) \ny_pred_rf = model_rf.predict(x_test) \n", "intent": "> ** Step : 6 - Model Evaluation**\n- Confusion Matrix\n- Classification Report\n"}
{"snippet": "realarray = df2.values\npredicted_rf_real = model.predict(realarray[:,0:len(df2.columns)-1])\nnp.savetxt(\"test_output/op.csv\", predicted_rf_real, header='bot' ,delimiter=\",\", comments='')\n", "intent": "                                    Classify Test Data\n"}
{"snippet": "def predictGame(team_1_vector, team_2_vector, home):\n    diff = [a - b for a, b in zip(team_1_vector, team_2_vector)]\n    diff.append(home)\n    return model.predict([diff]) \n", "intent": "Depending on the model you use, you will either need to return model.predict_proba or model.predict\n"}
{"snippet": "x_test = np.array(['not feeling great'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Make predictions using test set...\n"}
{"snippet": "validation_y_pred = regr.predict(validation_X_scaled)\n", "intent": "** Evaluate the performance of the model on the validation set **\n"}
{"snippet": "validation_y_poly_pred = regr_poly.predict(validation_polyFit)\n", "intent": "** Evaluate the model with polynomial features **\n"}
{"snippet": "validation_y_ridge_pred = ridge.predict(validation_polyFit)\n", "intent": "** Evaluate the ridge model **\n"}
{"snippet": "validation_y_lasso_pred = lasso.predict(validation_polyFit)\n", "intent": "** Evaluate the Lasso model **\n"}
{"snippet": "test_y_pred = regr.predict(test_X_scaled)\n", "intent": "** Test the model on the test set **\n"}
{"snippet": "mean_squared_error(preds, Y_test)\n", "intent": "Note: the MSE goes down because of the `log10` transform. However, that's exactly what we would have expected by definition.\n"}
{"snippet": "r2_score(preds, Y_test) \n", "intent": "The r-squared score goes up. Less skew is always good.\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(\n            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(d_loss_true, d_loss)\n    print(g_loss_true, g_loss)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum((gram_matrix(feats[style_layers[i]])  - style_targets[i]) **2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'],km.labels_))\nprint(classification_report(df['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "x_test = np.array(['not feeling good love'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "p5_predictions = [np.argmax(p5_model.predict(np.expand_dims(feature, axis=0))) for feature in test_step5]\ntest_accuracy = 100*np.sum(np.array(p5_predictions)==np.argmax(test_targets, axis=1))/len(p5_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    residual = tf.abs(predictions - labels)\n    small_res = 0.5 * tf.square(residual)\n    large_res = delta * residual - 0.5 * tf.square(delta)\n    return tf.cond(residual < delta, lambda: small_res, lambda: large_res)\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "def compute_loss(X, y, w):\n    margin = y * tf.tensordot(expand(X), w, axes=1)\n    maxs = tf.maximum(tf.zeros(shape=y.shape), tf.ones(shape=y.shape) - margin)\n    return tf.reduce_mean(maxs)\ndef compute_grad(X, y, w):\n    X_ = expand(X)\n    margin = y * tf.tensordot(X_, w, axes=1)\n    diff = tf.ones_like(y) - margin\n    mask = tf.where(diff >= 0, tf.ones_like(diff), tf.zeros_like(diff))\n    return tf.reduce_mean(- tf.expand_dims(mask * y, 1) * X_, axis=0)\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "crossval_gaussian = cross_val_score(gaussian, x_train, y_train, cv=5).mean()\ncrossval_random = cross_val_score(randomforest, x_train, y_train, cv=5).mean()\ncrossval_logreg = cross_val_score(logreg, x_train, y_train, cv=5).mean()\n", "intent": "Step 12: Cross Validation\n"}
{"snippet": "y_pred = pip_clf.predict(X_test)\n", "intent": "Prediction also requires a single line of code:\n"}
{"snippet": "print(classification_report(y_test, y_pred, labels=None, target_names=job_descriptions.target_names))\n", "intent": "And we obtain exactly the same results as in the previous section:\n"}
{"snippet": "predicted = model.predict(X_test)\nprint predicted\n", "intent": "Now for the test data, we predict its class (affair/no affair) for each set of feature values\n"}
{"snippet": "target_names= ['good', 'bad']\nprint(sklearn.metrics.classification_report(test_Y, predictions, target_names=target_names))\n", "intent": "We can also investigate other metrics, such as: \n"}
{"snippet": "y_predicted = clf.predict(X_test)\n", "intent": "Predict test examples ...\n"}
{"snippet": "predicted = model.predict(X_test)\nprint (predicted)\n", "intent": "Now for the test data, we predict its class (affair/no affair) for each set of feature values\n"}
{"snippet": "print(sklearn.metrics.classification_report(test_Y, predictions))\n", "intent": "We can also investigate other metrics, such as: \n"}
{"snippet": "scores = model.evaluate(X_Test, y_Test)\nprint scores\n", "intent": "We can evaluate how the trained model does on the test sample as follows:\n"}
{"snippet": "np.average(model.predict(X_test)==y_test)\n", "intent": "Performance on training set is perfect.\nWhat about test data not used on the fit?\n"}
{"snippet": "label=Y_test[0]\npredicted_label=model.predict(X_test[0].reshape(1,-1))\nprint(label,predicted_label)\n", "intent": "Let's predict one image\n"}
{"snippet": "nimages=X_test.shape[0]\nX_test=X_test.reshape((nimages,-1))\nprint(\"predicting...\",end=\"\")\nY_predicted=model.predict(X_test)\nprint(\"Done.\")\n", "intent": "Let's predict the full test sample $10,000$ images, this will be **slow**\n"}
{"snippet": "print(model.predict(shifted_images_test[idx].reshape(1,-1)),labels_test[idx])\n", "intent": "We can still predict\n"}
{"snippet": "nimages=shifted_images_test.shape[0]\nX_test=shifted_images_test.reshape((nimages,-1))\nprint(\"predicting...\",end=\" \")\nY_predicted=model.predict(X_test)\nprint(\"Done.\")\n", "intent": "But what is the overall accuracy?\n"}
{"snippet": "X_short_test=count_test_features[:50]\nY_short_test=test_documents[\"label\"][:50]\nY_short_pred=model.predict(X_short_test)\nnp.average(Y_short_pred==Y_short_test)\n", "intent": "Let's check of the first 50 documents\n"}
{"snippet": "print(model.predict(X_test[0].reshape(1,-1)),Y_test[0])\n", "intent": "Model makes some mistakes\n"}
{"snippet": "Y_predicted=model.predict(X_test.reshape((-1,nrows*ncols)))\nprint(\"accuracy\",np.mean(Y_predicted==Y_test))\n", "intent": "Accuracy is pretty low.\n"}
{"snippet": "Y_predicted=ber_model.predict(X_test.reshape((-1,nrows*ncols)))\nprint(\"accuracy\",np.mean(Y_predicted==Y_test))\n", "intent": "Bernoulli's Naive Bayes has has much better accuracy that Gaussian Naive Bayes\nfor this particular problem\n"}
{"snippet": "nimages=shifted_images_test.shape[0]\nX_test=shifted_images_test.reshape((nimages,-1))\nprint(\"predicting...\",end=\" \")\nY_predicted=ber_model.predict(X_test)\nprint(\"Done.\")\n", "intent": "Let's see how well we can predict now\n"}
{"snippet": "x=np.linspace(-1,1,101) \ny0=model0.predict(x)\ny1=model1.predict(x)\ny2=model2.predict(x)\n", "intent": "And then predict $\\hat{h}(x)$ the model's choice for a best fit across $x$'s domain $\\mathcal{X}=(-1,1)$\n"}
{"snippet": "Y_sk = model.predict(H)\ndY=(Y_sk-Y_pred)\nnp.dot(dY.T,dY)/len(dY)\n", "intent": "Results are identical to the `numpy` solution\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy Resnet50: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_x = test.copy()\ntest_y = test_x['ARTH1']\ntest_x.drop('ARTH1', axis=1, inplace=True)\ny_pred=model.predict(test_x)\nfrom sklearn.metrics import f1_score, accuracy_score\ny_true=[ 'No' if x is None else x for x in test_y ]\nprint(f1_score(y_true, y_pred, average='micro'))\n", "intent": "At the beginning we set some data aside that was not used to create the model. Now we can use that to test our model's accuracy.\n"}
{"snippet": "y_hat_shift = y_test.shift()[1:] \ny_test_shift = y_test[1:]\nr2_score(y_hat_shift, y_test_shift)\n", "intent": "Before fitting anything let's see what $R^2$ you get by predicting tomorrow with yesterday's value\n"}
{"snippet": "prediction = classifier.predict(test_data)\nfor i, pred in enumerate(prediction):\n    right = True if pred == test_labels[i] else False\n    print(str(i) + ': Predicted: ' + str(pred) + ', Label: ' + str(test_labels[i]) + ', Predicted?: ' + str(right))\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "ls = Lasso(alpha=0.4, normalize=True)\ncv_score = cross_val_score(ls, X, y, cv=5)\nprint(cv_score)\ncv_score2 = cross_val_score(ls, X, y, cv=5, \n                            scoring = make_scorer(mean_squared_error))\nprint(np.sqrt(cv_score2))\nprint('Average 5-fold R^2: {}'.format(np.mean(cv_score)))\nprint('Average 5-fold RMSE: {}'.format(np.mean(np.sqrt(cv_score2))))\n", "intent": "At alpha=0.4, lasso regressor thinks 'child_mortality' is the most important feature when prediction life expectancy.\n"}
{"snippet": "learn.load('resnet50.299')\nlearn = learn_size(learn, 299, 3e-2, [1/500, 1/50, 1/5])\nlearn.save('resnet50.299x2')\nlearn.load('resnet50.299x2')\nlog_preds,y = learn.TTA()\npreds = np.argmax(np.exp(log_preds), axis=1)\nprint(\"Test set accuracy: \", accuracy_score(y, preds))\n", "intent": "95.5% accuracy is looking promising, let's lower the learning rate and try a bit more.\n"}
{"snippet": "linreg.predict([[20]])\n", "intent": "Maybe I should study more?\n"}
{"snippet": "lr.predict(X_test.reshape(-1, 1))\n", "intent": "The `sklearn` built-in methods can make this `predict` process faster:\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\nprint('Test loss (closer to 0 ==  better):', test_loss)\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "prediction_prob_vects = model.predict(single_img_dataset)\nprint(predictions)\n", "intent": "Now predict the image:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for li in range(len(style_layers)):\n        G = gram_matrix( feats[ style_layers[li] ] )\n        A = style_targets[li] \n        style_loss += style_weights[li] * tf.reduce_sum( tf.pow( G - A, 2) )\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "gbm_preds = best_gbc.predict(X_test)\n", "intent": "<b>Test it on the test data</b>\n"}
{"snippet": "print(classification_report(y_test, gbm_preds))\n", "intent": "<b>Classification Report</b>\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, df, y, cv=10)\n", "intent": "K-Fold Cross Validation\n"}
{"snippet": "accuracy = accuracy_score(y_test, best_rfc.predict(X_test))\nprint \"Accuracy: \", accuracy\n", "intent": "Accuracy, Precision, and Recall\n"}
{"snippet": "print classification_report(y_test, best_rfc.predict(X_test))\n", "intent": "Precision and Recall\n"}
{"snippet": "predicted = SVC_W2V.predict(X_test)\nprint(\"Accuracy\", SVC_W2V.score(X_test,y_test))\nprint(\"Metrics\", metrics.classification_report(predicted,y_test))\n", "intent": "Pretty good results, before we had 30% of precision value on negative sets.\n"}
{"snippet": "predictions = clf.predict(X_test)\nacc = accuracy_score(predictions,np.array(y_test))\nprint('Our SVM Classifier %r percent accurate' %(acc*100))\n", "intent": "Then we test its predictive accuracy with the X_test and y_test\n"}
{"snippet": "forecast = model.predict('2015-09-04','2015-12-06',dynamic = True)\nplot = volume.ix['2015':].plot()\nplot = forecast.plot(ax=plot, style='r--', label='Prediction')\nplot.legend()\n", "intent": "This is how it looks after removing the seasonality.\n"}
{"snippet": "scores = cross_val_score(Logit, X_train, y_train, cv=5)\nprint(scores)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "print(metrics.accuracy_score(y_test,predicted))\nprint(metrics.roc_auc_score(y_test,prob[:,1]))\n", "intent": "The above shows that the classifier is predicting a 1 (having an affair) any time the probability in the second column is greater than 0.5.\n"}
{"snippet": "p=lm.predict(X)\npt.title('Scatter between predicted values and actual values in training set')\npt.scatter(Y,p,s=5)\npt.xlabel('actual value')\npt.ylabel('predicted value')\npt.show()\n", "intent": "Higher values denote a strong association and strong dependency.\n"}
{"snippet": "X_test_pca = pca.transform(X_test)\ny_pred = clf.predict(X_test_pca)\n", "intent": "Let's start with a qualitative inspection of the some of the predictions:\n"}
{"snippet": "test_data = np.random.random((10,10))\nfinal_model.predict(test_data)\n", "intent": "You can then generate out-of-sample predictions using this final, fully optimized model.\n"}
{"snippet": "def svm_loss(preds, ys, delta=0):\n    correct = ys.argmax()\n    score_correct = preds[correct]\n    loss = 0\n    for i, pred in enumerate(preds):\n        loss += max(0, pred + delta - score_correct)            \n    return loss\n", "intent": "To supervise training, we keep logging the loss:\n"}
{"snippet": "X, y = sample_suggestions(10000)\nrank_accuracy(y, model.predict(X))\n", "intent": "Still, the model is correct most of the time:\n"}
{"snippet": "y_train_pred_rf = rf_final.predict(xtrain)\ny_valid_pred_rf = rf_final.predict(xtest)\nprint('Random Forest Training MSE:', mean_squared_error(ytrain, y_train_pred_rf))\nprint('Random Forest Training R2:', r2_score(ytrain, y_train_pred_rf))\nprint('Random Forest Validation MSE:', mean_squared_error(ytest, y_valid_pred_rf))\nprint('Random Forest Validation R2:', r2_score(ytest, y_valid_pred_rf))\n", "intent": "It is seen that alcohol is the most important factor, while the density is the least important one.\n"}
{"snippet": "y_train_pred=clf.predict(x_tascale)  \n", "intent": "We can see that for class 3 and 9, the metric is zero, because the small number of instance for this class! <br>\nClass 5-7 has better metrics.\n"}
{"snippet": "sgd_clf.predict(X_train[0, np.newaxis])\n", "intent": "Predict on X_train[0]\n"}
{"snippet": "rf_clf.predict_proba(X_train[0, np.newaxis])\n", "intent": "Calculate the probability of each class for X_train[0].\n"}
{"snippet": "y_score_rf = cross_val_score(rf_clf, X_train, y_train, scoring='accuracy')\n", "intent": "Cacluate the cross validation score of random forest classifier\n"}
{"snippet": "y_pred_rf = cross_val_predict(rf_clf, X_train, y_train)\n", "intent": "Calculate the confusion matrix of the prediction from sgd_clf\n"}
{"snippet": "knn_clf.predict(X_train[0, np.newaxis])\n", "intent": "Predict on X_train[0]. Also check the true answer.\n"}
{"snippet": "reg_ridge.predict(X_poly[0:1])\n", "intent": "Predict by the ridge model at X_poly[0]\n"}
{"snippet": "reg_sgd_l2.predict(X_poly[0:1])\n", "intent": "Predict at X_poly[0]\n"}
{"snippet": "clf_softmax.predict([[5, 2]])\n", "intent": "Predict at [[5, 2]]. Also compute the probability of each class.\n"}
{"snippet": "svm_clf.predict([[5.5, 1.7]])\n", "intent": "Fit svm_clf and predict at [5.5, 1.7]\n"}
{"snippet": "clf_tree.predict_proba([[5, 1.5]])\n", "intent": "Predict the probability of each class for [5, 1.5]\n"}
{"snippet": "parameters={'gamma':np.linspace(0.01,0.2,10),'C':np.linspace(0.5,1.5,10)}\nclfGS = GridSearchCV(clf,parameters,cv=10,scoring=f1_scorer)\ntrain_predict(clfGS, X_train[:300], y_train[0:300], X_test, y_test)\nprint clfGS.best_params_ \n", "intent": "Apply Gridsearch now on a subset of the parameters around the best found. Note: Kernel rbf is the default kernel.\n"}
{"snippet": "split = 0.67\nX, XT, Z, ZT = loadDataset(split)\nprint 'Train set: ' + repr(len(X))\nprint 'Test set: ' + repr(len(XT))\nk = 3\nYT = predict(X, Z, XT, k)\naccuracy = getAccuracy(YT, ZT)\nprint('Accuracy: ' + repr(accuracy) + '%')\n", "intent": "Should output an accuracy of 0.95999999999999996%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    j = 0\n    for i in style_layers:\n        loss += style_weights[j]*tf.reduce_sum(tf.pow(style_targets[j] - gram_matrix(feats[i]), 2))\n        j += 1\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "best = None\nif  list_recall_lr[index_lr] > list_recall_nn[index_nn] and list_recall_lr[index_lr] > list_recall_xgb[index_xgb]:\n    best = list_models_lr[index_lr]\nelif  list_recall_nn[index_nn] > list_recall_lr[index_lr] and list_recall_nn[index_nn] > list_recall_xgb[index_xgb]:\n    best = list_models_nn[index_nn]\nelse :\n    best = list_models_xgb[index_xgb]\npredicted_test = fit_and_predict('Best Model', best ,X_test_dropped_features, Y_test, 5)\n", "intent": "After choosed the best method according to the previous results, it's time to apply the final test for it.\n"}
{"snippet": "pred= list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "def forward(X):\n    Z = X\n    for h in hidden_layers:\n        Z = h.forward(Z)\n    return tf.matmul(Z,W) + b \ndef predict(X): \n    act = forward(X)\n    return tf.argmax(act,1)\n", "intent": "Functions for tensorflow\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        layer_num = style_layers[i]\n        gram = gram_matrix(feats[layer_num])\n        style_loss += style_weights[i] * tf.reduce_sum((gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def compute_loss(self):\n    weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n    self.loss = tf.contrib.seq2seq.sequence_loss(\n        self.train_outputs.rnn_output, \n        self.ground_truth, \n        weights\n        )\n", "intent": "<p>And now the <b>loss function</b>:</p>\n"}
{"snippet": "estimation_2_point_5 = [2.5 * x for x in train_features]\nmean_squared_error(train_prices, estimation_2_point_5)\n", "intent": "and compared to `y = 2.5x`:\n"}
{"snippet": "predicted_values = lr.predict(x_test[:5])\npredicted_values\n", "intent": "What will our linear model predict? Let's see:\n"}
{"snippet": "r2_score(y_test, lr.predict(x_test))\n", "intent": "And also as the standalone [`r2_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n"}
{"snippet": "predicted = lr.predict(X_test)\n", "intent": "And we're ready to predict our testing set:\n"}
{"snippet": "print(np.sum(lm.predict(X) - np.mean(bos.PRICE)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "clf.predict(digits.data[-1:])\n", "intent": "Finally, we can predict the digit corresponding to a particular example:\n"}
{"snippet": "pred = model.predict(dtest)\npred = np.exp(pred) - 1\n", "intent": "The final step before submission is to make our predictions using the trained model:\n"}
{"snippet": "class RbfGaussian(RbfFunction):\n    def evaluate(self, x):\n        value = 0\n        width = self.width\n        for i in range(self.dimensions):\n            center = self.get_center(i)\n            value += ((x[i] - center) ** 2) / (2.0 * width * width)\n        return np.exp(-value)\n", "intent": "RBFs can take various shapes: quadratic, multi-quadratic, inverse multi-quadratic, mexican hat. Yet the most used is the Gaussian.\n"}
{"snippet": "def compute_score(clf, X,y, scoring='accuracy'):\n    xval=cross_val_score(clf, X, y, cv=5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "** To do that we'll define a small scoring function **\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    y_pred = clf.predict(features)\n    return f1_score(target.values, y_pred, pos_label= 1 )\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Here are some helper functions for what's to follow\n"}
{"snippet": "fpr, tpr, thresholds = metrics.roc_curve(y_test, clf.predict(X_test), pos_label=1)\nmetrics.auc(fpr, tpr)\n", "intent": "The accuracy of this model is bigger than that of our baseline model which was 0.886. This model is thus better. Let's compute its AUC then:  \n"}
{"snippet": "for n in [100, 200, 300, 400, 500]:\n    train_predict(clf, X_train[:n], y_train[:n], X_test, y_test)\n", "intent": "Let's now see how well our tuned model does on multiple training set sizes ranging from 100 to 500\n"}
{"snippet": "fpr, tpr, thresholds = metrics.roc_curve(y_test, clf.predict(X_test), pos_label=1)\nmetrics.auc(fpr, tpr)\n", "intent": "There has obviously been some improvements in the results; let's check this fact by also computing the Area under Curve (AUC) for our tuned model.\n"}
{"snippet": "our_linear_svm.predict(new_points)\n", "intent": "Our predicted values from new points are given by the method \"predict\" in our_linear_svm.\n"}
{"snippet": "SVM_success_report(our_quadratic_svm, points, labels)\n", "intent": "It works for a big number of points.\n"}
{"snippet": "fail_mask = (our_quadratic_svm.predict(points) != labels)\nfail_labels = labels[fail_mask]\nfail_points = points[fail_mask]\n(fail_points, np.sqrt(fail_points[:,0]**2 + fail_points[:,1]**2), fail_labels, \n our_quadratic_svm.predict(fail_points))\n", "intent": "If it doesn't work for a few points (because of an insufficient value of C), then these are the points that don't work\n"}
{"snippet": "def generate_images(generator_model,noise_dim, num_samples=1000):\n    predicted_samples = generator_model.predict(np.random.rand(num_samples, noise_dim))\n    pl.figure(figsize=(6,6))\n    pl.scatter(X_train[:,0], X_train[:,1],s = 40, alpha=0.2, edgecolor = 'k', marker = '+',label='original samples') \n    pl.scatter(predicted_samples[:,0], predicted_samples[:,1],s = 10, alpha=0.9,c='r', edgecolor = 'k', marker = 'o',label='predicted') \n    pl.xticks([], [])\n    pl.yticks([], [])\n    pl.legend(loc='best')\n    pl.tight_layout()    \n    pl.show()\n", "intent": "Before running the full model, we provide a function that will help in visualizing the generated samples\n"}
{"snippet": "results.loc[len(results)] = [\"Affinity Propogation\", \n                             n_clusters_, \n                             \"\",\n                             \"200\",\n                             \"%0.3f\" % metrics.silhouette_score(processed_numerical_p50_copy, labels, metric='sqeuclidean'),\n                             \"%0.3f\" % metrics.homogeneity_score(labels_true, labels),\n                             \"\"]\nresults\n", "intent": "Affinity propogation algorithm has also generated 3 clusters, conforming to output from kmeans\n"}
{"snippet": "results.loc[len(results)] = [\"DBSCAN\", \n                             n_clusters_, \n                             \"\",\n                             \"eps = 10\",\n                             \"NA\",\n                             \"%0.3f\" % metrics.homogeneity_score(labels_true, labels),\n                             \"\"]\nresults\n", "intent": "It seems DBSCAN is not able to generate any clusters\n"}
{"snippet": "results.loc[len(results)] = [\"Mini Batch KMeans - Kmeans++\", \n                             3, \n                             \"10\",\n                             \"300\",\n                             \"%0.3f\" % metrics.silhouette_score(scaled, labels),\n                             \"%0.3f\" % metrics.homogeneity_score(labels_true, labels),\n                             \"%0.3f s\" % (t_batch)]\nresults\n", "intent": "We clearly see that fit time has decreased considerably from 1.4 sec to 0.015 sec\nThis is huge improvement in terms of speed\n"}
{"snippet": "predicted_values = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "lin_reg.predict(6.5)\n", "intent": "**Predicting a new result with Linear Regression**\n"}
{"snippet": "y_pred = regressor.predict(6.5)\ny_pred\n", "intent": "**Predicting a new result**\n"}
{"snippet": "y_pred = classifier.predict(x_test)\n", "intent": "**Predicting the Test set results**\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n", "intent": "**Predicting the Test set results**\n"}
{"snippet": "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\nnew_prediction = (new_prediction > 0.5)\nnew_prediction\n", "intent": "**Predicting a single new observation**\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "**Predicting the Test set results**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"accuracies.mean(): \",accuracies.mean(), \"\\naccuracies.std(): \", accuracies.std())\n", "intent": "**Applying k-Fold Cross Validation**\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_VResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import helpers as hp\nX,y = hp.make_training_vectors(text_train, unroll=unroll, step=1)\nactivity = proj.predict(X)\ntophidden = nhidden[len(nhidden) - 1]\nif tophidden==2:\n    proj_2d = activity\nif tophidden>3:\n    from tsne import bh_sne\n    proj_2d = bh_sne(activity, theta=0.5) \n", "intent": "So here's the time series of hidden unit activity:\n"}
{"snippet": "est_elapsed_time = apr06.TaxiOut + air_est.predict(dtest_air) + tax_est.predict(dtest_tax)\npreds = apr06.DepDelay + est_elapsed_time - apr06.CRSElapsedTime\nprint('Three-Component Predictions with Boosted Decision Trees Performance: All Flights April 2006 \\n')\neval_performance(preds, apr06.ArrDelay)\n", "intent": "Finally, I put my models (trained on April 2005 data) to use and make some predictions on the April 2006 arrival times.\n"}
{"snippet": "knn.predict_proba([3,4,4,1])\n", "intent": "One can also know probablisitic distribution of the point or given features\n"}
{"snippet": "y_pred = model.predict(X)\ny_pred\n", "intent": "Random state must be fixed to generate a fixed random number generator each time the code is run.\n"}
{"snippet": "loanact_predict = df.loc[0:1,'activity'].values.astype(str).tolist() \ntfidf_matrix_predict = tfidf_vectorizer.transform(loanact_predict)\nloanact_classes = km.predict( tfidf_matrix_predict )\nprint(loanact_classes )\n", "intent": "The predictions are exactly those expected:\n"}
{"snippet": "y_pred_train = pipeline.predict(X_train)\ny_pred_test = pipeline.predict(X_test)\ndef get_threshold_metrics(y_true, y_pred):\n    roc_columns = ['fpr', 'tpr', 'threshold']\n    roc_items = zip(roc_columns, roc_curve(y_true, y_pred))\n    roc_df = pd.DataFrame.from_items(roc_items)\n    auroc = roc_auc_score(y_true, y_pred)\n    return {'auroc': auroc, 'roc_df': roc_df}\nmetrics_train = get_threshold_metrics(y_train, y_pred_train)\nmetrics_test = get_threshold_metrics(y_test, y_pred_test)\n", "intent": "MLPClassifier has no method called `decision_function()`, so need to use `pipeline.predict()`.\n"}
{"snippet": "ResNet50_predictions_breed = [np.argmax(ResNet50_model_breed.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50_breed]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions_breed)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions_breed)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "1 - mean_squared_error(y_test, y_predict)/np.var(y_test)\n", "intent": "ss_residual(Residual Sum of Squares)\nss_total(Total Sum of Squares)\n"}
{"snippet": "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\ntest_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\nprint(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\nprint(\"Test set accuracy: {accuracy}\".format(**test_eval_result))\n", "intent": "Run predictions for both training and test set.\n"}
{"snippet": "accuracy=cross_val_score(naiveBayes,test,test_class,cv=ea,scoring='accuracy')\nf1=cross_val_score(naiveBayes,test,test_class,cv=ea,scoring='f1_macro')\nroc=mean_roc_auc(naiveBayes,test,test_class,a)\nprint(accuracy.mean())\nprint(f1.mean())\nprint(roc[\"auc\"])\n", "intent": "The Naive Bayes classifier generates well on the test set.\n"}
{"snippet": "accuracy=cross_val_score(neuralNetwork,test,test_class,cv=ea,scoring='accuracy')\nf1=cross_val_score(neuralNetwork,test,test_class,cv=ea,scoring='f1_macro')\nroc=mean_roc_auc(neuralNetwork,test,test_class,a)\nprint(accuracy.mean())\nprint(f1.mean())\nprint(roc[\"auc\"])\n", "intent": "The artificial neural network classifier generates well on the test set.\n"}
{"snippet": "accuracy=cross_val_score(svm,test,test_class,cv=ea,scoring='accuracy')\nf1=cross_val_score(svm,test,test_class,cv=ea,scoring='f1_macro')\nroc=mean_roc_auc(svm,test,test_class,a)\nprint(accuracy.mean())\nprint(f1.mean())\nprint(roc[\"auc\"])\n", "intent": "The SVM classifier generates well on the test set.\n"}
{"snippet": "accuracy=cross_val_score(ensemble,test,test_class,cv=ea,scoring='accuracy')\nf1=cross_val_score(ensemble,test,test_class,cv=ea,scoring='f1_macro')\nroc=mean_roc_auc(ensemble,test,test_class,a)\nprint(accuracy.mean())\nprint(f1.mean())\nprint(roc[\"auc\"])\n", "intent": "The ensemble classifier generates well on the test set.\n"}
{"snippet": "athlete_predictions = clf.predict(athletes_test[['Height', 'Weight']])\nathlete_predictions\n", "intent": "Now that we trained the classifier we can use it to make predictions using the `predict` method\n"}
{"snippet": "predictions = []\nfor passenger_index, passenger in titanic_data.iterrows():\n        predictions.append(predict(passenger))\npredictions\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(titanic_data['Survived'], predictions)    \nprint('Accuracy: %4.2f%%' % (score * 100))\n", "intent": "You just need to execute the following code block to see how good your rule was.\n"}
{"snippet": "linkage_matrix_full = scipy.cluster.hierarchy.ward(newsgroupsCoocMat.toarray())\nhierarchicalClusters_full = scipy.cluster.hierarchy.fcluster(linkage_matrix_full, 4, 'maxclust')\nprint(\"For our complete clusters:\")\nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nprint(\"Adjusted Rand Score: {:0.3f}\".format(sklearn.metrics.adjusted_rand_score(newsgroupsDF['category'], hierarchicalClusters_full)))\n", "intent": "We can use this *get clusters* like we did with k-means. What if we do the full data set?\n"}
{"snippet": "x = @(image_224_batch.copy())\npreds = model.predict(x)\n", "intent": "`image_224_batch` is now compatible with the input shape of the neural network, let's make a prediction.\n"}
{"snippet": "y_predicted = model.predict(x_test)\ny_ = np.argmax(y_predicted,axis = -1)\nprint(\"test accuracy : \", np.mean(target_test == y_))\n", "intent": "**Exercices**\n- Compute model accuracy on test set\n"}
{"snippet": "stratified_kfold = StratifiedKFold(df_train['Survived'] == 1, n_folds = 10)\ncross_val_score(simple_pipeline, X, y, 'accuracy', cv = stratified_kfold)\n", "intent": "Looks good! How about k-fold stratified cross validation ?\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, \n                                                 cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n", "intent": "We'll tackle this more in the next section on **Model Performance and Evaluation.**  \nFor now, just let's say *higher* is *better*.  \n"}
{"snippet": "test_preds = best_alpha*X_test_level2[:,0] + (1-best_alpha)*X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test,test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "r2_train_stacking = r2_score(y_train_level2, train_pred)\nr2_test_stacking = r2_score(y_test, test_pred)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "print(metrics.classification_report(y_test, predicted, target_names=labels))\n", "intent": "To show the main classification metrics we will use the classification_report method from scikit-learn.\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "4.You predict new y values, given new x values\n"}
{"snippet": "y_pred = knn.predict(X)\n", "intent": "** Notice that we use only one neighbor. **\nNow we'll use this classifier to *predict* labels for the data\n"}
{"snippet": "X_test = test_df[features].values\ny_test = test_df[target].values\ny_pred = search.predict(X_test)\nmodel_test_score = spearman_scorer(search, X_test, y_test)\nmodel_test_score\n", "intent": "Now that we've tuned our model let's evaluate it on the test set.\n"}
{"snippet": "tree.predict(example.reshape(1,-1))\n", "intent": "And 0.481 should be what the tree predicts.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out the model on the test dataset of dog images.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==\n                           np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out the model on the test dataset of dog images. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n", "intent": "This ROC curve provides a nice visual way to assess your classifier's performance.\n"}
{"snippet": "print big_model.evaluate(train_data)['accuracy']\nprint big_model.evaluate(validation_data)['accuracy']\n", "intent": "Now, let us evaluate **big_model** on the training set and validation set.\n"}
{"snippet": "print metrics.roc_auc_score(Y_test, probs[:, 1])\n", "intent": "The accuracy is 98.6928%\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmse_scores = cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error')\nprint mse_scores\nprint np.mean(np.sqrt(-mse_scores))\n", "intent": "I next used a 10-fold cross-validation to calculate the RMSE for the linear regression model.  The RMSE when using all features was 0.0352. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmse_scores = cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error')\nprint mse_scores\nprint np.mean(np.sqrt(-mse_scores))\n", "intent": "I next used a 10-fold cross-validation to calculate the RMSE for the linear regression model.  The RMSE when using all features was 36.81. \n"}
{"snippet": "feature_cols = ['PCT_5_STAR',  'PCT_4_STAR', 'PCT_3_STAR',  'PCT_2_STAR',  'PCT_1_STAR',  'AVERAGE_OVERALL_RATING', \n                       'shop_REI GARAGE', 'TOTAL_SUBMITTED__RATINGS_ONLY_REVIEWS', 'STD',  'TOTAL_SUBMITTED__REVIEWS',  \n                       'PCT_RECOMMEND_TO_A_FRIEND',  'PCT_YES',  'shop_CAMPING',  'REVIEWS_5',  'shop_OUTDOOR ESSENTIALS', \n                       'shop_APPAREL', 'shop_HIKE & LIFESTYLE FOOTWEAR, SOCKS, BASICS',  'PRICE']\nX = prod_oct[feature_cols]\nprint np.sqrt(-cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error')).mean()\n", "intent": "A sampling of the other combinations I tried:\n"}
{"snippet": "predictions = model.predict(X_test, batch_size=batch_size)\n", "intent": "Make prediction on improved LSTM model\n"}
{"snippet": "trainScore = mean_squared_error(X_train, y_train)\nprint('Train Score: %.4f MSE (%.4f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = mean_squared_error(predictions, y_test)\nprint('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "measure accuracy of the prediction\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Make prediction using test data\n"}
{"snippet": "trainScore = model.evaluate(X_train, y_train, verbose=0)\nprint('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "Get the test score.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for idx, layer in enumerate(style_layers):\n        loss += style_weights[idx] * torch.sum((gram_matrix(feats[layer]) - \n                                           style_targets[idx])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(classification_report(Y_pred,y_pred))\n", "intent": "Above is the result for Logistic regression having 56% of precision.\n"}
{"snippet": "y_pred = rf_titanic.predict(X_titanic)\nC = confusion_matrix(y_titanic, y_pred)\nC = C / C.astype(float).sum().sum()\nprint(C)\n", "intent": "Look at how the data is being predicted:\n"}
{"snippet": "y_pred_test = rf_titanic.predict(test[cols_train])\n", "intent": "Now I will predict on the actual data and save.\n"}
{"snippet": "y_pred = multiple_linreg.predict(X_test)\nprint (y_pred)\n", "intent": "Final step for this process is trying to predict the outcome based on out of training samle data.\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import accuracy_score\naccuracy_score(test_labels[0:54147], prediction)\n", "intent": "Using the sklearn accuracy score to measure the accuracy of the prediction model. \n"}
{"snippet": "y_predicted = model.predict(X_test)\n", "intent": "We predict the power output on our test data based on the learned model\n"}
{"snippet": "mse = mean_absolute_error(y_train, model.predict(X_train))\nmse\n", "intent": "```\njoblib.dump(model, 'trained_house_classifier_model.pkl')\nmodel = joblib.load('trained_house_classifier_model.pkl')\n```\n"}
{"snippet": "print(\"====== Predict as all negative =======\")\nY_pred_all_neg  = np.zeros((len(Y_test),), dtype=np.int)\nprint(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n                                          score=accuracy_score(Y_test, Y_pred_all_neg)) )\nprint(\"====== Predict by random guess =======\")\nY_score_rand = np.random.uniform(0,1, (len(Y_test),))    \nprint(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n                                              score=average_precision_score(Y_test, Y_score_rand)) )\n", "intent": "Notice that the positive and negative examples in test is not balance (with Nr(pos)=1707, Nr(neg)=415 )\n"}
{"snippet": "print 'Predict time'\nprint''\nfrom sklearn.metrics import accuracy_score\nresult = ('ClassifyRF accuracy:---------->%.2f %%' % (accuracy_score(y_pred, y_validate)*100))\nprint result\n", "intent": "rfc = joblib.load('DB_INTENC/preproc/labeled/rfc.pkl')\n"}
{"snippet": "print 'Predict time'\nprint''\nfrom sklearn.metrics import accuracy_score\nresult = ('ClassifyRF accuracy:---------->%.2f %%' % (accuracy_score(y_pred, y_validate)*100))\nprint result\n", "intent": "Predict time\nCPU times: user 1.37 s, sys: 216 ms, total: 1.59 s\nWall time: 591 ms\nClassifyRF accuracy:---------->22.04 %\n"}
{"snippet": "x_test = np.array(['This assignment is easy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "evaluate_classification_error(my_decision_tree, test_data)\n", "intent": "Now, let's use this function to evaluate the classification error on the test set.\n"}
{"snippet": "prediction = knn.predict(X_new)\n", "intent": "predict returns the index of the target\n"}
{"snippet": "scores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\n", "intent": "cross_val_score refauls to 3 folds parameterized using cv\n"}
{"snippet": "from sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \",len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\n", "intent": "For smaller datasets, one data point can be a fold (like jackknife analysis)\n"}
{"snippet": "reload(extr)\nF1 = metrics.f1_score(SP.vstack(labels).ravel().astype('int'),pred.ravel()>.5, average=\"macro\")\nprint('Macro averaged F1 score %1.2f:' % F1)\nextr.plotPerfromance(pred.ravel(),  SP.vstack(labels).ravel())\n", "intent": "Finally, we can evaluate the model perfromacne on the test set by calculating the macro-averaged F1 score and plot an ROC curve\n"}
{"snippet": "X1 = normalized_data.iloc[top3modelno].values\nwith open('./rfc_E11.pkl', 'r') as f:\n    rfc = pickle.load(f)\npredictions = rfc.predict_proba(X1)[:,1]\n", "intent": "Load the Model and make a prediction.\n"}
{"snippet": "print (10**2 + 0**2 + 20**2 + 10**2)/4\nprint metrics.mean_squared_error(true,pred)\n", "intent": "**Mean Squared Error(MSE)** is the mean of the square erros:\n$$ \\frac{1}{n}=\\sum_{i=1}^{n}(|y_i -\\hat{y_i}|)^2 $$\n"}
{"snippet": "import numpy as np\nprint np.sqrt((10**2+0**2+20**2+10**2)/4)\nprint np.sqrt(metrics.mean_squared_error(true,pred))\n", "intent": "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n$$ \\sqrt(\\frac{1}{n}=\\sum_{i=1}^{n}(|y_i -\\hat{y_i}|)^2) $$\n"}
{"snippet": "n_tree = 100\nn_feature = [0.2, 0.4, 0.6, 0.8, 1.0]\nn_cv = 5\nrf_tune = []\nfor i in range(len(n_feature)):\n        rf = RandomForest(n_estimators=n_tree, max_features = n_feature[i])\n        score = sum(cross_val_score(rf, x_train, y_train, cv = n_cv, scoring = 'f1_macro'))/n_cv\n        rf_tune.append(score)\n", "intent": "Still, there doesn't seem to be hugh improvement.\n"}
{"snippet": "y_pred_dec_tree = dec_tree_clf.predict(np.array(X_test_multi_pref))\nprint(y_pred_dec_tree)\ny_pred_rnd_forest = rnd_forest_clf.predict(np.array(X_test_multi_pref))\nprint(y_pred_rnd_forest)\ny_pred_fcnn = (fcnn.predict(np.array(X_test_multi_pref)) > 0.5).astype(float)\nprint(y_pred_fcnn)\n", "intent": "Evaluating each model\n"}
{"snippet": "examples = ['You just won a prize!', 'Hello my friend']\nexample_vector = vectorizer.transform(examples)\npredictions = classifier.predict(example_vector)\nprint(predictions)\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "examples = ['Free travel, call only today!', 'Hello my friend']\nexample_vector = vectorizer.transform(examples)\npredictions = classifier.predict(example_vector)\nprint(predictions)\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "print(classification_report(y_test, predict))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "def validation_test(classifier, validation_set, expected_class):\n    columns = list(validation_set.columns)\n    columns.remove('slave_logs')\n    validation_set = validation_set.as_matrix(columns)\n    predictions = classifier.predict(validation_set[::,1::])\n    counts = collections.Counter(predictions)\n    percent_correct = (counts[expected_class]/(len(predictions))* 100)\n    print('Validation set was classified as', expected_class,\n          round(percent_correct,2), '% of the time')\n", "intent": "- check if slave logs from cliwoc data were classified correctly (want mostly classified as 1)\n- compare first column with slave_index\n"}
{"snippet": "y_hat = mlp.predict(X_train)\nprint (\"Train accuracy is {0:.2%}\".format(1. - np.mean(np.abs(y_hat - y_train))))\ny_hat = mlp.predict(X_test)\nprint (\"Test accuracy is {0:.2%}\".format(1. - np.mean(np.abs(y_hat - y_test))))\n", "intent": "Or you can get the probabilities of different labels using `.predict_proba()`\n"}
{"snippet": "from sklearn.metrics import r2_score\nR2 = r2_score(y_actual, predicted_values)\nR2\n", "intent": "Now, we can calculated R squared using the scikit learn function, as this is how you'd normally do it.\n"}
{"snippet": "y_ = np.argmax(y_test[0:100],1) \nprint metrics.classification_report(y_true= y_, y_pred= y_pred_lb)\nprint metrics.confusion_matrix(y_true= y_, y_pred= y_pred_lb)\nprint(\"Classification accuracy: %0.6f\" % metrics.accuracy_score(y_true= y_, y_pred= y_pred_lb) )\nprint(\"Log Loss: %0.6f\" % metrics.log_loss(y_true= y_, y_pred= y_pred, labels=range(4)) )\n", "intent": "Accuracy is depend on the number of epoch that you set in partametrs part.\n"}
{"snippet": "pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\ndf = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score'])\ndf\n", "intent": "Now iterate through classifiers and save the results\n"}
{"snippet": "y[logreg_scaled.predict(X_scaled) == 1]\n", "intent": "The code below compares the best two models 'winner' prediction  to the actual outcome of the primaries.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_X]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature,axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets,axis=1))/len(Xception_predictions)\nprint('Test Accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Check the test accuracy of trained model below. \n"}
{"snippet": "print(classifier.predict(count_vect.transform([\"This company refuses to provide me verification and validation of debt per my right under the FDCPA. I do not believe this debt is mine.\"])))\n", "intent": "After fitting the data, let's make some prediction\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, target_names=df['Product'].unique()))\n", "intent": "They are consistent within our expectation.\nFinally, we print out the classification report for each class:\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0)))\\\n                     for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "inception_predictions = [np.argmax(loaded_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "G = np.dot(X_test, X_train.T)\ny_pred = clf.predict(G)\n", "intent": "Predict some stuff.\n"}
{"snippet": "score = cnn_clf.evaluate(X_test, Y_test)\nprint('Loss: %s \\nAccuracy: %s' % (score[0], score[1]))\nmiscl = np.nonzero(cnn_clf.predict_classes(X_test).reshape((-1,)) != y_test)\nto_list = list(miscl)\nprint (len(to_list[0]))\n", "intent": "<b>Test the model and print results</b>\n"}
{"snippet": "nn_predictions = nn_clf.predict(X_test)\nsvm_predictions = svm_clf.predict(X_test)\nrf_predictions = rf_clf.predict(X_test)\n", "intent": "<b>Test classifiers<b>\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Once the model is fit, we estimate the performance of the model on unseen reviews.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))\n", "intent": "Now the model is already created. Let's try it out on a few instances from the training set:\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint(\"AUC score for simple IP address predictions = %.6f\"\n      % (roc_auc_score(df[\"is_attributed\"], df[\"predictions\"])))\n", "intent": "4 We can now check the AUC score for this prediction using sklearn metric roc_auc_score\n"}
{"snippet": "print(\"AUC score of predictions using ip on the whole dataset = %.6f\"\n      % (roc_auc_score(target, predictions)))\n", "intent": "Display AUC score for this simple prediction on training dataset\nPlease note this may take some time\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint(\"AUC score of predictions using ip on the whole dataset = %.6f\"\n      % (roc_auc_score(target, predictions)))\n", "intent": "Display AUC score for this simple prediction on training dataset\nPlease note this may take some time\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint(\"AUC score for click rate over a day : %.6f\" % roc_auc_score(train['is_attributed'], train['click_rate']))\n", "intent": "Now let's check if the click_rate over a day makes sense for the dataset\n"}
{"snippet": "predictions = model.predict(test_images)\n", "intent": "**Making Predictions**  \nOnce we have the model trained to our acceptable level we can then try and predict what an image will be.\n"}
{"snippet": "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0)\nprint(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n", "intent": "**Testing**  \nWe are now going to see how our model does with the test set.\n"}
{"snippet": "img = learn.data.train_ds[0][0]\nlearn.predict(img)\n", "intent": "To simply predict the result of a new image (of type [`Image`](/vision.image.html\n"}
{"snippet": "predictions = pulsar_classifier.predict(X_test)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPredict on Test Set \n<br><br></p>\n"}
{"snippet": "print('Decision Tree Accuracy score: {0:0.2f}'.format(accuracy_score(y_true = y_test, y_pred = predictions)*100) + \"%\")\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nMeasure Accuracy of the Classifier\n<br><br></p>\n"}
{"snippet": "net.evaluate()\nnet.zeroGradParameters()\npred = net.forward(X_valid)\naccuracy = accuracy_score(y_valid.argmax(axis=1), pred.argmax(axis=1))\nprint(f\"Accuracy score: {accuracy}\")\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "l= 100\npred = model.predict(X[l:l+4])\nsample = X[l:l+4]\nsample = sample.reshape(sample.shape[0],sample.shape[1],sample.shape[2] )\nplot_imgArr(sample,y[l:l+4], pred, gray=True)\npred\n", "intent": "Plot some images and the predicted steering angle for a quick comparison.\n"}
{"snippet": "jc_matrix = np.zeros(adjacency_repr.shape)\nfor u, v, p in nx.jaccard_coefficient(g_train): \n    jc_matrix[u][v] = p\n    jc_matrix[v][u] = p \njc_matrix = jc_matrix / jc_matrix.max()\njc_roc, jc_ap = get_roc_score(test_edges, test_edges_false, jc_matrix)\nprint('Jaccard Coefficient Test ROC score: ', str(jc_roc))\nprint('Jaccard Coefficient Test AP score: ', str(jc_ap))\n", "intent": "The probability of having a randomly selected neighbor in common. \n$$f_{ij} = \\frac{|N(e_i)|\\cap|N(e_j)|}{|N(e_i)|\\cup|N(e_j)|}$$\n"}
{"snippet": "score = -1*cross_validation.cross_val_score(regr, np.ones((n, 1)), y_PCA.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\nmse.append(score)\n", "intent": "Do one CV to get MSE for intercept alone (no PCs):\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsil_score2 = silhouette_score(X = dataToCluster_scaled, labels = kmeansModel_2.labels_, metric='euclidean', sample_size=None)\nsil_score3 = silhouette_score(X = dataToCluster_scaled, labels = kmeansModel_3.labels_, metric='euclidean', sample_size=None)\nprint(\"for n_clusters = 2, the average silhouette score is: \", sil_score2)\nprint(\"for n_clusters = 3, the average silhouette score is: \", sil_score3)\n", "intent": "But what if we want to try more clusters and evaluate them?\nhttp://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n"}
{"snippet": "predict_and_evaluate(xgb_model_opt, test_features, test_labels)\n", "intent": "Test on a testing set:\n"}
{"snippet": "price_pred = regr.predict([[2009,180000, 1896]])\nprint('The best price for VW Passat B6 1.9 TDI 2009 with 188 000 mileage is %.2f PLN' % price_pred[0][0])\n", "intent": "> - 1.6 = 1598\n> - 1.9 = 1896\n> - 2.0 = 1968\n"}
{"snippet": "from sklearn.metrics import completeness_score, homogeneity_score\nprint('Completeness score: {0:.2f}, Homogenity score: {1:.2f}'.format(completeness_score(y_test, y_pred), homogeneity_score(y_test, y_pred)))\n", "intent": "Classification metrics do not work for clustering.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(testY, res)))\n", "intent": "**More metrics**\nRoot mean-squared-error is in the same units as the target data, meaning our prediction may be off by around *xx* minutes. \n"}
{"snippet": "correct = 0\nfor i in range(len(preds)):\n    if (y_test[i] == preds[i]):\n        correct += 1\nacc = accuracy_score(y_test, preds)\nprint('Predicted correctly: {0}/{1}'.format(correct, len(preds)))\nprint('Error: {0:.4f}'.format(1-acc))\n", "intent": "Calculate obtained error\n"}
{"snippet": "cross_val_score(clf, data_v, label, cv=2, scoring='accuracy')\n", "intent": "Cross-validate results with full dataset. Because we have only 10 samples, we will perform 2-fold CV.\n"}
{"snippet": "cross_val_score(clf, data_m, label, cv=2, scoring='accuracy')\n", "intent": "Some score was obtained, we won't dig into it's interpretation.\nSee if things work also with missing values\n"}
{"snippet": "bst = xgb.train(params, dtrain, num_rounds)\ny_test_preds = (bst.predict(dtest) > 0.5).astype('int')\n", "intent": "Train the booster and make predictions.\n"}
{"snippet": "bst = xgb.train(params, dtrain, num_rounds)\ny_test_preds = (bst.predict(dtest) > 0.5).astype('int')\n", "intent": "Train the classifier and get predictions (same as in baseline):\n"}
{"snippet": "bst = xgb.train(params, dtrain, num_rounds)\ny_test_preds = (bst.predict(dtest) > 0.5).astype('int')\npd.crosstab(\n    pd.Series(y_test, name='Actual'),\n    pd.Series(y_test_preds, name='Predicted'),\n    margins=True\n)\n", "intent": "And like before, analyze the confusion matrix and obtained metrics\n"}
{"snippet": "R2BestDTTrain = r2_score(y_train, predBestDTTrain)\nR2BestDTTest = r2_score(y_test, predBestDTTest)\nMAEBestDTTrain = mean_absolute_error(y_train, predBestDTTrain)\nMAEBestDTTest = mean_absolute_error(y_test, predBestDTTest)\n", "intent": "* $R^2$ & $MAE$ Score:\n"}
{"snippet": "from sklearn.model_selection import KFold, cross_val_score\nkf = KFold(random_state=10)\ncross_val_score(tree, features, labels, cv=kf)\n", "intent": "Cross val, understand split matters for predict power\n"}
{"snippet": "predictions=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "class RegressionModel:\n    def fit(self, X, y):\n        pass\n    def predict(self, X):\n        return [0]*X.shape[0]\n", "intent": "Out model interface will consists of 2 mothods:\n  * fit(X, y)   - This function will train model\n  * predict(X)  - Predict value\n"}
{"snippet": "def score(y_true, y_pred):\n    return metrics.f1_score(y_true, y_pred, np.unique(y_pred))\nprint(score(df2017.flow1_anomalies, df2017.flow1_anomalies))\nprint(score(df2017.flow1_anomalies, df2017.flow2_anomalies))\n", "intent": "We use F1 score:\nhttp://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = knn.predict(X_new)\niris['target_names'][prediction]\n", "intent": "Then we'll try predicting using the knn object's predict method as follows:\n"}
{"snippet": "hyperparameters[\"DROPOUT_RNN_STATE_KEEP_PROB\"]=1\nhyperparameters[\"DROPOUT_RNN_INPUT_KEEP_PROB\"]=1\nhyperparameters[\"L2_REG_DENSE\"] = 0\nmetrics = local_estimator.evaluate(input_fn=lambda: _local_eval_input_fn(training_dir, hyperparameters),steps=evaluation_steps)\n", "intent": "The model created from local training can be evaluated (for accuracy) as demonstrated below:\n"}
{"snippet": "input_fnX = tf.estimator.inputs.numpy_input_fn(\n    x={'borrower_stats': df_pred_list_clean_conj}, shuffle=False)\npreds = list(model.predict(input_fnX))\nuniquepred=list(set(preds))\nprint(\"\\nunique list of prediction: \",uniquepred)\n", "intent": "<b>Predict!</b> Now is the time. We use the model we have trained to predict the loan_status from <i>test</i> data.\n"}
{"snippet": "print(\"Final Loss on the testing set: \")\npredictions_prev_set_new = []\nfor i in range(len(LABEL)):\n    ev = regressor_set[i].evaluate(input_fn=lambda: input_fn(test_conj,LABEL[i]), steps=1)\n    loss_score1 = ev[\"loss\"]\n    print(LABEL[i],\"{0:f}\".format(loss_score1))\n    y = regressor_set[i].predict(input_fn=lambda: input_fn(test_conj,LABEL[i]))\n    predictions_prev = list(itertools.islice(y, test_conj.shape[0]))\n    predictions_prev_set_new.append(predictions_prev)\nprint(\"predictions_prev_set_new length = \",len(predictions_prev_set))\n", "intent": "Predict the output of the test part of the training data.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint 'Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred)\nprint 'Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred)\nprint 'F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred)\n", "intent": "F!-score (F1): 2*(PRE*REC)/(PRE+REC)  \n"}
{"snippet": "MSE = np.sum((bos.PRICE - lm.predict(X)) ** 2)/(len(bos['RM']) - len(lm.coef_) - 1)\nMSE\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * 2 * tf.nn.l2_loss(gram_matrix(feats[style_layers[i]]) - \n                                                       style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import r2_score\nprint(\"Test Accuracy : \",r2_score(yhat_test,y_test))\nprint(\"Training Accuracy : \",r2_score(yhat_train,y_train))\n", "intent": "<h4>Evaluation</h4>\nWe will evaluate the predicted values with the actual values using the r square method.\n"}
{"snippet": "print(r2_score(yhat_p,y_test))\n", "intent": "Evaluation<br>\nTheir testing accuracy i given below.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Model test accuracy is %.1f%%' % test_accuracy)\n", "intent": "Test model on the test dataset of dog images.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Model test accuracy is %.1f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now test the trained model on the dataset of dog images.\n"}
{"snippet": "xception_predictions = [np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat == y)\nprint(\"Accuracy on training data = %f\" % acc)\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "y_ts_pred = regr.predict(X_ts)\nRSS_rel_ts = np.mean((y_ts_pred-y_ts)**2)/(np.std(y_ts)**2)\nprint(\"Normalized test RSS = {0:f}\".format(RSS_rel_ts))\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "print(\"Logistic Regression:\", accuracy_score(y_test, log_reg.predict(X_test_scaled)))\nprint(\"Softmax Regression:\", accuracy_score(y_test, smax_clf.predict(X_test_scaled)))\nprint(\"Random Forest:\", accuracy_score(y_test, rf_clf.predict(X_test_scaled)))\nprint(\"Neural Network (Fully Connected):\", network1.evaluate(X_test_scaled, y_test_cat)[1])\nprint(\"CNN:\", network.evaluate(X_test_scaled.reshape(-1, 28, 28, 1), y_test_cat))\n", "intent": "Let's test these models on the test data now!\n"}
{"snippet": "for expi in range(n_experiments):\n    if (expi + 1) % 20 == 0:\n        print('working on Experiment {0}'.format(expi + 1))\n    for i, alpha in enumerate(alpha_list):\n        for j, n_step in enumerate(n_step_list):\n            v_func = init_v_func(NUM_ALL_STATES)\n            for _ in range(n_episodes):\n                v_func = run_a_episode(v_func, alpha=alpha, n_step=n_step)\n            mse[expi, i, j] = mean_squared_error(V_TRUE, v_func[1:-1])\n", "intent": "This cell takes the most time to run\n"}
{"snippet": "ind = 80\nMin, Max = -2, 2\nnum_steps = 100\ngen_imgs = []\nfor val in np.linspace(Min, Max, num_steps):\n    noise[0, ind] = val\n    gen_img = model.predict(noise)\n    gen_img = gen_img*127.5+127.5\n    gen_imgs.append(gen_img[0, :, :, 0].astype(np.uint8))\n", "intent": "Change one of the variables of noise vector and see what happens :)  \n**Remember**: noise vector generated with 0 mean and $\\sigma$=1\n"}
{"snippet": "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\ny_test_ohe = one_hot_encode(y_test, 10)\ny_hat = np.argmax(loaded_model.predict(X_test), axis=1)\nprint('labels:  \\t', y_test[:30])\nprint('predictions:\\t', y_hat[:30])\n", "intent": "Feed test dataset to the persisted model to get predictions.\n"}
{"snippet": "pred_y = classifier.predict(test_X)\nprint(\"Fraction Correct\")\nprint(np.sum(pred_y == test_y) / float(len(test_y)))\n", "intent": "Now get the predictions for the iris types and print out the fraction that's correct\n"}
{"snippet": "print(np.mean((bos.PRICE-lm.predict(X))**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "prediction = nb_fit.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictiontfid = predict_tfid.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nclassification_report(y_test, predictions)\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "score, acc = model.evaluate(test_X, test_Y)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n", "intent": "We evaluate the model on test dataset and obtain the score and accuracy.  \nScore is the evaluation of the loss function for a given input.\n"}
{"snippet": "from time import time \nfrom sklearn.metrics import f1_score\ndef train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return y_pred, f1_score(target, y_pred, pos_label='H'), sum(target == y_pred) / float(len(y_pred))\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Import some kind of training functions (F1)\n"}
{"snippet": "predicted_toxic = toxic_classifier.predict(test_tfidf)  \npredicted_severe_toxic = severe_toxic_classifier.predict(test_tfidf)   \npredicted_obscene = obscene_classifier.predict(test_tfidf)   \npredicted_threat = threat_classifier.predict(test_tfidf)   \npredicted_insult = insult_classifier.predict(test_tfidf)   \npredicted_identity_hate = identity_hate_classifier.predict(test_tfidf)   \n", "intent": "Classify test data set\n"}
{"snippet": "if load_images == True: \n    print('Test Accuracy of SVC = ', round(scv_classyfier.score(X_test, y_test), 4))\n    t=time.time()\n    n_predict = 10\n    print(X_test[0])\n    print(len(X_test[0]))\n    print('My SVC predicts: ', scv_classyfier.predict(X_test[0:n_predict]))\n    print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n    t2 = time.time()\n    print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n", "intent": "Accuracy on test set yields in a promising classification accuracy: \n"}
{"snippet": "predict = logmod.predict(x_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\npredict = logModel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(classification_report(y_test, predict))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*float(np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1)))/float(len(dog_breed_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/float(len(VGG16_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/float(len(VGG19_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "for i, price in enumerate(clf.predict(students)):\n    print \"Is the Student {} predicted to pass the year: {}\".format(i+1, price)\n", "intent": "Prediction for them:\n"}
{"snippet": "log_fit.predict(x_test)\n", "intent": "The first results are not bad as well. Now we can make predictions with our model:\n"}
{"snippet": "prob = clf_forest.predict_proba(X_test)[:,1]\nprob[prob > 0.06] = 1\nprob[prob <= 0.06] = 0\n", "intent": "Rebuilding the random forest model with some correction\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytest, ypred))\n", "intent": "Classification report\n"}
{"snippet": "print(classification_report(ytest, ypred))\n", "intent": "Classification report\n"}
{"snippet": "logisticRegr.predict(test_img[0].reshape(1,-1))\n", "intent": "Uses the information the model learned during the model training process\n"}
{"snippet": "predictions = cross_validation.cross_val_predict(logistic_reg, x_train_deviation, y_train_reshaped, cv=50)\nscores = cross_validation.cross_val_score(logistic_reg, x_train_deviation, y_train_reshaped, cv=50)\nmetrics.classification_report(y_train_reshaped, predictions)\n", "intent": "Accuracy score of 68.7 % first try using a basic logistic regression model. Let's cross validate\n"}
{"snippet": "def least_squares_error(x, y, w, c):\n    return squared_error\nprint('Squared error for w = 1.5, c = 0.5 is ', \n      least_squares_error(x_train, y_train, w=1.5, c=0.5))\n", "intent": "Write a function that calculates the least squared error on the training data for a particular value of the parameters $w$ and $c$.\n"}
{"snippet": "VGG19_pred_array = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\nInceptionV3_pred_array = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy_VGG19 = 100*np.sum(np.array(VGG19_pred_array)==np.argmax(test_targets, axis=1))/len(VGG19_pred_array)\ntest_accuracy_InceptionV3 = 100*np.sum(np.array(InceptionV3_pred_array)==np.argmax(test_targets, axis=1))/len(InceptionV3_pred_array)\nprint('Test accuracy (VGG-19): %.4f%%' % test_accuracy_VGG19)\nprint('Test accuracy (InceptionV3): %.4f%%' % test_accuracy_InceptionV3)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = model.evaluate(x_original, y, verbose=0)\nprint('model test loss:    ', scores[0]*100)\nprint('model test accuracy:', scores[1]*100)\nmodel_accuracy = scores[1]*100\n", "intent": "Evaluate the model and calculated test accuracy\n"}
{"snippet": "train['PredictedAge'] = linear_mod.predict(train[LIN_MOD_FEATURES])\n", "intent": "Calculate the predicted age for __all__ the rows:\n"}
{"snippet": "sample1 = spam_train_df.iloc[9]\nsample2 = spam_train_df.iloc[4]\nspam_classifier.predict(sample1, spam_classifier.root)\nspam_classifier.predict(sample2, spam_classifier.root)\n", "intent": "<h2>Spam Prediction Splits</h2>\n"}
{"snippet": "print(accuracy_score(ytestlr, logreg_cv.predict(Xtestlr)))\n", "intent": "It gave me approximately the same accuracy score as above using my own loop.\n"}
{"snippet": "x_test = np.array(['paper'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "print 'Essay id_2\\'s Probablity of being CHN: ', clf.predict_proba(X_cn_dtm)[0][1]\nprint 'Essay id_16\\'s Probablity of being ENS: ', clf.predict_proba(X_en_dtm)[0][0]\n", "intent": "**To prove the above conclusion, use the same Logistic Regression model to predict the probablity of the sample essays' classes**\n"}
{"snippet": "print(ann.predict(X_test))\n", "intent": "Let's predict the output of the test set to see how it looks like.\n"}
{"snippet": "model.evaluate(X_test, y_test, batch_size=32)\n", "intent": "Finally, we test the model on the test data set.\n"}
{"snippet": "print('Confusion Matrix for simple, gradient boosted, and random forest tree classifiers:')\nprint('Simple Tree:\\n',confusion_matrix(target_test,simpleTree.predict(data_test)),'\\n')\nprint('Gradient Boosted:\\n',confusion_matrix(target_test,gbmTree.predict(data_test)),'\\n')\nprint('Random Forest:\\n',confusion_matrix(target_test,rfTree.predict(data_test)))\n", "intent": "**Confusion Matrix:**\n"}
{"snippet": "print (clf.predict(X_test[0:1]))\nprint (clf.predict(X_test[0:10]))\nprint (clf.score(X_test,Y_test))\n", "intent": "The training has been done successfully!\n"}
{"snippet": "import numpy as np\nsms_review_array= np.array([\"Chk in ur belovd ms dict\"])\nsms_review_vector= vectorizer.transform(sms_review_array)\nprint clf.predict(sms_review_vector)\n", "intent": "The output should be 0 which indicates that the SMS is spam or 1 which indicates that the SMS is not spam\n"}
{"snippet": "y_pred = predict(X_test, B_hat_digits)\naccuracy = np.mean(y_test == y_pred)\nprint 'Ordinary Least Squares on raw pixels final test set accuracy: %f' % (accuracy, )\nfrom sklearn import datasets, svm, metrics\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % ('OLS linear regressor', metrics.classification_report(y_test, y_pred)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred))\n", "intent": "Let's report the final test set accuracy. We'll also output a precision-recall summary and a confusion matrix.\n"}
{"snippet": "y_test_pred = best_softmax.predict(X_test)\ntest_accuracy = np.mean(y_test == y_test_pred)\nprint 'Softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )\nprint\nfrom sklearn import datasets, svm, metrics\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (best_softmax, metrics.classification_report(y_test, y_test_pred)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_test_pred))\n", "intent": "Let's report the final test set accuracy. We'll also output a precision-recall summary and a confusion matrix.\n"}
{"snippet": "y_test_pred = best_net.predict(X_test)\ntest_accuracy = np.mean(y_test == y_test_pred)\nprint '2-layer nn on raw pixels final test set accuracy: %f' % (test_accuracy, )\nprint\nfrom sklearn import metrics\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (best_net, metrics.classification_report(y_test, y_test_pred)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_test_pred))\n", "intent": "Let's report the final test set accuracy. We'll also output a precision-recall summary and a confusion matrix.\n"}
{"snippet": "y_test_pred = best_linear_svm.predict(X_test)\ntest_accuracy = np.mean(y_test == y_test_pred)\nprint 'Linear SVM on raw pixels final test set accuracy: %f' % (test_accuracy, )\nprint\nfrom sklearn import metrics\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (best_linear_svm, metrics.classification_report(y_test, y_test_pred)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_test_pred))\n", "intent": "Let's report the final test set accuracy. We'll also output a precision-recall summary and a confusion matrix.\n"}
{"snippet": "num_test = len(y_test)\ny_test_pred = np.zeros((num_test, num_classes))\nfor digit, model in models.iteritems():\n    y_test_pred[:, int(digit)] = model['model'].predict(X_test).T\nprint 'accuracy on held-out test set %f' % (np.mean(np.argmax(y_test_pred, axis=1) == y_test))\n", "intent": "Now it's time to classify the test set and check our accuracy.\n"}
{"snippet": "from sklearn import metrics\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % ('non-linear svm', metrics.classification_report(y_test, np.argmax(y_test_pred, axis=1))))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, np.argmax(y_test_pred, axis=1)))\n", "intent": "Let's output a precision-recall summary and a confusion matrix.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_hat = model_regress2.predict(X)\nplot(X,Y,'b.')\nplot(X,Y_true,'.r',linewidth=2)\nplot(X,Y_hat,'.g',linewidth=2)\nxlabel('X')\nylabel('Y')\n", "intent": "We then compare the fitted (green) and true (red) models.\n"}
{"snippet": "predictions = model.predict(test)\n", "intent": "* Predicting works *exactly* the same here as it did in linear models. \n"}
{"snippet": "predictions = model.predict(test)\npredictions[:5]\n", "intent": "* With the model in hand, we can now make some predictions. \n* Remember, we're always predicting on the test data frame.\n"}
{"snippet": "predictions = model.predict(test)\n", "intent": "* With the model in hand, we can now make some predictions. \n* Remember, we're always predicting on the test data frame.\n"}
{"snippet": "y_predicted = clf.predict(X_test)\n", "intent": "* **Note** the use of `tranform` here (**not** `fit_transform`). What would have happened if we were to call `fit_transform`?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_true = twenty_test.target\nprint(accuracy_score(y_true, y_predicted))\n", "intent": "Evaluating accuracy is easy:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_true, y_predicted,\n     target_names=twenty_test.target_names))\n", "intent": "The document collection D:\n<img src=\"pics/accuracy2.png\">\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, l in enumerate(style_layers):\n        G = gram_matrix(feats[l])\n        A = style_targets[i]\n        loss += style_weights[i] * ((G - A) ** 2).sum()\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "mod.predict_proba([\"hello are you ok?\"])\n", "intent": "Notice the type is an `sklearn` model, just as we created. \n"}
{"snippet": "print(\"Accuracy score: \", accuracy_score(y_validation, prediction))    \n", "intent": "Check a validation accuracy score.\n"}
{"snippet": "def show_metrics(y_test, y_pred):\n    print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred)))\n    print(\"Confusion matrix\")\n    print(confusion_matrix(y_test, y_pred))\n    fpr, tpr, thresholds = roc_curve(y_validation, prediction)\n    print(\"AUC: \" + str(auc(fpr, tpr)))\nshow_metrics(y_validation, prediction)  \n", "intent": "Lets check different metrics i.e. ***Accuracy***, ***Confusion matrix***, ***AUC***.\n"}
{"snippet": "clf.predict([[2009]])\n", "intent": "This means that North Korea is adding almost 17 thousand soldiers to its army every year\n"}
{"snippet": "predictions = []\npredicted_labels = []\nfor i, (img, y) in enumerate(validation_generator):\n    if i >= 800:\n        break\n    pp = loaded_model.predict(img)\n    label = int(pp[0] >= 0.5)\n    predictions.append(pp[0][0])\n    predicted_labels.append(label)  \n", "intent": "Here I create a dataframe of predictions for each image as well as predicted label and actual label.\n"}
{"snippet": "def get_geometric_mean_price():\n    return scipy.stats.mstats.gmean(dataset['cena'])\nps = [get_geometric_mean_price()] * len(ys)\nprint(msle(ys, ps))\nprint(l2_loss(ys, ps))\n", "intent": "Recall that outputing the mean minimzes $MSE$. However, we're now dealing with $MSLE$.\nThink of a constant that should result in the lowest $MSLE$.\n"}
{"snippet": "for price, prediction in zip(ys, predict(weights, xs)):\n    print(price, prediction)\n", "intent": "Let's check the predictions\n"}
{"snippet": "l2_loss(ys, ts)\n", "intent": "Please take a while to (roughly) guess the output before executing the cell below.\n"}
{"snippet": "y_pred = log_model.predict(X_test)\n", "intent": "And finally, we use this classifier to label the evaluation set we created earlier:\n"}
{"snippet": "sklearn.metrics.r2_score(y_test, predictions)\n", "intent": "Generating an R2 score\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy score on training data decrease from 0.92 to 0.8 which indicates that the model is more robust in predicting unseen data.\n"}
{"snippet": "expected = targets\npredicted = model.predict(inputs)\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "How does the tree perform on the training data?\n"}
{"snippet": "model_resnet50_prediction =[np.argmax(model_resnet50.predict(np.expand_dims(feature,axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(model_resnet50_prediction)==np.argmax(test_targets, axis=1))/len(model_resnet50_prediction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target,complete_pred)\navg_ar_score = adjusted_rand_score(iris.target,avg_pred)\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "result = model.evaluate(input_fn=test_input_fn) \n", "intent": "Train : 55,000    \nTest  : 10,000\n"}
{"snippet": "predictions = model.predict(input_fn=test_input_fn)\n", "intent": "<span style=\"color:\nPrediction\n</span>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_predict = model.predict(X_train)\nprint 'Training accuracy = ', accuracy_score(y_train, y_predict)\nclass_labels = [0,1]\nprint 'Class labels=', class_labels\nconfusion_matrix = confusion_matrix(y_train, y_predict, class_labels)\nprint 'Confusion matrix [known in lines, predicted in columns]=\\n', confusion_matrix\nbalance_classification_rate = 1/2 * ( float(confusion_matrix[1,1])/float(confusion_matrix[0,1] + confusion_matrix[1,1]) ) + ( float(confusion_matrix[0,0])/float(confusion_matrix[0,0] + confusion_matrix[1,0]))                              \nprint 'Balance Classification Rate = ', balance_classification_rate\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_predict = clf.predict(X_train)\nprint 'Training accuracy = ', accuracy_score(y_train, y_predict)\nclass_labels = clf.get_classes()     \nprint 'Class labels=', class_labels\nconfusion_matrix = confusion_matrix(y_train, y_predict, class_labels)\nprint 'Confusion matrix [known in lines, predicted in columns]=\\n', confusion_matrix\nbalance_classification_rate = 1/2 * ( float(confusion_matrix[1,1])/float(confusion_matrix[0,1] + confusion_matrix[1,1]) ) + ( float(confusion_matrix[0,0])/float(confusion_matrix[0,0] + confusion_matrix[1,0]))                              \nprint 'Balance Classification Rate = ', balance_classification_rate\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n", "intent": "**Classification accuracy:**\n"}
{"snippet": "predictions = naive_bayes.predict(testing_data)\n", "intent": "Now, we are ready to make predictions.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\nresnet50_model_test_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % resnet50_model_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def dcg_score(dup_ranks, k):\n    dcg = 0\n    for dr in dup_ranks:\n      temp = 1/(np.log2(1+dr))\n      temp *= dr<=k\n      dcg += temp\n    return dcg/len(dup_ranks)\n", "intent": "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n"}
{"snippet": "classifier =  KerasClassifier(build_fn = buildClassifier, batch_size= 500, epochs = 100)\nacuracies = cross_val_score(estimator = classifier,X = XTrain, y = yTrain, cv = 10)\n", "intent": "Do not add n_jobs = -1 if you use GPU, it will use only one CUDA core!\n"}
{"snippet": "classifier =  KerasClassifier(build_fn = buildClassifier, batch_size= 500, epochs = 200, verbose=1)\nacuracies = cross_val_score(estimator = classifier,X = XTrain, y = yTrain, cv = 10)\n", "intent": "Do not add n_jobs = -1 if you use GPU, it will use only one CUDA core!\n"}
{"snippet": "y_pred = my_model_q1.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\ndisplay(pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Evaluate the trained classifier\n"}
{"snippet": "my_model_q2 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_rf, training='label')\ncross_validation.cross_val_score(my_model_q2, X_test, y_test, cv=10, scoring='accuracy')\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "print(sklearn.metrics.classification_report(y_test, y_predicted))\n", "intent": "<p> The false positive and negative rates are roughly equal. </p>\n<br></br>\n<br></br>\nLet's compute general metrics.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = model.evaluate(X, y, verbose=0)\nprint(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "After we fit the model we can evaluate and summarize the performance on the entire training dataset.\n"}
{"snippet": "y_pred = model_vgg16.predict(image)\ny_pred.shape\n", "intent": "We can call the predict () function in the model to predict the probability that the image will belong to 1000 known object types\n"}
{"snippet": "X_new = [[3, 5, 4, 2]]\nknn.predict(X_new)\n", "intent": "**Step 4:** Make a prediction (predict)\n"}
{"snippet": "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\ny_pred_prob\n", "intent": "Binary Classification predict = (0, 1)\n"}
{"snippet": "metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "The larger the better\n"}
{"snippet": "print np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos.PRICE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "directory = os.fsencode(\"images\")\nfor file in tqdm(os.listdir(directory)):\n    filename = os.fsdecode(file)\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\"jpeg\"): \n        out_scores, out_boxes, out_classes = predict(sess, filename)\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "y_mult_pred = multiple_linreg.predict(X_mult)\nmetrics.r2_score(y_mult, y_mult_pred)\n", "intent": "Does this model have a better r^2 value?\n"}
{"snippet": "print \"MAE for fake data:\",metrics.mean_absolute_error(fake_y_true, fake_y_pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors/residuals:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print \"MSE for fake data:\",metrics.mean_squared_error(fake_y_true,fake_y_pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print \"RMSE for fake data:\",np.sqrt(metrics.mean_squared_error(fake_y_true, fake_y_pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "y_pred = dt.predict(X_test)\nnp.unique(y_pred)\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each test sample observation?\n"}
{"snippet": "print accuracy_score(y_occ_test,occupancy_tree.predict(X_occ_test))\n1- (float(y_occ_train.sum())/y_occ_train.shape[0])\n", "intent": "  * Evaluate the model using `accuracy_score` on the testing data.\n  * Is the accuracy score above chance? What is chance accuracy here?\n"}
{"snippet": "print \"Prediction: \",knn.predict(test)\n", "intent": "**what id \"should\" the classifier predict?**\nHopefully, you said id 2.\n"}
{"snippet": "print \"Classification Report:\\n\", metrics.classification_report(y_test,y_test_pred)\n", "intent": "Or we can compute the full classification report, which will give us precision/recall per-feature:\n"}
{"snippet": "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"neg_mean_squared_error\"))\nrmse_cross_val_scores = map(np.sqrt, cross_val_scores)\nprint \"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores)\nprint \"Std 10-fold rmse: \", np.std(rmse_cross_val_scores)\n", "intent": "And now let's run the whole pipe through the `cross_val_score` object:\n"}
{"snippet": "predicted_probs=gl_result.predict(smrkt_weekly_df[['Lag1','Lag2','Lag3','Lag4','Lag5','Volume']])\nboolean_predictions=predicted_probs>0.5\nboolean_truevalues = smrkt_weekly_df['Direction']=='Up'\nfrom sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(boolean_truevalues,boolean_predictions).ravel()\nprint(tn, fp, fn, tp)\n", "intent": "(b) Lag 2 is the only one with some correlation\n"}
{"snippet": "encoded_imgsHstak = encoderHstack.predict(np.array([[X_train_new[1,0,:,:]]]))\n", "intent": "How example[1] is transformed by the encoders :\n"}
{"snippet": "RSS=print(np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "MSE=print(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "x_test = np.array(['That was a great goal'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "y_hat = lr.predict(X_test)\n", "intent": "Using the classifier to make predictions is easy.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    print(feats)\n    total_loss = 0\n    for idx in range(len(style_layers)):\n        layer_gram = gram_matrix(feats[style_layers[idx]])\n        layer_loss = style_weights[idx] * tf.reduce_sum((layer_gram - style_targets[idx])**2)\n        total_loss += layer_loss\n    return total_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_data = np.arange(5,20,3)\ntest_data = test_data.reshape(len(test_data),1)\nregmod.predict(test_data)\n", "intent": "**prediction**  \n- test data can be the form 2-D array, 2-D list, Data Frame  \n- return a 2-D array\n"}
{"snippet": "fitted = regmod2.predict(X)\nresiduals = y - fitted\ntype(residuals), residuals.shape, fitted.shape\n", "intent": "**residuals and standard residuals**\n"}
{"snippet": "y_hat = model_ridge.predict(t_X)\n", "intent": "<font color = 'Red'>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(test_y, clf.predict(test_X)))\n", "intent": "After the model is trained it has to be evaluated.\n"}
{"snippet": "idx = [0]\npredictions = classifier.predict(x=np.array(test_data[idx]))\nfor i, p in enumerate(predictions):\n    print(\"Predicted %d, Label: %d\" % (p, test_labels[idx[i]]))\n    display(idx[i])\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_predictions, \n                             housing_labels)\nprint(\"Mean of Square Errors on training set: \", lin_mse)\nlin_rmse = np.sqrt(lin_mse)\nprint(\"Root of Mean of Squared Error on training set: \", lin_rmse)\n", "intent": "__Let's measure this regression model's RMSE on the whole training set using SkLearn's `mean_squared_error` function.__  \n"}
{"snippet": "scores = cross_val_score(forest_reg, \n                         housing_prepared, \n                         housing_labels,\n                        scoring = \"neg_mean_squared_error\", \n                         cv = 10,\n                        )\nforest_rmse_scores = np.sqrt(-scores)\ndisplay_scores(forest_rmse_scores)\n", "intent": "__Conducting Cross Validation with `RandomForestRegressor` to see if it underfits, overfits, or is just right__ \n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf,\n                                X_train_scaled,\n                                y_train,\n                                cv = 3,\n                                )\nconf_mx = confusion_matrix(y_train, \n                           y_train_pred \n                          )\n", "intent": "- i) Look at the confusion matrix. \n"}
{"snippet": "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\ndecision_boundary_point = X_new[y_proba[:, 1] >= 0.5][0]\ndecision_boundary_point\n", "intent": "__Looking at the model's expected probabilities for flowers with petal widths varying from 0 to 3 cm__:\n"}
{"snippet": "y_pred = bag_clf.predict(X_test)\naccuracy_score(y_pred, y_test)\n", "intent": "- According to this oob evaluation, this `BagginClassifier` is likely to achieve about 90.1% accuracy on the test set.  Let's verify this. \n"}
{"snippet": "train = fa_skeleton[fa_skeleton.X['Holdout']=='train']\ntrain.Y = train.X['Selfesteem']\ntest = fa_skeleton[fa_skeleton.X['Holdout']=='test']\ntest.Y = test.X['Selfesteem']\nstats_skeleton = train.predict(algorithm='svr',plot=True, **{'kernel':'linear'})\nfrom nltools.plotting import plotBrain\nplotBrain(stats_skeleton['weight_map'])\n", "intent": "Here, we will do the same proceedure on the skeletonized data. \n"}
{"snippet": "all_smooth = fa_smooth\nall_smooth.Y = fa_smooth.X['Selfesteem']\nsubject_id = all_smooth.X['SubjectID']\nstats_skeleton_all = all_smooth.predict(algorithm='svr', cv_dict={'type': 'loso','subject_id':subject_id}, plot=True,  **{'kernel':'linear'})\n", "intent": "The data below are modeled using the whole data set with leave-one-out cross-validation. \n"}
{"snippet": "mse = ((bos.PRICE - lm.predict(X))** 2).mean(axis=0)\nmse\n", "intent": "***\nThis is simply the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print(classification_report(y_test,predict))\n", "intent": "To get all the above metrics at one go, use the following function:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(scratch.predict(np.expand_dims(tensor, axis = 0) ) ) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions) == np.argmax(test_targets, axis = 1))/len(dog_breed_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16.predict(np.expand_dims(feature, axis = 0) ) ) for feature in test_VGG16]\nVGG16_accuracy = 100*np.sum(np.array(VGG16_predictions) == np.argmax(test_targets, axis = 1))/len(VGG16_predictions)\nprint('Test accuracy: %.2f%%' % VGG16_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception.predict(np.expand_dims(feature, axis = 0) ) ) for feature in test_Xception]\nXception_accuracy = 100*np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis = 1))/len(Xception_predictions)\nprint('Test accuracy: %.2f%%' % Xception_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        deviation = tf.reduce_sum(tf.square(style_targets[i] - gram))\n        style_loss += style_weights[i] * deviation\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.5f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "inputs = np.float64(patches)\ninputs = cam_net.data_preprocessing(inputs)\npredictions = bc_model.predict(inputs[:50])\n", "intent": "We predict the cancer probability for each one of the concept-patches\n"}
{"snippet": "optimal_theta = train(theta, X, Y, )\np = predict(np.array(optimal_theta), X)\nt = (p == Y)\nprint (\"test accuracy: \",np.mean(t)*100)\ny_test = predict(np.array(optimal_theta), crossX) \ncrosst = (y_test == crossY)\nprint (\"cross accuracy: \", np.mean(crosst)*100)\n", "intent": "Lets prepare the test data matricies and begin predicting!\n"}
{"snippet": "VGG19_model_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_model_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_predict = xgb_model.predict(dm_test)\nprint(y_predict)\n", "intent": "Make predictions on test data and evaluate the model.\n"}
{"snippet": "accuracy = accuracy_score(target_test, y_predict)\nprint(\"Accuracy: \" +  str(accuracy))\n", "intent": "Evaluate the performance of the model using the predicted data.\n"}
{"snippet": "def get_scores(model, X, y, X_train, X_test, y_train, y_test):\n    cv_score = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')\n    mse_cv = -np.mean(cv_score)\n    r_2_out = model.score(X_test, y_test)\n    y_pred_out = model.predict(X_test)\n    mse_out = metrics.mean_squared_error(y_test,y_pred_out)\n    r_2_in = model.score(X_train,y_train)\n    y_pred_in = model.predict(X_train)\n    mse_in = metrics.mean_squared_error(y_train,y_pred_in)\n    return cv_score, mse_cv, r_2_out, y_pred_out, mse_out, r_2_in, y_pred_in, mse_in\n", "intent": "**Make Functions to Retrieve & Print Metrics**\n"}
{"snippet": "y_pred_prob = model.predict(X_test_array)\n", "intent": "**Predicted Probabilites for Each Class**\n"}
{"snippet": "pred = rnc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = regressor.predict(X_test)  \n", "intent": "**Predict the test score values and compare with actual values**\n"}
{"snippet": "from sklearn import metrics  \nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))  \n", "intent": "**Evaluation Matrics**\n"}
{"snippet": "def evaluate(X_data, y_data, BATCH_SIZE):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy = sess.run(accuracy_operation, feed_dict={x_tf: batch_x, y_tf: batch_y, dropout_keep_prob:1.0})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "1. Runs the accuracy_operation on the validation set and reports total accuracy.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n", "intent": "**Step -7 : Evaluating**\n"}
{"snippet": "def accuracy(model):\n    kf = KFold(n_splits=5)\n    crv = cross_val_score(model, X_train, y_train, cv=kf,scoring='accuracy')\n    return crv.mean()*100\n", "intent": "Define a function to train and predict the accuracy of the model being used.\n"}
{"snippet": "from sklearn import metrics\npreds = logreg.predict(X2)\nprint(metrics.confusion_matrix(y, preds))\n", "intent": "Trying out a confusion matrix:\n"}
{"snippet": "def print_scores(model, X_features, Y_target):\n    scores = cross_val_score(model, X_features, Y_target, cv=5)\n    print('accuracies =', scores)\n    print('mean accuracy =', scores.mean())    \n", "intent": "(e) I use cross validation to see what is the accuracy of the model with basic settings\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_features]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error \nmean_absolute_error([np.mean(df_reviews_train.stars)] * len(df_reviews_val), df_reviews_val.stars)\n", "intent": "- Compare with benchmark\n"}
{"snippet": "def run_mesh(xx,yy,clf):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    return Z\n", "intent": "Run inference on mesh\n"}
{"snippet": "def run_mesh(xx,yy,clf):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    return Z\n", "intent": "run inference on mesh\n"}
{"snippet": "print confusion_matrix(y_test, best_rfc.predict(X_test))\n", "intent": "<p style=\"color:blue\">\nLet us look at confusion matrix for model\n</p>\n"}
{"snippet": "accuracy = accuracy_score(y_test, best_rfc.predict(X_test))\nprint \"Accuracy: \", accuracy\n", "intent": "<p style=\"color:blue\">\nCompute model accuracy score \n</p>\n"}
{"snippet": "print classification_report(y_test, best_rfc.predict(X_test))\n", "intent": "<p style=\"color:blue\">\nLet us look at classification report for model. This lists the precision, recall, f1-score and support\n</p>\n"}
{"snippet": "roc = roc_auc_score(y_test, best_rfc.predict_proba(X_test)[:,1])\nprint \"AUC Score: \", roc\n", "intent": "<p style=\"color:blue\">\nCompute AUC score for model\n</p>\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n", "intent": "<p style=\"color:blue\">\nRun K-Fold cross validation with 10 folds and obtain the cross validation score\n</p>\n"}
{"snippet": "y_pred_LR = lr.predict(X_test)\n", "intent": "<p style=\"color:blue\">\nUse Linear Regression model to predict values for test data\n</p>\n"}
{"snippet": "r2_LR = r2_score(y_test, y_pred_LR)\nprint(\"r^2 on test data for LR: %f\" %r2_LR)\n", "intent": "<p style=\"color:blue\">\nMeasure performance using R Squared (R^2)\n</p>\n"}
{"snippet": "mse_LR = mean_squared_error(y_test, y_pred_LR)\nprint(\"MSE on test data for LR: %f\" %mse_LR)\n", "intent": "<p style=\"color:blue\">\nMeasure performance using Mean Squared Error (MSE)\n</p>\n"}
{"snippet": "r2_LASSO = r2_score(y_test, y_pred_LASSO)\nprint(\"r^2 on test data for LASSO: %f\" %r2_LASSO)\n", "intent": "<p style=\"color:brown\">\nMeasure performance using R Squared (R^2)\n</p>\n"}
{"snippet": "mse_LASSO = mean_squared_error(y_test, y_pred_LASSO)\nprint(\"MSE on test data for LASSO: %f\" %mse_LASSO)\n", "intent": "<p style=\"color:brown\">\nMeasure performance using Mean Square Error (MSE)\n</p>\n"}
{"snippet": "print \"Logistic accuracy is %2.2f\" % accuracy_score(y_test,model.predict(X_test))\n", "intent": "<p style=\"color:blue\">\nCalculate Accuracy\n</p>\n"}
{"snippet": "logit_roc_auc = roc_auc_score(y_test, model.predict(X_test))\nprint \"Logistic AUC = %2.2f\" % logit_roc_auc\nprint classification_report(y_test, model.predict(X_test) )\n", "intent": "<p style=\"color:blue\">\nCalculate Precision, Recall and AUC\n</p>\n"}
{"snippet": "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n", "intent": "<p style=\"color:blue\">\nPlot ROC curve\n</p>\n"}
{"snippet": "roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n", "intent": "<p style=\"color:blue\">\nTest model accuracy with roc_auc_score\n</p>\n"}
{"snippet": "spam_array=np.array(['You have won a free mobile phone'])\nspam_vector = vectorizer.transform(spam_array)\nprint clf.predict(spam_vector)\n", "intent": "<p style=\"color:blue\">\nLet us verify using a sample message\n</p>\n"}
{"snippet": "knn1.predict(5000)\n", "intent": "Suppose that we want to predict the credit card balance of a customer that has a limit of 500 thousand dollars. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions1 = knn1.predict(test[['Limit']])\nmse1 = mean_squared_error(test['Balance'], predictions1)\nrmse1 = np.sqrt(mse1)\nprint(rmse1)\n", "intent": "We now assess the performance of our two models in the test data. \n"}
{"snippet": "from sklearn.metrics import r2_score\nrsquared1 = r2_score(test['Balance'], predictions1)\nrsquared2 = r2_score(test['Balance'], predictions2)\nprint(rsquared1)\nprint(rsquared2)\n", "intent": "We can then follow a similar procedure if we want to compute alternative measures. \n"}
{"snippet": "from sklearn.metrics import make_scorer\nscorer = make_scorer(mean_squared_error, greater_is_better=False)\nscores = cross_val_score(knn, train[['Limit']], train['Balance'], cv=kf, scoring = scorer)\nprint(scores)\n", "intent": "Another option is to create a scorer using [<TT>make_scorer</TT>](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n"}
{"snippet": "pred = test['tokens'].apply(lambda tokens: (feature in tokens))\nerror  = 1 - accuracy_score(pred, test['positive'])\nprint(error.round(3))\n", "intent": "The appearance of \"thank\" would therefore lead to us to classify a tweet as positive. Let's evaluate the performance of the model for the test data. \n"}
{"snippet": "losses=[]\nfor feature in features.index:\n    losses.append(training_error(feature))\nranked = pd.Series(losses, index=features.index)\nranked = ranked.sort_values()\nranked.head(20)\n", "intent": "The ranking of the features is as follows.\n"}
{"snippet": "from sklearn.metrics import recall_score, precision_score,  roc_auc_score\nprint(recall_score(y_test, y_pred).round(3))\nprint(precision_score(y_test, y_pred).round(3))\nauc = roc_auc_score(y_test, prob[:,1]).round(3)\nprint(auc)\n", "intent": "We can compute other individual metrics as follows. \n"}
{"snippet": "bst2 = xgb.train(param, train_scale_dm, num_round)\npreds_bst2 = bst2.predict(test_scale_dm)\n", "intent": "The above result does not do scaling. The result is not outstanding.\n"}
{"snippet": "prob_svm=svm_rbf.predict_proba(X_test_selected)[:,1]\nprob_rank_svm=np.flip(np.argsort(prob_svm),0)\nprint(np.sum(y_test2[prob_rank_svm[:10]])/10)\nprint(np.sum(y_test2[prob_rank_svm[:20]])/20)\n", "intent": "We tried setting class weight = balanced. It does not help on the misclassification rate.\n"}
{"snippet": "y_pred = clf.best_estimator_.predict(X_test)\nprint \"F1 score: {}\".format(f1_score(y_test, y_pred, pos_label='yes'))\nprint \"Best params: {}\".format(clf.best_params_)\n", "intent": "- What is the model's final F<sub>1</sub> score?\n"}
{"snippet": "predictions = clf.predict(X_test)\n", "intent": "Make our predictions!\n"}
{"snippet": "two_components_predictions = two_components_gmm.predict(data)\nbest_predictions= best_gmm.predict(data)\n", "intent": "<h5>Dataset Labelling</h5>\n"}
{"snippet": "df_test, _ = features(test_orders)\nprint('light GBM predict')\npreds = bst.predict(df_test[f_to_use])\ndf_test['pred'] = preds\nTRESHOLD = 0.22  \n", "intent": "build candidates list for test\n"}
{"snippet": "y_1 = regr_1.predict(urban_cons_test)\ny_2 = regr_2.predict(urban_cons_test)\ny_3 = regr_3.predict(urban_cons_test)\n", "intent": "Predict via test datas.\n"}
{"snippet": "mean_absolute_percentage_error(estimator.predict(X),Y)\n", "intent": "**All sample MAPE error and 20% error quartile score**\n"}
{"snippet": "mean_absolute_percentage_error(estimator.predict(Xtest),Ytest)\n", "intent": "**Out of Sample sample MAPE error and 20% error quartile score**\n"}
{"snippet": "mean_absolute_percentage_error(estimator.predict(X_),Y_)\n", "intent": "**All sample MAPE error and 20% error quartile score**\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_true = y_test\ny_pred = LOGREG.predict(X_test)\nprint classification_report(y_true, y_pred, target_names=['Low_Earner', 'High_Earner'])\n", "intent": "Getting a major improvement with the addition of sector. Now the new score to beat is 78%.\n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint(all_predictions)\n", "intent": " We've developed a model that can attempt to predict spam vs ham classification!\n"}
{"snippet": "simple_machine = SimpleMachine()\nsimple_machine.train(X, y)\npredictions = [simple_machine.predict(xi) for xi in X]\nprint_predictions(predictions, y)\n", "intent": "**Euclidean distance:**   \n```\n(0, 3)\n(1, 5)\n```\n$$\\sqrt{(0-1)^2 + (3-5)^2} = \\sqrt{5}$$\n"}
{"snippet": "score = dnn.evaluate(test_dataset, test_labels, show_accuracy=True, verbose=0)\nprint('Test accuracy (on %i samples): %.2f%%' % (len(test_dataset), score[1] * 100))\n", "intent": "**Finally, evaluate the fitted model on test data**\n"}
{"snippet": "metrics.f1_score(y_train, clf.predict(X_train), average='weighted')\n", "intent": "The over-fitting we saw previously can be quantified by computing the\nf1-score on the training data itself:\n"}
{"snippet": "y_predict = model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "result = dtcModel.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preds = dbn.predict(teX)\nprint classification_report(teY, preds)\n", "intent": "Then evaluate the the quality of the predictions for each digit:\n"}
{"snippet": "scores = model_selection.cross_val_score(classifier_pipeline, X, y, cv=3)\n", "intent": "Now apply the classifier pipeline to the feature and target data with KFold/StratifiedKFold cross validation.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_model = model.predict(Xtest) \naccuracy_score(ytest, y_model)\n", "intent": "[_describe how you are going to evaluate the results of your classifier and what it means_]\n"}
{"snippet": "t90 = np.rot90(test)\nt180 = np.rot90(t90)\nt270 = np.rot90(t180)\nprint \"Prediction \" + str(net.predict(t270))\n", "intent": "We expect the perceptron to return [1,0,1] for blue, red, blue\n"}
{"snippet": "    def predict(X):\n        return X.dot(self.theta)\n", "intent": "Here, we implement a predict function\n"}
{"snippet": "y_predicted = model2.predict(X_test)\n", "intent": "1. Create the y_predicted vector\n"}
{"snippet": "from sklearn import metrics \n        print k,nc\n        print(\"accuracy:\", metrics.accuracy_score(y_test, y_pred))\n        print(\"precision:\", metrics.precision_score(y_test, y_pred))\n        print(\"recall:\", metrics.recall_score(y_test, y_pred))\n", "intent": "We can also use the [`metrics` module](http://scikit-learn.org/stable/modules/classes.html\n"}
{"snippet": "result = knn.predict(Xtest)\nprint result\nprint \n", "intent": "Once $X$ is properly formatted, go ahead and do the prediction.  \nCan you output the *name* rather than the index of the iris type?\n"}
{"snippet": "accuracy_score(y_cv,pred_cv)\n", "intent": "Let us calculate how accurate our predictions are by calculating the accuracy.\n"}
{"snippet": "from numpy import array\nXnew = array([test_data[1]])\nynew = model.predict(Xnew)\nprint(\"Data (13 features): %s\\n Known target: %s\\n Predicted target=%s\" % (Xnew, test_targets[1], ynew) )\n", "intent": "Finally to create predictions using the model, we can use the predict() function.\n"}
{"snippet": "score = model.evaluate(x_test, \n                       y_test, \n                       verbose=0 \n                      )\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "We can evaluate our model on the test data.\n"}
{"snippet": "testFunction = tf.estimator.inputs.numpy_input_fn(\n    x={\"X\": test}, y=testLabels, shuffle=False)\neval_results = dnnClf.evaluate(input_fn=testFunction)\nprint(eval_results)\n", "intent": "1. Evaluate the proposed neural network model in the Complete Sina Weibo test dataset\n"}
{"snippet": "IterPred= dnnClf.predict(input_fn=testFunction)\ny_pred = list(IterPred)\nprint(y_pred[0])\n", "intent": "2\\. Show the evaluation for a specific test sample\n"}
{"snippet": "surv_pred = clf_log.predict(df_2)\n", "intent": "Now the test data is ready for prediction:\n"}
{"snippet": "skl.metrics.mean_squared_error(y_test,y_pred)\n", "intent": "Here are some scoring metrics for our first pass at this model:\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Here we find the mean squared error on our test subset, using all features\n"}
{"snippet": "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\nprint(\"\\n\\nLoss: %10.5f\\tAccuracy: %2.2f %%\" % (loss, accuracy*100))\n", "intent": "Here we run evaluation on the model we just trained with test data.\n"}
{"snippet": "prediction = np.around(model.predict(np.expand_dims(inputs_test[57], axis=0))).astype(np.int)[0]\nprint(\"Model prediction in hot vector output: %s\" % prediction)\nprint(\"Model prediction in actual Iris type: %s\" % outputs_vals[prediction.astype(np.bool)][0])\nprint(\"Actual Iris flower type in data set: Vector: %s Type: %s\" % (outputs_test[57].astype(np.int), outputs_vals[prediction.astype(np.bool)][0]))\n", "intent": "And finally we test the model by picking one flower and comparing prediction to actual flower type.\n"}
{"snippet": "kfold = KFold(n_splits=2, random_state=seed)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Result: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n", "intent": "**Cross-validation**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, Y_))\n", "intent": "We have 11389 correct predictions(9364 + 2025) and 2178 incorrect predictions (1400 + 778)\n"}
{"snippet": "start = 0\nx, y = generate_test_sample(val[feature_cols].as_matrix().astype(np.float32), val[feature_cols].as_matrix().astype(np.float32), \n                            2000, start, encoder_steps, encoder_steps)\npredictions = predictor.predict(x, predict_price=False)\n", "intent": "We will first show our autoencoder's encoding quality on the test data:\n"}
{"snippet": "print(r2_score(y_test, y_pred))\n", "intent": "We can use the $r^2$ metric to help us evaluate models during training. In fact sklearn has its own internal $r^2$ method.\n"}
{"snippet": "def make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n", "intent": "Helper functions to help generate our decision boundaries\n"}
{"snippet": "square_error_sum = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nnum_rows = bos.shape[0]\nmean_square_error = square_error_sum / num_rows\nprint 'Mean square error is {0}'.format(mean_square_error)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "test_acc = (best_net.predict(X_test) == y_test).mean()\nprint 'Test accuracy: ', test_acc\n", "intent": "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.\n"}
{"snippet": "flist = [2,8]\nregr = feature_subset_regression(x,y,[2,8])\nprint( \"w = \", regr.coef_)\nprint( \"b = \", regr.intercept_)\nprint( \"Mean squared error: \", mean_squared_error(y, regr.predict(x[:,flist])))\n", "intent": "Try using just features \n"}
{"snippet": "regr = feature_subset_regression(x,y,range(0,10))\nprint( \"w = \", regr.coef_)\nprint( \"b = \", regr.intercept_)\nprint( \"Mean squared error: \", mean_squared_error(y, regr.predict(x)))\n", "intent": "Finally, use all 10 features.\n"}
{"snippet": "predictions=clf.predict(x_test_original)\n", "intent": "**14** . Let's use the trained model to predict the testing data it has not seen during training\n"}
{"snippet": "print(\"Accuracy =\", accuracy_score(y_test_original,predictions))\n", "intent": "**15** . Prediction accuracy and some other relevant metrics\n"}
{"snippet": "y_new = df1.diagnosis\nx_new = df1.drop('diagnosis',axis=1)\npred1 = clf.predict(x_new)\n", "intent": "**16** . Let's use the trained model to predict the entire dataset and see how is its prediction accuracy\n"}
{"snippet": "print(\"Accuracy =\", accuracy_score(y_new,pred1))\n", "intent": "Prediction accuracy for the entire dataset:\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%'% test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "error_all = []\nfor n in xrange(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(train_data[target].to_numpy(), predictions.to_numpy())\n    error_all.append(error)\n    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Using the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "preds = pipeline.predict(X_test)\n", "intent": "** Now using the pipeline to predict from the X_test and create a classification report and confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit, target_names=faces.target_names))\n", "intent": "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label.\n"}
{"snippet": "pd.crosstab(y_test, fitted_alternative_models['rf'].predict(X_test), rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Gradient boosting gives the best score here as well. Let's look at the confusion matrix and the classification reports for both models:\n"}
{"snippet": "predicted_alternative_models = {}\nfor name, model in fitted_alternative_models.items():\n    y_pred = model.predict(X_test)\n    predicted_alternative_models[name] = accuracy_score(y_test, y_pred)\npredicted_alternative_models\n", "intent": "Both models perform much better than Logistic Regression, with Random Forest giving the best result. Let's look at the accuracy scores:\n"}
{"snippet": "pd.crosstab(y_test, fitted_alternative_models['rf'].predict(X_test), rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Confusion matrix for the best model:\n"}
{"snippet": "preds = model.predict(x_train)\n", "intent": "can we use the pretrained network directly ?\n"}
{"snippet": "from sklearn import metrics\ncomp = metrics.completeness_score(labelRegion, ck)\nhomo = metrics.homogeneity_score(labelRegion, ck)\nconfm = metrics.confusion_matrix(labelRegion, ck)\nprint(comp,'\\n',homo,'\\n',confm)\n", "intent": "The fitted cluster model performance can be evaluated using completeness and homogeneity score as follows:\n"}
{"snippet": "Xception_predictions = [np.argmax(transfer_Xception.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "gs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=2)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "<img src=\"http://ml4ef.github.io/images/06_07.png\" width=500>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) \\\n                        for feature in test_Resnet50]\ntest_accuracy = 100 \\\n            * np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "nbxtest=X_test\nif isspmatrix(X_test):\n    nbxtest=X_test.toarray()\nnbpred=nb.predict(nbxtest) \n", "intent": "**Testing the model**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(logreg_clf, X_train, y_train_5, cv=3, scoring=\"f1\")\n", "intent": "How did we do accros the whole training sample?\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\ny_train_pred = cross_val_predict(logreg_clf, X_train, y_train_5, cv=3)\nconf_mat=confusion_matrix(y_train_5, y_train_pred)\nconf_mat\n", "intent": "Lets look at the confussion matrix and calculate some accuracy metrics\n"}
{"snippet": "for thresh in np.arange(0.1, 0.201, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))\n", "intent": "Getting the best threshold based on validation sample.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit,\n                            target_names=faces.target_names))\n", "intent": "We can get a better sense of our estimator's performance using the `classification report`, which lists recovery statistics label by label:\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = mlp.predict(X_test)\nprint(pd.crosstab(Y_test, y_pred))\n", "intent": "This adjusted rand score has increased.  let's take a look at the contingency table for the model.\n"}
{"snippet": "auto_arima_30_aic = stepwise_fit_30.aic()\nprint('AIC: ' , auto_arima_30_aic)\nauto_arima_30_y_pred = stepwise_fit_30.predict(n_periods=90)\nauto_arima_30_mse = mean_squared_error(arima_test_data, auto_arima_30_y_pred)\nprint('MSE ',auto_arima_30_mse) \nauto_arima_30_model = 'SARIMAX(2, 1, 0)'\n", "intent": "The auto arima predicted a model with AR of order 2 with first order difference. Let's check the residuals and predictions.\n"}
{"snippet": "inputBMI = widgets.Text(description=\"BMI:\")\ndef predictLifeExpectancy(sender):\n    print(\"Life Expectancy:\",linear_reg_model.predict([[float(inputBMI.value)]])[0][0], end=\"\\r\")\ninputBMI.on_submit(predictLifeExpectancy)\ninputBMI\n", "intent": "Now that we have the data fit we can use this to predict what the life expectancy of a person is based on the BMI.\nWe will try and check out the life\n"}
{"snippet": "def rmse(model, X, y):\n    return np.sqrt(mean_squared_error(model.predict(X), y))\n", "intent": "A small function will help us calculate RMSE scores.\n"}
{"snippet": "def RMSE_score(model, X, y, cv=5):\n    return (np.mean(cross_val_score(prf, X_test, y_test, scoring='neg_mean_squared_error', cv=cv))*-1)**0.5\n", "intent": "Scoring with Root Mean Squared Error, as per the Kaggle competition.\nThe function 'RMSE_score' returns a 5x cross-validated RMSE for a given model.\n"}
{"snippet": "y_pred=classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix=confusion_matrix(y_test,y_pred)\nprint(confusion_matrix)\n", "intent": "The logistic regression model fit the dataset and the score is 0.67\n"}
{"snippet": "log_rmse =np.sqrt(metrics.mean_squared_error(Y_testpoly, y_pred))\nlog_rmse\n", "intent": "The confusion matrix result is telling us that we have 119+539 correct predictions and 86+236 incorrect predictions.\n"}
{"snippet": "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\ny_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\nz = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nz = z.reshape(xx.shape)\n", "intent": "Create a mesh grid to display the decision boundaries of the trained logistic regression model.\n"}
{"snippet": "y_predict = np.zeros(len(y_test))\nprint(log_loss(y_test, y_predict))\n", "intent": "About 37% of the labels are 1. We can guess all the predictions are 0, see what is the log loss.  \n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_train[train_is], y_pred[:, 1][train_is]))\n", "intent": "You can check that it is just a wrapper of `roc_auc_score`.\n"}
{"snippet": "X_train_array = fe.transform(X_train)\ny_pred = clf.predict_proba(X_train_array)\n", "intent": "Transform the _whole_ (training + validation) dataframe into a numpy array and compute the prediction.\n"}
{"snippet": "predictions = mlp.predict(test_image) \n", "intent": "First let's generate some predictions from our classifier\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(test_target,predictions))\nprint(confusion_matrix(test_target, predictions))\n", "intent": "Now of course sk learn has some easy prewritten functions to build a classification report. Isn't it great?\n"}
{"snippet": "predictions = linear_regression_model.predict(test_kaggle[features])\npredictions.shape\n", "intent": "Now we can use this improved model to predict the housing prices of Kaggle's external test data.\n"}
{"snippet": "predictions = ridge_regression_model.predict(test_kaggle[features])\nexport_csv(predictions, 'ridge_regression')\n", "intent": "If everything looks good, we can save our predictions to a csv file for submission to Kaggle.\n"}
{"snippet": "predictions = linear_regression_model.predict(test_kaggle[features])\npredictions.shape\n", "intent": "Now we can use this improved model to predict the housing prices of our test data.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\n", "intent": " 72%  Accuracy could be due to several highly correlated variables.\n"}
{"snippet": "x_test = np.array(['she likes to fart'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "df['result']=pipeline.predict(X_all)\n", "intent": "Here there is some code to visualize the final results.\n"}
{"snippet": "preds = np.expm1(model_lasso.predict(X_test))\n", "intent": "The residual plot looks pretty good.To wrap it up let's predict on the test set and submit on the leaderboard:\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:\n"}
{"snippet": "def clss_report(ytest, ypred):\n    report = classification_report(ytest, ypred)\n    print report \n", "intent": "<H1>ploykernel:</H1>\n"}
{"snippet": "_ = clss_report(y_test, y_pred)\n", "intent": "<H5>Classification Report (poly kernel):</H5>\n"}
{"snippet": "_ = clss_report(y_test, y_pred_minmax)\n", "intent": "<H5>Classification Report (poly kernel)-Minmax scaler:</H5>\n"}
{"snippet": "_ = clss_report(y_test_imp, y_pred_imp)\n", "intent": "<H5>Classification Report (poly kernel)-imputed:</H5>\n"}
{"snippet": "_ = clss_report(y_test_imp, y_pred_minmax_imp)\n", "intent": "<H5>Classification Report (poly kernel)-imputed:</H5>\n"}
{"snippet": "_ = clss_report(y_test_imp, y_pred_minmax_imp)\n", "intent": "<H3>Classification Report (rbf kernel):</H3>\n"}
{"snippet": "_ = clss_report(y_test_imp, y_pred_minmax_imp)\n", "intent": "<H3>Classification Report (best_C, best_gamma):</H3>\n"}
{"snippet": "_ = clss_report(y_test_imp, y_pred_minmax_imp)\n", "intent": "<H3>Classification Report :</H3>\n"}
{"snippet": "y_pred = model.predict(X_test)\ny_pred = np.exp(y_pred) - 1 \ny_pred[5:]\n", "intent": "we can also make predictions using our model\n"}
{"snippet": "X_test_imp = imp.transform(X_test)\npredicted_labels = clf.predict(X_test_imp)\n", "intent": "To test a model, we:\n1. First impute the missing values\n* Use the trained classifier to predict the labels\n"}
{"snippet": "train_df_predict_1 = lg_model_1.predict(sm.add_constant(X_train))\ntrain_df_predict_1.head()\ntrain_df_predict_2 = lg_model_1.get_prediction(sm.add_constant(X_train))\ntrain_df_predict_2.predicted_mean[0:5]\n", "intent": "Below is the output from model with dummy variable coding\n"}
{"snippet": "train_df_predict_3 = lg_model_2.predict(sm.add_constant(lg_train_df))\ntrain_df_predict_3.head()\ntrain_df_predict_4 = lg_model_2.get_prediction(lg_train_df)\ntrain_df_predict_4.predicted_mean[0:5]\n", "intent": "Here is the output with the model with no dummy variable coding\n"}
{"snippet": "example_predictions = lm.predict(train_data[example_features])\nprint example_predictions[0]\nprint train_data['price'][0]\n", "intent": "Naturally, there's a positive (not sure if it's linear) relationship between square footage and price. Let's make some predictions!\n"}
{"snippet": "def compute_RSS(model, data, outcome):\n    predictions = model.predict(data)\n    residuals = outcome - predictions\n    RSS = residuals**2\n    RSS = RSS.sum()\n    return RSS\n", "intent": "We can compute the RSS for all data points with a simple function.\n"}
{"snippet": "y_pred = clf.predict(X)\nprint (\"Classifier Accuracy: \",sum(y==y_pred).astype(float)/len(y))\n", "intent": "We can use this to make predictions.\n"}
{"snippet": "startTime = time()\ny_pred = cross_val_predict(clf, X_test, y_test, cv=n_folds)\nendTime = time()\nscore = accuracy_score(y_test, y_pred)\nprint(\"Time doing 10 fold cv took %f and the accuracy was %f\" % (endTime-startTime, score))\n", "intent": "Finally, let's see the test set error\n"}
{"snippet": "y_pred = cross_val_predict(clf, X_test, y_test, cv=n_folds)\naccuracy_score(y_test, y_pred)\n", "intent": "Finally, let's see the test set error\n"}
{"snippet": "pred, features_parts, bias = interpreter.predict(x)\n", "intent": "Note that we can get the numeric version :\n"}
{"snippet": "twits = [\n    \"This is really bad, I don't like it at all\",\n    \"I love this!\",\n    \":)\",\n    \"I'm sad... :(\"\n]\npreds = clf.predict(twits)\nfor i in range(len(twits)):\n    print(f'{twits[i]} --> {preds[i]}')\n", "intent": "Finally, let's run some tests :-)\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, style_indice in enumerate(style_layers):\n        generate_gram = gram_matrix(feats[style_indice])\n        target_gram = style_targets[i]\n        style_loss += content_loss(style_weights[i], generate_gram, target_gram)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_row = X_test[:1]\nprint(test_row)\nprint(test_row.shape)\nscore = loaded_model.predict(test_row)\nprint(score.shape)\nprint(score)\n", "intent": "The model constructed from storage can be used to predict\n"}
{"snippet": "test_row = X_test[:1]\nprint(test_row)\nprint(test_row.shape)\nscore = loaded_model.predict(test_row)\nprint(score.shape)\nprint(denormalize(score,min_max_dict_list))\n", "intent": "The model constructed from storage can be used to predict\n"}
{"snippet": "Y_submit = np.vstack((np.arange(X_submit.shape[0]), classifier.predict_proba(X_submit)[:,1])).T\nnp.savetxt('Y_submit.txt', Y_submit, '%d, %.2f', header='ID,Prob1',comments='',delimiter=',')\n", "intent": "Run this code and submit the `Y_submit.txt` file generated.\n"}
{"snippet": "gs = GridSearchCV(estimator = pipe_svc,\n                 param_grid = param_grid,\n                 scoring = 'accuracy',\n                 cv = 2,\n                 n_jobs = -1)\nscores = cross_val_score(gs, X_train, y_train, scoring = 'accuracy', cv = 5)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "***A 5x2 nested cross-validation***\n"}
{"snippet": "def predict(Y_hat):\n    return np.argmax(Y_hat, axis=0).reshape(-1,1)\ndef accuracy(Y_predict, Y):\n    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]\n", "intent": "$$\naccuracy = \\frac{\\sum_i^m (y^{[i]}==\\hat{y}^{[i]})}{m}\n$$\n"}
{"snippet": "print('In-sample error rate: {}'.format(np.sum(my_model.predict(train_X) != train_y) / len(train_y)))\nprint('Out-of-sample error rate: {}'.format(np.sum(my_model.predict(test_X) != test_y) / len(test_y)))\n", "intent": "Check how many blood types were predicted correctly in the training and test sets:\n"}
{"snippet": "fea = dsnmf.get_features().T \npred = kmeans.fit_predict(fea)\nscore = sklearn.metrics.normalized_mutual_info_score(gnd, pred)\nprint(\"NMI: {:.2f}%\".format(100 * score))\n", "intent": "Evaluate it in terms of clustering performance using \nthe normalised mutual information score.\n"}
{"snippet": "Y_pred = clf.predict(X_test)\n", "intent": "This predicts new data using a testing set\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(Y_pred, Y_test)\n", "intent": "Now we can get an accuracy score, based on the predicted Y values and actual Y values!\n"}
{"snippet": "score   = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:',    score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Now score the model using the test set\n------------------------------------\n"}
{"snippet": "y_pred = log_model.predict(X_test)\n", "intent": "And finally, we use log_model to label the evaluation set we created earlier:\n"}
{"snippet": "roc_auc_score(test_df.y_true, test_df.y_predict)\n", "intent": "The following blocks show scores for test set on metrics such as roc-auc, f1-score, accuracy\n"}
{"snippet": "Ypred = clf.predict(Xtest)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(Ypred[:,0], Ytest[:,0]))\nprint(\"RMSE is: \",rms)\n", "intent": "Let's test this model on test data\n"}
{"snippet": "predicted = model.predict(test_data)\nprint(\"Evaluation report: \\n\\n%s\" % metrics.classification_report(test_labels, predicted))\n", "intent": "You can check your **model quality** now. To evaluate the model, use **test data**.\n"}
{"snippet": "predictions = loadedModelArtifact.model_instance().predict(score_data)\n", "intent": "In this subsection you will score *predict_data* data set.\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "result = knn.predict([[3, 5],])\nprint(iris.target_names[result])\n", "intent": "That's it. We've trained our classifier! Now let's try to predict a new, previously-unobserved iris:\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Predictions and Evaluations\nTesting Data value predictions\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\n", "intent": "Classification Report for model\n"}
{"snippet": "Yp = model.predict(x_test_scaled, verbose=1)\n", "intent": "Use the `predict(self, x, batch_size=32, verbose=0)` function to do predictions on the test sample with the trained model.\n"}
{"snippet": "y_pred = net_regr.predict(X_regr[:5])\ny_pred\n", "intent": "You may call `predict` or `predict_proba` on the fitted model. For regressions, both methods return the same value.\n"}
{"snippet": "weights3_swc_sw_rms_prop = linear_rms_prop(feats3_swc[best3 + all_dummy_names], lab3_swc, 80, int(len(table3)/5), decay_rate=.01,\n                                    epsilon=1*(10**14), epsilon_delta=.1, max_iter=5000)\nfeat_mat3_swc = np.insert(np.matrix(feats3_swc[best3 + all_dummy_names]), 0, 1, axis=1)   \npredicted3_swc = feat_mat3_swc * weights3_swc_sw_rms_prop\nr2 = r_squared(np.asarray(predicted3_swc.transpose())[0], np.array(lab3_swc))\nsse = sum_squared_error(np.asarray(predicted3_swc.transpose())[0], np.array(lab3_swc))\nprint('RMSProp with all categoricals r squared is {:0.4f}'.format(r2))\nprint('regression with all categoricals sum squared error: {:0.4f}'.format(sse))\nprint('regression with all categoricalsmean squared error: {:0.4f}'.format(sse/len(table3)))\nprint('regression with all categoricals mean error: ${:0.4f}'.format((sse/len(table3))**.5))\n", "intent": "Lets run Linear RMSProp with all the best numericals and all the categorical columns:\n"}
{"snippet": "weights2_swc_sw_rms_prop = linear_rms_prop(feats2_swc[best2 + all_dummy_names], lab2_swc, .9, int(len(table2)/5), decay_rate=.01,\n                                    epsilon=7*(10**14), epsilon_delta=10, max_iter=5000)\nfeat_mat2_swc = np.insert(np.matrix(feats2_swc[best2 + all_dummy_names]), 0, 1, axis=1)   \npredicted2_swc = feat_mat2_swc * weights2_swc_sw_rms_prop\nr2 = r_squared(np.asarray(predicted2_swc.transpose())[0], np.array(lab2_swc))\nsse = sum_squared_error(np.asarray(predicted2_swc.transpose())[0], np.array(lab2_swc))\nprint('RMSProp with all categoricals r squared is {:0.4f}'.format(r2))\nprint('regression with all categoricals sum squared error: {:0.4f}'.format(sse))\nprint('regression with all categoricalsmean squared error: {:0.4f}'.format(sse/len(table2)))\nprint('regression with all categoricals mean error: ${:0.4f}'.format((sse/len(table2))**.5))\n", "intent": "Lets run Linear RMSProp with all the best numericals and all the categorical columns:\n"}
{"snippet": "clf.predict([[1.1, 0.5, 0.2]])\n", "intent": "Now, based on this random forest classifier, we can see that recommended action is 0 when market factor combination is $[1.1, 0.5, 0.2]$ \n"}
{"snippet": "def find_Prediction(trainX, testX, model):\n    trainPredict = model.predict(trainX)\n    testPredict = model.predict(testX)\n    return trainPredict, testPredict\ntrainPredict, testPredict = find_Prediction(trainX, testX, model)\n", "intent": "Now we use this LSTM NN to make prediction for both train and test sets:\n"}
{"snippet": "row['rank_home'] = world_cup_rankings.loc['Denmark', 'rank']\nhome_points = world_cup_rankings.loc['Denmark', 'weighted_points']\nopp_rank = world_cup_rankings.loc['Portugal', 'rank']\nopp_points = world_cup_rankings.loc['Portugal', 'weighted_points']\nrow['rank_difference'] = row['rank_home'] - opp_rank\nrow['point_difference'] = home_points - opp_points\nDenmark_winning_prob = model.predict_proba(row)[:,1][0]\nPortugal_winning_prob=1 -Denmark_winning_prob\nPortugal_winning_prob\n", "intent": "Portugal wins with a chance of 0.55\n"}
{"snippet": "ax = houses.plot.scatter(x='RM', y='price')\nax.plot(houses['RM'], lr.predict(houses[['RM']]), 'r-')\n", "intent": "This is not a great score. There are many points far from the linear regression line.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=1)\nprint('\\t'+ str(model.metrics_names))\nprint('\\t'+str(score))\n", "intent": "- Test for accuracy across all test data\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, rf.predict(X_test)))\n", "intent": "Random Forest Measures\n"}
{"snippet": "preds = net1.predict(X_test)\npreds.shape\n", "intent": "Let's grab out the predictions!\n"}
{"snippet": "prob = pb.predict_proba(test_data)\nprint 'ROC AUC:', roc_auc_score(test_labels, prob[:, 1])\n", "intent": "again, we could proceed with training and use new dataset\n```\nnl.partial_fit(new_train_data, new_train_labels)\n```\n"}
{"snippet": "y_pred_prob = xgb1.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nmetrics.auc(fpr, tpr)\n", "intent": "It is important to plot the precision curve for the model. It illustrates how the model learned the data and recalled. \n"}
{"snippet": "log_preds = learn.predict()\n", "intent": "__validation accuracy__\n"}
{"snippet": "flist = [2,8]\nregr = feature_subset_regression(x,y,[2,8])\nprint (\"w = \", regr.coef_)\nprint (\"b = \", regr.intercept_)\nprint (\"Mean squared error: \", mean_squared_error(y, regr.predict(x[:,flist])))\n", "intent": "Try using just features \n"}
{"snippet": "regr = feature_subset_regression(x,y,range(0,10))\nprint (\"w = \", regr.coef_)\nprint (\"b = \", regr.intercept_)\nprint (\"Mean squared error: \", mean_squared_error(y, regr.predict(x)))\n", "intent": "Finally, use all 10 features.\n"}
{"snippet": "clf.set_params(n_neighbors=grid.best_params_['n_neighbors'])\ncross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n", "intent": "We verify that the grid search has indeed chosen the right parameter value for $k$.\n"}
{"snippet": "predicted = model.predict(test_X)\nprobs = model.predict_proba(test_X)\n", "intent": "- **0.8898** is the accuracy on the training set. It's good, but we want to know the **prediction power** of the model\n"}
{"snippet": "y_pred = model.predict(X_test.values)\ncnf_matrix = confusion_matrix(y_test,y_pred)\ncnf_matrix\n", "intent": "As we thought, this model comes out with a 99.91% accuracy. Which sounds great? But now let's look at the confusion matrix: \n"}
{"snippet": "y_prob = gnb.predict_proba(X)\nprint y_prob.shape\n", "intent": "Building an ROC curve requires to use the probability estimates for the test data points *before* they are thresholded.\n"}
{"snippet": "print \"Accuracy: %.3f\" % metrics.accuracy_score(y, y_pred_cv)\n", "intent": "**Question:** Evaluate the performance of your model (accuracy, AUC, ROC-AUC). How does it compare to the one you previously obtained?\n"}
{"snippet": "print(accuracy_score(y_true = y, y_pred = LogReg.predict(X)))\n", "intent": "Let's see what the benchmark accuracy score is?\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Evaluating the model against the test set:\n"}
{"snippet": "n_folds = 10\ndef cross(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "- We'll use the cross val score method to train our algorithm\n- But before it we will shuffle the train data\n"}
{"snippet": "predict = lor.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print (classification_report(y_test, predict))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "p = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print (confusion_matrix(y_test, p))\nprint (classification_report(y_test, p))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "predict = dtc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predict_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test, predict_rfc))\nprint (confusion_matrix(y_test, predict_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predict = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print (confusion_matrix(college_df['Cluster'], km.labels_))\nprint (classification_report(college_df['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predict = NB.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print (confusion_matrix(y_test, predict))\nprint (classification_report(y_test, predict))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "pipe_predict = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "scores = {}\nfor i in range(sample.shape[1] - 1):\n    scores[i] = roc_auc_score(sample[:, -1], sample[:, i])\n", "intent": "Below hypothesis that some bits of fingerprints allows separating geroprotectors, is tested.\n"}
{"snippet": "scores = {}\nfor i in range(X_train.shape[1]):\n    scores[i] = roc_auc_score(y_train, X_train[:, i])\nweights = np.array([abs(scores[key] - 0.5) for key in scores.keys()])\nweighted_X_train = X_train * weights\nweighted_X_test = X_test * weights\n", "intent": "To generate PCA-based features, `y_test` can not be used and `X_test` can be transformed, but it is better to avoid fitting PCA to it.\n"}
{"snippet": "score=model.evaluate(x=test_Resnet50,y=test_targets)\nscore[1]*100\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.8f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.8f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.8f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "from sklearn import metrics\nprint (metrics.accuracy_score(dhar_accuracy, Y_test))\n", "intent": "    6. h Obtain the test data accuracy\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint r2\n", "intent": "Let's measure the r-squared error:\n"}
{"snippet": "target_variable = 2\nwindow_size = 5\nbatch_size = 500\nSPY2_X_LSTM, SPY2_y_LSTM = window_transform_series(np.array(SPY2_df_X), np.array(SPY2_df_y['Adj Close 1day pct_change']), window_size = window_size)\ngood_size_train = SPY2_X_LSTM.shape[0] - SPY2_X_LSTM.shape[0] % batch_size\nSPY2_X_LSTM = SPY2_X_LSTM[-good_size_train:]\nSPY2_y_LSTM = SPY2_y_LSTM[-good_size_train:]\nerror = mean_squared_error(SPY_2_model.predict(SPY2_X_LSTM, batch_size = batch_size), np.array(SPY2_y_LSTM)) / np.mean(SPY2_y_LSTM)\nprint('Error: {}% of mean'.format(error * 100))\nSPY2_perf['Adj Close 1day pct_change'] = error\n", "intent": "layer1 = 1024, layer2 = 256, epochs = 100, batch_size = 500\n"}
{"snippet": "print(classification_report(y_test, predict))\n", "intent": "The classification report displays the precision, recall, F1 score and the support for the model.\n"}
{"snippet": "RESDOG_predictions = [np.argmax(RESDOG.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(RESDOG_predictions)==np.argmax(test_targets, axis=1))/len(RESDOG_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Here we test the model, with the original assignment being to achieve over 60% test accuracy.\n"}
{"snippet": "pred = fitted_models['gb'].predict(X_test)\nprint( confusion_matrix(y_test, pred) )\n", "intent": "The model with the greatest AUROC is gradient boosted trees. Let's look at its confusion matrix.\n"}
{"snippet": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "We first define a rmsle evaluation function\n"}
{"snippet": "def classify_videos(classifier, pop_df, nonPop_df):\n    trainData, testData = setupTrainingAndTestingData(pop_df, nonPop_df)\n    matrix = testData.drop('video_id', 1).as_matrix()\n    prediction = classifier.predict(matrix)\n    return prediction\nclassifier = run_random_forest(train_df)\ny_pred = classify_videos(classifier, df_pop, df_nonpop)\n", "intent": "Finally, we set up a method that combines this entire process into one easy flow given a set of popular or nonpopular videos.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('The Inception Test accuracy id: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "num_folds = 3\nav = cv.cross_val_score(classifier, X, y, scoring = 'accuracy', cv = num_folds)\nprint(\"Accuracy: \" + str(round(100 * av.mean(), 2)) + \"%\")\npv = cv.cross_val_score(classifier, X, y, scoring = 'precision_weighted', cv = num_folds)\nprint(\"Precision: \" + str(round(100 * pv.mean(), 2)) + \"%\")\nrv = cv.cross_val_score(classifier, X, y, scoring = 'recall_weighted', cv = num_folds)\nprint(\"Recall: \" + str(round(100 * rv.mean(), 2)) + \"%\")\nfv = cv.cross_val_score(classifier, X, y, scoring = 'f1_weighted', cv = num_folds)\nprint(\"F1: \" + str(round(100 * fv.mean(), 2)) + \"%\")\n", "intent": "Values:\n- accuracy\n- precision\n- recall\n- F1\n"}
{"snippet": "ptmod.plot_loss(loss_by_epoch, 'CNN 700', plot_values='epoch')\n", "intent": "Let's plot loss values and look at train/test scores while we're at it:\n"}
{"snippet": "print(\"Prec %f\" % cross_val_score(clf3 , fil1, fil['highcrime'], cv=10, scoring='precision' ).mean())\nprint (\"Recall %f\" %cross_val_score(clf3 , fil1, fil['highcrime'], cv=10, scoring='recall' ).mean())\nprint (\"Accuracy %f\" %cross_val_score(clf3 , fil1, fil['highcrime'], cv=10, scoring='accuracy' ).mean())\n", "intent": "i.\t What is the 10-fold cross-validation accuracy, precision, and recall for this method?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(fil['ViolentCrimesPerPop'], prediction)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "ridge_predict = ridge.predict(fil1)\nmean_squared_error(fil['ViolentCrimesPerPop'], ridge_predict)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "mean_squared_error(fil['ViolentCrimesPerPop'], y_pred)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "model_transfer_predictions = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(model_transfer_predictions)==np.argmax(test_targets, axis=1))/len(model_transfer_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Finally, we test the performance over the test set:\n"}
{"snippet": "score = model4.evaluate(x_test, y_test)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Finally, we test the performance over the test set:\n"}
{"snippet": "score = model_2.evaluate(X_test, Y_test)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Al evaluar el modelo 2 se tiene que el accuracy mejora alcanzando un valor 0.38\n"}
{"snippet": "score = model_3.evaluate(X_test_vgg16, Y_test)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Utilizando la red preentrenada VGG16 adaptada al problema se consigue un accuracy de 0.57 al emplear 50 epochs.\n"}
{"snippet": "score = model_4.evaluate(X_test_vgg19, Y_test)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Utilizando la red preentrenada se consigue un accuracy de 0.57 al emplear 80 epochs que es similar a lo obtenido con VGG16\n"}
{"snippet": "from sklearn.metrics import fbeta_score\nfbeta_score(y_test, y_pred, beta=2)\n", "intent": "Derive the initial F2 score.\n"}
{"snippet": "from sklearn.metrics import fbeta_score\ny_pred = model.predict(X_test)\nfbeta_score(y_test, y_pred, average='binary', beta=2)\n", "intent": "Derive the F2 score.\n"}
{"snippet": "from sklearn.metrics import fbeta_score\ny_pred = optimized_model.predict(X_test)\nfbeta_score(y_test, y_pred, average='binary', beta=2)\n", "intent": "Derive the F2 score.\n"}
{"snippet": "def score_confusion_matrix(classifier):\n    print (\"Score: {}\".format(classifier.score(X_test, y_test)))\n    y_predicted = classifier.predict(X_test)\n    print (\"Confusion Matrix: \", confusion_matrix(y_test, y_predicted.round()))\nscore_confusion_matrix(classifier)\n", "intent": "**Confusion matrix:**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Less accuracy, but no longer overfitting\n"}
{"snippet": "ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\nvDNN = vanillaDNN(config, training = False)\nlabels, preds, output = vDNN.predict(ckpt)\n", "intent": "Classification on a new instance is given by the softmax of the output of the final readout layer.\n"}
{"snippet": "ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\nnetwork_output = vDNN.predict(ckpt)\nnp.save(os.path.join(directories.checkpoints, '{}_{}_y_pred.npy'.format(config.channel, config.mode)), network_output)\nnp.save(os.path.join(directories.checkpoints, '{}_{}_y_test.npy'.format(config.channel, config.mode)), df_y_test.values)\n", "intent": "Classification on a new instance is given by the softmax of the output of the final readout layer.\n"}
{"snippet": "ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\nvDNN = vanillaDNN(config, training = False)\nnetwork_output = VDNN.predict(ckpt)\nnp.save(os.path.join(directories.checkpoints, '{}_{}_y_pred.npy'.format(config.channel, config.mode)), network_output)\nnp.save(os.path.join(directories.checkpoints, '{}_{}_y_test.npy'.format(config.channel, config.mode)), df_y_test.values)\n", "intent": "Classification on a new instance is given by the softmax of the output of the final readout layer.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's get a Mean Square Error estimate over the whole training set\n"}
{"snippet": "y_pred = regr.predict(X_test)\nmatrix = confusion_matrix(list(y_test.values), list(y_pred))\nprint(matrix)\nprint('Correct predictions: %d' % (matrix[0][0] + matrix[1][1]))\nprint('Incorrect predicitons: %d' % (matrix[0][1] + matrix[1][0]))\n", "intent": "Finally we will use a confusion matrix to get the number of correct and incorrect predictions made by our model.\n"}
{"snippet": "print(classification_report(y_test_class,y_pred_class))\n", "intent": "Here are the scorse computed based on what we talked about in class.  \n"}
{"snippet": "predictions = model.predict(X_test)\npredictions_probs = model.predict_proba(X_test)\nprint(predictions[:10])\nprint(predictions_probs[:10])\n", "intent": "All you have to do to get predictions is\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "**Predictions from the model I created:**\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Ensure that the test accuracy is greater than 1%.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Ensure that the test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv1 = cross_val_score(classifier, train_X, train_y, cv=2, scoring='accuracy')\ncv10 = cross_val_score(classifier, train_X, train_y, cv=10, scoring='accuracy')\nprint(\"Two-fold cross validation:\", cv1)\nprint(\"Ten-fold cross validation:\", cv10)\n", "intent": "- Teh `cv` parameter is the number of folds.\n- Scoring type depends on whether classification, clustering, regression.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n", "intent": "Getting performance metrics\n"}
{"snippet": "print(estimator.evaluate(input_fn=input_fn))\n", "intent": "Here we evaluate how well our model did. In a real example, we would want\nto use a separate validation and testing data set to avoid overfitting.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = test_set.drop(\"median_house_value\", axis=1)\ny_test = test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)   \n", "intent": "Here we will evaluate the performance of our model on the test set\n"}
{"snippet": "cross_val_score(forest_clf, X_train_scaled, y_train, cv = 3, scoring = \"accuracy\")\n", "intent": "We could easily reach 90% accuracy by just standardizing our data\n"}
{"snippet": "item_item = gl.recommender.item_similarity_recommender.create(train, \n                                  user_id=\"userId\", \n                                  item_id=\"title\", \n                                  target=\"rating\",\n                                  only_top_k=5,\n                                  similarity_type=\"cosine\")\nrmse_results = item_item.evaluate(test)\n", "intent": "Now we will build an item similarity recommender using graphlab. We will tell it which parameters to use and then evaluate it.\n"}
{"snippet": "mlp_prediction_PCA = mlp_classifier_PCA.predict(P_test)\n", "intent": "We will test now individual character recognition scores for each classifier using the test batch.\n"}
{"snippet": "rbm_mlp_prediction = rbm_mlp_classifier.predict(X_test_chars)\nmlp_prediction_HOG = mlp_classifier_HOG.predict(fd_test)\nmlp_prediction_PCA = mlp_classifier_PCA.predict(P_test)\nmlp_prediction = mlp_classifier.predict(X_test_chars)\n", "intent": "We will test now individual character recognition scores for each classifier using the test batch.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(d_Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print metrics.accuracy_score(y_test, log_pred_search_pred)\nprint metrics.classification_report(y_test, log_pred_search_pred)\n", "intent": "Prediction Breakdown Analysis\n* Precision: True Pos / (True pos + False Positives). \n* Recall: True Pos / (true pos + false negatives)\n"}
{"snippet": "y_new_logreg = logreg_grid_search.predict(x_new_dtm,delegate='estimator')\n", "intent": "- Created stored predictions for out of sample data based on each model\n"}
{"snippet": "y_pred_nb = nb_clf.predict(X_test_vect)\n", "intent": "Create a variable for vectorized test predictions\n"}
{"snippet": "print metrics.accuracy_score(y_test, y_pred_nb)\nprint metrics.classification_report(y_test, y_pred_nb)\n", "intent": "Classification Report\n"}
{"snippet": "y_pred_reg = nb_nb.predict(X_test)\nconfusion_matrix(y_test, y_pred_reg)\n", "intent": "Unlike TFDIF with Naive Bayes, made predictions for each target category and had slighlty higher accuracy score of 73.6%\n"}
{"snippet": "nn.predict(test_data)\n", "intent": "We can then predict the results using the predict method\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_IN]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "yh_best_model_test = np.argmax(best_model.predict(X_test), axis = 1)\n", "intent": "<h3>Analysis</h3>\n<p>Here I evaluate the testing recall of the model.</p>\n"}
{"snippet": "yhat_chained = np.argmax(chained.predict(X_test), axis = 1)\nyhat_best_model = np.argmax(best_non_chained.predict(X_test), axis = 1)\n", "intent": "<h3>Testing Performance and Confusion Matrices</h3>\n"}
{"snippet": "Xcept_predictions = [np.argmax(Xcept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xcept]\ntest_accuracy = 100*np.sum(np.array(Xcept_predictions)==np.argmax(test_targets, axis=1))/len(Xcept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print \"k=3:\", round(accuracy_score(Y_test,Y_pred3),4)\nprint \"k=5:\", round(accuracy_score(Y_test,Y_pred5),4)\nprint \"k=10:\", round(accuracy_score(Y_test,Y_pred10),4)\nprint \"k=20:\", round(accuracy_score(Y_test,Y_pred20),4)\nprint \"k=50:\", round(accuracy_score(Y_test,Y_pred50),4)\nprint \"k=100:\", round(accuracy_score(Y_test,Y_pred100),4)\n", "intent": "Calculating Accuracy, Precision and recall for different k values.\n"}
{"snippet": "def calculate_loss(x, y, model):\n    square_sums = (y - model.evaluate(x))**2\n    n_inv = 1./x.shape[0]\n    return n_inv * square_sums.sum()\n", "intent": "$$\\mathcal{L}(\\tilde{\\alpha}, \\tilde{\\beta}) =\\frac{1}{N}\\sum_{n=1}^{N}{\\left(y - (\\hat{\\alpha} x + \\hat{\\beta}) \\right)^2}$$\n"}
{"snippet": "def calculate_gradient(batch_x, batch_y, model):\n    n_inv_coeff = - 2./batch_x.shape[0]\n    result_diff = (batch_y - model.evaluate(batch_x))\n    grad_alpha = n_inv_coeff * (result_diff * batch_x).sum()\n    grad_beta = n_inv_coeff * result_diff.sum()\n    return np.array([grad_alpha, grad_beta])\n", "intent": "We are going to reuse it in this function as follows:\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=120):\n    for i in range(n):\n        pair = pairs[i]\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "def implicit_cf_loss(y_true, y_pred, alpha=alpha):\n    P = y_true > 0\n    C = 1 + alpha*y_true\n    return K.mean( C*(P - y_pred)**2 )\n", "intent": "This is simply the IMF objective we'd like to minimize.\n"}
{"snippet": "predictions = clf.predict(X_test)\ny_vals = y_test.ravel()\nconfusion_matrix = pd.crosstab(predictions, y_vals, rownames=['y_vals'], colnames=['predictions'])\nrf_metrics = {\n    'sensitivity': round(float(confusion_matrix[1][1]) / (float(confusion_matrix[0][1] + confusion_matrix[1][1])),2),\n    'specificity': round(float(confusion_matrix[0][0]) / (float(confusion_matrix[0][0] + confusion_matrix[1][0])),2),\n    'accuracy': round(float(confusion_matrix[0][0] + confusion_matrix[1][1]) / float(confusion_matrix[0][0] + confusion_matrix[0][1] + confusion_matrix[1][0] + confusion_matrix[1][1]),2)\n}\n", "intent": "Calculate the sensitivity, specificity and general accuracy of the model\n"}
{"snippet": "predictions = linear_regressor.predict(my_feature, as_iterable=False)\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nprint \"Mean Squared Error (on training data): %0.3f\" % mean_squared_error\nprint \"Root Mean Squared Error (on training data): %0.3f\" % math.sqrt(mean_squared_error)\n", "intent": "Let's make predictions on that training data, to see how well we fit the training data.\n"}
{"snippet": "print \"Class predictions according to Scikit Learn:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "we now verify that the class predictions obtained by your calculations are the same as that obtained from SciKit Learn.\n"}
{"snippet": "print \"Class predictions according to SciKit Learn:\" \nprint sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "**Checkpoint**: we make sure that probability predictions match the ones obtained from Scikit Learn.\n"}
{"snippet": "prediction = kmeans.predict(df.ix[:,['actcount','med_ml_num','buynum', 'grind','quests_speed']])\nwith open('clusters.csv','wb') as f:\n    writer = csv.writer(f, delimiter='|',\n                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    writer.writerow(['id','cluster_index'])\n    for i in xrange(len(prediction)):\n        writer.writerow([df['id'].ix[i],prediction[i]])\n    f.close()\n", "intent": "Dividing of users on clusters\n"}
{"snippet": "import keras\npreds2 = model2.predict(X_test)\nkeras.metrics.categorical_accuracy(y_test, preds2).eval()\n", "intent": "The performance of model 2 and model 3 respectively is as follows:\n"}
{"snippet": "from basic_model_tf import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\nclass supervised_training:\n    input_sequence = tf.placeholder('int32',[None,None])\n    reference_answers = tf.placeholder('int32',[None,None])\n    logprobs_seq = model.symbolic_score(input_sequence, reference_answers)\n    crossentropy = - select_values_over_last_axis(logprobs_seq,reference_answers)\n    mask = infer_mask(reference_answers, out_voc.eos_ix)\n    loss = tf.reduce_sum(crossentropy * mask)/tf.reduce_sum(mask)\n    train_step = model.weights\ninitialize_uninitialized(s)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "predictions = model_5.predict(validation_X)\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].values, y_pred=predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_true=test_data['sentiment'].values, y_pred=predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(y_true=test_data['sentiment'].values, y_pred=predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true=test_data['sentiment'].values, y_pred=predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "baby_matrix = vectorizer.transform(baby_reviews['review_clean'])\nprobabilities = model.predict_proba(baby_matrix)[:,1]\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "print(classification_report(predictions,label_test))\n", "intent": "<img \nsrc='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=\"400\" />\n"}
{"snippet": "model.predict(trainX, batch_size=batch_size)\n", "intent": "This same batch size must then be used later when evaluating the model and making predictions\n"}
{"snippet": "val_pseudo = bn_model.predict(conv_val_feat, batch_size=batch_size)\n", "intent": "To do this, we simply calculate the predictions of our model on the validation weights from the conv layers:\n"}
{"snippet": "dtree_pred = dtree.predict(X_test)\n", "intent": "As you can see this model's score is higher hinting that it is probably more accurate. Let's take a look at the classification report.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L = []\n    for i, l in enumerate(style_layers):\n        A = style_targets[i]\n        G = gram_matrix(feats[l], normalize=True)\n        L.append(style_weights[i]*tf.square(tf.norm(A - G)))\n    loss = tf.add_n(L)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\nprint(answers['tv_out'])\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "predicted_population_growth_rate_2017 = population_growth_rate_mod.predict([[2017]])\npredicted_population_growth_rate_2017 = predicted_population_growth_rate_2017[0][0]\nprint 'The population growth rate of 2017 is expected to be',predicted_population_growth_rate_2017\n", "intent": "As the population does not increase with a good linear model, we need to calculate the population of 2017 from population growth rate via kNN model.\n"}
{"snippet": "predicted_y = clf_rf.predict(test_X)\n", "intent": "<br/>\nWe use our previously trained model to make the predictions on our Test data.\n"}
{"snippet": "y_pred = km.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\n", "intent": "In this case we don't have the identifiability problem as indicated by the confusion matrix.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=5)\nscores\n", "intent": "__For a 5-fold validation__\n"}
{"snippet": "yprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n", "intent": "__In general the boundary in Gaussian naive Bayes is quadratic__.\n__It naturally allows for probabilistic calssification__\n"}
{"snippet": "from sklearn.metrics import f1_score\ndef predict_scores_ec(clf, features, target):\n    print (\"Predicting labels using {}...\".format(clf.__class__.__name__))\n    start = time.time()\n    y_pred_train_ec = clf.predict(features)\n    end = time.time()\n    score = f1_score(target, y_pred_train_ec, pos_label=None, average=\"weighted\")\n    return score\n    print (\"\\nPrediction time (secs): {:.3f}\".format(end - start))\n", "intent": "A function to predict F1 Scores for Expected Confusion \n"}
{"snippet": "from sklearn.metrics import f1_score\ndef predict_scores_ac(clf, features, target):\n    print (\"Predicting labels using {}...\".format(clf.__class__.__name__))\n    start = time.time()\n    y_pred_train_ac = clf.predict(features)\n    end = time.time()\n    score = f1_score(target, y_pred_train_ac, pos_label=None, average=\"weighted\")\n    return score\n    print (\"\\nPrediction time (secs): {:.3f}\".format(end - start))\n", "intent": "A Function to predict F1 Scores for Actual COnfusion \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for indx,l_num in enumerate(style_layers):\n        loss += style_weights[indx] * torch.sum(torch.pow(gram_matrix(feats[l_num]) - style_targets[indx],2))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:, 1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "y_pred = pipeline.predict(X_test.values)\n", "intent": "We can now make predictions on test data and evaluate the model.\n"}
{"snippet": "y_pred = clf.predict(X_test.values)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n", "intent": "We can see the accuracy of the best parameter combination on test set.\n"}
{"snippet": "predicted = model.predict(test_data)\nprint(\"Evaluation report: \\n\\n%s\" % metrics.classification_report(test_labels, predicted))\n", "intent": "Check the quality of your model. \nTo evaluate the model, use **test data**.\n"}
{"snippet": "test_predictions = loaded_model.predict(test_data[:10])\n", "intent": "You can run a test prediction to verify that that model has been loaded correctly.\n"}
{"snippet": "predicted = model.predict(test_data)\nprint(\"Evaluation report: \\n\\n%s\" % metrics.classification_report(test_labels, predicted))\n", "intent": "You can check your **model quality** now. To evaluate the model, use the **test data**.\n"}
{"snippet": "predictions = loadedModelArtifact.model_instance().predict(score_data)\n", "intent": "In this subsection you will score the `predict_data` data set.\n"}
{"snippet": "scores_logR = cross_val_score(logR, X_train, y_train, cv=10)\nprint('Cross Validation Accuracy Scores: {:.3f}(+/- {:.2f})'.format(scores_logR.mean(), \n                                                                      scores_logR.std()*2))\n", "intent": "These accuracy values seem less overfit than the Random Forest model.  Let's check cross validation scores with the training set.\n"}
{"snippet": "full_pred3 = mlp3.predict(X)\npd.crosstab(Y, full_pred3) \n", "intent": "Again, adjusted rand score is high, and cross validation variance reduced slightly.\n"}
{"snippet": "full_pred4 = mlp4.predict(X)\npd.crosstab(Y, full_pred4) \n", "intent": "Our adjusted rand scores decreased, and the variance increased.  Let's try some other adjustments in hyperparameters.\n"}
{"snippet": "full_pred5 = mlp5.predict(X)\npd.crosstab(Y, full_pred5) \n", "intent": "The model with a single layer of 1000 still has a higher score than this model. \n"}
{"snippet": "predictions = np.round(model.predict(test_x))\npredictions = np.squeeze(predictions).T\naccuracy = sum(test_y == predictions) / test_y.shape\nprint(\"Percent accuracy on test data = %.2f\"%(100 * accuracy))\n", "intent": "So after calibration the model can account for 74% of the outcomes in the training dataset.  Lets see how it performs on the remainder of the data\n"}
{"snippet": "pred_tuned = sal_tree_tuned.predict(data_test)\nprint(metrics.confusion_matrix(target_test.iloc[:,1], pred_tuned))\nprint(metrics.classification_report(target_test.iloc[:,1], pred_tuned))\naccuracy_score(target_test.iloc[:,1], pred_tuned)\n", "intent": "Now, run the cell below so we can see how well the pre-tuned model did.\n"}
{"snippet": "clf.predict_proba(x_ts)\n", "intent": "A Random Forest classifier also outputs the prediction **probabilities** for each class:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_ts, y_pred)\n", "intent": "Luckily Scikit-learn provides a quite handy alternative:\n"}
{"snippet": "y_raw = classify_and_evaluate(X, y)\n", "intent": "Before trying out our autoencoder, we simply try to classify our raw vectors.\n"}
{"snippet": "X_latent = encoder.predict(X)\n", "intent": "Now we can classify our pages using there autoencoded versions to see what we get.\n"}
{"snippet": "X_latent = encoder.predict(X)\ny_latent = classify_and_evaluate(X_latent, y)\n", "intent": "Now we can classify our pages using there autoencoded versions to see what we get.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    print (student_output.shape, correct.shape)\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.reduce_sum([style_weights[i]*tf.reduce_sum(tf.square(gram_matrix(feats[l]) - style_targets[i])) for i,l in enumerate(style_layers)])\n    return style_loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X, y = y, cv = 10)\n", "intent": "conduct 10-fold cross-validation\n"}
{"snippet": "outputs_test = list(Yoh_test.swapaxes(0,1))\nscore = model.evaluate(Xoh_test, outputs_test) \nprint('Test loss: ', score[0])\n", "intent": "The final training loss should be in the range of 0.02 to 0.5\nThe test loss should be at a similar level.\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:, 0] + (1 - best_alpha) * X_test_level2[:, 1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.)\n    for i in range(len(style_layers)):\n        content_current = gram_matrix(feats[style_layers[i]])\n        style_loss += content_loss(style_weights[i], content_current, style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def print_score(classifier, X_train, y_train, X_test, y_test):\n    print('Train set score :', classifier.score(X_train, y_train))\n    print('Test set score :', classifier.score(X_test, y_test))\n", "intent": "Let's define a function to print the model's accuracy :\n"}
{"snippet": "def print_score(classifier, X_train, y_train, X_test, y_test):\n    print('Train set score :', classifier.score(X_train, y_train))\n    print('Test set score :', classifier.score(X_test, y_test))\n", "intent": "Define function to print accuracy of the model\n"}
{"snippet": "silhouette_score(X_test, clust_model.predict(X_test))\n", "intent": "And now computing the silhouette score\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nsum_error = 0\nfor classifiers in ensemble.Ensemble.estimators_:\n    y_hat = classifiers.predict(X_test)\n    sum_error += (1 - accuracy_score(y_test, y_hat))\nerror_rate = sum_error/20\nprint(error_rate)\n", "intent": "* Collecting the average base classifier error rate\n"}
{"snippet": "from scipy.misc import comb\nimport math\ndef ensemble_error(n_classifier, error):\n    k_start = math.ceil(n_classifier/2.0)\n    probs = [comb(n_classifier, k) *\n            error**k *\n            (1-error)**(n_classifier - k) for k in range (k_start, n_classifier + 1)]\n    return(sum(probs))\n", "intent": "* Collecting the theoretical ensemble error rate assuming classifiers are independent\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nsum_error = 0\nfor classifiers in ensemble.Ensemble.estimators_:\n    y_hat = classifiers.predict(X_test)\n    sum_error += (1 - accuracy_score(y_test, y_hat))\nerror_rate = sum_error/20\nprint(error_rate)\n", "intent": "* Error rate of base classifiers\n"}
{"snippet": "y_hat = ensemble.predict(X_test)\n1 - accuracy_score(y_test, y_hat)\n", "intent": "* Collecting the ensemble error rate\n"}
{"snippet": "from scipy.misc import comb\nimport math\ndef ensemble_error(n_classifier, error):\n    k_start = math.ceil(n_classifier/2.0)\n    probs = [comb(n_classifier, k) *\n            error**k *\n            (1-error)**(n_classifier - k) for k in range (k_start, n_classifier + 1)]\n    return(sum(probs))\nensemble_error(n_classifier=20, error=error_rate)\n", "intent": "* Collecting the theoretical ensemble error rate assuming classifiers are independent\n"}
{"snippet": "def nn2(im, k, order = None, imgs=img ):\n    d = nn1_score(im, imgs, order)\n    return np.argsort(d)[:k]\n", "intent": "Instead of using a single nearest neighbor, sometimes it is useful to consider the consensus (majority vote) of k-nearest neighbours. \n"}
{"snippet": "yy_pred=lr.predict(income[['Age', 'fnlwgt', 'Education-Num', 'Age2']])\n", "intent": "se generan las predicciones para el modelo de varias variables\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy_InceptionV3 = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_InceptionV3)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def hinge_loss(y_true, y_pred, margin=1):\n    return max(0, margin-(y_true*y_pred))\n", "intent": "Also max-margin loos (proper of SVMs).\n$$l(y, \\hat{y}) = max(0, 1-(y \\cdot \\hat{y}))$$\nAs can be observed, $y = \\pm1$\n"}
{"snippet": "best_cnn_model.evaluate(x_test,y_test)\n", "intent": "The best accuracy with CNN is 83%. This is achieved by number of filters equal to 100, kernel size equal to 7 and `relu` activation function.\n"}
{"snippet": "pred_BenchmarkModel = [np.argmax(BenchmarkModel.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(pred_BenchmarkModel)==np.argmax(test_targets, axis=1))/len(pred_BenchmarkModel)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "y_pred = log_model.predict(X_test)\n", "intent": "Now we use the classifier to label the evaluation set we created earlier:\n"}
{"snippet": "lines = testDf[\"line\"]                    \nX_test = vectorizer.transform( lines ) \npredIdx = multNB.predict(X_test)       \npredC = analyze[predIdx]                  \n", "intent": "That worked out well, but how do we know, how good the predictions work over all?\nLuckily, sklearn comes with some nice metric functions.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(  realValue\n               , predicted)\n", "intent": "In common sense we would just calculate the accuracy (in German: Treffsicherheit) like:\n``` \n  sum of true / sum of all = 3 /4 = 0.75\n```\n"}
{"snippet": "realValue = [0,0,1,1]\npredicted = [0,1,1,1] \nprint classification_report( realValue \n                            ,predicted\n                            ,target_names=[\"ZERO\",\"ONE\"])\n", "intent": "But let's have a look at the first example:\n"}
{"snippet": "y_pred = gs.predict(X_test)\nprecision_score(y_test, y_pred)\n", "intent": "* The precision is 0.30, which is based on default threshold 50%\n"}
{"snippet": "mean_squared_error(y, y_lr)\n", "intent": "Let's calculate the MSE for simple regression model:\n"}
{"snippet": "mean_squared_error(y, y_lrp)\n", "intent": "Let's calculate the MSE for multiple regression model:\n"}
{"snippet": "mean_squared_error(y_new, y_lr_new)\n", "intent": "MSE for simple regression on new data:\n"}
{"snippet": "mean_squared_error(y_new, y_lrp_new)\n", "intent": "MSE for multiple regression on new data:\n"}
{"snippet": "import scipy as scs\npredictions = results3.predict()\nscs.stats.describe(predictions)\n", "intent": "From here, we predict the ratings and examine to determine how the model performs.\n"}
{"snippet": "predict_proba = model.predict(X_test)\ny_pred = [np.argmax(x) for x in predict_proba]\ny_true = [np.argmax(x) for x in y_test]\nprint(\"Accuracy:\", accuracy_score(y_pred, y_true))\nprint(\"Precision:\", precision_score(y_pred, y_true, average='macro'))\nprint(\"Recall:\", recall_score(y_pred, y_true, average='macro'))\n", "intent": "Let's compute the model's accuracy, average precision and average recall on the test set.\n"}
{"snippet": "target = 'Outcome'\nbaseline = np.full(536, 0.5)\nprint(\"Baseline: \",log_loss(df_test[target],baseline))\nprint(\"Completely Wrong: \",log_loss(df_test[target],1-df_test[target]))\n", "intent": "- Log Loss value for all predicted probabilites == 0.5\n- Log Loss value if every prediction was wrong with 100% confidence\n"}
{"snippet": "preds = clf.predict_proba(X_test)[:,1]\nclipped_preds = np.clip(preds, 0.05, 0.95)\ndf_sample_sub.Pred = clipped_preds\ndf_sample_sub.head()\n", "intent": "Create predictions using the logistic regression model we trained.\n"}
{"snippet": "target = 'Outcome'\nbaseline = np.full(134, 0.5)\nprint(\"Baseline: \",log_loss(df_test_2018[target],baseline))\nprint(\"Completely Wrong: \",log_loss(df_test_2018[target],1-df_test_2018[target]))\n", "intent": "Just to reground ourselves to the baseline log-loss scores...\n"}
{"snippet": "probs1_final = lr.predict_proba(ss.transform(df_2018[features1]))\nprobs2_final = lr2.predict_proba(df_2018[features2])\nX_ens_final = np.asarray((probs1_final[:, 1],probs2_final[:, 1])).T\nprobs_ens_final = ens1.predict_proba(X_ens_final)\n", "intent": "Final predictions on all potential matchups, instead of only observed games.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.zeros([])\n    for i in range(len(style_layers)):\n        cur_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(cur_gram - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "user_id = 3; movie_id = 5\np = model.predict([np.array([user_id]), np.array([movie_id])])\nprint ('User %d would likely rate movie \\'%s\\' at %f'%(user_id, movie_names[movies[movie_id]], p))\n", "intent": "We can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, the following predicts that user \n"}
{"snippet": "print (cross_val_score(clf, X, y, cv=5))\n", "intent": "A better way to evaluate\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(purchaseAmount, p4(pageSpeeds))\nprint(r2)\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n", "intent": "Once the model is fit, we can estimate the performance of the model on the train and test datasets.\n"}
{"snippet": "pred_X = gcf.predict(X_te)\nprint(pred_X)\n", "intent": "<p>Now checking the prediction for the test set:<p>\n"}
{"snippet": "pred_X = gcf.predict(X_te)\nprint(pred_X)\n", "intent": "<p> ... and predicting classes ... </p>\n"}
{"snippet": "print((FP + FN) / (TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred))\n", "intent": "Also known as \"Misclassification rate\"\n"}
{"snippet": "print(TP / (TP + FN))\nprint(metrics.recall_score(y_test, y_pred))\n", "intent": "How \"sensitive\" is the classifier to detecting positive instances?\nAlso known as \"True Positive Rate\" or Recall\n"}
{"snippet": "print(TP / (TP + FP))\nprint(metrics.precision_score(y_test, y_pred))\n", "intent": "How \"precise\" is the classifier when predicting positive instances?\n"}
{"snippet": "y_pred_list = pipeline2.predict(X_test).tolist()\ny_test_list = y_test.tolist()\nfor index in range(len(y_pred_list))[:10]:\n    if y_pred_list[index] != y_test_list[index]:\n        print('[pred]: ', y_pred_list[index])\n        print('[true]: ', y_test_list[index])\n        print('[doc ]: ', X_test.iloc[index])\n        print()\n", "intent": "The score seems not so great. Let's look at some misclassified documents to see what is going on...\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)  \n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "RMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction))\nprint(RMSE)\n", "intent": "A common measure to evaluate Linear Regression Accuracy is to use the Root Mean Square Error.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions=lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "We can measure the performance of this model using Root Mean Squared Error (RMSE).\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(ytrain_4,y_scores)\n", "intent": "The graph shows that higher the recall, higher the specificity. We can also measure the area under the curve (AUC).\n"}
{"snippet": "roc_auc_score(ytrain_4,y_scores_forest)\n", "intent": "Now you can see that the RandomForestClassifier ROC curve looks much better. We can confirm this by looking at the ROC AUC score:\n"}
{"snippet": "cross_val_score(sgd_clf, Xtrain,ytrain,cv=3,scoring=\"accuracy\")\n", "intent": "Just with any other classifier, we want to evaluate it on cross-validation. Let's look at *SGDClassifier*'s accuracy:\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\nprint metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "By looking at its accuracy and its [area under the curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n"}
{"snippet": "from sklearn import metrics\npredicted_from_val = clf.predict(X_val)\nexpected_from_val = y_val\nmetrics.accuracy_score(expected_from_val, predicted_from_val)\n", "intent": "**Step 6:** Identify the model's accuracy score on the validation and test sets, and the precision, recall, and F1 scores on the test set:\n"}
{"snippet": "y_ = np.argmax(y_test,1) \nprint metrics.classification_report(y_true= y_, y_pred= y_pred_lb)\nprint metrics.confusion_matrix(y_true= y_, y_pred= y_pred_lb)\nprint(\"Classification accuracy: %0.6f\" % metrics.accuracy_score(y_true= y_, y_pred= y_pred_lb) )\nprint(\"Log Loss: %0.6f\" % metrics.log_loss(y_true= y_, y_pred= y_pred) )\n", "intent": "Accuracy is depend on the number of epoch that you set in partametrs part.\n"}
{"snippet": "dev_predictions = predict(dev_x, saved_weights, saved_biases, conv_net)\n", "intent": "Didn't help at all...let's look at some of the bad predictions from the dev set.\n"}
{"snippet": "predicted_values = model.predict(df_raw[['TotalSF']])\n", "intent": "We can get our predicted values using the `predict` method, instead of calculating it ourselves:\n"}
{"snippet": "def get_encoding(model, img):\n    imag = image_preprocess(image_path + img)\n    prediction = model.predict(imag)\n    features = np.reshape(prediction, prediction.shape[1])\n    return features\n", "intent": "To actually make predictions we have to pass this array into the VGG16 Model. This is done in the function below.\n"}
{"snippet": "prfs(dftest['Class'], model.predict(df_pca_features_test[features2]))\n", "intent": "The accuracy is still 96%.\n"}
{"snippet": "fpr2, tpr2, thresholds2 = roc_curve(dftest['Class'], model.predict_proba(df_pca_features_test[features2])[:,1], drop_intermediate=False)\n", "intent": "The recall remains the same.\n"}
{"snippet": "y_sj_test = pd.Series(map(int, map(round, best_sj_model.predict(X_sj_temp_test))))\ny_iq_test = pd.Series(map(int, map(round, best_iq_model.predict(X_iq_temp_test))))\n", "intent": "Our predictions are much more muted and unable to capture major fluctuations in total dengue cases across the two areas.\n"}
{"snippet": "sensitivity = TP / float(FN + TP)\nprint(sensitivity)\nprint(metrics.recall_score(y_test,bayes.predict(X_test)))\n", "intent": "The accuracy for this model is about 78%\n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, svc.predict(X_test)))\n", "intent": "From the confusion matrix, we observe that using this model:\nWe misclassified 14 spam as non-spam emails and 3 non-spam emails as spam emails.\n"}
{"snippet": "sensitivity = TP / float(FN + TP)\nprint(sensitivity)\nprint(metrics.recall_score(y_test,svc.predict(X_test)))\n", "intent": "The accuracy for this model is about 72%\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, svc.predict(X_test)))\n", "intent": "The classifier is highly specific, which means it is good at predicting mails as spam. \n"}
{"snippet": "some_data = X_train[1:6]\nsome_labels = y_train[1:6]\nprint(\"Predictions:\", lr.predict(some_data))\n", "intent": "Visualize what the model is predicting compared to the real labels:\n"}
{"snippet": "lr_mae = mean_absolute_error(y_train, train_X_predictions)\nlr_mae\n", "intent": "An RMSE above 0.5 is not a good indication of the model fit.\n"}
{"snippet": "lin_scores = cross_val_score(lr, X_train, y_train,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Grid search will use cross_val_score based on maximizing a utility performance measure. So we maximize(-MSE) here.\n"}
{"snippet": "x1_test = uniform(-1.0, -0.001, 5)\nx2_test = uniform(0.001, 1.0, 5)\nlabels1_test = classifier_1d.predict(x1_test)\nlabels2_test = classifier_1d.predict(x2_test)\nprint(\"prediction x1_test\")\nprint(labels1_test) \nprint(\"prediction x2_test\")\nprint(labels2_test) \n", "intent": "Generating some test data to verify that the neural network can classify successfully.\n"}
{"snippet": "start = time.time()\nlabels = kmeans_fit.labels_\nsilhouette=metrics.silhouette_score(tfs, labels, metric='euclidean')\nend = time.time()\nprint end-start\nprint \"Score:\",silhouette\n", "intent": "<img src=\"https://github.com/Yagwar/TAMD/blob/master/full%202.PNG?raw=true\">\n"}
{"snippet": "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_mlr, prediction)))\nprint('Mean Absolute Error:', mean_absolute_error(y_mlr, prediction))\nprint('Correlation Coefficient:', np.corrcoef(y_mlr, prediction)[0][1])\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "[accuracy_score(tree.predict(X_test), y_test) for tree in rf.estimators_]\n", "intent": "Check accuracy of each single tree\n"}
{"snippet": "[accuracy_score(tree.predict(X_test), y_test) for tree in rf.estimators_]\n", "intent": "Check accuracy of the single trees\n"}
{"snippet": "[accuracy_score(tree.predict(X_test), y_test) for tree in rf.estimators_]\n", "intent": "Again, check accuracy on individual trees\n"}
{"snippet": "accuracy_score(y_pred, y_test)\n", "intent": "Check overall accuracy\n"}
{"snippet": "[accuracy_score(y_pred, y_test) for y_pred in gb.staged_predict(X_test)]\n", "intent": "Overall accuracy is a bit better, let's check the accuracy at every stage.\n"}
{"snippet": "knn.predict([[3, 5, 4, 2]])  \n", "intent": "<h3> Make prediction\n"}
{"snippet": "prediction = react.predict(\"Happy Mother's Day from the Chicago Cubs!\")\nprint(prediction)\n", "intent": "The `predict` function returns an `OrderedDict` of the five predicted reactions to the given text, sorted from strongest to weakest.\n"}
{"snippet": "np.mean((bos.PRICE-lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\ntest_accuracy = np.mean(predictions == testY[:,0], axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "This shows the accuracy of our model.\n"}
{"snippet": "auc_knn = sklearn.metrics.roc_auc_score(y_test, y_scoreknn)\nprint(\"AUROC value:\", auc_knn)\n", "intent": "And, we calculate the AUROC value below:\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['<=50K','>50K']\nprint(classification_report(y_test,y_pred,target_names=target_names))\n", "intent": "<h4>Classification Report</h4>\n"}
{"snippet": "pred=model.predict(X_test)\nprint(classification_report(y_test,pred))\nprint ('\\n')\nprint (confusion_matrix(y_test,pred))\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predict=rfc.predict(X_test)\n", "intent": "** Predicting the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test,predict))\nprint ('\\n')\n", "intent": "**Classification report from the results**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_train, y_train_predictions)\n", "intent": "*Evaluate training performance*\n"}
{"snippet": "y_pred = lr.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "The 5-fold cross validation accuracy score is lower than our baseline. The model fares even worse when used on the test set:\n"}
{"snippet": "small_model.predict(newX)[1]\n", "intent": "Let's visualize the small tree here to do the traversing for this data point.\n"}
{"snippet": "ypred = model_5.predict(Xval)\nvalidation_data['pred']=ypred\nvalidation_data[['pred','safe_loans']].head()\nFP=validation_data.ix[validation_data['pred']==1][validation_data['safe_loans']==-1]\nnumFP=len(FP)\nprint('number of False positive :', numFP)\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "error_all = []\nfor n in range(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(train_data[target], predictions)\n    error_all.append(error)\n    print(\"Iteration %s, training error = %s\" % (n, error_all[n-1]))\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true = test_data['sentiment'].as_matrix(), \n                                                   y_pred = predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_true = test_data['sentiment'].as_matrix(),\n                                             y_pred = predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(y_true = test_data['sentiment'].as_matrix(), \n                                        y_pred = predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true = test_data['sentiment'].as_matrix(),\n                                        y_pred = predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "score = mlp.evaluate(p.X, to_categorical(p.y))\nprint(\"\\n\\nloss: {} | train acc: {}\".format(score[0], score[1]))\n", "intent": "Keras also provides a handy function that alows us to eveluate our models on the metrics that we previously specified. For example:\n"}
{"snippet": "train_score = mlp.evaluate(X_train, to_categorical(y_train))\nprint(\"\\n\\ntrain loss: {} | train acc: {}\\n\".format(train_score[0], train_score[1]))\ntest_score = mlp.evaluate(X_test, to_categorical(y_test))\nprint(\"\\n\\ntest loss: {} | test acc: {}\".format(test_score[0], test_score[1]))\n", "intent": "As you may recall, it is good practice to only compute test scores when you have already chosen the *best* model and only compute it **once**.\n"}
{"snippet": "test_data_features = count_vectorizer.transform(test_df['phrase_cleaned']).toarray()\nprint(test_data_features.shape)\nexport_to_kaggle(naive_bayes_clf.predict(test_data_features), 'naive_bayes_02')\n", "intent": "Surprisingly, Naive Bayes performed best using bag-of-words vectors! Let's first evaluate on Kaggle to get test result accuracy.\n"}
{"snippet": "ols.predict([[10,2000]])\n", "intent": "Fitted values can be obtained by calling ```predict()```:\n"}
{"snippet": "model.evaluate(x=X_test,y=y_test)\n", "intent": "Out-of-sample test can be conducted with ```model.evaluate()```:\n"}
{"snippet": "output = forest.predict(X_test)\n", "intent": "To inspect by hand the values of the prediction, use the method predict:\n"}
{"snippet": "y_test_pred = model.predict(X_norm_test)\n", "intent": "Predicting the labels on the test images.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_5, y_train_pred)\n", "intent": "$$ recall = \\frac{TP}{TP+FN}$$\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "$$ F_1 = 2\\times \\frac{precision\\times recall}{precision+recall}$$\n"}
{"snippet": "forest_clf.predict_proba([some_digit])\n", "intent": "You can call predict_proba() to get the list of probabilities that the classifier assigned to each instance for each class:\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\nprint(\"AUC: {}\".format(roc_auc_score(y_test==2, y_pred_prob)))\n", "intent": "AUC is essentially just a single metric that displays the area under the ROC curve.\n"}
{"snippet": "uboost_preds = uboost_dalitz.predict_proba(testX)\nada_preds = base_ada.predict_proba(testX[train_f])\n", "intent": "Make the predictions and plot the results\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n    target_names=twenty_test.target_names))\n", "intent": "scikit-learn further provides utilities for more detailed performance analysis of the results:\n"}
{"snippet": "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n", "intent": "The result of calling fit on a GridSearchCV object is a classifier that we can use to predict:\n"}
{"snippet": "pred = clf.pred(X_test)\nclf.check_error(pred, y_test.values)\n", "intent": "The iterations vs cost plot is used to check if the cost is being reduced per iteration\n"}
{"snippet": "print(\"Training error: \", metrics.mean_squared_error(regr.predict(X_train),y_train))\nprint(\"Test     error: \", metrics.mean_squared_error(regr.predict(X_test),y_test))\n", "intent": "The coefficients and the bias are now computed\n"}
{"snippet": "model_scores = {}\nfor name, model in models.items():\n    scores = cross_val_score(model, train, y, cv=3)\n    model_scores[name] = scores.mean()\n    print(name, model_scores[name])\n", "intent": "Now we'll just run the data through our models.\nWe use [cross validation](http://scikit-learn.org/stable/modules/cross_validation.html\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = torch.zeros(1)\n    for layer,target,weight in zip(style_layers, style_targets, style_weights):\n        gram = gram_matrix(feats[layer])\n        style_loss += weight * torch.sum((gram-target)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "nscore = accuracy_score(ny_true, ny_pred)\nscore = accuracy_score(y_true, y_pred)\nprint(\"% Normal relu accuracy predicting whether an employee has left or not:\", np.round(nscore,4)*100)\nprint(\"% Leaky ReLU accuracy predicting whether an employee has left or not:\", np.round(score,4)*100)\n", "intent": "Over all our data, it seems that the Leaky ReLU model seems to perform about 2% better.\n"}
{"snippet": "svm_params = {\"C\": 1, \"dual\": True}\npredictor = SVMPredictor(**svm_params)\nsplit_loss = predictor.evaluate(train_x, train_ys, method='split')\nprint(\"Split CV log loss: {}\".format(split_loss))\n", "intent": "Let's how our reduced input does using an (untuned) classifier from `sklearn`.\n"}
{"snippet": "print(\"Training set:\")\npred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\nprint('Test set:')\npred_test = predict(X_test, Y_test, W, b, word_to_vec_map)\n", "intent": "Let's examine model performance on the test set.\n"}
{"snippet": "predictions = model.predict(data_prepared)\n", "intent": "Let's see why some of the SMSs were flagged as not spam while in actual fact it was spam\n"}
{"snippet": "Resnet50_predictions = [np.argmax(transfer_Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50Data]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def content_loss(content, combination):\n    return backend.sum(backend.square(content-combination))\n", "intent": "The content loss is caculated as the Euclidean distance between feature representations of the content and the combination images.\n"}
{"snippet": "def  total_variation_loss(x):\n    a=backend.square(x[:,:height-1,:width-1,:]-x[:,1:,:width-1,:])\n    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return backend.sum(backend.pow(a + b, 1.25))\n", "intent": "To encourage spatial smoothness, we will add another term called [total_variation_loss](https://arxiv.org/abs/1412.0035) (a regularization term).\n"}
{"snippet": "dtrain = xgb.DMatrix(X, label = y, missing = float('NaN'))\nfitted_model = xgb.train(\n                 {x.replace('_xgb', ''):y for x,y in best.iteritems()},\n                 dtrain,\n                 int(best['num_boost_round_xgb'])\n               )\nypred = fitted_model.predict(xgb.DMatrix(Xtest, missing = float('NaN')))>.5\nypredcv = fitted_model.predict(xgb.DMatrix(Xcv, missing = float('NaN')))>.5\n", "intent": "We train our tree and look at its performance metrics. Surprisingly, we didn't actually improve much over just looking at the random forest!\n"}
{"snippet": "def determineAnalysis(true, pred, regressor): \n    mae = mean_absolute_error(true, pred)\n    rmse = sqrt(mean_squared_error(true, pred))\n    r2 = r2_score(true, pred)\n    true, pred = np.array(true), np.array(pred)\n    print('Mean absolute error is ',mae)\n    print('Root mean squared error is ',rmse)\n    print('RSquare is ',r2)\n", "intent": "**Below are the methods written for determining summary metrics after analysis and plotting graphs of Actual values vs Predicted values**\n"}
{"snippet": "all_models = [dt_model, rf_model, gb_model]\nfor m in all_models:\n    print(cross_val_score(m, X, y, scoring=auc_scorer, cv=5)) \n", "intent": "It's good to check if our model will generalize to new data\n"}
{"snippet": "def gain_score(y_test,y_predict):\n     try:\n        return precision_score(y_test,y_predict)\n     except:\n        return 0.0\n", "intent": "> Gain = PV+ = lift*pi1 = TP/(TP+FN)\n"}
{"snippet": "rf_scores = cross_val_score(decision_tree_classifier, all_inputs, all_classes, cv=10)\nsb.boxplot(rf_scores)\nsb.stripplot(rf_scores, jitter=True, color='white')\n", "intent": "A striplot drawa strips of observations on top of a box plot:\n"}
{"snippet": "Machado_16 = [[157, 696, 37, 105, 96, 0, 6.9, 17.2, .239, .309, .294, .343, .533, 86, 236, 7, 23, .979, 0, 0, 1, 0, 0, 0, 0, 0]]\nMachado_WAR = model_2.predict(Machado_16)\nMachado_WAR\n", "intent": "Fangraphs had Kris Bryant at 6.6 WAR\n"}
{"snippet": "Russell_16 = [[151, 598, 21, 67, 95, 5, 9.2, 22.6, .179, .277, .238, .321, .417, 152, 388, 14, 58, .975, 0, 0, 0, 0, 0, 0, 0, 1]]\nRussell_WAR = model_2.predict(Russell_16)\nRussell_WAR\n", "intent": "Fangraphs had Manny Machado at 6.2 WAR\n"}
{"snippet": "Abreu_14 = [[145, 622, 36, 80, 107, 3, 8.2, 21.1, .264, .356, .317, .383, .581, 970, 69, 6, 100, .994, 1, 0, 0, 0, 0, 0, 0, 0]]\nAbreu_WAR = model_2.predict(Abreu_14)\nAbreu_WAR\n", "intent": "Fangraphs had Addison Russell at 3.3 WAR\n"}
{"snippet": "Alexei_14 = [[145, 506, 6, 38, 48, 8, 4.2, 12.5, .092, .265, .241, .277, .333, 162, 317, 14, 83, .972, 0, 0, 0, 0, 0, 0, 0, 1]]\nAlexei_WAR = model_2.predict(Alexei_14)\nAlexei_WAR\n", "intent": "Fangraphs had Jose Abreu at 5.3 WAR\n"}
{"snippet": "Heyward_16 = [[142, 592, 7, 61, 49, 11, 9.1, 15.7, .094, .266, .230, .306, .325, 218, 4, 2, 0, .991, 0, 0, 0, 0, 0, 0, 1, 0]]\nHeyward_WAR = model_2.predict(Heyward_16)\nHeyward_WAR\n", "intent": "Fangraphs Alexei Ramirez at -1.5 WAR\n"}
{"snippet": "from sklearn import metrics\nrmse1 ={}\ny_pred_lassocv_fit_scale = lassocv_fit_scale.predict(X_test_scaled)\nrmse1['Lasso_CV_Scaled']=np.sqrt(metrics.mean_squared_error(y_test, y_pred_lassocv_fit_scale))\ny_pred_lassocv_fit = lassocv_fit.predict(X_test_scaled)\nrmse1['Lasso_CV']=np.sqrt(metrics.mean_squared_error(y_test, y_pred_lassocv_fit))\ny_pred_elasticcv_fit_scale = elasticcv_fit_scale.predict(X_test_scaled)\nrmse1['ElasticNet_CV_Scaled']=np.sqrt(metrics.mean_squared_error(y_test, y_pred_elasticcv_fit_scale))\nrmse1\n", "intent": "Interpret Regression Metrics for each of your models. Choose one of the following:\n* R2\n* MSE / RMSE\n* MAE\nWhat are your top 3 performing models? \n"}
{"snippet": "label_B_predict = svc.predict(data_B_stack)\n", "intent": "Make predictions for the entire data in test data for Guatemala Earthquake.\n"}
{"snippet": "label_sem_B_predict = svc_sem.predict(data_sem_B_stack)\n", "intent": "Predict the labels for test data.\n"}
{"snippet": "ws = np.linspace(-10,40,100)\nfor b in [0]: \n    errs = []\n    for w in ws:\n        errs.append(mean_squared_error(y_train, predict(x_train, w, b)))\n    pylab.plot(ws, errs)\npylab.legend(['bias = 0'])\npylab.grid()\npylab.xlabel('Weight w')\npylab.ylabel('Cost: Mean squared error')\n", "intent": "Error function shows how the weight *w* affects the error. Notice that we did not study bias term *b* in the sake of simplicity.\n"}
{"snippet": "x_test = np.array(['I have completed my homeworks'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, train_x, train_y_5, cv = 4, scoring = 'accuracy')\n", "intent": "it is not a good measure in this case, since the true values (equals 5) is only about 10% of total data set\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(features, axis=0))) for features in test_Resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_predictions = loaded_model.predict(x_test[:5])\n", "intent": "Make test predictions to check that the model has been loaded correctly.\n"}
{"snippet": "print ('Percent correct on Test Data : {}'.format(accuracy_score(y_true=y_test, y_pred=dt_test_pred)*100) )\npd.crosstab(index=np.ravel(y_test), columns=np.ravel(dt_test_pred), rownames=['Survived'], colnames=['Predicted'])\n", "intent": "Just Looking at Test Results\n"}
{"snippet": "num_trees = []\ntrain_errs = []\nfor i, y_pred in enumerate(gbt.staged_predict(X_train)):\n    num_trees.append(i)\n    train_errs.append(zero_one_loss(y_train, y_pred))\ntest_errs = []\nfor i, y_pred in enumerate(gbt.staged_predict(X_test)):\n    test_errs.append(zero_one_loss(y_test, y_pred))\n", "intent": "GBT has a staged_predict fn that shows what the prediction would make after each tree in the ensemble run\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict  = model.predict(X_test)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "print(classification_report(y_test, y_test_hat, digits=6))\n", "intent": "With the function `classification_report()` we print get the precision, recall per each class.\n"}
{"snippet": "y_hat_10 = (y_test_hat_probs > 0.10)*1\nprint('Confusion matrix:\\n', confusion_matrix(y_test, y_hat_10))\nprint(classification_report(y_test, y_hat_10, digits=4))\n", "intent": "If we set the threshold down to 10%, we can detect around 75% of all fraud case but almost double our false positive rate (now 25 false alarms)\n"}
{"snippet": "roc_auc_score(y_test, y_test_hat) * 100\n", "intent": "The AUC score of the test date is around 98%.\n"}
{"snippet": "probs_keras = model.predict(X_test)\n", "intent": "The prediction is also a one liner\n"}
{"snippet": "loan_df['forecast_final_log'] = results_AR.predict(start = loan_df_shape_old, end = loan_df.shape[0], dynamic= True)\nloan_df['forecast_final_cumsum'] = loan_df['forecast_final_log'].cumsum()\nloan_df['forecast_final_Running_cumsum'] = predictions_ARIMA_log.ix[-1] + loan_df['forecast_final_log'].cumsum()\nloan_df['forecast_final'] = np.exp(loan_df['forecast_final_Running_cumsum'])\n", "intent": "Finally we predict the amount of funds needed in the next 24 months and plot the results.\n"}
{"snippet": "Y_web_test_predicted, probabilities = predict(model_parameters,training_parameters,model,X_web_test)\n", "intent": "`predict` returns predicted labels and associated probabilities of the trained model for each image it considers:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted = clf.predict(X_dev_counts)\n", "intent": "Predict the classes of dev data\n"}
{"snippet": "f1_baseline = f1_score(Y_dev,Y_baseline, average='weighted')\nf1_dev_logistic = f1_score(Y_dev, y_predicted_dev_logistic, average='weighted')\nf1_dev_SVM = f1_score(Y_dev, y_predicted_dev_SVM, average='weighted') \nprint(\"===== Traditional ML algorithms F1-scores ====\")\nprint(\"Logistic Regression: {0:.2f}\".format(f1_dev_logistic))\nprint(\"Support Vector Machine: {0:.2f}\".format(f1_dev_SVM))\n", "intent": "As you can see the ML algorithms performans slightly better than the baseline\n"}
{"snippet": "f1_baseline = f1_score(Y_test,Y_baseline, average='weighted')\nf1_dev_logistic = f1_score(Y_test, y_predicted_dev_logistic, average='weighted')\nf1_dev_SVM = f1_score(Y_test, y_predicted_dev_SVM, average='weighted') \nprint(\"===== Traditional ML algorithms F1-scores ====\")\nprint(\"Logistic Regression: {0:.2f}\".format(f1_dev_logistic))\nprint(\"Support Vector Machine: {0:.2f}\".format(f1_dev_SVM))\n", "intent": "As you can see the ML algorithms performans slightly better than the baseline\n"}
{"snippet": "print (np.average((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "client_data = [[5, 17, 15], \n               [4, 32, 22], \n               [8, 3, 12]]  \nfor i, price in enumerate(reg.predict(client_data)):\n    print (\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))\n", "intent": "- Earlier I chose max-depth instance to be at 4. For better traning score and lower validation error. This shows my choice is right\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "After we test the model again we see that we improved test accuracy up to 51%\n"}
{"snippet": "start = time.time()\ntrain_features = conv_base.predict(train_img, verbose=1)\nstop = time.time()\nprint(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")\n", "intent": "    Preprocess dataset with VGG model\n"}
{"snippet": "index = 5000\npredictions = model.predict(np.reshape(conv_base.predict(val_img[index].reshape([1,img_size,img_size,3])), (1, 6 * 6 * 512)))\n", "intent": "    Predict for one image\n"}
{"snippet": "predict('chair')\n", "intent": "--------------------------------------------------------------------------------------------------------------------------------\n"}
{"snippet": "predict('bathtub')\n", "intent": "--------------------------------------------------------------------------------------------------------------------------------\n"}
{"snippet": "train_img = train_images.reshape((nb_img_train, img_size * img_size))\ntrain_img = train_img.astype('float32') / 255\ntest_imgs = test_images.reshape([-1,img_size*img_size])\ntest_imgs = test_imgs.astype('float32') / 255\ntest_loss, test_acc = network.evaluate(test_imgs, test_lab)\n", "intent": "    Run on Test Set\n"}
{"snippet": "y_pred=linreg(['TV','Radio'],'Sales',X_test)\nrmse=round(np.sqrt(metrics.mean_squared_error(y_test,y_pred)),2)\nrmse\n", "intent": "- Run each code cell twice: to update X_test.\n"}
{"snippet": "C_value = 2.0\nLR = joblib.load('LRno_seasons.pkl') \npp = LR.predict_proba(test_data_no_seasons)\n", "intent": "These are run separately and then the final cells at the end export them to a Kaggle friendly format.\n"}
{"snippet": "C_value = 2.0\nLR = joblib.load('LRwith_L2.pkl')\npp = LR.predict_proba(test_data_all)\n", "intent": "No seasons kaggle score: 2.55255\n"}
{"snippet": "C_value = 2.0\nLR = joblib.load('LRno_months.pkl')\npp = LR.predict_proba(test_data_no_months)\n", "intent": "Kaggle score of 2.55263\n"}
{"snippet": "test_acc = (net.predict(X_test) == y_test).mean()\nprint('Test accuracy: ', test_acc)\n", "intent": "Now it's time to finally evaluate our model, with test set data.\n"}
{"snippet": "def evaluate(Y_pred, Y_true):\n    acc = tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y_true, 1))\n    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n    return acc\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint('MSE:', metrics.mean_squared_error(income[\"Income\"], y_pred))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "model.predict(train.FullDescription)\n", "intent": "Every time we predict the model will run the tf-idf part first, already fitted on the train set and then use the ridge regression model. \n"}
{"snippet": "InceptionV3_predictions = [np.argmax(mymodel_InceptionV3.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3] \ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "get_error(y,predictions)\n", "intent": "Hey, that looks better! What's the error look like? \n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "Step 4: evaluate your model\nNext we'll test the accuracy of our model using `classification_report`\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Step 3: predict using your test data\nNext make a variable called y_pred. y_pred will be a `predict`ion on `classifier` using the `X_test` data. \n"}
{"snippet": "def run_classifier(clf, X, y, penalty=None, random=1):\n    start_time = time()\n    kf = KFold(X.shape[0], random_state=random, shuffle=False)\n    predictions = cross_val_predict(clf, X, y, cv=kf) \n    metrics = compute_metrics(clf.__class__.__name__, predictions)\n    duration = int(round((time() - start_time) * 1000))\n    return metrics, duration, predictions\n", "intent": "Create a Generic Classifier Function\n"}
{"snippet": "print('Scores with all features: ')\nprint(cross_val_score(rfc, X_train, Y_train, cv=4))\nprint('Scores with feature importances: ')\nprint(cross_val_score(rfc, X_train_rfc, Y_train, cv=4))\nprint('Scores with RFE: ')\nprint(cross_val_score(rfc, X_train_rfe, Y_train, cv=4))\nprint('Scores with Select K Best: ')\nprint(cross_val_score(rfc, X_train_skb, Y_train, cv=4))\nprint('Scores with PCA: ')\nprint(cross_val_score(rfc, X_train_pca, Y_train, cv=4))\n", "intent": "Now let's see how they all compare! Again, we'll test them out with an unmodified Random Forest.\n"}
{"snippet": "def calculate_error(output):\n    n_examples = output.shape[0]\n    prediction = output[:, N_ftr]\n    target = output[:, N_ftr + 1]\n    misclass = np.where(target == prediction, 0, 1)\n    error = np.mean(misclass)\n    precision = 1. - error\n    return error, precision\n", "intent": "We now define a helper function that calculates the error (misclassification rate) and the accuracy. It will evaluate each tree that we build.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i,l in enumerate(style_layers):\n        style_current = gram_matrix(feats[l])\n        loss += style_weights[i]*tf.reduce_sum((style_current-style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X, y, cv=10)\nprint(\"Cross-validation scores: {}\".format(scores))\n", "intent": "Is it already stratified???\n"}
{"snippet": "X_test = [] \ny_pred = clf.predict(___) \ntarget_names[y_pred]\n", "intent": "Add some of your own data and make a prediction in the cell below.\nData could be a single x, y point or array of x, y points. e.g. `[[0, 5]]`\n"}
{"snippet": "print(accuracy_score(ytest, ypred))\nprint(confusion_matrix(ytest, ypred))\n", "intent": "What is our prediction accuracy ? How best to see it ?\n* Accuracy score\n* Confusion Matrix\n"}
{"snippet": "for name, model in fitted_models.items():\n    pred = model.predict_proba(X_test)\n    pred = [p[1] for p in pred]\n    fpr, tpr, thresholds = roc_curve(y_test, pred)\n    print('{} has an auc score of {}'.format(name, auc(fpr, tpr)))\n", "intent": "<span id='auroc'></span>\n"}
{"snippet": "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "This diagram indicates that the chosen model was correct.\n"}
{"snippet": "labels = kmeans.predict(df)\nlabels\n", "intent": "**Findings the label (here essentially the group number or the cluster number)**\n"}
{"snippet": "xtest = np.linspace(10, 15, 5)\nxtest=xtest[:, np.newaxis]\nytest = model.predict(xtest)\nytest\n", "intent": "Now let us try to predict values from x=10 to x=15\n"}
{"snippet": "preds = gnb.predict(test)\nprint(preds)\n", "intent": "Now lets pass the test data and get some predictions \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(test_labels, preds))\n", "intent": "**Calculating Accuracy Score**\n"}
{"snippet": "start = time()\nscore = cross_val_score(knn, X_test3, y_test3, cv=5)\nprint(\"KNN unweighted took %.2f seconds .\" % ((time() - start)))\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n", "intent": "Cross validation of the features dataframe (unweighted)\n"}
{"snippet": "start = time()\nscore_w = cross_val_score(knn_w, X_test3, y_test3, cv=5)\nprint(\"KNN weighted took %.2f seconds .\" % ((time() - start)))\nprint(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))\n", "intent": "Cross validation of the features dataframe (weighted)\n"}
{"snippet": "start = time()\nscore_3 = cross_val_score(knn, X_test3_f, y_test3_f, cv=5)\nprint(\"KNN unweighted took %.2f seconds .\" % ((time() - start)))\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score_3.mean(), score_3.std() * 2))\n", "intent": "The one that performed the best above was the unweighted full dataframe - let's try the cross validation of that:\n"}
{"snippet": "y_predict = clf_.predict( X_test )\ny_score = clf_.predict_proba( X_test )\n", "intent": "Now let's validate.\n"}
{"snippet": "print('R^2: ', r2_score(y_test, pred))\nprint('MAE: ', mean_absolute_error(y_test, pred))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "model.predict(X)\n", "intent": "Next, let's call the <code style=\"color:steelblue\">.predict()</code> function.\n"}
{"snippet": "pred = model.predict_proba(X[:10])\npred\n", "intent": "Call <code style=\"color:steelblue\">.predict_proba()</code> on the first 10 observations and display the results.\n"}
{"snippet": "pred = model.predict_proba(raw_data)\n", "intent": "**Let's see what happens when we try to apply our model to this raw dataset.**\n"}
{"snippet": "pred = model.predict_proba(augmented_data)\nprint( pred[:5] )\n", "intent": "**Predict probabilities for <code style=\"color:steelblue\">augmented_data</code> using your model.**\n* Then print the first 5 predictions.\n"}
{"snippet": "_, pred1 = retention_model.predict_proba(raw_data, clean=True, augment=True)\n_, pred2 = retention_model.predict_proba(cleaned_data, clean=False, augment=True)\n_, pred3 = retention_model.predict_proba(augmented_data, clean=False, augment=False)\n", "intent": "If implemented correctly, these next three statements should all work.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.7f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((gram-style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred, sigma = gp.predict(x_pred2D, return_std=True)\n", "intent": "** Get mean and eith standard deviation (return_std=True) or covariance (return_cov=True) of conditional predictive distribution. **\n"}
{"snippet": "gp.compute(x, yerr)\ny_pred, sigma = gp.predict(y, x_pred)\n", "intent": "** recompute covariance matrix with best fit hyperparameters.\nGet mean and covariance of conditional predicitve distribution **\n"}
{"snippet": "gp.compute(x, yerr)\ny_pred, sigma = gp.predict(y, x_pred, return_var=True)\n", "intent": "** recompute covariance matrix with new hyperparameters, get mean and variance of predictive distribution **\n"}
{"snippet": "print(\"mean squered error:\",metrics.mean_squared_error(yc, rec_yc))\n", "intent": "We can compute MSE on our reconstruction to check it similarity with the original signal\n"}
{"snippet": "print(\"mean squered error:\",metrics.mean_squared_error(np.real(rec_wave), np.real(clean_signal)))\n", "intent": "We can evaluate our model using such a metrics as for example mean squered error. We can find this metric in sklearn library\n"}
{"snippet": "predictions = logmodel.predict(x_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(clf.predict(X)[:10])\n", "intent": "clf.**predict()** gives the **predicted label**\nclf.**predict_proba()** gives the **predicted probability**\n"}
{"snippet": "def cutoff_predict(clf, X, cutoff):\n    return (clf.predict_proba(X)[:,1] > cutoff).astype(int)\n", "intent": "Function to compute prediction from **trained classifier, training dataset** and **cutoff value(threshold)**.\n"}
{"snippet": "def custom_f1(cutoff):\n    def f1_cutoff(clf, X, y):\n        ypred = cutoff_predict(clf, X, cutoff)\n        return sklearn.metrics.f1_score(y, ypred)\n    return f1_cutoff\n", "intent": "**Evaluate different cutoff values under different train/test splittings** using **cross-validation**.\n"}
{"snippet": "VGG16_predictions = [np.argmax(vgg_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_predictions = best_clf.predict(X_test)\nsummary_metrics(y_test, test_predictions)\n", "intent": "Let's look at our test set to see how we would perform with this model\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n", "intent": "Creating confusion matrix for the test data, we get an accuracy of 99%\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\ny_pred_perceptron = cross_val_predict(perceptron_clf, X, y, cv=10)\ndef percentageConfMat(confMat):\n    return np.around((confMat/np.sum(confMat,axis=1)[:,None])*100,2)\nprint('Average accuracy:')\nprint(metrics.accuracy_score(y, y_pred_perceptron))\nprint('Percentual confusion Matrix:')\nconf_mat_perceptron = metrics.confusion_matrix(y, y_pred_perceptron)\npercentageConfMat(conf_mat_perceptron)\n", "intent": "Let's take a look where are our mistakes. A confusion matrix is suitable to answer those questions;\n"}
{"snippet": "y_pred_perceptron_1hot = cross_val_predict(perceptron_clf, X_1hot, y, cv=10)\nprint('Average accuracy:')\nprint(metrics.accuracy_score(y, y_pred_perceptron_1hot))\nprint('Percentual confusion Matrix:')\nconf_mat_perceptron = metrics.confusion_matrix(y, y_pred_perceptron_1hot)\npercentageConfMat(conf_mat_perceptron)\n", "intent": "Let's see how the Perceptron perfoms under a dataset one-hot encoded.\n"}
{"snippet": "y_pred_best_knn = cross_val_predict(grid_search_knn.best_estimator_, X_1hot, y, cv=10)\nprint('\nprint('Average accuracy:')\nprint(metrics.accuracy_score(y, y_pred_best_knn))\nprint('Percentual confusion Matrix:')\nconf_mat_bestKnn = metrics.confusion_matrix(y, y_pred_best_knn)\nprint(percentageConfMat(conf_mat_bestKnn))\n", "intent": "Does not seem much improvment by looking at the overall accuracy.\n"}
{"snippet": "scores_ols_reg = cross_val_score(ols_reg, X_train, y, scoring='neg_mean_squared_error', cv=10)\nscores_sgd_reg = cross_val_score(sgd_reg, X_train, y, scoring='neg_mean_squared_error', cv=10)\nscores_rf_reg = cross_val_score(rf_reg, X_train, y, scoring='neg_mean_squared_error', cv=10)\n", "intent": "As said before, no need to worry about hyperparameters tunning **right now**. Scikit-learn has proper values by default.\n"}
{"snippet": "X_test = data_test_std[:,:-1]\ny_test_target = data_test_std[:,-1]\ny_pred = sgd_reg.predict(X_test)\ny_pred_x0 = sgd_reg_x0.predict(X_test[:,0].reshape((X_test[:,0].shape[0],1)))\n", "intent": "Now, let's fed in our model!\n"}
{"snippet": "X_test = data_test_std[:,:-1]\ny_test_target = data_test_std[:,-1]\ny_pred = grid_search_sgd.best_estimator_.predict(X_test)\ny_pred_newFeature = sgd_reg_newFeatures.predict(selector.transform(X_test))\n", "intent": "Now, let's fed in our model!\n"}
{"snippet": "item_prediction = predict(train_matrix_small, item_similarity, type='item')\nuser_prediction = predict(train_matrix_small, user_similarity, type='user')\n", "intent": "Then can make our predictions!\n"}
{"snippet": "expected_dtest = y_dtest\npredicted_dtest = model.predict(X_dtest)\nresults = ['correct class' if e==p else 'wrong class' for e, p in zip(expected_dtest, predicted_dtest)]\nCounter(results)\n", "intent": "The test data set is used on the classification model in order to test its performance.\n"}
{"snippet": "expected_dtest = y_dtest\npredicted_dtest = model.predict(X_dtest)\nresults = ['correct class' if e==p else 'wrong class' for e, p in zip(expected_dtest, predicted_dtest)]\nCounter(results)\n", "intent": "Test data set is used on the classification model in order to test its performance.\n"}
{"snippet": "cr_dtest = metrics.classification_report(expected_dtest, predicted_dtest)\ncm_dtest = metrics.confusion_matrix(expected_dtest, predicted_dtest, labels=class_names)\nprint cr_dtest\nprint cm_dtest\n", "intent": "The models classifies all samples of the training data set correctly: precision, recall and f1-score of 1, and a confusion matrix as diagonal matrix.\n"}
{"snippet": "for name,model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    means.append(cv_results.mean())\n    stds.append(cv_results.std())\n", "intent": "http://scikit-learn.org/stable/modules/cross_validation.html\n"}
{"snippet": "y_pred = superlearnerClf.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the trained classifier with predict funtion of superlearner classifier\n"}
{"snippet": "y_pred = superlearnerClf.predict_proba(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred.argmax(axis=1)) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred.argmax(axis=1)))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_pred.argmax(axis=1), rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the trained classifier with predict_proba funtion of superlearner classifier\n"}
{"snippet": "scores=cross_val_score(superlearnerClf,X_train_plus_valid,y_train_plus_valid,cv=10)\nprint(scores)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "y_tuned_pred = my_tuned_superlearner.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_tuned_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_tuned_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_tuned_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the performance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "print(\"Evaluating DeepTune ...\", file=sys.stderr)\ndeeptune = evaluate(DeepTune())\ndeeptune.groupby('Platform')['Platform', 'Speedup', 'Oracle'].mean()\n", "intent": "Summary of results:\n"}
{"snippet": "score = accuracy_score(final_boost.predict(indep_test_data), dep_test_data)\nprint(\"Boost Accuracy on test data: {}\".format(score))\n", "intent": "<a id='boost_test_result'></a>\nLastly, let's run the model on the test data.\n"}
{"snippet": "def displayAccuracy(pred, label_test):\n    acc = accuracy_score(pred, labels_test)\n    print('Accuracy: {:.2%}'.format(acc))\n", "intent": "Metrics function to calculate the performace of our classifiers.\n"}
{"snippet": "msefull = np.mean((bos.PRICE - lm.predict(x))** 2)\nmsefull\n", "intent": "Let's calculate mean squared error\n"}
{"snippet": "print ' training error ' ,np.mean((Y_train - lm.predict(X_train))**2)\nprint ' testing error ', np.mean((Y_test - lm.predict(X_test))**2)\n", "intent": "Mean squared error for training and test data\n"}
{"snippet": "mn_nb_prediction = mn_nb.predict(X_test)\n", "intent": "<div class=\"span5 alert alert-info\">\n"}
{"snippet": "print('Accuracy score for MN NB classifier on training data: {:2.4f}%'\\\n      .format(accuracy_score(y_train, train_pred, )*100))\nprint('Accuracy score for MN NB classifier on test data: {:2.4f}%'\\\n      .format(accuracy_score(y_test, mn_nb_prediction)*100))\nprint('')\nprint('Classification report:\\n {}'.format(metrics.classification_report(y_test, mn_nb_prediction)))\nprint('')\nprint('Confusion Matrix:\\n {}'.format(metrics.confusion_matrix(y_test, mn_nb_prediction)))\n", "intent": "<div class=\"span5 alert alert-info\">\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "Classification accuracy: percentage of correct predictions\n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve**:`\n"}
{"snippet": "knn.predict([3, 5, 4, 2])\n", "intent": "- New observations are called \"out-of-sample\" data\n- Uses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "- Proportion of correct predictions\n- Common evaluation metric for classification problems\n"}
{"snippet": "pred_class = clf_dt_10.predict(X_2)\npred_proba = clf_dt_10.predict_proba(X_2)\n", "intent": "Lets understand first just the difference between **Class** prediction and **Class Probabilities**\n"}
{"snippet": "hit = \"hit: {1568:6 1416:5 3230:6 787:8 2757:4 0:13 980:4 3116:4 l}\"\nprint '(6,30)'\nprint regressor.predict(hit)\n", "intent": "<span>\nAnd finally test\n</span>\n"}
{"snippet": "y_pred = clf.predict(X_blind)\nblind['Prediction'] = y_pred\n", "intent": "Now it's a simple matter of making a prediction and storing it back in the dataframe:\n"}
{"snippet": "y_unknown = clf.predict(X_unknown)\nwell_data['Facies'] = y_unknown\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "import numpy as np\nfrom mlxtend.evaluate import lift_score\ny_target =    np.array([0, 0, 1, 0, 0, 1, 1, 1, 1, 1])\ny_predicted = np.array([1, 0, 1, 0, 0, 0, 0, 1, 0, 0])\nlift_score(y_target, y_predicted)\n", "intent": "This examples demonstrates the basic use of the `lift_score` function using the example from the *Overview* section.\n"}
{"snippet": "softmax_predictions = model.predict(test_data[:5])\nsoftmax_predictions\n", "intent": "Doing the inference\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy', verbose=100)\n", "intent": "Using the built-in CV\n"}
{"snippet": "y_train_pred_90 = (y_scores > 150000)\nprecision_score(y_train_5, y_train_pred_90)\n", "intent": "If we want a precision over 90%, we can select ~ 150000 as a threshold, there for, we don't use the defaut predict method, we use:\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy',verbose=100)\n", "intent": "Now let's evaluate our models\n"}
{"snippet": "preds = m.predict(tfidf_test)\n(preds==y_test).mean()\n", "intent": "Evaluating on test set\n"}
{"snippet": "test_preds = model.predict(test_sequences)\n", "intent": "Assessing where the model went wrong\n"}
{"snippet": "test_preds = model2.predict(test_sequences)\n", "intent": "Assessing where the model went wrong\n"}
{"snippet": "print ('Base Model Accuracy:', accuracy_score(y2_test, RF_clf_predict))\nprint ('New Model Accuracy :', accuracy_score(y2_test, new_model_predict))\n", "intent": "Compare Accurracy of both models\n"}
{"snippet": "print metrics.accuracy_score(y_test, predicted)\n", "intent": "Now let's generate some evaluation metrics.\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test, predicted)\n", "intent": "The accuracy comes out to be 46%.\nWe can also see the confusion matrix and a classification report with other metrics to improve our model.\n"}
{"snippet": "scores = cross_val_score(model2, X, y, scoring='accuracy', cv=10)\nprint scores\nprint scores.mean()\n", "intent": "<h2>Model Evaluation Using Cross-Validation</h2><br>\nNow let's try 10-fold cross-validation, to see if the accuracy holds up more rigorously.\n"}
{"snippet": "acc  = sklearn.model_selection.cross_val_score(model, training_features, training_labels, cv=5)\n", "intent": "Perform 5-fold cross validation (cv=5):\n"}
{"snippet": "predicted_labels = model.predict(features_test)\n", "intent": "Compute the predicted labels:\n"}
{"snippet": "predicted_labels = model.predict(test_features)\n", "intent": "To test the classifier on a set of (test) feature vectors, use the `predict` method:\n"}
{"snippet": "sklearn.metrics.recall_score(test_labels, predicted_labels)\n", "intent": "Compute the recall score, and verify it is correct:\n"}
{"snippet": "sklearn.metrics.precision_score(test_labels, predicted_labels)\n", "intent": "Compute the precision score, and verify:\n"}
{"snippet": "sklearn.metrics.f1_score(test_labels, predicted_labels)\n", "intent": "Compute the [F-measure](https://en.wikipedia.org/wiki/F1_score), and verify:\n"}
{"snippet": "y_test = model.predict(X_test)\n", "intent": "Use the trained neural network to make predictions from the test data:\n"}
{"snippet": "lin_arr=\"can I use this with an iphone\".split()\nclassifier.predict(model.infer_vector(lin_arr))\n", "intent": "<h1> Test on a sentence </h1>\n"}
{"snippet": "import warnings\nwarnings.filterwarnings('ignore')\nprediction=[]\nfor line_arr in qs_yn[-int(len(qs_yn)/2):]:\n    try:\n        prediction.append(classifier.predict(QmodelB[line_arr[0]]))\n    except KeyError:\n        prediction.append(1)\nsum(prediction)/len(prediction)\n", "intent": "<h3> test on real questions </h3>\n"}
{"snippet": "import warnings\nwarnings.filterwarnings('ignore')\nprediction=[classifier.predict(model.infer_vector(line_arr)) for line_arr in qs_yn[-2000:]]\n", "intent": "<h1> Test on data not used in traning </h1>\n"}
{"snippet": "lin_arr=\"do I need to\".split()\nclassifier.predict(model.infer_vector(lin_arr))\n", "intent": "<h1> user input </h1>\n"}
{"snippet": "y_pred = regr.predict(Xts)\nRSS = np.mean((y_pred-yts)**2)/np.std(yts)**2\nprint(\"Normalized RSS = \", RSS)\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "y_cpred = logreg_c.predict(Xs)\nprint(\"Multi-Class accuracy =\",np.mean(y_cpred==y_c))\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "testprediction=model.predict(features_test)\n", "intent": "That took a while, but it was still an awful lot shorter than the 15 minutes it took with C=1\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)\nprint(probs[:8, 1])\nprint(probs[:8, 0])\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size, verbose=0)\nprobs = model.predict_proba(val_data, batch_size=batch_size, verbose=0)\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\ncat_feature_position = 0\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,cat_feature_position]\nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "residual_sum_sqrs = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nresidual_sum_sqrs\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "msePTRATIO = np.mean((bos.PRICE - lm.predict(X[['PTRATIO']])) ** 2)\nprint (msePTRATIO)\n", "intent": "***\nTry fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town)\nCalculate the mean squared error. \n"}
{"snippet": "def plot_gen(G, n_ex=16):\n    plot_multi(G.predict(noise(n_ex)).reshape(n_ex, 28, 28), cmap='gray')\n", "intent": "This is just a helper to plot a bunch of generated images.\n"}
{"snippet": "def data_D(sz, G):\n    real_img = X_train[np.random.randint(0,n,size=sz)]\n    X = np.concatenate((real_img, G.predict(noise(sz))))\n    return X, [0]*sz+[1]*sz  \n", "intent": "Create a batch of some real and some generated data, with appropriate labels, for the discriminator.\n"}
{"snippet": "def eval_keras(input):\n    preds = model.predict(input, batch_size=128)\n    predict = np.argmax(preds, axis = 2) \n    return (np.mean([all(real==p) for real, p in zip(labels_test, predict)]), predict)\n", "intent": "To evaluate, we don't want to know what percentage of letters are correct but what percentage of words are.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\ny_pred = make_prediction(X_test, W)\nprint(classification_report(y_test, y_pred))\n", "intent": "It would be nice to get the f1-score of 0.9 or higher\n"}
{"snippet": "labels = km.predict(rdd).collect() \ncluster_centers = km.clusterCenters \n", "intent": "Gather the results from our model:\n"}
{"snippet": "pred = de_one_hot(SignClassifier.predict(X_test))\nactual = de_one_hot(y_test)\n", "intent": "As a final step, let's take a look at some incorrectly classified images from our non-googled testing set to analyze the main source of our error.\n"}
{"snippet": "def predict(x, w, b):\n    return x*w + b\n", "intent": "This method takes x, w, b and returns the value of y. <br/>\n\\begin{equation}\ny_i = w*x_i+b\n\\end{equation}\n"}
{"snippet": "y_pred = lm_model.predict(x)\ny_pred = [1 if p > 0.2 else 0 for p in y_pred]\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y, y_pred)\n", "intent": "Looks like 0.2 is a good cutoff? Let's pick that and look at the accuracy of the model\n"}
{"snippet": "predictions=clf.predict(pred_test)\n", "intent": "making preditions for the test set. \n"}
{"snippet": "def model_pred(data):\n    return (model.predict_proba(data = data)[:,1]).astype(bool)\n", "intent": "We can also create a prediction function if we want to consider probabilities:\n"}
{"snippet": "shap_values = model.predict(xgb_test, pred_contribs = True)\n", "intent": "Because the Tree SHAP algorithm is implemented in XGBoost we can compute exact SHAP values quickly over thousands of samples.\n"}
{"snippet": "x_new = rnorm(size=(100, 1))\ny_hat = m.predict(x_new)[0].reshape((100, 1))\n", "intent": "Prediction is fairly simple using `GPy`. We just plugin some new data into the `predict` function of the GP.\n"}
{"snippet": "residual_sum_of_squares = mean_squared_error(y, y_hat)\nr_squared = r2_score(y, y_hat)\nprint(\"RSS: \", residual_sum_of_squares)\nprint(\"R^2: \", r_squared)\n", "intent": "Compute the residual sum of squares and coefficient of determination $R^2$ to estimate the model quality:\n"}
{"snippet": "def predict(x_new):\n    x_new = x_new.reshape((1, x.shape[1]))\n    ret = 0\n    for i in range(x.shape[0]):\n        ret += alpha[i] * y[i] * rbf_kernel(x[i, :].reshape((1, x.shape[1])), x_new)\n    return numpy.sign(ret[0])\n", "intent": "Finally let's benchmark our implementation vs `sklearn`. First we implement a function to predict a new label\n"}
{"snippet": "y_new_hat = numpy.array([predict(xt)[0] for xt in x_test])\n", "intent": "Then we predict $\\hat{y}_{\\text{new}}$ for our implementation and the `sklearn` SVM.\n"}
{"snippet": "X_new = [[3,5,4,2],[5,4,3,2]]\nknn.predict(X_new)\n", "intent": "**Step 4:** Predict the response for a new observation\n"}
{"snippet": "X_test_encoded = encoder.predict(X_test)\n", "intent": "Now, we can compress MNIST images.\n"}
{"snippet": "X_test_decoded = decoder.predict(X_test_encoded)\n", "intent": "Let's decompress the compressed MNIST images.\n"}
{"snippet": "X_test_decoded = autoencoder.predict(X_test)\nshow_images(X_test, X_test_decoded)\n", "intent": "We just want to see the quality of compression/decompression.  So, let's just feed forward test images to see how the restored digits look like.\n"}
{"snippet": "X_test_decoded = autoencoder.predict(X_test_noisy)\nshow_images(X_test_noisy, X_test_decoded)\n", "intent": "We pass the noisy images to the autoencoder to see the restored images.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.median_absolute_error(true, pred)\n", "intent": "$$\\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i| $$\n"}
{"snippet": "print metrics.mean_squared_error(true, pred)\n", "intent": "$$\\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i)^2 $$\n"}
{"snippet": "import numpy as np\nprint np.sqrt(metrics.mean_squared_error(true, pred))\n", "intent": "$$\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i)^2}$$\n"}
{"snippet": "y_pred = linreg.predict(X_test)\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": " **RMSE for our sales predictions:**\n"}
{"snippet": "object_pftas_cols = [col for col in bbbc021_feature.columns if \"haralick\" not in col]\nscores = cross_validation.cross_val_score(classifier, \n                                          bbbc021_feature[object_pftas_cols].values, \n                                          bbbc021_merged[\"moa\"].values,\n                                          cv=5)\nsum(scores / 5)\n", "intent": "Wow! Object and PFTAS features do great on their own. What if we tried both?\n"}
{"snippet": "regression.predict(35)[0]\n", "intent": "Now that we have fit our prediction model to our data lets do cool stuff to it.\n"}
{"snippet": "school_enrollment_percentage = 80\nprint(\"Predicted Life Expectancy for School Enrollment % of {}%: \".format(school_enrollment_percentage),\n      regr1.predict(school_enrollment_percentage)[0])\n", "intent": "This is how we can predict specific values on our model:\n"}
{"snippet": "for i in range(1, 11):\n    print(\"School Enrollment: {}\".format(i*10), regr3.predict([i * 10, 100, 100]))\n    print(\"Sanitation Access: {}\".format(i*10), regr3.predict([100, i * 10, 100]))\n    print(\"Water Access: {}\".format(i*10), regr3.predict([100, 100, i * 10]))\n    print(\"-----\\n\")\n", "intent": "What can we precict with this new model?\n"}
{"snippet": "CNN_error = C.classification_error(CNN_Model, label)\n", "intent": "We also record the classification error between output and label (ground truth) during training.\n"}
{"snippet": "LogReg_error = C.classification_error(LogRegModel, label)\n", "intent": "We also record the classification error between output and label (ground truth) during training.\n"}
{"snippet": "MLP_error = C.classification_error(MLP_Model, label)\n", "intent": "We also record the classification error between output and label (ground truth) during training.\n"}
{"snippet": "predictions = cd_classifier.predict(X_test, as_iterable=True)\n", "intent": "Here we run the test set against the model we have created.\n"}
{"snippet": "Resnet50_pred = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_pred)==np.argmax(test_targets, axis=1))/len(Resnet50_pred)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = LR.predict(X)\n", "intent": "These are the coefficients for age, male and the constant.\nNow let's see, how the model is doing...\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.6f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.0001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.7f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        deviation = tf.reduce_sum(tf.square(gram - style_targets[i]))\n        style_loss += style_weights[i]*deviation\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.7f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "xception_predictions = [np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(X_val, Y_val, verbose=0)\nprint('Validation Accuracy:', score[1])\n", "intent": "**Validation Accuracy**:\n"}
{"snippet": "score = model.evaluate(X_val, Y_val, verbose=0)\nprint('Validation Accuracy:', score[1])\n", "intent": "**Validation Accuracy**: (fill in here)\n"}
{"snippet": "score = model.evaluate(X_val, Y_val, verbose=0)\nprint('Validation Accuracy:', score[1])\n", "intent": "**Best Validation Accuracy:**\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(test, axis=0))) for test in X_test]\n", "intent": "Try out the model on the test dataset of fashion-mnist images\n"}
{"snippet": "def z_score(df, column, mean, std):\n    return (df[column] - df[column].mean())/df[column].std(ddof=0)\nmean = hist.distance.mean()\nstd = hist.distance.std()\nfor d in datasets:\n    d['distance_normalized'] = z_score(d, 'distance', mean, std)\n", "intent": "In this section I'm going to use `z_score` normalization to `distance` column.\n"}
{"snippet": "y = model_int.predict(x)\n", "intent": "We then run `predict` method to get the outputs with the inputs `x`.\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat == y)\nprint(\"Accuracy on training data = %f\" % acc)\n", "intent": "We can next plot the accuracy on the training data.  We see we get over 96%.\n"}
{"snippet": "print(\"Acc: {:.3f}%\".format(100*np.mean(logreg.predict(Xs) == y)))\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "y_hat_test = regr.predict(Xtest)\n", "intent": "Measure the normalized RSS on the test data.  Is it substantially higher than the training data?\n"}
{"snippet": "y_tr_pred = regr.predict(X_tr)\nRSS_tr = np.mean((y_tr_pred-y_tr)**2)/(np.std(y_tr)**2)\nRsq_tr = 1-RSS_tr\nprint(\"RSS per sample = {0:f}\".format(RSS_tr))\nprint(\"R^2 =            {0:f}\".format(Rsq_tr))\n", "intent": "We next compute the RSS (per sample) and the R^2 on the training data\n"}
{"snippet": "score, acc = model.evaluate(Xts, yts, verbose=0)\nprint(\"accuracy = %f\" % acc)\n", "intent": "We can test the performance on the test data set.\n"}
{"snippet": "def predict(X,w):\n    z = X.dot(w[1:]) + w[0]\n    yhat = (z > 0)\n    return yhat\nyhat = predict(Xts,w)\nacc = np.mean(yhat == yts)\nprint(\"Test accuracy = %f\" % acc)\n", "intent": "We can measure the accuracy of the final estimate by creating a predict method.\n"}
{"snippet": "yhat = predict(Xts,w)\nacc = np.mean(yhat == yts)\nprint(\"Test accuracy = %f\" % acc)\n", "intent": "Finally we measure the accuracy.\n"}
{"snippet": "Xts = X[Iperm[ntr:],:]\nyts = y[Iperm[ntr:]]\nyhat = logreg.predict(Xts)\nacc = np.mean(yhat == yts)\nprint('Accuaracy = {0:f}'.format(acc))\n", "intent": "Now, we can measure the accuracy on the test data.\n"}
{"snippet": "yhat_ts = svc.predict(Xts)\nacc = np.mean(yhat_ts == yts)\nprint('Accuaracy = {0:f}'.format(acc))\n", "intent": "Measure the accuracy on the test data.  The prediction can take several minutes too -- SVMs are *very* slow!\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake).data.cpu().numpy()\n    g_loss = ls_generator_loss(score_fake).data.cpu().numpy()\n    print(\"Maximum error in d_loss: %.8g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %.8g\"%rel_error(g_loss_true, g_loss))\ndef np_to_var(input):\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    for i, layer_index in enumerate(style_layers):\n        style_current = gram_matrix(feats[layer_index])\n        layer_loss = style_weights[i] * (style_current - style_targets[i]).pow(2).sum()\n        style_loss = layer_loss if i == 0 else (style_loss + layer_loss)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted,pos_label=0))) \nprint('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted,pos_label=0)))\nprint('F1: {:.2f}'.format(f1_score(y_test, tree_predicted,pos_label=0)))\n", "intent": "Let's apply all the metrics using the tree_predicted array from the decision tree classifier: \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('Random class-proportional (dummy)\\n', \n      classification_report(y_test, y_classprop_predicted, target_names=['Malignant', 'Benign']))\nprint('SVM\\n', \n      classification_report(y_test, svm_predicted, target_names = ['Malignant', 'Benign']))\nprint('Logistic regression\\n', \n      classification_report(y_test, lr_predicted, target_names = ['Malignant', 'Benign']))\nprint('Decision tree\\n', \n      classification_report(y_test, tree_predicted, target_names = ['Malignant', 'Benign']))\n", "intent": "It allows you to print a full summary about the performance of your classifier towards all your labels in the target column\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_most_frequent=dummy.predict(X_test)\nconfusion_most_frequent=confusion_matrix(y_test, y_most_frequent) \nprint(confusion_most_frequent)\n", "intent": "let's see the confusion matrix for our dummy classifier :\n"}
{"snippet": "y18_pred = model.predict(pred18_df[['prev_month']])\ny18_pred\n", "intent": "**With this Model, get prediction for 2018 data**\n"}
{"snippet": "lr.predict(X_test)\n", "intent": "**Compare Predicted values to real values from test set**\n"}
{"snippet": "dfensembletest=pd.DataFrame.from_dict({'knn':knn_predictions,\n                                       'log':log_predictions, \n                                       'svm':svm_predictions,\n                                       'rf':rf_predictions,\n                                       'nb':nb_predictions, 'y':y_test.values})\nepreds = est.predict(dfensembletest[['knn', 'log', 'svm', 'rf', 'nb']].values)\ntestactual = y_test.values\n", "intent": "Now let's run the ensemble on the test values.\n"}
{"snippet": "train_predictions = grid.predict(X_train)\n", "intent": "The random forest model has a 91% R^2 score on the training data.\n"}
{"snippet": "test_predictions = grid.predict(X_test)\n", "intent": "The model has a 77.8% R^2 score on the test data.\n"}
{"snippet": "predictions = regressor.predict(x_train[numeric_column_names])\nprint(predictions[:5])\nsns.distplot(predictions);\n", "intent": "Before we do anything significant, let's take a look at the sales predictions with the current model:\n"}
{"snippet": "def mape_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "This competition is based on the MAP error.\n"}
{"snippet": "print(metrics.classification_report(testdata.target, predicted))\nAccuracy = metrics.accuracy_score(testdata.target, predicted)\nprint(\"Accuracy = \" + str(Accuracy))\nprint('Error Rate = ' + str(1-Accuracy))\n", "intent": "Using the array given by the prediction model, we find our accuracy is around 82%\n"}
{"snippet": "final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))\n", "intent": "Now try on the entire validation dataset\n"}
{"snippet": "from sklearn import metrics\nprint('--------SVM-------' + '\\n')\nprint(metrics.classification_report(label_dev, predicted_svm)+ '\\n')\nprint('--------MNB-------' + '\\n')\nprint(metrics.classification_report(label_dev, predicted_mnb)+ '\\n')\n", "intent": "<h3>Classification Report and Confustion Matrix</h3>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i,layer in enumerate(style_layers):\n        gram = gram_matrix(feats[layer])\n        style_loss += style_weights[i]*tf.reduce_sum(tf.square(gram-style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    print(\"correct: {} \\nmy answer:{}\".format(correct,student_output))\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_Inception]\ntest_accuracy = 100*np.sum(Inception_predictions == np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint(\"Test accuracy: {:.4f}%\".format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(tensor)) for tensor in test_Inception]\ntest_accuracy = 100*np.sum(Inception_predictions == np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint(\"Test accuracy: {:.4f}%\".format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction_mlp = model.predict(sentiment_count)\nprediction_mlp = prediction_mlp.argmax(axis=1)\nprint(\"the accuracy of training dataset\")\nprint(round(accuracy_score(y_first40, prediction_mlp),2))\nprint(\"\\n\")\nprint(\"confusion matrix from multi-layer perceptron model\")\nprint(confusion_matrix(y_first40, prediction_mlp))\nprint(\"\\n\")\nprint(\"the overview of performance metrics\")\nprint(classification_report(y_first40, prediction_mlp))\n", "intent": "<a id=\"step10\"></a>\n"}
{"snippet": "test_prediction_mlp = model.predict(test_sentiment_count)\ntest_prediction_mlp = test_prediction_mlp.argmax(axis=1)\nprint(\"the accuracy of testing dataset\")\nprint(round(accuracy_score(y_test, test_prediction_mlp),2))\nprint(\"\\n\")\nprint(\"confusion matrix from multi-layer perceptron model (in remaining test dataset)\")\nprint(confusion_matrix(y_test, test_prediction_mlp))\nprint(\"\\n\")\nprint(\"the overview of performance metrics (in remaining test dataset)\")\nprint(classification_report(y_test, test_prediction_mlp))\n", "intent": "<a id=\"step11\"></a>\n"}
{"snippet": "lasso_default = Lasso()\nscorer = make_scorer(r2_score, greater_is_better=True)\nscores = cross_val_score(lasso_default, X_train, y_train, cv=5)\nprint(\"average cross-validation R^2 score of default Lasso model: {:.4f}\".format(scores.mean()))\n", "intent": "<a id=\"step4\"></a>\nDefault parameter for Lasso model:\n* alpha = 1\n"}
{"snippet": "all_pred = spam_detect_model.predict(sms_tfidf)\nall_pred\n", "intent": "Let us check how our model performs:\n"}
{"snippet": "accuracy_score = classifier.evaluate(input_fn=get_test_inputs, steps=1)[\"accuracy\"]\nprint(\"\\nTest Accuracy: {0:.1f}%\\n\".format(100*accuracy_score))\n", "intent": "Testing the neuronal network and showing the achieved accuracy score.\n"}
{"snippet": "print(metrics.confusion_matrix(trainLabel,predictedTrainLabel))\nprint(metrics.accuracy_score(trainLabel, predictedTrainLabel))\n", "intent": "<h1> Train Data - Print Prediction Result </h1>\n"}
{"snippet": "print(metrics.confusion_matrix(testLabel,predictionTestLabel))\nprint(metrics.accuracy_score(testLabel, predictionTestLabel))\n", "intent": "<h1> Test Data - Print Prediction Result </h1>\n"}
{"snippet": "RSS = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nMSE = np.sqrt(RSS)\nprint('MSE: ', MSE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "tl_predictions = [np.argmax(tl_model.predict(np.expand_dims(feature, axis=0))) for feature in test_bf]\ntest_accuracy = 100*np.sum(np.array(tl_predictions)==np.argmax(test_targets, axis=1))/len(tl_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred = model_orig.predict(X_test)\nc_matrix = confusion_matrix(y_test, pred)\nprint(c_matrix)\n", "intent": "Now we can use the X_test to test the accuracy of the SVC model.\n"}
{"snippet": "print \"Raw Data: %.4f\" % silhouette_score(X,km_forest.fit_predict(X),metric='euclidean')\nfrom sklearn import preprocessing\nX_normalized = preprocessing.normalize(X, norm='l1')\nprint \"Normalised Data (L2): %.4f\" % silhouette_score(X_normalized,km_forest.fit_predict(X_normalized),metric='euclidean')\n", "intent": "Generate a silhouette score. Does this score change if you normalize your data? (Should you normalize your X and Y in this instance?)\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint \"%.4f\" % cross_val_score(model_exercise2,X_exercise2.iloc[test],y_exercise2.iloc[test],cv=100).mean()\n", "intent": "Cross validate an SVC model. Remember cross_val_score \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\npredicted = clf.predict(features_test)\nprint(\"Accuracy score of SVM model: {}\\n\".format(accuracy_score(data_labels_test,predicted)))\nprint(classification_report(data_labels_test, predicted))\n", "intent": "Obtain the accuracy scores \n"}
{"snippet": "predictions = [model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in test_tensors]\n", "intent": "The model is tried on the test dataset of driver images.\n"}
{"snippet": "VGG16_predictions = [VGG16_model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in bottleneck_features_test_VGG16]\n", "intent": "The model is tried on the test dataset of driver images.\n"}
{"snippet": "VGG16_predictions2 = [VGG16_model2.predict(np.expand_dims(tensor, axis=0))[0] for tensor in bottleneck_features_test2_VGG16]\n", "intent": "The model is tried on the test dataset of driver images.\n"}
{"snippet": "VGG16_predictions_fine_tuned = [model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in test_tensors]\n", "intent": "The model is tried on the test dataset of driver images.\n"}
{"snippet": "h = .02     \nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n", "intent": "<p>\n    Now, we draw the background color of each cluster\n</p>\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n    target_names=twenty_test.target_names))\n", "intent": "We achieved 91.3% accuracy using the SVM. scikit-learn provides further utilities for more detailed performance analysis of the results:\n"}
{"snippet": "loss = tf.losses.log_loss(y, y_proba)  \n", "intent": "But we might as well use TensorFlow's tf.losses.log_loss() function:\n"}
{"snippet": "print(\"The available metrics are: {}\".format([s for s in dir(sklearn.metrics) if s[0] != '_']))\nprint(\"for our clusters:\")\nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(newsgroupsDF['category'], km.labels_)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(newsgroupsDF['category'], km.labels_)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(newsgroupsDF['category'], km.labels_)))\n", "intent": "Once we have the clusters, we can evaluate them with a variety of metrics that\nsklearn provides. We will look at a few.\n"}
{"snippet": "print(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(natregimesDF['category'], natregimesKMeans.labels_)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(natregimesDF['category'], natregimesKMeans.labels_)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(natregimesDF['category'], natregimesKMeans.labels_)))\n", "intent": "Look at the how well it it works:\n"}
{"snippet": "print(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(natregimesDF['category'], natregimesKMeans2.labels_)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(natregimesDF['category'], natregimesKMeans2.labels_)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(natregimesDF['category'], natregimesKMeans2.labels_)))\n", "intent": "And looking at the metrics\n"}
{"snippet": "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) \nprint(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) \nprint(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) \n", "intent": "Let's calculate the precision, recall, and F-measures.\n"}
{"snippet": "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) \nprint(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) \nprint(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) \n", "intent": "Let's get the precision, recall, and F-measure.\n"}
{"snippet": "print(sklearn.metrics.precision_score(test.target, labels, average = 'weighted')) \nprint(sklearn.metrics.recall_score(test.target, labels, average = 'weighted')) \nprint(sklearn.metrics.f1_score(test.target, labels, average = 'weighted')) \n", "intent": "The precision, recall, and F-measure.\n"}
{"snippet": "print(clf.predict(test_redditDf['tfVect'][82]))\nprint(test_redditDf['subreddit'][82])\n", "intent": "And a prediction from the testing set\n"}
{"snippet": "targetRow = 10\nprint(redditDfTesting.iloc[targetRow])\nprint(\"Is predicted to be:\")\nprint(decisionTree.predict(redditDfTesting['tfVect'][targetRow]))\n", "intent": "We now have a fully trained decision tree, lets test it on one of our testing data:\n"}
{"snippet": "decisionTree.predict_proba(redditDfTesting['tfVect'][targetRow])\n", "intent": "We can also get the model's probabilties for each class, it's very boring for trees\n"}
{"snippet": "print(sklearn.metrics.precision_score(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'], average = 'weighted'), 'precision')\nprint(sklearn.metrics.recall_score(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'], average = 'weighted'), 'recall')\nprint(sklearn.metrics.f1_score(redditDfTesting['subreddit'], redditDfTesting['predictionsDT'], average = 'weighted'), 'F-1 measure')\n", "intent": "Then we can look at a couple metrics\n"}
{"snippet": "redditDfTesting['predictionsBoost'] = boosting.predict(np.stack(redditDfTesting['tfVect'], axis=1)[0])\n", "intent": "get the predictions\n"}
{"snippet": "linreg.predict(18)\n", "intent": "<h3>Predict Ground Temperatures from Chirps/Second</h3>\n"}
{"snippet": "bb_data.predict(7)\n", "intent": "<h3>Predict body size with brain size</h3>\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    score = r2_score(y_true, y_predict)\n    return score\n", "intent": "- use all the training data(split into train and test)\nhttp://scikit-learn.org/stable/modules/grid_search.html\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix\ncm_train = confusion_matrix(train_target,\n                            clf_tf_idf.predict(vectorizer_tf_idf.transform(train_data)), labels = ['easy', 'medium'])\ncm_train\n", "intent": "Let's calculate the precision on the test data.\n"}
{"snippet": "cm_test = confusion_matrix(test_target,\n                           clf_tf_idf.predict(vectorizer_tf_idf.transform(test_data)),\n                           labels = ['easy', 'medium'])\ncm_test\n", "intent": "Let's consider the confusion matrix for the 2 most common labels, easy and medium.  \n"}
{"snippet": "X_test = vectorizer_count.transform(test_data)\ny_predict = clf.predict(X_test)\n", "intent": "The 10-fold cross-validation training and testing gives us an averaged accuracy of the model.\n"}
{"snippet": "correct = 0\nfor instance in range(len(mnist.test.images)):\n    if np.argmax(mnist.test.labels[instance]) == predict(w1,w2,mnist.test.images[instance]):\n        correct += 1\nprint(correct/len(mnist.test.images))\n", "intent": "Finally, by executing the cell below, calculate the accuracy of your ANN on the test data. It should be around 0.8843 .\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    stloss = 0\n    i=0\n    for l in style_layers:\n        b = gram_matrix(feats[l])\n        stloss += style_weights[i] * tf.reduce_sum(( b- style_targets[i])*(b - style_targets[i]))\n        i +=1\n    return stloss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def get_test_misclass(model,x_test,y_test):\n    y_true = y_test\n    y_pred = model.predict(x_test)\n    print(classification_report(y_true, y_pred))\n", "intent": "**Test Misclass Rate**\n"}
{"snippet": "y_pred = log.predict(x_test)\n", "intent": "<a id='predlab'></a>\n"}
{"snippet": "final_l=[]\nfor l in estimator.predict(input_fn=input_test_func):\n    final_l.append(l['class_ids'][0]) \n", "intent": "** Each item in your list will look like this: **\n"}
{"snippet": "final_l=[]\nfor l in estimator.predict(input_fn=input_test_func):\n    final_l.append(l[class_ids][0]) \n", "intent": "** Each item in your list will look like this: **\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "<a id='prediction_step'></a>\n"}
{"snippet": "print('Precision score of the training subset: {:.3f}'.format(precision_score(y_test, pred, average='micro'))) \n", "intent": "<a id='traintest_precision'></a>\n"}
{"snippet": "pred = model.predict(X5)\n", "intent": "<a id='prediction_step'></a>\n"}
{"snippet": "print('Precision score of the prediction subset: {:.3f}'.format(precision_score(consensus_GT, predict, average='micro'))) \n", "intent": "<a id='predict_precision_score_matrix'></a>\n"}
{"snippet": "rfc_prediction = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, rfc_prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "knn_pred = knn_model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred = log_model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predict = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "threshold_approx = (threshold[0] + threshold[1])/2\ny_predict = make_predict(model, X_test, threshold_approx)\nprint classification_report(y_true=y_test, y_pred=y_predict)\n", "intent": "The threshold can be approximated by taking the mid-point of these two values and then used to make predicitons on the test data.\n"}
{"snippet": "i = [0,1,2,3]\nx = X_test[i,:]\nprint x\nyhat = predictor.predict(x)\na = np.array(zip(yhat,y_test[i]))\nprint a\n", "intent": "Now that we trained `predictor` we can use it to provide predictions on any example `x`. \n"}
{"snippet": "bhat = predictor.predict(X_test)\nb = np.array(zip(bhat,y_test))\ncount2=0\nfor x in range(len(b)):\n    if b[x][0]==b[x][1]:\n        count2=count2+1\nprint \"Accuracy:\",((count2/float(len(b)))*100.0),\"%\"\n", "intent": " **Answer to Question 8:**\n"}
{"snippet": "training_predictions = model.predict(x_train)\nprint(np.mean((training_predictions - y_train) ** 2))\n", "intent": "<h4>Generate predictions in-sample error</h4>\n"}
{"snippet": "lolo = LeaveOneLabelOut(train['strain_1_cluster'])\ncvscores = cross_val_score(clf4, xtrain, ytrain, cv = lolo)\nprint(cvscores)\n", "intent": "The errors here are pretty skewed towards overestimating the distances - interesting\n"}
{"snippet": "customcvscores = cross_val_score(clf4, xtrain, ytrain, cv = cvlist)\nprint(customcvscores)\n", "intent": "Using the custom cross validation object I made earlier to try crossvalidation\n"}
{"snippet": "y_pred=model.predict(df_train.x.reshape(-1,1))\nzip(df_train.y, y_pred)\n", "intent": "Now we have the model fit to the data we can predict.  Let's predict the y values for the original x:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"accuracy = {0:4.1f}% \".format(accuracy_score(pred, target_test)*100))\n", "intent": "We can use metrics from sklearn to quantify performance:\n"}
{"snippet": "import functools\ndef poly_loss(y, y_hat, p, binary_classification=False):\n    if binary_classification:\n        assert set(y.ravel()).issubset(set((0,1)))\n        y_hat = y_hat >= 0.5\n    return np.abs(y - y_hat)**p\n", "intent": "$\\ell_{p}(y, \\hat{y}) \\triangleq \\lvert y - \\hat{y} \\lvert^{\\,p}$, where $y,\\hat{y}\\in\\reals$.\n"}
{"snippet": "import functools\ndef zerone_loss(y, y_hat, binary_classification=False):\n    if binary_classification:\n        assert set(y.ravel()).issubset(set((0,1)))\n        y_hat = y_hat >= 0.5\n    return y != y_hat\n", "intent": "$\\ell_{01}(y,\\hat{y}) \\triangleq \\mathbb{1}(y \\neq \\hat{y})$.\n"}
{"snippet": "def hinge_loss(y, y_hat):\n    assert set(y.ravel()).issubset(set((0,1)))\n    y = 2 * y - 1   \n    return np.maximum(0, 1 - y * y_hat)\n", "intent": "$\\ell_{h}(y,\\hat{y}) \\triangleq \\max(0, 1 - y\\hat{y})$ where $y\\in\\{\\pm 1\\},\\,\\hat{y}\\in\\reals$.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nsentiment_predictions = clf.predict(tf_idf)\ncm = confusion_matrix(sentiment,sentiment_predictions)\nprint cm\n", "intent": "Let's look at the confusion matrix to see how our true positive, true negative, false positive, and false negative rate is\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X))** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "opt = Adam(lr=0.0001)\nloss = get_custom_loss(1.0)\nmodel.compile(loss=loss, \n              optimizer=opt, \n              metrics=[binary_crossentropy_n_cat, 'accuracy', batch_GAP])\n", "intent": "I manually decreased the learning rate during training, starting at about 0.001 for training the `top_model` (on a larger `batch_size` of 128 or so).\n"}
{"snippet": "target_test_linear_SVM = clf_LSVC.predict(features_test)\n", "intent": "Now that we have trained our classifier, we can predict the result for our testing dataset.\n"}
{"snippet": "LSTM1.load_weights(lstm_weights_path)\nscore, acc = LSTM1.evaluate(test_data,test_labels)\nprint('Test score: %.2f' % score)\nprint('Test accuracy: %.2f' % acc)\n", "intent": "Looks like the model is overfitting at this point. Let's see how the best weights do on the testing set.\n"}
{"snippet": "predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xfit = np.linspace(35, 60)\nXfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\nyfit[:3]\n", "intent": "Predict mortality rates using a new set of latitude data and apply it to the trained data model.\n"}
{"snippet": "net2.evaluate()\npred2 = net2.forward(X_val)\npred2 = np.argmax(pred2, axis=1)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(pred2, np.argmax(y_val, axis=1))\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "print('R^2:', r2_score(y_test, pred))\nprint('MAE:', mean_absolute_error(y_test, pred))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "Y_pred = model.predict(Xtest)\n", "intent": "**5. Apply the model and evaluate**\n"}
{"snippet": "print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\nprint(metrics.classification_report(y_test, y_pred))\n", "intent": "We now calculate a series of statistics that allow us to gauge how well the algorithm will perform on unseen data.\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test accuracy:%.4f'%score[1])\n", "intent": "**Validation Accuracy**: (fill in here)\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0, batch_size=200)\nprint('Test accuracy:%.4f'%score[1])\n", "intent": "**Validation Accuracy**: (fill in here)\n"}
{"snippet": "pred_Y = attn_model.predict(test_X, \n                          batch_size = 16, \n                          verbose = True)\n", "intent": "Here we evaluate the results by loading the best version of the model and seeing how the predictions look on the results. We then visualize spec\n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n    return (\"Mean score: {0:.3f} (+/- {1:.3f})\").format(np.mean(scores), sem(scores))\nprint mean_score(scores)\n", "intent": "We obtained an array with the k scores. We can calculate the mean and the standard error to obtain a final figure:\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": "A high score on a trained set indicates some over-fitting\n"}
{"snippet": "predictions = loaded_model_1.predict(diff1718)\n", "intent": "Here we predict who the winners will be. \n"}
{"snippet": "precision = cross_val_score(logistic, X, y, cv=10, scoring = \"precision\")\nrecall = cross_val_score(logistic, X, y, cv=10, scoring = \"recall\")\nprint(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\nprint(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))\n", "intent": "<span style=\"color:blue\">We evaluate precision with cross-validation.\n"}
{"snippet": "inceptionv3_predictions = [np.argmax(inceptionv3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inceptionv3]\ntest_accuracy = 100*np.sum(np.array(inceptionv3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionv3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(X_test, Y_test,\n                       verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Display the accuracy, i.e. the metric we compiled for.\n"}
{"snippet": "validation_size = 1500\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))\n", "intent": "Extracting a validation set, and measuring score and accuracy.\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n", "intent": "Let us encode a few images, decode them again and plot them for comparison\n"}
{"snippet": "pd.crosstab(pfail, clf.predict(temps.reshape(-1,1)), rownames=[\"Actual\"], colnames=[\"Predicted\"])\n", "intent": "The failures in prediction are, exactly where you might have expected them to be, as before.\n"}
{"snippet": "metrics.roc_auc_score(c, c_hat)\n", "intent": "The ROC curve illustrates the poor quality of this classifier.\n"}
{"snippet": "incept_predictions = [np.argmax(incept_model.predict(np.expand_dims(feature, axis=0)))\\\n                      for feature in test_incept]\ntest_accuracy = 100*np.sum(np.array(incept_predictions)==np.argmax(test_targets, axis=1))/len(incept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(dog_breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res50]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = clf.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the performance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "predictions = []\nfor tree in rf.estimators_:\n    predictions.append(tree.predict_proba(X_val)[None, :])\n", "intent": "Step 2: Get predictions for each tree in Random Forest separately.\n"}
{"snippet": "preds = learner.predict()\nactuals = learner.data.val_y\nsns.jointplot(preds, actuals, kind=\"hex\", stat_func=None, size=7)\n", "intent": "Looking good. Now it's time to try predicting. I'll compare the predictions with the validation set.\n"}
{"snippet": "svm = joblib.load('svm-model.pkl')\npredicted_labels = svm.predict_proba(principal_components)[0]\n", "intent": "Using SVM model trained on bosphorus dataset.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n    return f1_score(target.values, y_pred, pos_label='yes')\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "The code below initializes some helper functions used to test the models later.\n"}
{"snippet": "roc_auc_score(yv,0.5*(pred_seq+pred_bid))\n", "intent": "The two sets of predictions are fairly highly correlated for our validation split, but we can still try mean ensembling the results.\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        gdiff = gram_matrix(feats[style_layers[i]]).sub(style_targets[i])\n        loss += style_weights[i]*torch.sum(gdiff*gdiff)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredicted_intents = predictions[0].argmax(1)\ntruth_intents = test_i.argmax(1)\naccuracy_score(truth_intents, predicted_intents)\n", "intent": "Intent classification accuracy\n"}
{"snippet": "predictions = ner_model.predict(x=[x_test, x_char_test, test_lengths], batch_size=1)\neval = get_conll_scores(predictions, y_test, {v: k for k, v in dataset.y_labels.vocab.items()})\n", "intent": "Once the model has trained. Run CONLLEVAL to see how well it performs.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()\n", "intent": "- Because AUC is a useful metric for choosing between models, it is available as a scoring function for **cross_val_score()**\n"}
{"snippet": "def computeTestScores(test_x, test_y, clf, cv):\n    kFolds = sklearn.model_selection.KFold(n_splits=cv)\n    scores = []\n    for _, test_index in kFolds.split(test_x):\n        test_data = test_x[test_index]\n        test_labels = test_y[test_index]\n        scores.append(sklearn.metrics.accuracy_score(test_labels, clf.predict(test_data)))\n    return scores\n", "intent": "**A much better solution**\n"}
{"snippet": "clflog.predict_proba(Xtest)\n", "intent": "The probabilities output by the classifier can be accessed as below.\n"}
{"snippet": "clflog.predict_proba(Xtest)[:,1]\n", "intent": "The second column (`[:,1]` in numpy parlance, i.e. skip 1) gives the probability that the sample is a 1 (or +ive), here Male.\n"}
{"snippet": "results = model.evaluate(test_data, test_labels)\nprint(results)\n", "intent": "Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(lr, X, y, cv=5)\n", "intent": "Looks like logistic regression is the winner, but there's room for improvement.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(bos.PRICE,a)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=0.0\n    for i in range(len(style_weights)):\n        style_loss+=style_weights[i]*tf.reduce_sum((gram_matrix(feats[style_layers[i]])-style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "probabilities = sentiment_model.predict_proba(sample_test_matrix)\nprint probabilities\n", "intent": "We now calculate the probability by using the sklearn function of the Logistic Regressor that we built:\n"}
{"snippet": "predictions_sentiment = sentiment_model.predict(train_matrix)\npredictions_simple = simple_model.predict(train_matrix_word_subset)\n", "intent": "We are going to compute the accuracy in the TRAIN DATASET:\n"}
{"snippet": "predictions_sentiment_test = sentiment_model.predict(test_matrix)\npredictions_simple_test = simple_model.predict(test_matrix_word_subset)\n", "intent": "Now, we will repeat this exercise on the test_data. Start by computing the classification accuracy of the sentiment_model on the test_data.\n"}
{"snippet": "predictions = boosting_classifier_model_5.predict(validation_matrix)\ncorrects = sum(predictions == validation_data['safe_loans'])\nprint 'accuracy: ', corrects/float(len(validation_data))\n", "intent": "- La calculamos \"a mano\":\n"}
{"snippet": "range_n_clusters = range(2,10)\nZ = linkage(dfw, 'single')\nfor i in range_n_clusters:\n    S_labels=fcluster(Z, i, criterion='maxclust')    \n    silhouette_avg = silhouette_score(dfw, S_labels)\n    print(\"When n_clusters = {},\".format(i)+\" the average silhouette_score is : {}\".format(silhouette_avg))\n", "intent": "* Bottom-up hierarchical clustering with \"single link\" distance metric\n"}
{"snippet": "Z2 = linkage(dfw, 'complete')\nfor i in range_n_clusters:\n    C_labels=fcluster(Z2, i, criterion='maxclust')    \n    silhouette_avg = silhouette_score(dfw, C_labels)\n    print(\"For n_clusters = {},\".format(i)+\" the average silhouette_score is : {}\".format(silhouette_avg)) \n", "intent": "* Bottom-up hierarchical clustering with \"complete link\" distance metric.\n"}
{"snippet": "broadcastCAF = sc.broadcast(caf.collectAsMap())\nbroadcastCAF.value\ndef predforest(k,p):\n    x = [i.features for i in p]\n    yhat = broadcastCAF.value[k].predict(x)\n    return yhat\n", "intent": "And we're done!  For example, we can now use the trees to 'predict' response values.\n"}
{"snippet": "preds = model.predict(test_sample)\nprint test_sample.shape\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    query_img = test_sample[i]\n    _, result = model.kneighbors([query_img], n_neighbors=4)\n    show(query_img)\n    show(train[result[0],:], len(result[0]))\n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "preds = model.predict(test_sample)\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nerr_rate = float(len(errors))/len(preds)\nprint err_rate\n", "intent": "Now let's calculate the prediction error rates of the KNN classifier.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\npredictions_mech = mlp_etensor.predict(np.vstack(x_test_mech))\npredictions_therm = mlp_keff.predict(np.vstack(x_test_therm))\n", "intent": "Use the predict feature in the scikit learn tool\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Evaluate model performance by predicting off the test values!\nPredict off the X_test set of the data\n"}
{"snippet": "rn50_breed_predictions = [np.argmax(tl_model.predict(np.expand_dims(t, axis=0))) for t in test_resnet_50]\ntest_accuracy = 100*np.sum(np.array(rn50_breed_predictions)==np.argmax(test_targets, axis=1))/len(rn50_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = pump_classifier.predict(X_test)\n", "intent": "Predict on Test Set\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\ny_true = dtest.get_label()\ny_pred = clf.predict(dtest).ravel()\n", "intent": "import pickle\npickle.dump(clf, open('/mnt/HARD/MinMax94/models/pickle/xgboost_air.pickle', \"wb\"))\n"}
{"snippet": "import numpy as np\nimport pickle\nstd = np.std(y_true - y_pred)\nprint(mean_absolute_error(y_true, y_pred))\nupper = y_pred + std * 4\nlower = y_pred - std * 4\ndf_test['label_predict'] = ((y_true >= upper) | (y_true <= lower))\ndf_test['decision_function'] = np.abs(y_true - y_pred)\n", "intent": "import pickle\nclf = pickle.load(open('/mnt/HARD/MinMax94/models/pickle/xgboost.pickle', 'rb'))\n"}
{"snippet": "Y_pred = model.predict(X_test)\ny_pred = np.argmax(Y_pred, axis=1)\nyt = np.argmax(y_test, axis=1)\nfrom sklearn.metrics import classification_report\nprint(classification_report(yt, y_pred))\n", "intent": "It's always good to inspect some additional metrics of predictive performance. If you're not familiar with them you could check their definition:\n"}
{"snippet": "pp = model.predict(Y)\n", "intent": "Now, let's make the predictions for all of them using our classifier:\n"}
{"snippet": "def evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n", "intent": "Now, after our model is \n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nerror = mean_absolute_error(y_true,np.exp(model.predict(test_data))-shift)\nprint error\n", "intent": "Now that our model is trained, we find the predictions of the test data\n"}
{"snippet": "test_iter = mx.io.NDArrayIter(mnist['test_data'], None, batch_size)\nprob = lenet_model.predict(test_iter)\ntest_iter = mx.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size)\nacc = mx.metric.Accuracy()\nlenet_model.score(test_iter, acc)\nprint(acc)\nassert acc.get()[1] > 0.98\n", "intent": "Finally, we'll use the trained LeNet model to generate predictions for the test data.\n"}
{"snippet": "Xception_predictions = [np.argmax(merged_model.predict(np.expand_dims(feature, axis=0))) for feature in test_tensors]\ntest_accuracy = 100 * np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis=1)) / len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dtree_predict=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rfc_predict = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,rfc_predict))\nprint('\\n')\nprint(confusion_matrix(y_test,rfc_predict))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "word_a = 'graph'\nword_b = 'trees'\noutput = keras_model.predict([np.asarray([model.wv.vocab[word_a].index]), np.asarray([model.wv.vocab[word_b].index])])\nprint output\n", "intent": "Now, we input the two words which we wish to compare and retrieve the value predicted by the model as the similarity score of the two words. \n"}
{"snippet": "input_text = 'artificial intelligence'\nmatrix = process_text(input_text)\npredictions = model.predict(matrix)\nscoredict = {}\nfor idx, classlabel in zip(range(len(classlabels)), classlabels):\n    scoredict[classlabel] = predictions[0][idx]\nprint scoredict\n", "intent": "Our classifier is now ready to predict classes for input data.\n"}
{"snippet": "y_pred_class = logreg.predict(X_test)\n", "intent": "Making Predictions for the test\n"}
{"snippet": "print metrics.accuracy_score(y_test,y_pred_class)\n", "intent": "77% accuracy is significantly better than the previous 66%. Let's see for the AUC metrics as well as sensitivity.\n"}
{"snippet": "print metrics.roc_auc_score(y_test,y_pred_prob_1)\n", "intent": "The AUC improves by around 12% from 72% before.\n"}
{"snippet": "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n", "intent": "This will print out the predicted probabilities of Functional Needs Repair\n"}
{"snippet": "pred_labels_ = rfc_.classes_[ rfc_.predict_proba( X_test ).argmax( axis = 1 ) ]\n", "intent": "Predict on the test dataset.\n"}
{"snippet": "pred_labels_ = gnb_.classes_[ gnb_.predict_proba( X_test ).argmax( axis = 1 ) ]\n", "intent": "Predict the labels ...\n"}
{"snippet": "pred_labels_ = lorl2_.predict( scl_.transform( X_test ) )\nprint \"Accuracy score is %.3f%%\" % ( 100*np.mean( pred_labels_ == y_test ), )\n", "intent": "Predict on the test.\n"}
{"snippet": "pred_labels_ = pipe_.predict( X_test )\n", "intent": "Now test the pipeline.\n"}
{"snippet": "def get_staged_accuracy(ensemble, X, y):\n    prob_ = np.stack([est_.predict_proba(X)\n                      for est_ in ensemble.estimators_],\n                     axis=1).astype(float)\n    pred_ = np.cumsum(prob_[..., 1] > 0.5, axis=1).astype(float)\n    pred_ /= 1 + np.arange(ensemble.n_estimators).reshape((1, -1))\n    return np.mean((pred_ > .5).astype(int) == y[:, np.newaxis], axis=0)\nbag_scores_ = get_staged_accuracy(bag_, X_test, y_test)\nrdf_scores_ = get_staged_accuracy(rdf_, X_test, y_test)\nada_scores_ = np.array(list(ada_.staged_score(X_test, y_test)))\n", "intent": "Get the prediction as a function of the memebers in the ensemble.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model2.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "gbm_pipe.set_params(gradientboostingregressor__n_estimators=240,\n                    gradientboostingregressor__subsample=0.7,\n                    gradientboostingregressor__loss='ls',\n                    gradientboostingregressor__learning_rate=0.02,\n                    gradientboostingregressor__max_depth =4,\n                    gradientboostingregressor__max_features=3,\n                    gradientboostingregressor__min_samples_leaf=4)\nResultsTuned['GBM'] = cross_val_score(gbm_pipe, Xtrain, ytrain, cv=kfold, scoring=scoring)\nmodelsTuned['GBM']  = gbm_pipe.steps[-1][-1]\nsns.boxplot(data=ResultsTuned)\n", "intent": "Let's add the gradient boosting model to our tuned results, and visualize:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels,housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(lin_rmse)                                                 \n", "intent": "Let's use Root Mean Square as the evaluation metric for our algorithms.\n"}
{"snippet": "score = bic_score(test,like,dim)\nprint(score)\n", "intent": "Calculate the score:\n"}
{"snippet": "Y_predict_multifit = lm.predict(Z)\nmean_squared_error(df['price'], Y_predict_multifit)\nprint (R_squared(Z, Y))\n", "intent": "We can say that ~ 80.896 % of the variation of price is explained by this multiple linear regression \n"}
{"snippet": "print (\"Predicted values:\", p(x))\nmean_squared_error(y, p(x))\n", "intent": "previously we had:\n```\nx = df['highway-mpg']\ny = df['price']\nf = np.polyfit(x, y, 3)\np = np.poly1d(f)\nprint(p)\n```\n"}
{"snippet": "Rcross2 = cross_val_score(lre, x_data[['horsepower']], y_data, cv=2)\nprint(Rcross2[1])\n", "intent": "Calculate the average R^2 using two folds, find the average R^2 for the second fold utilizing the horsepower as a feature :\n"}
{"snippet": "from sklearn import metrics\nprint (\"Neigh's Accuracy:\", metrics.accuracy_score(y_testset, pred))\nprint (\"Neigh23's Accuracy:\", metrics.accuracy_score(y_testset, pred23))\nprint (\"Neigh90's Accuracy:\", metrics.accuracy_score(y_testset, pred90))\n", "intent": "Awesome! Now let's compute neigh's <b>prediction accuracy</b>. We can do this by using the <b>metrics.accuracy_score</b> function\n"}
{"snippet": "predTree = skullsTree.predict(X_testset)\nlen(predTree)\n", "intent": "Let's make some predictions on the testing dataset and store it into a variable called predTree.\n"}
{"snippet": "print (((np.mean((y_testset - LinReg.predict(X_testset))**2)))**0.5)\n", "intent": "And lastly, <b>RMSE</b>:<br>\n<img src = \"https://ibm.box.com/shared/static/hc55qagftbvl8yb306vcx78a738n2ylc.gif\", height = 100, align = 'left'>\n"}
{"snippet": "from sklearn import metrics\nprint(\"Neigh's Accuracy: \"), metrics.accuracy_score(y_testset, pred)\n", "intent": "Awesome! Now let's compute neigh's <b>prediction accuracy</b>. We can do this by using the <b>metrics.accuracy_score</b> function\n"}
{"snippet": "print(\"RandomForests's Accuracy: \"), metrics.accuracy_score(y_testset, predForest)\n", "intent": "Let's check the accuracy of our model. <br>\nNote: Make sure you have metrics imported from sklearn\n"}
{"snippet": "predTree = skullsTree.predict(X_testset)\n", "intent": "Let's make some <b>predictions</b> on the testing dataset and store it into a variable called <b>predTree</b>.\n"}
{"snippet": "print (\"Predicted Probabilities for the 3 class iris dataset:\\n\\n{}\".\n       format(gbrt.predict_proba(x_test[:10])))\n", "intent": "    Higher values represent a higher certainty that the sample is in that particular class\n"}
{"snippet": "loss = tf.losses.mean_squared_error(labels=y_target, predictions=my_output)\n", "intent": "For this, we choose the L2 loss.  We can easily choose the L1 loss by replacing `tf.square()` with `tf.abs()`.\n"}
{"snippet": "loss = tf.losses.mean_squared_error(labels=y_target, predictions=my_output)\nmy_opt = tf.train.GradientDescentOptimizer(0.02)\ntrain_step = my_opt.minimize(loss)\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "We use the L2 loss, and the standard Gradient Descent Optimization with a learning rate of 0.02.\n"}
{"snippet": "model_output = tf.add(tf.matmul(x_data, A), b)\nloss = tf.losses.mean_squared_error(labels=y_target, predictions=model_output)\n", "intent": "We add the model operations (linear model output) and the L2 loss.\n"}
{"snippet": "loss_l2 = tf.losses.mean_squared_error(\n    labels=y_target, predictions=model_output)\nmy_opt_l2 = tf.train.GradientDescentOptimizer(learning_rate)\ntrain_step_l2 = my_opt_l2.minimize(loss_l2)\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Here is the loss function, variable initialization, and optimization functions.\n"}
{"snippet": "preds = model.predict(test_sample)\nprint test_sample.shape\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    query_img = test_sample[i]\n    _, result = model.kneighbors(query_img, n_neighbors=4)\n    show(query_img)\n    show(train[result[0],:], len(result[0]))\n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "predictions = mlp.predict(X_test)\n", "intent": "Now that we have a model it is time to use it to get predictions! We can do this simply with the predict() method off of our fitted model:\n"}
{"snippet": "def content_loss_func(sess, model):\n    def _content_loss(p, x):\n        N = p.shape[3]\n        M = p.shape[1] * p.shape[2]\n        return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2))\n    return _content_loss(sess.run(model['conv4_2']), model['conv4_2'])\n", "intent": "Define the equation (1) from the paper to model the content loss. We are only concerned with the \"conv4_2\" layer of the model.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_acc  = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy: {}\" .format(test_acc))\n", "intent": "We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called accuracy:\n"}
{"snippet": "y_pred = model.predict(xval)\n", "intent": "As before, we need to coerce these X values into a [n_samples, n_features] features matrix.\n"}
{"snippet": "random_image_button = widgets.Button(description='New Image!')\ndef display_face_and_prediction(b):\n    index = randint(0, 400)\n    face = faces.images[index]\n    imshow(face, cmap='gray')\n    print(\"Image Index:\", index)\n    B = face.reshape(1, -1)\n    print(\"This person is happy\", \"\\n[0 = NO; 1 = YES]:\", svc_1.predict(B))\nrandom_image_button.on_click(display_face_and_prediction)\ndisplay(random_image_button)\n", "intent": "Create GUI to visually assess the performance of the classifier.\n"}
{"snippet": "y_hat=est.best_estimator_.predict(df_in_oos)\nprint(np.corrcoef(df_tar_oos.T,y_hat.T)[0][1])\ni=y_hat<0\ncumul_eq=df_tar_oos.copy()\ncumul_eq[i]=cumul_eq[i]*-1\ncumul_eq.cumsum().plot(label='model')\nax=df_tar_oos.cumsum().plot(c='red', label='long')\nax.legend();\n", "intent": "Plot the cumulative returns for model vs long\n"}
{"snippet": "predict = clf.predict(X_test)\naccuracy= clf.score(X_test, y_test)\nprint \"Accuracy = {0:.2f}%\".format(accuracy*100)\n", "intent": "Predict from the Test data and Check accuracy\n--\n"}
{"snippet": "plot(xx, yy, 'k-')\nplot(xx, yy_down, 'k--')\nplot(xx, yy_up, 'k--')\nscatter(clfr.support_vectors_[:, 0], clfr.support_vectors_[:, 1], s=80, facecolors='none')\nscatter(X_train[:, 0], X_train[:, 1], c=clfr.predict(X_train), cmap=cm.Paired, alpha=0.2, edgecolor='None')\nscatter(X_test[:, 0], X_test[:, 1], c=clfr.predict(X_test), cmap=cm.Paired, alpha=0.5, edgecolor='None')\nxlim(-2.5*xmax,2.5*xmax)\nylim(-2.5*xmax,2.5*xmax)\n", "intent": "**Plot the training data, testing data, seperating hyperplane, and closest parallel hyperplanes**\n"}
{"snippet": "preds = model.predict(x_test)\n", "intent": "Get predictions for the test set.\n"}
{"snippet": "scores = cross_validation.cross_val_score(knn,\n                                          features, target,\n                                          scoring=\"roc_auc\",\n                                          n_jobs=6,\n                                          cv=3);\n\"Accuracy: %0.5f (+/- %0.5f)\"%(scores.mean(), scores.std())\n", "intent": "Cross Validation score\n"}
{"snippet": "print(svm.predict(X_test))\n", "intent": "3) Apply / evaluate\n"}
{"snippet": "clf_predict = clf.predict(x_test)\n", "intent": "clf contains all the co-efficients belongs to the trained model. So lets predict our dataset by using testing sample to predict the House Sale Price\n"}
{"snippet": "clf_predict = clf_train.predict(x_test)\nprint(clf_predict[:100])\n", "intent": "<h3>Evaluate Model Accuracy</h3>\n"}
{"snippet": "from sklearn.metrics import classification_report\nreport = classification_report(y_test, clf_predict)\nprint(report)\n", "intent": "<img src=\"clf_report.png\" /img width=400>\n"}
{"snippet": "training_music[str(k_chosen)] = models[k_chosen].predict(scaled_features)\n", "intent": "Get labels for each training observation \n"}
{"snippet": " print(np.sum((bos.PRICE - lm.predict(X))**2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X))**2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = model.predict(x=data.test.images)\ny_pred_cls = np.argmax(y_pred,axis=1)\nplot_example_errors(y_pred_cls)\n", "intent": "**Mis-classified Images**\n"}
{"snippet": "y_pred = fun_model.predict(x=data.test.images)\ny_pred_cls = np.argmax(y_pred,axis=1)\nplot_example_errors(y_pred_cls)\n", "intent": "**Mis-classified Images**\n"}
{"snippet": "print('Mean accuracy: %.1f%%' % np.mean(100*cross_val_score(\n    lr, X, y, cv=BootstrapOutOfBag(n_splits=200, random_seed=456))))\n", "intent": "In practice, it is recommended to run at least 200 iterations, though:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmodel_output_0 = []\nmodel_output_1 = []\nfor row in input_data:\n    model_output_0.append(predict_with_network(row, weights_0))\n    model_output_1.append(predict_with_network(row, weights_1))\nmse_0 = mean_squared_error(target_actuals, model_output_0)\nmse_1 = mean_squared_error(target_actuals, model_output_1)\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)\n", "intent": "**Scaling up to multiple data points**\n"}
{"snippet": "import numpy as np\ndef compute_log_loss(predicted, actual, eps=1e-14):\n predicted = np.clip(predicted, eps, 1 - eps)\n loss = -1 * np.mean(actual * np.log(predicted)\n + (1 - actual)\n * np.log(1 - predicted))\n return loss\n", "intent": "**Computing log loss with NumPy**\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\nprint(\"The loss value:\", round(score[0],3), \"\\nThe metric value:\", round(score[1],3))\n", "intent": "Finally we will evaluate the model with the test set.\n"}
{"snippet": "y_pred_class = lin_clf.predict(X_test_dtm)\n", "intent": "Predicting the data from test data by the model.\n"}
{"snippet": "from sklearn import metrics\nacc1=metrics.accuracy_score(y_test, y_pred_class)\nacc1\n", "intent": "Accuracy Score - the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1']\nprint(classification_report(y_test, y_pred_class, target_names=target_names))\nclassreport=classification_report(y_test, y_pred_class, target_names=target_names)\n", "intent": "Classification Report - Summary of the Precision, Recall, f1score and support.\n"}
{"snippet": "from sklearn.metrics import hamming_loss\nhl1=hamming_loss(y_test, y_pred_class)\nhl1\n", "intent": "Hamming Loss - The Hamming loss is the fraction of labels that are incorrectly predicted.\n"}
{"snippet": "y_pred_class = nb.predict(X_test_dtm)\n", "intent": "Prediciting the values using test data-set using the trained model.\n"}
{"snippet": "from sklearn import metrics\nacc2=metrics.accuracy_score(y_test, y_pred_class)\nacc2\n", "intent": "Accuracy Score - the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1']\nprint(classification_report(y_test, y_pred_class, target_names=target_names))\nMNBclassreport= classification_report(y_test, y_pred_class, target_names=target_names)\n", "intent": "Classification Report - Summary of the Precision, Recall, f1score and support.\n"}
{"snippet": "from sklearn.metrics import hamming_loss\nhl2=hamming_loss(y_test, y_pred_class)\nhl2\n", "intent": "Hamming Loss -The Hamming loss is the fraction of labels that are incorrectly predicted.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmae2=mean_absolute_error(y_test, y_pred_class)\nmae2\n", "intent": "Mean Absolute Error -  used to measure how close forecasts or predictions are to the eventual outcomes.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2s2=r2_score(y_test, y_pred_class)\nr2s2\n", "intent": "R2 score -is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable.\n"}
{"snippet": "from sklearn import metrics\nacc3=metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "Accuracy Score - the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1']\nprint(classification_report(y_test, y_pred_class, target_names=target_names))\nGNBclassreport= classification_report(y_test, y_pred_class, target_names=target_names)\n", "intent": "Classification Report - Summary of the Precision, Recall, f1score and support.\n"}
{"snippet": "from sklearn.metrics import hamming_loss\nhl3=hamming_loss(y_test, y_pred_class)\n", "intent": "Hamming Loss - The Hamming loss is the fraction of labels that are incorrectly predicted.\n"}
{"snippet": "from sklearn.metrics import precision_score\npr3_m=precision_score(y_test, y_pred_class, average='macro')\npr3_m\n", "intent": "Precision Score - The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmae3=mean_absolute_error(y_test, y_pred_class)\n", "intent": "Mean Absolute Error -  used to measure how close forecasts or predictions are to the eventual outcomes.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2s3=r2_score(y_test, y_pred_class)\n", "intent": "R2 score -is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable.\n"}
{"snippet": "y_train_pred = dt_reg.predict(X_train)\ny_test_pred = dt_reg.predict(X_test)\nregr_plot(18,8,y_test_pred,y_test,title=\"Predicted VS Actual\",xlabel=\"House Price\",\n          ylabel=\"Predicted Price\");\ndt_r = r2_score(y_test,y_test_pred)\ndt_mape = mape_vectorized(y_test_pred,y_test)\n", "intent": "Our Decision Tree Regressor has a MAPE of 9.3% and an r-squared of 0.60.\n"}
{"snippet": "y_train_pred = svm_reg.predict(X_train)\ny_test_pred = svm_reg.predict(X_test)\nregr_plot(18,8,y_test_pred,y_test,title=\"Predicted VS Actual\",xlabel=\"House Price\",\n          ylabel=\"Predicted Price\");\nsvm_r = r2_score(y_test,y_test_pred)\nsvm_mape = mape_vectorized(y_test_pred,y_test)\n", "intent": "Our SVM Regressor has a MAPE of 9.2% and an r-squared of 0.64.\n"}
{"snippet": "y_train_pred = rf_reg.predict(X_train)\ny_test_pred = rf_reg.predict(X_test)\nregr_plot(22,8,y_test_pred,y_test,title=\"Predicted VS Actual\",xlabel=\"House Price\",\n          ylabel=\"Predicted Price\");\nrf_r = r2_score(y_test,y_test_pred)\nrf_mape = mape_vectorized(y_test_pred,y_test)\n", "intent": "Our Random Forest Regressor has a very impressive MAPE of only 6.6% and an r-squared of 0.74.\n"}
{"snippet": "my_stumps = []\nfor nb_points in range(30,250,50):\n    A = np.random.randint(1,nb_points/2,(nb_points,2)).astype(float)\n    labels = np.random.choice(2,nb_points)\n    uni = 1./(nb_points)*np.ones(nb_points)\n    stump = myStump()\n    stump.learn(np.c_[A,labels],uni)\n    my_stumps.append([stump, cross_valid_score(stump,np.c_[A,labels],distro=uni),A,labels])\n", "intent": "**Then**, a more large-scale comparison:\n"}
{"snippet": "y_predict = model.predict(x_test)\ny_predict[:5]\n", "intent": "We can also look individual predictions.  Let's run the final trained model over the validation set:\n"}
{"snippet": "print (clf.predict([[160, 0]]))\n", "intent": "In the end, we use one example and predict it using the train classifier. \n"}
{"snippet": "Xcep_predictions = [np.argmax(Xcep_model.predict(np.expand_dims(feature, axis=0))) \n                    for feature in test_xception ]\ntest_accuracy_Xcep = np.sum(np.array(Xcep_predictions) == np.argmax(\n                     test_targets, axis=1))/ len(Xcep_predictions) * 100\nprint('Test accuracy: %.3f%%' % test_accuracy_Xcep)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "models = ['hist_mean', 'hist_median', 'prophet']\nfcst_period = 2\ntrain_prepared = prepare_data_fitting(train)\ny_test = test.groupby(['date']).sum()['prod_vProd']\nfor m in models:\n    y_pred = sales_forecasting(fcst_period, train_prepared, m, plot=True)['yhat'].iloc[-fcst_period:] \n    mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n    print('{} MSE: {:,.0f}'.format(m, mse))\n", "intent": "Let's evaluate the models for our holdout period (equivalent of 2-period forecast)\n"}
{"snippet": "class CustomMetric(object):\n    def get_final_error(self, error, weight):\n        return 0.0\n    def is_max_optimal(self):\n        return True\n    def evaluate(self, approxes, target, weight):\n        return 0.0, 0.0\n", "intent": "To set a custom metric function for overfitting detector and best model selection, create an object that implements the following interface:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmodel = CatBoostClassifier(iterations=400, loss_function='MultiClass', custom_loss='Accuracy')\nscores = cross_val_score(model, x_train, y_train, scoring='accuracy', n_jobs=-1, fit_params=None)\n", "intent": "Each of the arrays contains loss in each iteration (like `staged_predict`):\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_score = cross_val_score(regressor, X_train, y_train, cv=10)\ncross_score\n", "intent": "Now I'm going to look at the average accuracy using **cross validation**\n"}
{"snippet": "x_test = x_fit[:, np.newaxis]\ny_pred = model.predict(x_test)\n", "intent": "As before, we need to coerce these *x* values into a ``[n_samples, n_features]`` features matrix, after which we can feed it to the model:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(test.label, predicted3))\n", "intent": "For the given random_state we found that Multinomial Naive Bayes gave better accuracy. Let explore more using the metrics function.\n"}
{"snippet": "positive_review_transformed = bow_transformer.transform([positive_review])\nnb.predict(positive_review_transformed)[0]\n", "intent": "Seems like someone had the time of their life at this place, right? We can expect our model to predict a rating of 5 for this review.\n"}
{"snippet": "negative_review_transformed = bow_transformer.transform([negative_review])\nnb.predict(negative_review_transformed)[0]\n", "intent": "This is a slightly more negative review. So, we can expect our model to rate this a 1-star.\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Now we predict values for the testing data:\n"}
{"snippet": "print(clf.predict_proba(X_train_std[0].reshape(1, -1)))\nprint(clf.predict_proba(X_train_std[1200].reshape(1, -1)))\nprint(clf.predict_proba(X_train_std[9900].reshape(1, -1)))\n", "intent": "What are the `[home-team-win, draw, away-team-win]` probabilities for some random games.\n"}
{"snippet": "x = im.reshape(batch_shape)\ny = model.predict(x)\n", "intent": "We can now run the image through the convolutional layer and display the outputs.  We get the same outputs as we did earlier.\n"}
{"snippet": "prob=clf.predict_proba(X_test)\n", "intent": "Running it on actual test data\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        loss += style_weights[i]*tf.reduce_sum((gram_matrix(feats[style_layers[i]])-style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = [np.argmax(breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_bn]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = final_clf.predict(new_test_file)\nprint(predictions)\ntype(predictions)\n", "intent": "Now, predicting the results of survival rate of passengers using the random forest classifier trained model.\n"}
{"snippet": "y_hat =  lin(10,5,x)\nmean_square_error(y_hat, y)\n", "intent": "Suppose we believe $a = 10$ and $b = 5$ then we can compute `y_hat` which is our *prediction* and then compute our error.\n"}
{"snippet": "def print_score(m):\n    res = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "We now have something we can pass to a random forest!\n"}
{"snippet": "def print_score(m):\n    res = [m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "pred = regr.predict(x)\n", "intent": "Training set error:\n"}
{"snippet": "regr_metrics(y_valid, regr.predict(x_valid))\n", "intent": "It will be helpful to have some metrics on how good our prediciton is.  We will look at the mean squared norm (L2) and mean absolute error (L1).\n"}
{"snippet": "model.predict(img)\n", "intent": "As was the case in that notebook, a 0 indicates cat and 1 indicates dog.\n"}
{"snippet": "print(\"Input :\")\nprint(X_val)\nprint(\"\\n\\n Output :\")\ny_pred = model.predict(X_val)\nprint(y_pred)\n", "intent": "The loss is \"stable\" even if it didn't reached yet the best point. We can also take a look to the output based on the X_val we generated\n"}
{"snippet": "run = exp.trials[0].runs[0][0]\nfitted_classifier = run.clf\nM_test = diogenes.utils.cast_np_sa_to_nd(M[run.test_indices])\nlabels_test = labels[run.test_indices]\nscores = fitted_classifier.predict_proba(M_test)[:,1]\n", "intent": "Now, we will extract a single run, which gives us a single fitted classifier and a single set of test data.\n"}
{"snippet": "print trial_0.average_score()\nrun_0 = trial_0.runs_flattened()[0]\nprint run_0.f1_score()\nfig = run_0.prec_recall_curve()\n", "intent": "In general, metrics and graphs are available at the `Run` level and aggregations of metrics are available at the `Trial` level\n"}
{"snippet": "Y_train_predict = model.predict(X_lstm_in_train_)\nY_test_predict = model.predict(X_lstm_in_test_)\n", "intent": "I guess some more training would have helped.\n"}
{"snippet": "train_score = math.sqrt(mean_squared_error(Y_train_[0:Y_train_predict.shape[0]], Y_train_predict[:,0]))\nprint('Train score: %.2f RMSE' % (train_score))\ntest_score = math.sqrt(mean_squared_error(Y_train[0:Y_test_predict.shape[0]], Y_test_predict[:,0]))\nprint('Test score: %.2f RMSE' % (test_score))\n", "intent": "Print the RMSE error (0 = best)\n"}
{"snippet": "clf.predict_proba([[2., 2.]])\n", "intent": "You can also get the predicted probability:\n"}
{"snippet": "with open('submission_file.csv','w') as f:\n    f.write('id,label\\n')\nwith open('submission_file.csv','a') as f:\n    for data in tqdm(test_data):\n        img_num = data[1]\n        img_data = data[0]\n        orig = img_data\n        data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n        model_out = model.predict([data])[0]\n        f.write('{},{}\\n'.format(img_num,model_out[1]))\n", "intent": "Alright, so we made a couple mistakes, but not too bad actually! \nIf you're happy with it, let's compete!\n"}
{"snippet": "rs = cross_validation.ShuffleSplit(len(data), n_iter=10, test_size=10)\nscores = cross_validation.cross_val_score(lm, x_train, y_train, cv=rs)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "At first glance my score and price seem reasonable, per my human eyes\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.zeros(1, tf.float32)\n    for i, style_layer in enumerate(style_layers):\n        loss += content_loss(style_weights[i], gram_matrix(feats[style_layer]), style_targets[i])\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "yhat = np.round(model.predict(X_ints_test + [X_test_num]))\nprint(mt.confusion_matrix(y_test,yhat),mt.recall_score(y_test,yhat))\n", "intent": "And finally, check out how it did.\n"}
{"snippet": "model.predict_proba([[0,0,0,0,0]])\n", "intent": "which gives $p = 0.5$. Indeed, we can validate this:\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Now we can score our model\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nprint('training wall clock time: %5.2f min' % ((finish-start)/60.))\n", "intent": "Now we can score our model\n"}
{"snippet": "cross_val_score(clf, X, y)  \n", "intent": "Of course, usually we care about cross-validated scores.\n"}
{"snippet": "subFile = pd.concat([test.datetime, pd.Series(np.array(model.predict(testToRun)))], axis = 1)\nsubFile.columns = ['datetime', 'count']\n", "intent": "Predict test and subFile out\n"}
{"snippet": "subFile2 = pd.concat([test.datetime, pd.Series(np.array(modelCasual.predict(testToRun))), pd.Series(np.array(modelRegistered.predict(testToRun)))], axis = 1)\nsubFile2.columns = ['datetime', 'casual', 'registered']\nsubFile2.head()\n", "intent": "Predict and subFile2 out\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(\"Accuracy score : {}\".format(accuracy_score(y_test, y_pred)))\nprint(\"Precision score : {}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall score : {}\".format(recall_score(y_test, y_pred)))\n", "intent": "The best parameters correspond to $C=100$ and the best kernel between rbf and linear is the linear kernel.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.to_float(tf.constant([0]))\n    for i in range(len(style_layers)):\n        curr_gram = gram_matrix(feats[style_layers[i]])\n        style_loss = tf.add(style_loss,style_weights[i]* \n                            tf.reduce_sum(tf.square(tf.subtract(curr_gram, style_targets[i]))))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(College_data['Cluster'],kmeans_model.labels_))\nprint(classification_report(College_data['Cluster'],kmeans_model.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "titanic_test_data_X['Survived'] = voting_est.predict(titanic_test_data_X)\n", "intent": "**Predict the output**\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Test accuracy: 74.1627%\n"}
{"snippet": "range_n_clusters = range(5,31)\nplot_s_score(df15D,range_n_clusters)\n", "intent": "<a id='s_score'></a>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    sum = 0\n    for i in range(len(style_layers)):\n        A = gram_matrix(feats[style_layers[i]])\n        G = style_targets[i]\n        w = style_weights[i]\n        sum += w * tf.square(tf.norm(A - G))\n    return sum\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def dice_coef(y_true, y_pred):\n    intersection = mx.sym.sum(mx.sym.broadcast_mul(y_true, y_pred), axis=(1, 2, 3))\n    return mx.sym.broadcast_div((2. * intersection + 1.),(mx.sym.sum(y_true, axis=(1, 2, 3)) + mx.sym.sum(y_pred, axis=(1, 2, 3)) + 1.))\ndef dice_coef_loss(y_true, y_pred):\n    intersection = mx.sym.sum(mx.sym.broadcast_mul(y_true, y_pred), axis=1, )\n    return -mx.sym.broadcast_div((2. * intersection + 1.),(mx.sym.broadcast_add(mx.sym.sum(y_true, axis=1), mx.sym.sum(y_pred, axis=1)) + 1.))\n", "intent": "We will be using the dice coefficient as our loss by optimizing it's negative.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0029.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "model_predictions = [np.argmax(pretrain_model.predict(np.expand_dims(feature, axis=0))) for feature in test]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions=model.predict(X_test)\n", "intent": "Now that we have fit our model to our data, let's use it to predict our tail risk and our results.\n"}
{"snippet": "print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions)) \n", "intent": "Now we're ready to print out our reports.\n"}
{"snippet": "Y_HAT=stem_model.predict(test_data)\n", "intent": "use stem_model.predict\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nMSE_Test=Pred_Test=metrics.mean_squared_error(test_data[\"ShareWomen\"], Y_HAT)\nTRAIN_HAT=stem_model.predict(train_data)\nMSE_Train=Pred_Test=metrics.mean_squared_error(train_data[\"ShareWomen\"], TRAIN_HAT)\nprint(MSE_Train)\n", "intent": "metrics.mean_absolute_error(test_data[\"ShareWomen\"], Y_HAT))\n"}
{"snippet": "def model_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.score(X_train, y_train)\n    print('Train R^2 Score: %.5f ' % (trainScore))\n    testScore = model.score(X_test, y_test)\n    print('Test R^2 Score: %.5f ' % (testScore))\n    return trainScore, testScore\nmodel_score(model, X_train, y_train, X_test, y_test)\n", "intent": "Calculate R^2 score for training and testing datasets\n"}
{"snippet": "FM_predictions = [np.argmax(FM_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(FM_predictions)==np.argmax(test_targets, axis=1))/len(FM_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "X_test = np.c_[sales_test[example_features]]\nY_test = np.c_[sales_test['price']]\npredictions_test = model_exact.predict(X_test, parameters_exact)\nRSS = model_exact.scores(Y_test, predictions_test)\nprint(\"RSS when using example features on TEST data = %1.3e\" % RSS)\n", "intent": "We can now make predictions given by the model, let's compute the RSS on TEST data for the example model:\n"}
{"snippet": "predictions_simple_test = model_simple.predict(X_test, parameters_simple)\n", "intent": "Use your newly estimated weights to compute the predictions on all the TEST data:\n"}
{"snippet": "predictions_2_test = model_2.predict(X_test, parameters_2)\n", "intent": "Use your newly estimated weights to compute the predictions on all the TEST data:\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\npred_proba = sentiment_model.predict_proba(X_test_vectorized)[:,1]\nprint('AUC: ', roc_auc_score(y_test, pred_proba))\n", "intent": "We calculate area under curve (AUC) below to check how well the model we trained can generalize to new dataset.\n"}
{"snippet": "prob_val1 = mlp1.predict_proba(X_norm_val)[:,1]\nprob_val1 = prob_val1.reshape(m_val,1)\n", "intent": "I consider ROC-AUC score to evaluate models of consideration.\n"}
{"snippet": "r2_score(two_years.Close, two_years.yhat)\n", "intent": "Checking the R Square scores\n"}
{"snippet": "cvscore = cross_val_score(cv=5,estimator=xgb1,X = X_train,y = y_train, n_jobs = -1)\n", "intent": "Running the CV for the ensemble regressor crashes it; get an indication on one regressor.\n"}
{"snippet": "with tf.name_scope(\"MSE\"):\n    y_true = tf.placeholder(\"float32\", shape=(None,), name=\"y_true\")\n    y_predicted = tf.placeholder(\"float32\", shape=(None,), name=\"y_predicted\")\n    mse = tf.losses.mean_squared_error(y_true, y_predicted)\ndef compute_mse(vector1, vector2):\n    return mse.eval({y_true: vector1, y_predicted: vector2})\n", "intent": "Your assignment is to implement mean squared error in tensorflow.\n"}
{"snippet": "pred = WC_rf.predict(X_test)\n", "intent": "Let's then predict outcomes on our test set, and see how well we do (measured by root mean squared error).\n"}
{"snippet": "    def predict(self, X):\n        return self._compute_labels(X, self.centroids_)\n", "intent": "If we are given a new data point, we can determine to which cluster it belongs by using the `_compute_labels` function we defined previously.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    layer_losses = []\n    for i in range(len(style_layers)):\n        layer = style_layers[i]\n        gram = gram_matrix(feats[layer])\n        layer_loss = tf.reduce_sum(tf.square(tf.subtract(gram, style_targets[i])))\n        layer_loss = tf.scalar_mul(style_weights[i], layer_loss)\n        layer_losses.append(layer_loss)\n    return tf.add_n(layer_losses)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.10f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.10f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    sumLoss = 0\n    I = 0\n    for idx in style_layers:\n        gram = gram_matrix(feats[idx])\n        sumLoss += content_loss(style_weights[I],gram,style_targets[I])\n        I += 1\n    return sumLoss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.10f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "print((10 + 0 + 20 + 10) / 4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n    $$\\frac{1}{m}\\sum_{i=1}^{m}|y^{(i)}-y'^{(i)}|$$\n"}
{"snippet": "import numpy as np\nprint((10**2 + 0**2 + 20**2 + 10**2) / 4)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error (MSE)** is the mean of the squared errors:\n    $$\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-y'^{(i)})^2$$\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint(\"Test accuracy: \", test_accuracy * 100, \"%\")\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\nnum_test_samples = len(dog_breed_predictions)\nprint(\"The Number of Test images are: {}\".format(num_test_samples))\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "predictions = [np.argmax(Network_model.predict(np.expand_dims(feat, axis=0))) for feat in test_network]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "transform_testing_set = lda.transform(X_test)\ny_pred = clf.predict(transform_testing_set)\n", "intent": "Now that we have trained our SVM on our LDA transformed dataset, we should transform our testing dataset and use the SVM to make predictions:\n"}
{"snippet": "predictions = pl.predict(txt_test)\n", "intent": "We then use that fitted model to make predictions on our testing data.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(label_test,predictions))\n", "intent": "Finally, using Scikit-Learn's classification_report feature, we're going to check what kind of result those predictions gave us.\n"}
{"snippet": "n_clusters = 3\ny_lower = 10\ncluster_labels = kmeans.fit_predict(X)\nsilhouette_avg = silhouette_score(X, cluster_labels)\nsilhouette_values = silhouette_samples(X, cluster_labels)\n", "intent": "$s_i = \\frac{b(i)-a(i)}{max(a_i, b_i)}$\n"}
{"snippet": "rgan_5d = load_rgans_loss(8,'Q network')\nrgan_5d_copy3 = load_rgans_loss(12,'Q network')*2\nrgan_5d_copy4 = load_rgans_loss(14,'Q network')\n", "intent": "RGAN on GAN and CGAN\n"}
{"snippet": "print (\"Accuracy is \", accuracy_score(target_test, target_pred, normalize = True)*100)\n", "intent": "Our model is giving an accuracy of 60%. This is not bad with a simple implementation. \n"}
{"snippet": "predictions = dtree.predict(X_test)\nprint(classification_report(y_test,predictions))\n", "intent": "Measure the accuracy of the resulting decision tree model using your test data.\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Now we predict using the trained model.\n"}
{"snippet": "def lsh_score(lsh_model, Y_train, X_test, Y_test):\n    _, k_neighbors = lsh_model.kneighbors(X_test)\n    Y_pred, _ = mode(Y_train[k_neighbors], axis=1)\n    return accuracy_score(Y_test, Y_pred)\n", "intent": "Before we begin our validation process, we define a function to score a locality sensitive hashing model.\n"}
{"snippet": "start = timer()\nprint(f\"Best LSH Test Accuracy: {lsh_score(lsh_models[best_lsh], Y_train, X_test, Y_test)}\")\nend = timer()\n", "intent": "Using the best model, we want to score the test data set (i.e. get the accuracy of the model) and record the prediction time.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0001.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "DogXception_predictions = [np.argmax(DogXception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXception]\ntest_accuracy = 100*np.sum(np.array(DogXception_predictions)==np.argmax(test_targets, axis=1))/len(DogXception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "flist = [2,8]\nregr = feature_subset_regression(x,y,[2,8])\nprint(\"w = \", regr.coef_)\nprint(\"b = \", regr.intercept_)\nprint(\"Mean squared error: \", mean_squared_error(y, regr.predict(x[:,flist])))\n", "intent": "Try using just features \n"}
{"snippet": "regr = feature_subset_regression(x,y,range(0,10))\nprint(\"w = \", regr.coef_)\nprint(\"b = \", regr.intercept_)\nprint(\"Mean squared error: \", mean_squared_error(y, regr.predict(x)))\n", "intent": "Finally, use all 10 features.\n"}
{"snippet": "dtrain_predictions = lr.predict(X_test_lr)\nrmse = np.sqrt(mean_squared_error(y_test_lr, dtrain_predictions))\nprint (\"RMSE : %.4g\" % rmse)\nprint(\"Coefficient of determination R^2 of the prediction(Score): %.4g\" % lr.score(X_train_lr,y_train_lr))\n", "intent": "Second is to evaluate our model\n"}
{"snippet": "submission_lr = np.round_(lr.predict(X_t),decimals=2)\nnp.savetxt(\"submission_lr123.csv\",submission_lr, delimiter=\",\")\nlr.predict(X_t)\n", "intent": "Save the predicted labels into a CSV file for Kaggel submission.\n"}
{"snippet": "submission_lr = np.round_(rf3.predict(X_t),decimals=2)\nnp.savetxt(\"submission_lr124.csv\",submission_lr, delimiter=\",\")\nrf3.predict(X_t)\n", "intent": "Save the predicted labels into a file for Kaggel submission\n"}
{"snippet": "pred = ens.predict(xval)\n", "intent": "Predictions are generated as usual:\n"}
{"snippet": "print(classification_report(y_test, ypred_test))\nprint(confusion_matrix(y_test, ypred_test))\nprediction_titles = [('predicted: %s\\ntrue: %s' % (ypred_test[i], y_test[i])) \n                    for i in range(ypred_test.shape[0])]\nplot_gallery(X_test, prediction_titles, h, w)\n", "intent": "Quantitative evaluation of the model quality on the test set\n"}
{"snippet": "x1 = new_logistic_regr.predict_log_proba(lag2_training)\nx2 = lda.predict_log_proba(lag2_training)\nfor num in range(5): \n    print(x1[num], x2[num])\n", "intent": "This is the exact same confusion matrix as the last logistc regression model.  Let's see if they have the same predictions.\n"}
{"snippet": "yt = model.predict(trainX1, batch_size = 32) \ns4 =0\nfor i in range(len(yt)):\n    y1 = yt[i]\n    y1 = list(y1)\n    idx = y1.index(max(y1))\n    if trainY[i][idx] == 1:\n        s4 += 1\nprint(s4/len(yt))\n", "intent": "this is the code used to check the in sample (training) performance of the Recurrent Neural Network, high training performance means overfitting.\n"}
{"snippet": "yts_hat = logreg.predict(Xts)\nacc = np.mean(yts_hat == yts)\nprint(\"Accuracy: {:.03f}\".format(acc))\n", "intent": "Now, let's predict the test data and calculate the accuracy\n"}
{"snippet": "predictions = xgc.predict(X_test)\npredictions[:10]\n", "intent": "Here we do the usual, use the trained model to make predictions on the test dataset\n"}
{"snippet": "xgb_model.predict(xgb.DMatrix(X_test.iloc[[j]]))\n", "intent": "Out-of-the-box LIME cannot handle the requirement of XGBoost to use xgb.DMatrix() on the input data\n"}
{"snippet": "y_pred = lr_model.predict(X_test)\n", "intent": "Generate predictions\n"}
{"snippet": "y_pred = xgb_model.predict(X_test)\n", "intent": "Generate predictions\n"}
{"snippet": "predictions = model.predict(image)\n", "intent": "Now that our image is ready, generate predictions by using `.predict` as usual.\n"}
{"snippet": "predictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) +\\\n                               np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "pred_labels = lr.predict(X)\n", "intent": "Plot the probabilities and the predictions.\n"}
{"snippet": "cm = confusion_matrix(y, lr.predict(X))\nsns.heatmap(cm, annot=True)\n", "intent": "Create confusion matrix for the Spotify data and calculate recall and precision scores\n"}
{"snippet": "precision = float(precision_score(y, lr.predict(X)))\nrecall = float(recall_score(y, lr.predict(X)))\nprint(\"The precision is {:.1f}% and the recall is {:.1f}%.\".format(precision * 100, recall * 100))\n", "intent": "If you were a spotify data scientist, would you want a model that produces more false negatives or false positives?\n"}
{"snippet": "y_pred_3yrs = logistic_regression_model_3yrs.predict(test_x_ind_2017)\nconfusion_matrix_3yrs = confusion_matrix(test_y_ind_2017, y_pred_3yrs)\nprint(confusion_matrix_3yrs)\n", "intent": "Above results are scores of applying our logistic regression to testing information of 2017.\n"}
{"snippet": "y_pred = logistic_regression_model_2yrs.predict(test_x_ind_2017)\nconfusion_matrix = confusion_matrix(test_y_ind_2017, y_pred)\nprint(confusion_matrix)\n", "intent": "The result is telling us that we have 187135 correct predictions and 3875+2 incorrect predictions.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100.0*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "correct = 0\nfor i in range(100,500):\n    X, y = get_one_clickstream_sample(clickstream_samples, i, length, n_features_in, n_features_out)\n    Z = get_one_session_sample(session_samples, i, session_length, session_features_in)\n    dataZ = np.array(Z)\n    dataZ = dataZ.reshape(1, session_features_in)\n    yhat = model.predict([X,dataZ])\n    if one_hot_decode_label(yhat) == one_hot_decode_label(y):\n        correct += 1\nprint('Accuracy: %f' % ((correct/400)*100.0))\n", "intent": "And finally a test with data used during training and data not used for training.\n"}
{"snippet": "y_test_preds = gbmr.predict(X_test)\ny_test_preds=(np.exp(y_test_preds)-1)\n", "intent": "SUBMISSION\n-------\nPredicting Test set\n"}
{"snippet": "VInception_predictions = [np.argmax(VInception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(VInception_predictions)==np.argmax(test_targets, axis=1))/len(VInception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import fbeta_score\ncount = int(income.count())\nnaivepred = np.ones(count, dtype=int)\naccuracy1 = accuracy_score(income, naivepred)\nfscore1 = fbeta_score(income, naivepred, beta=0.5)\nprint \"Sklearn Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy1, fscore1)\n", "intent": "** Extra: Lets try the same with sklearn metrics library now to confirm what we did manually above is correct **\n"}
{"snippet": "ge_760 = data_without_656[data_without_656[CASID_COLUMN] >= 760]\nbetween_760_779 = ge_760[ge_760[CASID_COLUMN] <= 779]\nclusters = kmeans.predict(between_760_779)\nunique(between_760_779[CASID_COLUMN].values)\n", "intent": "As we can see, there are a lot of records between the CasId 760 and 779. So, we are going to know what CasId are:\n"}
{"snippet": "ge_45 = data_without_656[data_without_656[SUBCATEGORYID_COLUMN] >= 45]\nbetween_45_49 = ge_45[ge_45[SUBCATEGORYID_COLUMN] <= 49]\nclusters = kmeans.predict(between_45_49)\nunique(between_45_49[SUBCATEGORYID_COLUMN].values)\n", "intent": "Like it happended with the CasIds, there are a lot of records between the SubCategoryId 45 and 49. So, we are going to know what SubCategory are:\n"}
{"snippet": "Combined_predictions = [np.argmax(Combined_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Combined]\ntest_accuracy = 100*np.sum(np.array(Combined_predictions)==np.argmax(test_targets, axis=1))/len(Combined_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\t  \n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: {:.2%}'.format(test_accuracy))\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Resnet50_predict = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predict)==np.argmax(test_targets, axis=1))/len(Resnet50_predict)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "browsing_data = np.array([1,1,1,1,1,1,0,1,0]).reshape(1, -1)\nprint(\"On comparing products: purchase propensity :\",classifier.predict_proba(browsing_data)[:,1] )\n", "intent": "The propensity changes. With each event such as studying customer review the propensity should change if the event is a precursor to purchase\n"}
{"snippet": "from keras import backend as K\ndef max_margin_loss(y_true, y_pred):\n    loss_ =\n    return loss_\n", "intent": "\\begin{equation*}\nloss = \\sum_i{max(0, 1 -p_i + n_i)}\n\\end{equation*}\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, X2train, y2train, cv=10)\nprint (lasso_scores)\nprint (np.mean(lasso_scores))\n", "intent": "Cross-validate the Lasso $R^2$ with the optimal alpha\n"}
{"snippet": "Ypred = best.predict(Xtest)\nnp.savetxt('result_validate.txt', Ypred)\n", "intent": "Now, we can predict the output of the validation data and write it to a file.\n"}
{"snippet": "y_pred_logreg = gs_logreg.predict(X_val)\nwrite_res(y_pred_logreg,\"logreg\")\n", "intent": "KNN\n1\tSun, 13 Nov 2016 17:55:43\n"}
{"snippet": "y_pred_bagging = gs_bagging.predict(X_val)\nwrite_res(y_pred_bagging,\"bagging\")\n", "intent": "Logistic Regression\n2\tSun, 13 Nov 2016 17:58:37\n"}
{"snippet": "y_pred_tree = gs_tree.predict(X_val)\nwrite_res(y_pred_tree,\"tree\")\n", "intent": "Bagging\n3\tSun, 13 Nov 2016 18:01:12 (-0h)\n"}
{"snippet": "results = model_dnn.evaluate(input_fn=lambda: input_fn(train_file_names_full, test_batch_size, \n                                                       test_num_epochs, test_randomize_input), \n                             steps=test_steps)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))\n", "intent": "Test the Deep Neural Network, using the train sample\n"}
{"snippet": "summaries[\"train\"] = dict()\nsummaries[\"train\"][\"results\"] = results \nsummaries[\"train\"][\"predict_proba\"] = model_dnn.predict_proba(\n    input_fn=lambda: input_fn(train_file_names_full, test_batch_size, \n                              test_num_epochs, test_randomize_input))\n", "intent": "Save the output summaries\n"}
{"snippet": "results = model_dnn.evaluate(input_fn=lambda: input_fn(test_file_names_full, test_batch_size, \n                                                       test_num_epochs, test_randomize_input), \n                             steps=test_steps)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))\n", "intent": "Test the Deep Neural Network, using the test sample\n"}
{"snippet": "summaries[\"test\"] = dict()\nsummaries[\"test\"][\"results\"] = results \nsummaries[\"test\"][\"predict_proba\"] = model_dnn.predict_proba(\n    input_fn=lambda: input_fn(test_file_names_full, test_batch_size, \n                              test_num_epochs, test_randomize_input))\n", "intent": "Save the output summaries\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nX_test, Y_test = test_data[:, :n_features], test_data[:, n_features:]\npredicted = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint(rmse)\nprint('The RMSE error was: {:.2f}'.format(rmse))\n", "intent": "This model can be tested using the test samples we generated above:\n"}
{"snippet": "n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "Define a cross-validation strategy\n"}
{"snippet": "err = []\nfor i in np.arange(1,11,2):\n\tfit = one_vs_all(i,train,0.01,2)\n\tclf = fit[0]\n\tY = fit[2]\n\tgram = fit[1]\n\ttrain_pred = clf.predict(gram)\n\terror = 1-np.sum(np.sign(train_pred) == np.sign(Y))/train_pred.shape[0]\n\terr.append(error)\nprint(err)\n", "intent": "Q2. As shown in the output, 0 has the highest $E_{in}$\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_train_all)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the final clustering with the optimal k value.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_all_norm_np)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the final clustering with the optimal k value.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_train_all)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the above clustering.\n"}
{"snippet": "print 'Clustering Completeness Score:', completeness_score(target_all, clusters)\n", "intent": "Compute the completeness of the k = 2 clustering assignments and the class variable.\n"}
{"snippet": "print 'Clustering Homogeneity Score:', homogeneity_score(target_all, clusters)\n", "intent": "Compute the homogeneity of the k = 2 clustering assignments and the class variable.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_all_norm_np)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the above clustering.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_train_np)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the clustering above.\n"}
{"snippet": "print 'Clustering Completeness Score:', completeness_score(target_train_np, clusters)\n", "intent": "Compute the completeness of the k = 2 clustering assignments and the class variable.\n"}
{"snippet": "print 'Clustering Homogeneity Score:', homogeneity_score(target_train_np, clusters)\n", "intent": "Compute the homogeneity of the k = 2 clustering assignments and the class variable.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_train_norm_np)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the clustering above.\n"}
{"snippet": "print 'Clustering Homogeneity Score:', homogeneity_score(target_train_np, clusters)\n", "intent": "Compute the homogeniety of the k = 2 clustering assignments and the class variable.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_full_norm_trans)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the final clustering with the optimal k value.\n"}
{"snippet": "sse_center_dist = calc_cluster_error(centers, clusters, data_full_norm_trans)\nprint 'Total Clustering Sum Squared Error:'\nprint sse_center_dist\n", "intent": "Calculate the sum square error of the above clustering.\n"}
{"snippet": "tweet = \"I am very happy :)\"\ntest_vector = vectorizer.transform([tweet])\nprediction = classifier_linear.predict(test_vector)\nprint('\"' + tweet + '\"')\nprint(prediction[0])\n", "intent": "As you can see for our trivial tweet, our SVM correctly predicts the sentiment to be positive:\n"}
{"snippet": "predictions = clf.predict(X_val)\ncount = 0\nfor x in range(len(predictions)):\n    if y_val[x] != predictions[x]:\n        count += 1\nprint(\"\nprint(\"\nprint(\"% accuracy: \" + str(float(len(y_val)-count)/float(len(y_val))))\n", "intent": "Now that the classifier has been trained we can run the classifier on the test set and see how accurate the decision tree model was on this dataset.\n"}
{"snippet": "model0.predict(numpy.array(images))\n", "intent": "And now we give these images to our model and take a look at what the filter has found. \n"}
{"snippet": "def predict_images(images):\n    resized_images = []\n    for image in images:\n        resized_images.append(numpy.resize(image, (image_size, image_size, 1)))\n    return model1.predict(numpy.array(resized_images))\n", "intent": "Again, run our test images through the model to see what the filters output.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(clf,data,target,cv=10)\nconf_mat = confusion_matrix(target,y_pred)\n", "intent": "GOOD!! It was improved around 8%.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(clf,data,target,cv=10)\nconf_mat = confusion_matrix(target,y_pred)\nconf_mat\n", "intent": "Just 1% improved....\n"}
{"snippet": "x_test = np.array(['You the great'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n    source = np.expand_dims(source, axis=0)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "print(metrics.confusion_matrix(y_test, rf_predict_test, labels=[1, 0]) )\nprint(\"\")\nprint(\"Classification Report\")\nprint(metrics.classification_report(y_test, rf_predict_test, labels=[1,0]))\n", "intent": "suffering from overfitting\n"}
{"snippet": "def music_score(score):\n    y = np.zeros(0)\n    for i in score:\n        t = np.linspace(0, 1, i[1] * 16) \n        x = np.sin(2 * np.pi * i[0] * t)\n        y = np.append(y, x)\n    return y    \n", "intent": "Let's having fun a while.\n"}
{"snippet": "preds = reg.predict(X_test)\nr2 = performance_metric(y_test, preds)\nprint \"Model has a coefficient of determination, R^2, of {:.3f}.\".format(r2)\n", "intent": "The maximum depth for the optimal model is 3, close to my previous guest of 4.\n"}
{"snippet": "print(classification_report(y_true, y_pred, target_names=labels))\n", "intent": "Classification report\n"}
{"snippet": "encoded_X0 = encoder.predict([X_train[0].reshape((1, 784)), y_train[0].reshape((1, 10))])\nprint(encoded_X0)\n", "intent": "Now, let's see how this model represents that same digit in the latent space by passing it through the encoder.\n"}
{"snippet": "reg.predict(X_test) - y_test\n", "intent": "Finally, for good measure, the errors in table form:\n"}
{"snippet": "scores_knn = cross_val_score(knn_model, X_train, y_train, cv=10)\nprint(scores_knn)\n", "intent": "__**Cross-validation Scores (stratified k-fold):**__\n"}
{"snippet": "scores_log_reg = cross_val_score(log_reg, X_train, y_train, cv=10)\nprint(scores)\n", "intent": "__*Cross-validation Scores (stratified k-fold):*__\n"}
{"snippet": "scores_svm = cross_val_score(svm_model, X_train, y_train, cv=10)\nprint(scores_svm)\n", "intent": "__**Cross-validation Scores (stratified k-fold):**__\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Step 9: Evaluate the model on the test dataset (10,000 images)\n"}
{"snippet": "loaded_model.compile(loss='categorical_crossentropy',\n              optimizer=OPTIMIZER,\n              metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, Y_test, verbose=VERBOSE)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1])\n", "intent": "[Optional] Step 13: Compile and evaluate loaded model\n"}
{"snippet": "loaded_model.compile(loss='categorical_crossentropy',\n              optimizer=OPTIMIZER,\n              metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, Y_test, verbose=VERBOSE)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1])\n", "intent": "[Optional] Step 15: Compile and evaluate loaded model\n"}
{"snippet": "preds = model.predict(image)\nP = imagenet_utils.decode_predictions(preds)\nfor (i, (imagenetID, label, prob)) in enumerate(P[0]):\n    print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n", "intent": "Step 1: Call predict\n<br>Step 2: Decode the predictions\n<br>Step 3: Print the top 5 predictions and their probabilities\n"}
{"snippet": "x_pred = np.arange(min(Xtrain), max(Xtrain), len(Xtrain)/3).reshape(-1, 1)\ny_pred = reg_bias.predict(x_pred)\n", "intent": "Easy enough. Let's look a little closer at the fitted model.\n"}
{"snippet": "optimal_ell = search.best_estimator_\npred = optimal_ell.predict(X2)\n", "intent": "Remember, we're in $R^{11}$ so we'll need our PCA-compressed representations to visualize these results.\n"}
{"snippet": "for product in labels.columns.values:\n    print(\"Evaluating\", product)\n    label = labels[product]\n    print()\n    %time print(\"Score:\", cross_val_score(pipeline, features, label, cv=10, n_jobs=-1).mean())\n    print()\n", "intent": "To check our pipeline, we should run it against all of our products(may take a while)\n"}
{"snippet": "scores = cross_val_score(knn, X, y, cv=20, scoring='accuracy')\ncross_validation=scores.mean()\nprint(cross_validation) \n", "intent": "Cross-validation on 20 Folds\n"}
{"snippet": "scores = cross_val_score(knn, X, y, cv=20, scoring='accuracy')\ncross_validation=scores.mean()\nprint(cross_validation) \n", "intent": "Cross-validation on 20 folds\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint('Confusion Matrix', '\\n'*2,confusion_matrix(df['Cluster'],kmeans.labels_), '\\n')\nprint('Classification Report','\\n'*2 ,classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\ny_pred = grid_search.predict(X_test)\nlabels=['non-cars', 'cars']\nprint(classification_report(y_test, y_pred, target_names=labels))\nprint(confusion_matrix(y_test, y_pred, labels=range(2)))\n", "intent": "<a id='modeleval'>\n</a>\nback to [Table Of Contents](\n"}
{"snippet": "models = {'age + subreddit + title':gs_rf.predict_proba(X_test_full),\n         'age + subreddit':as_rf.predict_proba(X_test_as)}\nanother_roc_plotter(models, y_test, \"RoC | Random Forest Classifiers on Varying Features\")\n", "intent": "An improvement!  More evidence that titles are less important than age and subreddit.  Let's compare the RoC curves here.\n"}
{"snippet": "cross_val_score(clf, X_test, y_test, cv=5)\n", "intent": "90% accuracy, once again using gradient boosting. Let's look at a cross validation. \n"}
{"snippet": "batch_size = 100\nbatch_indices = np.random.randint(limit) * np.ones((batch_size,), dtype=int)\nZ = vae.encoder.predict(X[batch_indices])\nZ = Z / 100.\nfor i in xrange(1,100):\n    Z += vae.encoder.predict(X[:batch_size])/100.\nprint Z.shape\nprint batch_indices[0]\n", "intent": "Encode first batch of data.\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "Predict the labels for the test set.\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "What is the accuracy of this new model?\n"}
{"snippet": "all_merge_model_predictions = []\nfor i in range(test_resnet50.shape[0]):\n    feature = [np.expand_dims(test_vgg16[i] , axis=0),\n               np.expand_dims(test_vgg19[i], axis=0),\n               np.expand_dims(test_resnet50[i], axis=0),\n               np.expand_dims(test_inceptionv3[i], axis=0),\n               np.expand_dims(test_xception[i], axis=0)]\n    all_merge_model_predictions.append(np.argmax(all_merge_model.predict(feature)))\ntest_accuracy = 100*np.sum(np.array(all_merge_model_predictions)==np.argmax(test_targets, axis=1))/len(all_merge_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "metrics.confusion_matrix(train_final_y, rf.predict(train_final_x))\n", "intent": "[precision](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n"}
{"snippet": "predicted = model.predict(X_test)\nerror = numpy.sqrt(((predicted - y_test) ** 2).mean(axis=0))\n", "intent": "Evaluate Model\n--------------\n"}
{"snippet": "transfer_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_bottleneck]\ntest_accuracy = 100*np.sum(np.array(transfer_predictions)==np.argmax(test_targets, axis=1))/len(transfer_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "im = Image.open(Y_test_filepath)\nY_file_vals = translate_n0r(list(im.getdata()), im.getpalette())\nprint(mse_euclidian_error(Y_file_vals, predicted_radar))\n", "intent": "Finding the MSE and Euclidian Error rates of the last (visual) example - the predicted frame versus the actual next frame.\n"}
{"snippet": "print(mse_euclidian_error(Y_file_vals, Y_file_vals))\n", "intent": "The lowest error rate can be seen below, demonstrated as a comparison between two identical arrays.\n"}
{"snippet": "random_1 = []\nrandom_2 = []\nfor i in range(400):\n    random_1.append(random.randrange(0,76))\n    random_2.append(random.randrange(0,76))\nprint(mse_euclidian_error(random_1, random_2))\n", "intent": "Below: a high error rate generated by comparing two randomly-generated arrays of the same size as our 20-by-20 scaled-down radar images.\n"}
{"snippet": "yts_pred = regr.predict(Xts)\nprint(yts_pred)\nRSS_ts = np.mean((yts_pred-yts)**2)/(np.std(yts)**2)\nprint(\"RSS per sample = {0:f}\".format(RSS_ts))\nprint(Xts.shape)\nprint(Xts[105])\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def cutoff_predict(clf, X, cutoff):\n    return (clf.predict_proba(X)[:, 1]> cutoff).astype(int)\ndef custom_f1(cutoff):\n    def f1_cutoff(clf, X, y):\n        ypred = cutoff_predict(clf, X, cutoff)\n        return sklearn.metrics.f1_score(y, ypred)\n    return f1_cutoff\n", "intent": "Determine the a good cutoff point from prediction probability for class membership \n"}
{"snippet": "print(metrics.flat_classification_report(\n    y_test, y_pred, labels=labels, digits=3\n))\n", "intent": "Inspect per-class results in more detail:\n"}
{"snippet": "crf = rs.best_estimator_\ny_pred = crf.predict(X_test)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=labels, digits=3\n))\n", "intent": "As you can see, quality is improved. Here estimator is for 60 fits and 100 iterations max.\n"}
{"snippet": "crf = rs.best_estimator_\ny_pred = crf.predict(X_test)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=labels, digits=3\n))\n", "intent": "90 fits and 200 iterations max\n"}
{"snippet": "def predict(model, X):\n    return np.apply_along_axis(model, axis=1, arr=X)\ny_pred = predict(naive_decision_rule, X)\nprint(y_pred)\n", "intent": "The following function takes a decision rule (or model) and a matrix of data points to generate the predictions for this matrix.\n"}
{"snippet": "cnn_predictions = [np.argmax(cnn.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy1 = 100*np.sum(np.array(cnn_predictions)==np.argmax(test_targets, axis=1))/len(cnn_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy1)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum((G - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = lg.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)\n", "intent": "The confusion matrix shows that 223(25%) of non-spam data is predicted as spam. The model is overall weak at detecting spam messages.\n"}
{"snippet": "print (\"Confusionn Matrix:\")\ny_c_pred = lg.predict(X_c_test)\ny_c_pred_arr = y_c_pred.tolist()\ny_c_test_arr = np.array(y_c_test.label_spam).tolist()\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_c_test, y_c_pred)\nprint(confusion_matrix)\nprint (\"Classification Report: \")\nprint(classification_report(y_c_test, y_c_pred))\n", "intent": "The false positive rate is still high.\n"}
{"snippet": "my_Resnet50_predictions = [np.argmax(my_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(my_Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(my_Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ypred=clfsvm.predict(Xtest)\nconfusion_matrix(ytest, ypred)\n", "intent": "Our classifier above had, as one of its printed outputs, a confusion matrix. It looked like this:\n"}
{"snippet": "svmval_averagepp=average_profit_pp(ytest, clfsvm.predict(Xtest), u)\nsvmval_averagepp\n", "intent": "Using the SVM classifier we calculated, `clfsvm`, calculate the profit we can make\n"}
{"snippet": "clflog_val_averagepp = average_profit_pp(ytest, clflog.predict(Xtest), u)\nclflog_val_averagepp\n", "intent": "Calculate the profit that this classifier gives us:\n"}
{"snippet": "average_profit_pp(ytest,clflog.predict(Xtest), u)\n", "intent": "**(a) Average profit per person for t=0.5 (the usual case)**\n"}
{"snippet": "confusion_matrix(ytest,t_repredict(clflog, 0.05, Xtest))\n", "intent": "**(b) Confusion Matrix and average profit per person for t=0.05**\n"}
{"snippet": "average_profit_pp(ytest, t_repredict(clflog, 0.95, Xtest), u)\n", "intent": "**(c) average profit per person for t=0.95**\n"}
{"snippet": "def log_likelihood(clf,x,y):\n    prob = clf.predict_log_proba(x)\n    pos = y\n    neg = ~pos\n    return prob[neg, 0].sum() + prob[pos, 1].sum()   \n", "intent": "Using clf.predict_logproba, write a function that computes the log-likelihood of a dataset\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(\n            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(\"Maximum error in d_loss: %1.15e\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %1.15e\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:1.15e}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for j in range(len(style_layers)):\n        i = style_layers[j]\n        G = gram_matrix(feats[i])        \n        style_loss += style_weights[j]*torch.sum((G-style_targets[j])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.)\n    for j in range(len(style_layers)):\n        i = style_layers[j]\n        style_loss += style_weights[j]*tf.norm(gram_matrix(feats[i]) - style_targets[j])**2\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "totals['predicted1'] = lr1.predict(X1)\ntotals['predicted2'] = lr2.predict(X2)\ntotals['predicted3'] = lr3.predict(X3)\n", "intent": "After calling ```.predict``` on the three models above, let's plot the actual 2016 and the predicted values from the models.\n"}
{"snippet": "best_model = models.__getmodel__('{0}_modelsearch_example_f1'.format(current_date))\nprediction = [p[1] for p in best_model.predict_proba(x_test[features])]\nhistogram = hist(prediction)\n", "intent": "Here I'll predict performance on the test set.  You'll make a holdout set before doing the test/train split above, and then score that here.\n"}
{"snippet": "import sklearn.metrics as mt\nprint(mt.confusion_matrix(dfClg['Cluster'],km.labels_))\nprint('\\n')\nprint(mt.classification_report(dfClg['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_true = df['TARGET CLASS']\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "metrics.mean_absolute_error(y_true = y_test,\n                            y_pred = y_pred)\n", "intent": "These numbers seem very small, let's calculate them again using the sklearn metrics.\n"}
{"snippet": "y_pred = lm.predict(X)\n", "intent": "The results are identical. Let's make predictions on the whole dataset.\n"}
{"snippet": "y_pred = r_forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(mt.classification_report(y_test,y_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0., tf.float32)\n    for i in range(len(style_layers)):\n        gram_mat = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((gram_mat - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "train_data['says_please'] = train_data['request_text_edit_aware'].str.lower().str.contains(\"please\")\nsuccess_vs_fail_report('says_please')\ntrain_data['says_need'] = train_data['request_text_edit_aware'].str.lower().str.contains(\"need\")\nsuccess_vs_fail_report('says_need')\n", "intent": "As a warm up, let's investigate the hypothesis that being polite makes a difference.\n"}
{"snippet": "VGG19_breed_prediction = [np.argmax(VGG19_breed_classifier.predict(np.expand_dims(bt_features, axis=0)))\n                          for bt_features in test_VGG19]\ntest_accuracy = 100*np.sum(np.asarray(VGG19_breed_prediction) == np.argmax(test_targets, axis=1))/len(VGG19_breed_prediction)\nprint(\"Test accuracy is {0:0.4f}\".format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict1=model1.predict(x_test1)\npredict1[predict1<0]=0.05\npredict2=model2.predict(x_test2)\npredict2[predict2<0]=0.05\n", "intent": "Looking at the predictions, we see that some are negative which is impossible. I'll set them as 0,05\n"}
{"snippet": "predict1=model1.predict(x_test1)\npredict1[predict1<0]=0.05\npredict2=model2.predict(x_test2)\npredict2[predict2<0]=0.05\nprint(\" Mean absolute error :\")\nprint(\"Model 1:\",mean_absolute_error(predict1,y_test1))\nprint(\"Model 2:\",mean_absolute_error(predict2,y_test2))\n", "intent": "Slighlty better scores for model 2, but I feel we can do a lot better with other models ! Let's see the mean absolute errors\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nconfusion_matrix(y_test, lm.predict(x_test))\n", "intent": "Produce the accuracy score of the logistic regression from the test set\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "that's it, and we have a working model. Let's try it out:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "it's working, but not great.. Let's measure the error:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\ndisplay_scores(tree_rmse_scores)\n", "intent": "We can use cross validation to better estimate the models error. Here we will run validation 10 times on different splits:\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n    scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Let's measure the linear regression:\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n", "intent": "We can now test our finely tuned model against the original holdout set:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "We can use X-Validation just like in Chapter 2:\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "~ 95% accuracy seems good, but also 10% of items are 5 anyways. We can create a classifier that classifies everything as not-5 for comparison:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(precision_score(y_train_5, y_train_pred))\nprint(recall_score(y_train_5, y_train_pred))\n", "intent": "We can calculate precision and recall directly:\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "It's about 90% sure it's a 5, but 10% chance it could be a 0.\nLet's check how well SGD performs in x-val:\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n", "intent": "It can be useful to see what kinds of errors your model makes. Let's look at the confusion matrix:\n"}
{"snippet": "Y_pred = model.predict(X_test)\naccuracy = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n", "intent": "Next we will use the model to predict churning on our 30% validation random sample:\n"}
{"snippet": "predicted = cross_validation.cross_val_predict(lsvc_l1, X, y, cv=10)\nprint(metrics.accuracy_score(y, predicted))\nclassification_results = metrics.classification_report(y, predicted)\nprint(classification_results)\n", "intent": "Assess the goodness of the model.\n"}
{"snippet": "test_comment = 'Fuck you!'\ny_pred = clf_pipeline.predict([test_comment])\nprint'%s: %s' % (test_comment, class_names[y_pred[0]])\n", "intent": "Like we did before, we can test how well our classifier is doing by inputing our own comments.\n"}
{"snippet": "test_comment = 'I do not agree!'\ny_pred = knn_pipeline.predict([test_comment])\nprint'%s: %s' % (test_comment, class_names[y_pred[0]])\n", "intent": "Let's test and see how well new comments are classified on our `knn_pipeline`.\n"}
{"snippet": "from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_sample(pre_process_data, label)\ncross_val_score(rf, X=X_resampled, y=y_resampled, cv=10).mean()\n", "intent": "Since it's an unbalanced dataset we can use RandomOverSampler to generate new samples in the classes which are under-represented\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, predict))\nprint('\\n')\nprint(confusion_matrix(y_test, predict))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "item_prediction = predict(train_data_matrix, item_similarity, kind='item')\nuser_prediction = predict(train_data_matrix, user_similarity, kind='user')\nitem_prediction[item_prediction > 5] = 5\nitem_prediction[item_prediction < 1] = 1\nuser_prediction[user_prediction > 5] = 5\nuser_prediction[user_prediction < 1] = 1\nRMSE['User-based CF'] = rmse(user_prediction, test_data_matrix)\nRMSE['Item-based CF'] = rmse(item_prediction, test_data_matrix)\nprint 'User-based RMSE: ', RMSE['User-based CF']\nprint 'Item-based RMSE: ', RMSE['Item-based CF']\n", "intent": "I will use the function to calculate the user and item predictions then evaluate the accuracy.\n"}
{"snippet": "train_resids = np.exp(y_train) - np.exp(best_lasso.predict(X_train))\ntest_resids = np.exp(y_test) - np.exp(best_lasso.predict(X_test))\n", "intent": "From the best model we can get the residuals.\n"}
{"snippet": "y_score = gs_ap.decision_function(X_test)\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\naverage_precision = average_precision_score(y_test, y_score)\n", "intent": "[See the sklearn plotting example here.](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n"}
{"snippet": "skm.roc_auc_score(y_test, gbc.predict(X_test))\n", "intent": "This looks a bit better.\n"}
{"snippet": "pred = knn.predict([[14,14,88,566,1,0.08,0.06,0.04,0.18,0.05]])\nprint(pred)\n", "intent": "Finally, we can make our prediction model on the patient with all the available features.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(x, bernoulli.cdf(x,p))\n", "intent": "<li>The above block of code checks the accuracy of the Cumulative Density Function and the Percent Point Function\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "Compute precision, recall, F-measure and support\n"}
{"snippet": "lr.predict(min_income)\n", "intent": "We can use the `predict` method to predict a credit card decision value given a yearly annual income value.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Predicing the test set \n"}
{"snippet": "cmDecisionNB1 = confusion_matrix(y_test1,classifierNB.predict(X_test1))\ncmDecisionNB1\n", "intent": "<h4><li>Evaluating the model using <b>First Data Set</b>.</li></h4>\n"}
{"snippet": "cmDecisionNB2 = confusion_matrix(y_test2,classifierNB.predict(X_test2))\ncmDecisionNB2\n", "intent": "<h4><li>Evaluating the model using <b>Second Data Set</b>.</li></h4>\n"}
{"snippet": "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "A lot of this code is from a blog I found. The next cell is to make a metric for a graph later.\n"}
{"snippet": "predictions = lr.predict(X_test)\ndisplay((y_test - predictions)/y_test, np.mean((y_test - predictions)/y_test))\nmean_squared_error(y_true = y_test, y_pred = predictions)\n", "intent": "some error statistics on the most recent model I guess, let me know if you want a particular one or more info in general\n"}
{"snippet": "print(\"Accuracy:\\t\", metrics.accuracy_score(df['cluster'], kmeans.labels_)) \nprint(\"Homogeneity:\\t\", metrics.homogeneity_score(df['cluster'], kmeans.labels_)) \nprint(\"Completeness:\\t\", metrics.completeness_score(df['cluster'], kmeans.labels_)) \nprint(\"V:\\t\\t\", metrics.v_measure_score(df['cluster'], kmeans.labels_)) \nprint(\"RI:\\t\\t\", metrics.adjusted_rand_score(df['cluster'], kmeans.labels_)) \nprint(\"AMI:\\t\\t\", metrics.adjusted_mutual_info_score(df['cluster'], kmeans.labels_), \"\\n\")  \nprint(\"Confusion matrix:\\n\", metrics.confusion_matrix(df['cluster'], kmeans.labels_), \"\\n\") \nprint(\"Classification report:\\n\", metrics.classification_report(df['cluster'], kmeans.labels_)) \n", "intent": "<b><h2> Evaluation </h2></b>\n"}
{"snippet": "prediction = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "pred2= pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "xc_predictions = [np.argmax(xc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xc]\ntest_accuracy = 100*np.sum(np.array(xc_predictions)==np.argmax(test_targets, axis=1))/len(xc_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = model.evaluate(X, y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "We then evaluate how well the model has done.**evaluate()** Returns the loss value & metrics values for the model in test mode.\n"}
{"snippet": "y_pred = classifier.predict(X_test)  \nprint(classification_report(y_test, y_pred))\n", "intent": "We will now predict the X_test values and then print the classification report .Where we can see the recall,precision and f1 score metrics\n"}
{"snippet": "preds_per_model = {}\nall_y = []\nfor year, season in season_data:\n    print('Season year: ', year)\n    _, sess_predicts, sess_res = season_report(season, roc_visualization=True)\n    preds_per_model = {m: preds_per_model.get(m, []) + list(preds)\n                   for m, preds in sess_predicts}\n    all_y += list(sess_res)\n    print()\n", "intent": "Running all seasons report and there testing all attributes together\n"}
{"snippet": "preds = target.predict(x_test_adv)\nacc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\nprint(\"\\nAccuracy on adversarial samples: %.2f%%\" % (acc * 100))\n", "intent": "Now let's evaluate our NN with adversarial examples on the tensorflow and keras based NN\n"}
{"snippet": "def spam_filter(message):\n    if clf.predict(vectorizer.transform([preprocess_text(message)])):\n        return 'spam'\n    else:\n        return 'not spam'\n", "intent": "Lets create a function that takes message as input and tells us whether the message is spam or ham.\n"}
{"snippet": "precision_score(y_train_5, y_train_pred)\n", "intent": "$$\nprecision = \\frac {True Positives} {True Positives + False Positives}\n$$\n$$\nrecall = \\frac {True Positives} {True Positives + False Negatives}\n$$\n"}
{"snippet": "predictions = predict(parameters, X)\nprint(predictions.shape)\nprint(Y.shape)\nprint(Y.size)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "print \"Root Mean Squared Logarithmic Error Train: \", rmsele(rf_opt.predict(X_train), y_train)\nprint \"Root Mean Squared Logarithmic Error Test: \", rmsele(rf_opt.predict(X_test), y_test)\nprint \"Training accuracy: %0.2f%%\" % (100*rf_opt.score(X_train, y_train))\nprint \"Test accuracy: %0.2f%%\" % (100*rf_opt.score(X_test, y_test)) + \"\\n\"\n", "intent": "The above plot shows that there is a high correlation between actual/predicted rental counts\n"}
{"snippet": "predict(count_vectorizer, logreg, test_data)\n", "intent": "Nothing impressive - only 2% better better than the classifier that thinks that everything is a comedy.\n"}
{"snippet": "predict(n_gram_vectorizer, logreg, test_data)\n", "intent": "The results are worse than using a tokenizer and bag of words. Probably due to not removing the stop words.\n"}
{"snippet": "predicted = knn.predict(X_test)\n", "intent": "__Warning__: 10 minutes runtime on 7 cores, 1 hour on 4 core laptop\n"}
{"snippet": "y_tr_pred = regr.predict(X_tr)\nRSS = np.mean((y_tr_pred-y_tr)**2)/(np.std(y_tr)**2)\nprint(\"Normalized RSS={0:f}\".format(RSS))\n", "intent": "Compute the error on the training data\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\n", "intent": "We can then use the RFC object to predict the X test set again.\n"}
{"snippet": "print(classification_report(y_test,rfc_pred))\n", "intent": "In this case it proves that that Model is better in a forest and predicted better results when compared to the training data.\n"}
{"snippet": "example_client = example_client.reshape(1, -1) \nprediction = clf.predict(example_client)\nprint(prediction)\n", "intent": "All we do is pass this array into the clf model and ask it to predict the class.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Now use the Predict function and assign the output to an object for review.\n"}
{"snippet": "print(classification_report(le.transform(Y_test), predicted, target_names=le.classes_, digits=2))\n", "intent": "Below we show precision recall and f1-score of classifier.\n"}
{"snippet": "def nll_true(theta, X):\n    g.set_value(theta[0])\n    return (p0.nnlf(X) - p1.nnlf(X)).sum()\ndef nll_approx(theta, X):\n    g.set_value(theta[0])\n    return -np.sum(cc_decomposed.predict(X, log=True))\n", "intent": "Next let us construct the log-likelihood curve for the artificial dataset. \n"}
{"snippet": "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "predict the class for each (x,y) pair in the grid\n"}
{"snippet": "y_preds = lr.predict(X_test)\n", "intent": "<a id='Plot the best model result'></a>\n"}
{"snippet": "svc.grid_predict(X_test, Y_test)\n", "intent": "then I can test the quality of the prediction with respect to the test data:\n"}
{"snippet": "predictions = votingC.predict(X_test)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y_test, predictions)))\n", "intent": "Finally, we can create a prediction for this model:\n"}
{"snippet": "classifiers = [(svc, 'Support Vector Machine'),\n                (lr, 'Logostic Regression'),\n                (knn, 'k-Nearest Neighbors'),\n                (tr, 'Decision Tree'),\n                (rf, 'Random Forest'),\n                (gb, 'Gradient Boosting')]\nfor clf, label in classifiers:\n    print(30*'_', '\\n{}'.format(label))\n    clf.grid_predict(X, Y)\n", "intent": "It remains only to examine the predictions of the different classifiers that have been trained in section 5:\n"}
{"snippet": "y_preds = c.predict(X_test)\nprint(classification_report(y_test,y_preds))\n", "intent": "So, our test accuracy increased with adjusting the hyper parameter\n"}
{"snippet": "predicted_resp = model.predict(predictfrom_ind)\nprint(\"predicted_resp.shape = \",predicted_resp.shape)\n", "intent": "The model is trained!\nLet's make a prediction.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nmean_squared_error(y, y_hat), np.sqrt(mean_squared_error(y, y_hat))\n", "intent": "Package `sklearn` has convinient functions that help calculate $MSE$ and $R^2$. Let's look at them. \n"}
{"snippet": "y_train_hat = reg.predict(X_train)\nprint(\"On train: RMSE={}, R2={}\".format(\n    np.sqrt(mean_squared_error(y_train, y_train_hat)), \n    r2_score(y_train, y_train_hat)))\n", "intent": "Let's compare it to performance on train.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_test_hat)\n", "intent": "90%! not bad. \nThe package `sklearn` has also convenient function that calculate accuracy score.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_test, y_test_hat), recall_score(y_test, y_test_hat)\n", "intent": "Again we have convenient functions for calculating it.\n"}
{"snippet": "f_score(precision_score(y_test, y_test_hat), recall_score(y_test, y_test_hat))\n", "intent": "So the second one seems to be slightly better.\nNow let's come back to cancer example and calculate f-score for it.\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_test, y_test_hat)\n", "intent": "Or using function form `sklearn`.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, y_test_score)\n", "intent": "Finally the area under the ROC curve is __AUC__. Let's use a function from `sklearn` to calculate it.\n"}
{"snippet": "np.sqrt(mean_squared_error(y_dev, y_dev_hat))\n", "intent": "This time it looks better.\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test, tuned_forest_predictions)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "results = test.map(lambda lp: (lp.label, float(model.predict(lp.features))))\nprint(results.take(10))\ntype(results)                       \n", "intent": "Run it on the test data\n"}
{"snippet": "valuesAndPreds = test.map(lambda p: (float(model.predict(p.features)), p.label))\nmetrics = RegressionMetrics(valuesAndPreds)\nprint(\"MSE = %s\" % metrics.meanSquaredError)\nprint(\"RMSE = %s\" % metrics.rootMeanSquaredError)\nprint(\"R-squared = %s\" % metrics.r2)\nprint(\"MAE = %s\" % metrics.meanAbsoluteError)\nprint(\"Explained variance = %s\" % metrics.explainedVariance)\n", "intent": "Now validate the model on the test set, and check the Root Mean Squared Error.\n"}
{"snippet": "num_rounds_train = 600\nlgb_model=lgb.train(params,dlgb,num_rounds_train)\nypred_lgbm = lgb_model.predict(X_na)\n", "intent": "Based on the CV results it looks like the model performance plateaus around 600 rounds. This is what we will use for our model.\n"}
{"snippet": "pred = model_tfidf.predict(rev1_dtm)\n", "intent": "Have the model make predictions on rev1\n"}
{"snippet": "predictions = clf.predict(features_test)\n", "intent": "**6 - Predictions**\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(labels_test, predictions)\n", "intent": "Surprisingly, not that bad for such a dummy model!\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogInceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix , classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "best_model = grid_search.best_estimator_\nevaluate(best_model, x_train, y_train, x_test, y_test)\n", "intent": "The grid seach also picks the best model for you automatically which you can then use with the optimal parameters from the grid search\n"}
{"snippet": "test_predictions_array = learner.predict(is_test=True)\n", "intent": "Set the is_test flag to True to tell the model to predict on the test set.<br>\n"}
{"snippet": "precision_score(y_test, preds) \n", "intent": "`3.` Additional metrics to test model\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, layer in enumerate(style_layers):\n        loss += style_weights[i] * torch.sum((gram_matrix(feats[layer]) - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        style_loss += style_weights[i] * 2 * tf.nn.l2_loss((gram_matrix(feats[layer]) - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix ,accuracy_score\ncm=confusion_matrix(y_test,y_pred)\nacc=accuracy_score(y_test,y_pred)\n", "intent": "Evaluating the model\n"}
{"snippet": "model.predict(test_sents)\n", "intent": "it seems that it predicts only neutral (second column has the max. value) below\n"}
{"snippet": "predict1 = model.predict('2009','2012')\npredict1\n", "intent": "8\\. Compute prediction for years 2009-2012 and analyze their fit against actual values. (1 point)\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_predit = exported_pipeline.predict(X)\n", "intent": "Using all the data (test and training), the model explained variance score is 0.91. The best possible score is 1.0.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_true = np.ones((1331,1), dtype=np.int)\naccuracy_score(y_true, y_pred_train)\n", "intent": "<b> 15) Measuring the accuracy of ONE CLASS SVM </b>\n   <b> The accuracy achieved is 86% <b>\n"}
{"snippet": "fpr, tpr, thresholds = roc_curve(Y_test, best_lrc.predict_proba(X_test)[:,1])\nprint \"ROC Area Under Curve (AUC) = %0.2f\" % auc(fpr, tpr)\n", "intent": "***\nPlotting the ROC curve and calculating AUC will help us to compare later different classification models.\n"}
{"snippet": "confusion_matrix(Y_test, knc.predict(X_test))\n", "intent": "The kNN classifier doesn't perform better than the previous Logistic Regression classifier with this dataset.\n"}
{"snippet": "pred = rg.predict(X_poly)\nconfusion_matrix(y_train_v,pred)\n", "intent": "The coeffiecient and interception are much smaller than them while we use SGDClassifier. Therefore, RidgeClassifier can compute much faster.\n"}
{"snippet": "print(sorted(zip(map(lambda x: round(x, 4),\n                     rf.feature_importances_), X), reverse=True))\ny_predict = rf.predict(Xtest)\nr2 = r2_score(ytest, y_predict)\n", "intent": "When plotting the true vs. predited values, we get an r_squared value of roughly 0.8, not bad.. on a first run.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Accuracy =\", accuracy_score(y_test, clf.predict(X_test)))\n", "intent": "Definition: The accuracy is the proportion of correct predictions.\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import fbeta_score\nprint(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\nprint(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\nprint(\"F-Score =\", fbeta_score(y_test, clf.predict(X_test), beta=1))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$\n$$Recall = \\frac{TP}{TP + FN}$$\n$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, clf.predict(X_test))\n", "intent": "Definition: number of samples of class $i$ predicted as class $j$.\n"}
{"snippet": "Ince_predictions = [np.argmax(Ince_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Ince]\ntest_accuracy = 100*np.sum(np.array(Ince_predictions)==np.argmax(test_targets, axis=1))/len(Ince_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xcept_predictions = [np.argmax(Xcept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xcept]\ntest_accuracy = 100*np.sum(np.array(Xcept_predictions)==np.argmax(test_targets, axis=1))/len(Xcept_predictions)\nprint('Test accuracy: %.3f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cv_scores = []\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        cv_scores.append(log_loss(val_y, preds))\n        print(cv_scores)\n        break\n", "intent": "Now let us do some cross validation to check the scores. \nPlease run it in local to get the cv scores. I am commenting it out here for time.\n"}
{"snippet": "def classification_error(probabilites):\n    return 1. - np.max(probabilites)\ncex = np.apply_along_axis(classification_error, 0, X)\n", "intent": "The classification error (CE) is defined as the uncertainty of all but the most probable variable. \n$$ CE = 1 - \\max_i p_i$$\n"}
{"snippet": "evaluate(x_=xzero, l_=lzero)\nevaluate(x_=xstar, l_=lstar)\n", "intent": "Check objective function values at points of interest.\nNote that the constraint error norm and the gradient norm are 0 at $(x^*, \\lambda^*)$.\n"}
{"snippet": "evaluate(x_=xinit, l_=linit, u_=uinit)\nevaluate(x_=xstar, l_=lstar, u_=ustar)\n", "intent": "Check objective function values at points of interest.\nNote that the constraint error norm and the gradient norm are 0 at $(x^*, \\lambda^*)$.\n"}
{"snippet": "DogResnet_predictions = [np.argmax(DogResnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet]\ntest_accuracy = 100*np.sum(np.array(DogResnet_predictions)==np.argmax(test_targets, axis=1))/len(DogResnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "iris_y_pred = regr.predict(iris_X_test)\n", "intent": "> Put your code here\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % mean_squared_error(iris_y_test, iris_y_pred))\n", "intent": "> Put your code here\n"}
{"snippet": "print('Variance score: %.2f' % r2_score(iris_y_test, iris_y_pred))\n", "intent": "> Put your code here\n"}
{"snippet": "lm.predict(x_train)\n", "intent": "Predicted values of target variable are obtained by\n"}
{"snippet": "lm.predict(x_test)\n", "intent": "To access the prediction of this regression model, we use the observations in the testing set.\n"}
{"snippet": "sklearn.metrics.r2_score(y_train, lm.predict(x_train))\n", "intent": "To access the goodness of fit for the training set, the $R^2$ measure is used.\n"}
{"snippet": "sklearn.metrics.mean_squared_error(y_test, lm.predict(x_test))\n", "intent": "To access the predictive performance of this model, the mean squared error measure can be used.\n"}
{"snippet": "yhat = clf.predict(X)\nprint(yhat)\n", "intent": "Predict the data you used for training/fiting the classifer\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "__Evaluation Scores__\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores_benchmark = cross_val_score(model, X, y, cv=5) \nprint(\"Cross-Val Scores (Accuracy): {}\".format(scores_benchmark))\nprint(\"Cross-Val Mean (Accuracy): {}\".format(scores_benchmark.mean()))\n", "intent": "=> Indeed only slightly better than random guess, ~ 0.05 percentage points (AUC)\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nprint(\"Avg Silhoutte score:\", silhouette_score(X_bow, pred), \"(novel collections)\")\n", "intent": "Silhoutte coefficient: [-1,1], where 1 is most dense and negative vals correspond to ill seperation.\n"}
{"snippet": "print(\"AVG Silhoutte score\", silhouette_score(X_bow, corpus.target), \"(original collections)\")\n", "intent": "Compared to original collections by Sir Arthur Conan Doyle:\n"}
{"snippet": "new_prediction = classifier.predict(sc.transform(np.array([[0,0,600,1,40,3,60000,2,1,1,50000]])))\nnew_prediction = (new_prediction > 0.5)\nnew_prediction\n", "intent": "** New prediction ** \n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))\n", "intent": "This model is definitely an improvement.\n"}
{"snippet": "clf.predict(digits.data[-1:])\n", "intent": "We predict the last image of the model to see if it predicts well\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\n", "intent": "Evaluate model on test data.\n"}
{"snippet": "print(model.predict([p]))\n", "intent": "Show prediction model.\n"}
{"snippet": "X_new = [[3,5,4,2], [5,4,3,2]]\nknn.predict(X_new)\n", "intent": "* Returns a NumPy array, and we keep track of what the numbers 'mean'\n* Can predict for multiple observations at once\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error (MSE)** is the mean of the squared errors:\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n"}
{"snippet": "glass['household_pred_prob'] = logreg.predict_proba(X)[:,1]\n", "intent": "What if we wanted the predicted probabilities instead of just the class predictions, to understand how confident we are given a prediction?\n"}
{"snippet": "def vae_loss(y_true, y_pred):\n", "intent": "Then, we need to translate our loss into Keras code:\n"}
{"snippet": "from sklearn.metrics.pairwise import euclidean_distances\nkMatch          = 10\npredOutput      = dlObj.predict(linkGal,euclidean_distances(dataGalTest,dataGalTrain))\nindSDSS,weiSDSS = dlObj.match(linkSDSS,predOutput,kMatch=kMatch)\nr,c             = np.shape(dataGalTest)\nsedTest         = np.zeros((r,c))\nfor ii in range(r):\n    for jj in range(kMatch):\n        sedTest[ii,:] = sedTest[ii,:]+weiSDSS[ii,jj]*dataSDSS[indSDSS[ii,jj],:]/np.sum(weiSDSS[ii,:])\n", "intent": "__Convert test data in query space (Galacticus) to test data in reference space (SDSS)__\n"}
{"snippet": "y_train_abc_pred  = abc_best.predict(X_train)\ny_train_abc_proba = abc_best.predict_proba(X_train)\n", "intent": "Is there overfitting in this model?\n"}
{"snippet": "y_train_gbc_pred  = gbc_best.predict(X_train)\ny_train_gbc_proba = gbc_best.predict_proba(X_train)\nprint(\"Gradient Boosting Train Confusion Matrix\")\nprint(skm.confusion_matrix(y_train, y_train_gbc_pred))\nprint(\"\\nnGradient Boosting Train Classification Report\")\nprint(skm.classification_report(y_train, y_train_gbc_pred))\nprint(\"\\nnGradient Boosting Train ROC AUC\")\nprint(skm.roc_auc_score(y_train, y_train_gbc_proba[:,1]))\nprint(\"\")\n", "intent": "Is there overfitting with this model?\n"}
{"snippet": "predictions = dtc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "pred_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,pred_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "svc_pred = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "new_x = [ [1.2,  3.0,  5.4,  4.2], [1.2,  3.0,  5.4,  4.2] ]\nmodel.predict(new_x)\n", "intent": "Make a prediction with our new model\n"}
{"snippet": "clu_pred = clu.predict(X)\nprint clu_pred\n", "intent": "We can also predict the *label* of each point:\n"}
{"snippet": "print(\"Test classification rate %0.05f \" % model.evaluate(X_test, Y_test)[1])\n", "intent": "And now it is ready to be used again:\n"}
{"snippet": "import sklearn.metrics as metrics\nprob2_x_true_Kmean = predfilterdUserClassesIndexed\nprob2_x_pred_Kmean = prob2_kmeansFit_predict\nprint(\"confusion_matrix: \")\nprint(metrics.confusion_matrix(prob2_x_true_Kmean, prob2_x_pred_Kmean))\nprint(\"Precision\", metrics.precision_score(prob2_x_true_Kmean, prob2_x_pred_Kmean, average='weighted'))\nprint(\"Recall: \", metrics.recall_score(prob2_x_true_Kmean, prob2_x_pred_Kmean, average='weighted'))\nprint(\"F-Score:\", metrics.f1_score(prob2_x_true_Kmean, prob2_x_pred_Kmean, average='weighted'))   \n", "intent": "- 1) The k-means algorithm\n"}
{"snippet": "import sklearn.metrics as metrics\nprob2_x_true_MaxB = predfilterdUserClassesIndexed\nprob2_x_pred_MaxB = prob2_SSE_based_ag_labels\nprint(\"confusion_matrix: \")\nprint(metrics.confusion_matrix(prob2_x_true_MaxB, prob2_x_pred_MaxB))\nprint(\"Precision:\", metrics.precision_score(prob2_x_true_MaxB, prob2_x_pred_MaxB, average='weighted'))\nprint(\"Recall: \", metrics.recall_score(prob2_x_true_MaxB, prob2_x_pred_MaxB, average='weighted'))    \nprint(\"F-Score:\", metrics.f1_score(prob2_x_true_MaxB, prob2_x_pred_MaxB, average='weighted'))         \n", "intent": "- 2) MAX-based agglomerative hierarchical clustering\n"}
{"snippet": "import sklearn.metrics as metrics\nprob2_x_true_SSEB = predfilterdUserClassesIndexed\nprob2_x_pred_SSEB = MAX_based_ag_labels\nprint(\"confusion_matrix: \")\nprint(metrics.confusion_matrix(prob2_x_true_SSEB, prob2_x_pred_SSEB))\nprint(\"Precision:\", metrics.precision_score(prob2_x_true_SSEB, prob2_x_pred_SSEB, average='weighted'))\nprint(\"Recall: \", metrics.recall_score(prob2_x_true_SSEB, prob2_x_pred_SSEB, average='weighted'))     \nprint(\"F-Score:\", metrics.f1_score(prob2_x_true_SSEB, prob2_x_pred_SSEB, average='weighted'))\n", "intent": "- 3) SSE-based agglomerative hierarchical clustering\n"}
{"snippet": "x, y = make_xy(critics, vectorizer)\nprob = clf.predict_proba(x)[:, 0]\npredict = clf.predict(x)\nbad_rotten = np.argsort(prob[y == 0])[:5]\nbad_fresh = np.argsort(prob[y == 1])[-5:]\n", "intent": "We can see mis-predictions as well.\n"}
{"snippet": "print('{:8.3f}'.format(calinski_harabaz_score(X, hi3Labs)))\n", "intent": "Let's take a look at a couple of metrics:\n"}
{"snippet": "test_pred = XGB_Model.predict(feature_space_test)\naccuracy_score(target_space_test, test_pred)\n", "intent": "Results: Average Accuracy = 80.55%, Variance = 0.000004, Std. Dev. = 0.001984 \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    generate_vars = []\n    for idx in style_layers:\n        generate_vars.append(gram_matrix(feats[idx]))\n    l = len(generate_vars)\n    for idx in range(l) :\n        loss += tf.reduce_sum(tf.pow((generate_vars[idx]-style_targets[idx]),2)) * style_weights[idx]\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "for colname, th in th_dict.items():\n    print(colname, \" -- Threshold:\", th, \"\\n--------------\")\n    print(classification_report(y_true=radar['Type'], \n                                y_pred=radar[colname]), '\\n')\n", "intent": "Due to their usefulness, these measures are summarized in a special report available by the _classification_\\__report()_ function.\n"}
{"snippet": "x_test = np.array(['let suck the marrow in of life'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "scores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Evaluation Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Store and archive the model on notebook filesystem.\n"}
{"snippet": "X_new = [ 5.5,3.5,1.4,0.2]\nprediction =  classifier.predict([ [5.5,3.5,1.4,0.2]])\nprint(prediction)\n", "intent": "Now let us predict unlabled data\n"}
{"snippet": "y_pred_train = regressor.predict(_______)\n", "intent": "And predict. First let us try the training set:\n"}
{"snippet": "y_pred_test = regressor.predict(_______)\n", "intent": "The line is able to capture the general slope of the data, but not many details.\nLet's try the test set:\n"}
{"snippet": "X_check = valid_dataset[:10].reshape(10, 784)\npredicted_classes = regr.predict(X_check)\nvisualize_weights(X_check, predicted_classes)\n", "intent": "Now, let's ask the model to predict 10 letters.\n"}
{"snippet": "predict = np.zeros(photo.shape)\nall_coordinates = np.array(list(itertools.product(np.arange(photo.shape[0]), np.arange(photo.shape[1]))))\nall_results = model.predict(all_coordinates).astype(int)\nfor x in range(all_coordinates.shape[0]):\n    predict[all_coordinates[x,0], all_coordinates[x,1]] = all_results[x]\nshow_matrix(predict.astype(np.uint8))\n", "intent": "Let's see the image predicted\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "The average Silhouette Coefficient of the overall clustering outcome is reported by:\n"}
{"snippet": "y_pred = XGBmodel.predict(X_test)\nfrom sklearn.metrics import classification_report\ntarget_names = ['Fake', 'Real']\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint('Accuracy:',XGBmodel.score(X_test, y_test))\n", "intent": "**v=0.2 Further Tuned Result**\n"}
{"snippet": "df1['Defaulted_pred_prop'] = logreg.predict_proba(X)[:, 1]\n", "intent": "**predicted probabilities** instead of just the **class predictions**\n"}
{"snippet": "y_hat = model.predict(X_train)\n", "intent": "* $\\frac{1}{N} \\sum_n |\\hat{y}_{n}-y_n|$ - mean absolute error\n* $\\frac{1}{N} \\sum_n (\\hat{y}_{n}-y_n)^2$ - mean squared error\n"}
{"snippet": "accuracy_whole = accuracy_score(y, y_predict)\nprint(\"Test accuracy of KNN classifier: \", accuracy_whole)\n", "intent": "Calculate the classification quality for the entire sample:\n"}
{"snippet": "X_new = np.array([[0.11, 0.37], \n                  [-1.0, 1.4]])\ny_predicts_new = []\nfor knn in classifiers:\n    y_pred = knn.predict(X_new)\n    y_predicts_new.append(y_pred)\ny_predicts_new = np.array(y_predicts_new)\nprint(\"All predictions:\\n\", np.array(y_predicts_new))\nprint(\"\\nAverage prediction:\", y_predicts_new.mean(axis=0) )\n", "intent": "Get predictions for all classifiers\n"}
{"snippet": "y_test_knn = knn.predict(X_test)\ny_test_dt = dt.predict(X_test)\ny_test_logreg = logreg.predict(X_test)\n", "intent": "Make prediction of target **label**.\n"}
{"snippet": "y_test_proba_knn = knn.predict_proba(X_test)[:, 1] \ny_test_proba_dt = dt.predict_proba(X_test)[:, 1]\ny_test_proba_logreg = logreg.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction of **probability** of customer's positive response.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_test_predict = reg.predict(X_test.reshape(-1, 1))\n", "intent": "Make prediction on test sample\n"}
{"snippet": "print \"The in-sample prediction R^2 is %.4f\" % linreg.score(X,y)\nprint \"The mean squared error in model is %.4f\" % metrics.mean_squared_error(y,linreg.predict(X))\n", "intent": "**In-sample prediction metric**:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Train acc: {}\".format(accuracy_score(y_train, train_predictions)))\nprint(\"Test acc: {}\".format(accuracy_score(y_test, test_predictions)))\n", "intent": "One of the easiest metrics to understand when it comes to classification is accuracy: what fraction did you correctly predict.\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=5, beam_width=30, show=5):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_sentences, attentions = evaluate(encoder, decoder, pair[0], beam_width=beam_width)\n        for j in range(min(len(output_sentences), show)):\n            print('<', output_sentences[j])\n        print()\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nhx = (x_max-x_min)/1000.\nhy = (y_max-y_min)/1000.\nxx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\nZ = clusters.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "The time complexities are similar.\n"}
{"snippet": "import numpy as np\nnaive_count_predicted = clf_naive_count.predict(X_test_counts)\nnaive_tfidf_predicted = clf_naive_tfidf.predict(X_test_tfidf)\nprint(\"\\ncount_accuracy:\",np.mean(naive_count_predicted == twenty_test.target))\nprint(\"\\ntfidf_accuracy:\",np.mean(naive_tfidf_predicted == twenty_test.target))\n", "intent": "As expected tfidf features gives some better accuracy\n"}
{"snippet": "client_data = [[5, 17, 15], \n               [4, 32, 22], \n               [8, 3, 12]]  \nfor i, price in enumerate(reg.predict(client_data)):\n    print(\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))\n", "intent": "** Hint: ** The answer comes from the output of the code snipped above.\n**Answer: **\n"}
{"snippet": "(lasso_model, cross_val_score1) = build_lasso_model_and_cross_validate(train_data_one_hot, X_columns=predictor_cols,\n                                                                       Y_column='SalePrice', alpha=2000)\nprint_model_stats(lasso_model, cross_val_score1)\npredictions = lasso_model.predict(train_data_one_hot[predictor_cols])\nprint(\"Number of -ve predictions :  {0}\".format(np.sum(predictions < 0)))\n", "intent": "Now, the question remains as to which value of alpha we should use 2000 or 5000. Let us run them separately.\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % np.mean((lm.predict(X) - bos.PRICE) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "@np.vectorize\ndef BScorr(prob, score, n=1000):\n    clf = Expert(brier_score=score)\n    y = bernoulli.rvs(p=prob, size=n)\n    p = step_function(clf.predict(y), 0.5)\n    return np.corrcoef(np.vstack((y, p)))[0][1]\n", "intent": "* Define the function, which will calculate the correlations between probabilities and scores\n"}
{"snippet": "brier_score(y_prob, y)\n", "intent": "*Ensemble expert* predicts better, than each expert in the group:\n"}
{"snippet": "brier_score(y_prob, y)\n", "intent": "Our version of ensemble expert makes prediction even worser:\n"}
{"snippet": "print(accuracy_score(y, step_function(y_prob, 0.5)))\n", "intent": "We also obtained fair accuracy score using our weighted ensemble expert:\n"}
{"snippet": "accuracy = accuracy_score(y_test, predicted)\nprint(\"Accurcay score is: %f\" %(accuracy))\nprint(classification_report(y_test, predicted, target_names= ['0','1','2']))\n", "intent": "Let us see how Random forest perform on the test data\n"}
{"snippet": "[i[0] > 0 for i in model.predict(X)]\n", "intent": "We get an accuracy of 99.9%\nNow let's check whether our model works\n"}
{"snippet": "Y_pred = model.predict(X_train)\nprint(math.sqrt(mean_squared_error(Y_train, Y_pred)))\nprint (model.score(X_test,Y_test))\n", "intent": "Check accuracy with r^2 and mean square error\n"}
{"snippet": "print(\"Scoring SMC curve using LMC data\")\nline_score(model_svm_3, x_svm_4, y_svm_4)\nprint(\"\\nScoring LMC curve using SMC data\")\nline_score(model_svm_4, x_svm_3, y_svm_3)\n", "intent": "Let's try scoring each of the regressions using the data from the other dataset to see if they generalize.\n"}
{"snippet": "rf_predictions = rf.predict(testing_data)\n", "intent": "> **Step 4:** Use `predict` to test the model on previously unseen data.\n"}
{"snippet": "print('Random Forest scores:')\nprint('Accuracy score: ', format(accuracy_score(y_test, rf_predictions)))\nprint('Precision score: ', format(precision_score(y_test, rf_predictions)))\nprint('Recall score: ', format(recall_score(y_test, rf_predictions)))\nprint('F1 score: ', format(f1_score(y_test, rf_predictions)))\n", "intent": "> **Step 5:** Score the predictions.\n"}
{"snippet": "print('Decision Tree scores:')\nprint('Accuracy score: ', format(accuracy_score(y_test, predictions)))\nprint('Precision score: ', format(precision_score(y_test, predictions)))\nprint('Recall score: ', format(recall_score(y_test, predictions)))\nprint('F1 score: ', format(f1_score(y_test, predictions)))\n", "intent": "Let's re-print the Decision Tree scores again so we can look at them side-by-side.\n"}
{"snippet": "try:\n    f_S_union_i.predict(np.array([1,1]))\nexcept Exception as e:\n    print(e)\n", "intent": "Remember, for the sample input for which we'll calculate feature importance, we chose values of 1 for all features.\n"}
{"snippet": "pred_S_union_i = f_S_union_i.predict(sample_input)\npred_S_union_i\n", "intent": "The prediction of the model when it has features 0 and 1 is:\n"}
{"snippet": "pred_S = f_S.predict(sample_input)\npred_S\n", "intent": "The model's prediction when it is only training on feature 1 is:\n"}
{"snippet": "def model_score(m, X_train, y_train, X_valid, y_valid):\n    train_score = m.score(X_train, y_train)\n    valid_score = m.score(X_valid, y_valid)\n    oob_score = m.oob_score_\n    print(\"train: %f, oob: %f, valid: %f\" % (train_score, oob_score, valid_score))\n    return train_score, valid_score, oob_score\n", "intent": "Create a function for scoring the model.\n"}
{"snippet": "script = (dml(scriptPredict).input(data=testData, C=1, Hin=28, Win=28, W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3, W4=W4, b4=b4)\n                            .output(\"predictions\"))\npredictions = ml.execute(script).get(\"predictions\").toNumPy()\nprint classification_report(testData[:,0], predictions)\n", "intent": "Use trained model and predict on test data, and evaluate the quality of the predictions for each digit.\n"}
{"snippet": "train_resid = y_train - lm.predict(X_train)\ntest_resid = y_test - predictions\nlm_residuals = train_resid\nlm_residuals.append(test_resid)\nlm_residuals.shape\n", "intent": "Getting residuals from the first model:\n"}
{"snippet": "y_hat_gr = gridsearch.predict(X_test)\nscore = accuracy_score(y_test, y_hat_gr)\nprint 'Accuracy score:', score\n", "intent": "The Gridsearch returned the best parameters for the logistic regression model: L1 penalty and value of C of approxomately 1.78.\n"}
{"snippet": "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X_train, km.labels_, sample_size=1000))\nprint()\n", "intent": "We now evaluate the clustering result.\n"}
{"snippet": "print (\"Records in our dataset (rows): \", len(data.index))\nprint (\"Then we predict one cluster per record, which means length of: \", len(model.predict(data)) )\n", "intent": "What clusters do we get? Let's get \"predictions\" of the model:\n"}
{"snippet": "mnb.predict_proba(X_test)\nX_train.columns\n", "intent": "__Naive Bayes- Prediction__\n"}
{"snippet": "binary_example_clf.predict(test_features_original)\n", "intent": "Then we can use `NaiveBayesEnhancedClassifier.predict()` to (1) apply the `NB-transformation` and make our prediction.\n"}
{"snippet": "multiclass_example_clf.predict(multiclass_test_features)\n", "intent": "Then let's get predictions for each test document.\n"}
{"snippet": "multiclass_example_no_decision_function.decision_function_predict_proba(multiclass_test_features)\n", "intent": "Likewise, if the `classifier` chosen does *not* have a `.decision_function()` method, then `.predict_proba()` is used.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Resnet-50 Test accuracy: %.4f%%' % test_accuracy)\nInceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('InceptionV3 Test accuracy: %.4f%%' % test_accuracy)\nXception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Xception Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'true positive rate:', (best.predict(X_test.loc[y_test==True]) == 1).mean()\nprint 'true negative rate:', (best.predict(X_test.loc[y_test==False]) == 0).mean()\nprint 'mean accuracy:', np.append((best.predict(X_test.loc[y_test==True]) == 1), \n                                  (best.predict(X_test.loc[y_test==False]) == 0)).mean()\n", "intent": "do final evaluation on the holdout set\n"}
{"snippet": "y_pred = lr_model.predict(X_test)\n", "intent": "<b>We now make predictions on the testing set</b>\n"}
{"snippet": "from nltk.corpus import brown\nfrom nltk import UnigramTagger\nimport nltk\nbrown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\nbrown_train = brown_news_tagged[100:]\nbrown_test = brown_news_tagged[:100]\nut3 = UnigramTagger(brown_train)\nut3.evaluate(brown_test)\n", "intent": "Just for our testing purposes let's go back go the news category in the brown tagset, and use the most basic UnigramTagger we've seen in class.\n"}
{"snippet": "noun_tagger = DefaultTagger('NOUN')\nUG_with_DF_as_noun = UnigramTagger(brown_train, backoff=noun_tagger)\nprint('Unigram tagger with backoff accuracy: %4.1f%%' % ( 100.0 * UG_with_DF_as_noun.evaluate(brown_test)))\n", "intent": "This is basicly a tagger that by default tags every word as passed tag, in this case, 'XYZ'\n"}
{"snippet": "y_pred = model.predict(X_test[:500])\n", "intent": "Predict part of the test data:\n"}
{"snippet": "score = model.evaluate(X_train, y_train, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "---\nLet's take a look at the error and accuracy history of this model and evaluate its score on the training set.\n"}
{"snippet": "prediction, bias, contributions = ti.predict(best_rf, X)\n", "intent": "In order to know the influence of each variable on the model, feature contribution algorithm is applied. \n"}
{"snippet": "uniques, idx = np.unique(data.beer_beerid, return_index=True)  \npred = pd.Series(model.predict(X[idx, :]), index=data.beer_beerid[idx], name=\"predictions\") \\\n    .sort_values(ascending=False, inplace=False)\npred_name = pd.Series(pred.values, beer_names[pred.index], name=\"predictions\")\nprint \"Top recommendations for {}.\".format(user)\nprint pred_name[:5]\n", "intent": "Pretty bad cross-validation scores, but mind you we have only a handful reviews.\n"}
{"snippet": "beer = np.random.choice(beer_names.index)\nbeer_idx = (data.beer_beerid == beer).values\nX_beer = X[beer_idx, :][0]  \nprint \"{} will give beer {} a rating of {:.1f}.\".format(user, beer_names[beer], model.predict(X_beer.reshape(1,-1))[0])\n", "intent": "Pretty spectacular. \nJust for the fun of it, how will this user rate a random beer?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, val in enumerate(style_layers):\n        loss = tf.norm(gram_matrix(feats[val]) - style_targets[idx])\n        style_loss += style_weights[idx] * loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print('Graph of Batch Gradient Descent (and with momentum) loss per epoch')\nnames = ['BGD', 'BGD+Momentum']\nlosses = [loss_per_epoch_bgd, loss_per_epoch_momentum]\ngraph_loss(losses, train_epochs, names)\n", "intent": "Ok! Let's compare the results!\n"}
{"snippet": "best_est = joblib.load('best_est.pkl')\nm_X_test = get_test_data() \na_y = best_est.predict(m_X_test)\nnp.savetxt('comp_testY.dat', a_y, fmt='%.5f')\nprint(\"Written to file 'comp_testY.dat'.\")\nprint(a_y)\n", "intent": "This is what generates the prediction for the exercise\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0, name='loss')\n    for idx, layer in enumerate(style_layers):\n        layer_loss = tf.reduce_sum((gram_matrix(feats[style_layers[idx]]) - style_targets[idx])** 2) * style_weights[idx]\n        style_loss += layer_loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef compute_cost(A2, Y, parameters):\n    m = Y.shape[1] \n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    cost = sqrt(mean_squared_error(A2, Y))       \n    assert(isinstance(cost, float))\n    return cost\n", "intent": "change cost function\n"}
{"snippet": "import numpy as np\ncorrect = y\nestimated_proba = clfLR.predict_proba(X)\nestimated = clfLR.predict(X)\nprint ('Correct: {0}\\nTotal: {1}\\nPercentage:'.format(compared.sum(), len(correct)))\n", "intent": "Evaluating obtained model\n"}
{"snippet": "from sklearn import metrics\nprint \"Precision: {0}\".format(metrics.precision_score(y_test, y_pred))\nprint \"Recall:    {0}\".format(metrics.recall_score(y_test, y_pred))\nprint \"F1 score:  {0}\".format(metrics.f1_score(y_test, y_pred))\n", "intent": "Combining together precision and recall into [F1 Score](https://en.wikipedia.org/wiki/F1_score).\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred, target_names=['Stars', 'Quasars (QSOs)'])\n", "intent": "Printing table with all matrics embedded into.\n"}
{"snippet": "REZNET50_predictions = [np.argmax(REZNET50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(REZNET50_predictions)==np.argmax(test_targets, axis=1))/len(REZNET50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.linear_model import ???\ny_train = data_scaled[:,0]\nX_train = data_scaled[:,1:]\nncv = 10 \nlin_reg = ???() \nlin_reg.??? \nyhat = lin_reg.??? \nscores = cross_validation.cross_val_score(lin_reg,\n    X_train, y_train, scoring=\"mean_squared_error\", cv=ncv)\n", "intent": "Like before, find the appropriate function for linear regression (this time in the `sklearn.linear_model` submodule)\n"}
{"snippet": "class KeywordClassifier(object):\n    def __init__(self):\n        self.keywords = set()\n    def predict(self, text):\n        prediction = 0\n        return prediction\n", "intent": "Complete the function below.\n"}
{"snippet": "class KeywordClassifier(object):\n    def __init__(self):\n        self.keywords = set()\n    def predict(self, text):\n        prediction = 0\n        for keyword in self.keywords:\n            if keyword in text:\n                prediction = 1\n        return prediction\n", "intent": "Now we put everything together to display errors:<br/><br/>\n"}
{"snippet": "rf_predict_train = rf_model.predict(xTrain)\nprint(\"Acuracy: {0:.4f}\".format(metrics.accuracy_score(yTrain, rf_predict_train)))\n", "intent": "Predict Training Data\n"}
{"snippet": "lr_predict_train = lr_model.predict(xTrain)\nprint(\"Acuracy: {0:.4f}\".format(metrics.accuracy_score(yTrain, lr_predict_train)))\n", "intent": "Predict Training Test\n"}
{"snippet": "print(\"{0}\\n\".format(metrics.confusion_matrix(yTest, lr_predict_test, labels=[1,0])))\nprint(\"Classification Report\")\nprint(\"{0}\\n\".format(metrics.classification_report(yTest, lr_predict_test, labels=[1,0])))\n", "intent": "Confusion Matrix for Logistic Regression\n"}
{"snippet": "clf_predict_mod = clf.predict(x_test)\nprint(clf_predict_mod[:5])\nprint(y_test[:5])\n", "intent": "<b>Model Prediction</b>\n"}
{"snippet": "predict_default_param = rfc.predict(X_test)\n", "intent": "- we compute the prediction using the model just built on the input test set:\n"}
{"snippet": "accuracy_score(y_test, predict_default_param)\n", "intent": "- Below you can see the accuracy score of our model on the test set:\n"}
{"snippet": "predict = rfc.predict(X_test)\naccuracy_score(y_test, predict)\n", "intent": "- We build our new prediction on the input test set and then we show our accuracy score on the test set:\n"}
{"snippet": "(clf.predict(X)==y).mean()\n", "intent": "We can compute the prediction accuracy based on the predictions as well as the observations.\n"}
{"snippet": "X_new = [[ 5.0,  3.6,  1.3,  0.25]]\nclf2.predict(X_new)\n", "intent": "With this fitted model, we can predict the class for a given new observation.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Now with our new model lets make some predictions!\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))\n", "intent": "And with our predictions made, lets evaluate them!\n"}
{"snippet": "cluster_label = clusterer.predict(pixels)\ncluster_label[10]\n", "intent": "Predict (or label) the cluster that each pixel will belong to and associate the new image array with the new pixel cluster centroids.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        current_im_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((current_im_gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = xgb_grid_model.predict(test_x)\nprint('single model XGBoost R2:',r2_score(test_y, prediction))\n", "intent": "**1. Best Single Model: XGBoost - performance on test dataset**\n"}
{"snippet": "for clf_name in ['LinearSVC', 'LogisticRegression', 'DecisionTreeClassifier', 'SVC', 'RandomForestClassifier']:\n    print \"Training\", clf_name\n    start = time()\n    scores = cross_validation.cross_val_score(\n        best_estimator_dict[clf_name], X_train_gs, y_train_gs, cv=5, scoring='roc_auc') \n    end = time()\n    duration = end - start\n    print \"Average CV performance for {}: {:.6} (in {:.6} seconds)\".format(\n        clf_name, scores.mean(), duration)\n", "intent": "To further validate this setup a 5-fold cross-validation is performed.\n"}
{"snippet": "X_test_dtm = cv.transform(X_test)\ny_pred = clf.predict(X_test_dtm)\n", "intent": "__PREDICTING TEST SET__\n"}
{"snippet": "X_test_dtm = cv.transform(X_test)\ny_pred = clf.predict(X_test_dtm)\n", "intent": "__PREDICTING TEST__\n"}
{"snippet": "print \"Class predictions according to GraphLab Create:\" \nprint sentiment_model.predict(sample_test_data)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "print \"Class predictions according to GraphLab Create:\" \nprobabilities = sentiment_model.predict(sample_test_data, output_type='probability')\nprint probabilities\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "test_data['predicted_probability'] = sentiment_model.predict(test_data, output_type = 'probability')\ntest_data.topk('predicted_probability', 20, reverse = True).print_rows(num_rows=20)\npredictions = test_data.topk('predicted_probability', 20)\nResult_Q4 = np.array(predictions['name'])\n", "intent": "**Quiz Question**: Which of the following products are represented in the 20 most negative reviews?  [multiple choice]\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\npredictions = model_5.predict(validation_data)\nfalse_positives = sum((predictions == +1) & (validation_data['safe_loans'] == -1))\nfalse_negatives = sum((predictions == -1) & (validation_data['safe_loans'] == +1))\nconf_matrix = confusion_matrix(np.array(validation_data['safe_loans']), np.array(predictions))\nResult_Q3 = conf_matrix[0,1]\nprint('Q3: False Positive: %s' %Result_Q3)\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "pred = clf.best_estimator_.predict_proba(airplane_test[:,:-1])[:,1]\na = pred[::2]\nb = pred[1::2]\ncorrect_rate = ((a>b).sum())/len(a)\ncorrect_rate\n", "intent": "Test earthquake article\n"}
{"snippet": "pred = clf.best_estimator_.predict_proba(earthquake_test[:,:-1])[:,1]\na = pred[::2]\nb = pred[1::2]\ncorrect_rate = ((a>b).sum())/len(a)\ncorrect_rate\n", "intent": "Test airplace articles\n"}
{"snippet": "ridge_pred = ridge_pipe.predict(X_test).clip(0, )\nlasso_pred = lasso_pipe.predict(X_test).clip(0, )\nrf_pred = rf.predict(X_test)\nridge_mse = (mean_squared_error(y_test, ridge_pred)).round(2)\nlasso_mse = (mean_squared_error(y_test, lasso_pred)).round(2)\nrf_mse = (mean_squared_error(y_test, rf_pred)).round(2)\nridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred)).round(2)\nlasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred)).round(2)\nrf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred)).round(2)\n", "intent": "We use mean squared error and root mean squared error for model performance measure\n"}
{"snippet": "alexnet = models.alexnet(pretrained=True)\nlabels = get_imagenet_labels()\nprediction = predict(resized_image, alexnet, labels)\nprint(\"Alexnet: The category of your input image is {0}\".format(prediction))\n", "intent": "Ok, it's the dog we saw before. Now it's time for Alexnet to guess.\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=1)\nprint(score)\n", "intent": "Now, we can test our network on our test dataset, to determine how well it generalizes.\n"}
{"snippet": "accuracy = accuracy_score(y, clf.predict(X))\nprint(accuracy)\n", "intent": "The following code evaluates a classifier based on the validation data.\n"}
{"snippet": "y_pred = gs.predict(feature_set)\nprint(gs.best_estimator_.coef_)\nprint(gs.best_estimator_.intercept_)\n", "intent": "Now, call predict on the estimator with the best found parameters.\n"}
{"snippet": "model_2_predictions = [np.argmax(model_2.predict(np.expand_dims(feature, axis=0))) for feature in test_Model_2]\ntest_accuracy = 100*np.sum(np.array(model_2_predictions)==np.argmax(test_targets, axis=1))/len(model_2_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def print_wer_report(session, caption, tower_decodings, tower_labels, show_example=True):\n    items, mean = calculate_wer(session, tower_decodings, tower_labels)\n    print \"%s WER: %f09\" % (caption, mean)\n    if len(items) > 0 and show_example:\n        print \"Example (WER = %f09)\" % items[-1][2]\n        print \" - source: \\\"%s\\\"\" % items[-1][0]\n        print \" - result: \\\"%s\\\"\" % items[-1][1] \n    return items, mean\n", "intent": "Plus a convenience method to calculate and report the WER bundle all at once.\n"}
{"snippet": "with tf.device('/cpu:0'):\n    test_decodings, test_labels = decode_batch(ted_lium.test)\n    _, test_wer = print_wer_report(session, \"Test\", test_decodings, test_labels)\n", "intent": "Finally the trained model is tested using an unbiased test set.\n"}
{"snippet": "pred = classifier.predict(X_test)\n", "intent": "Use these parameters to predict on the test set\n"}
{"snippet": "rng = np.random.RandomState(0) \nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2) \nynew = model.predict(Xnew)\n", "intent": "To see how this is working, let's see how a bunch of random points are classified:\n"}
{"snippet": "y_pred_bag = bagging_cl.predict(testing_data)\ny_pred_rndfrst = rndfrst_cl.predict(testing_data)\ny_pred_ada = adaboos_cl.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def precision(actual, preds):\n    tp = np.sum(np.logical_and(preds == 1, actual == 1))\n    fp = np.sum(np.logical_and(preds == 1, actual == 0))\n    return tp /(tp + fp)\nprint(precision(y_test, naive_bayes_pred))\nprint(precision_score(y_test, naive_bayes_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = np.sum(np.logical_and(preds == 1, actual == 1))\n    return tp / np.sum(actual == 1)\nprint(recall(y_test, naive_bayes_pred))\nprint(recall_score(y_test, naive_bayes_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    precision_v = precision(preds, actual)\n    recall_v = recall(preds, actual)\n    return 2 * (precision_v * recall_v)/(precision_v + recall_v)\nprint(f1(y_test, naive_bayes_pred))\nprint(f1_score(y_test, naive_bayes_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "print(mean((bos.PRICE - lm.predict(X))**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"sv.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        feats_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((feats_gram - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_predict = random_forest_clasf.predict(test)\n", "intent": "The predictions will be done with the algorithm Random Forest because it was the one with the highest score.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = torch.zeros(1).type(dtype)\n    for l in range(len(style_layers)):\n        layer = style_layers[l]\n        gram_curr = gram_matrix(feats[layer])\n        diff = gram_curr - style_targets[l]\n        style_loss += style_weights[l]*((diff*diff).sum())\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def my_func(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\nmy_scorer = make_scorer(my_func, greater_is_better=True)\n", "intent": "I selected a list of floats to test for the slack penalty, [0.01, 0.1, 1., 5., 10., 25., 50., 100.].\n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\ny_pred\n", "intent": "***\nPredicting a token value.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "The classifer guessed correctly. What is the models performance like?\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "Getting around 95% accuracy. What happens if we make a dumb classifier that predicts everything as \"not 5\"\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method=\"decision_function\")\n", "intent": "How do you decide which threshold to use? We need to get the scores for all instances in the training set using `cross_val_predict`\n"}
{"snippet": "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n", "intent": "Now we have an ensemble of three trees. It can make predictions on a new instance simply by adding up the predictors of all the trees\n"}
{"snippet": "with graph.as_default():\n  l2_loss = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)    \n  loss = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(logits2, tf_train_labels)) +  0.000001 * l2_loss  \n", "intent": "In order to improve accuracy and avoid overfitting, I'll add regularization to this model and iterate more times.\n"}
{"snippet": "mlp.evaluate(X_test, y_test)\n", "intent": "Once the model is trained, we can evaluate its performance on the test data.\n"}
{"snippet": "mlp2.evaluate(X_test, y_test)\n", "intent": "Did you notice anything about the accuracy? Let's train it some more.\n"}
{"snippet": "mlp.evaluate(X_test, y_test, show_accuracy=True)\n", "intent": "Once the model is trained, we can evaluate its performance on the test data.\n"}
{"snippet": "def evaluate(y_pred_cls,y_true_cls):\n    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")\n    return accuracy\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "print(\"Residual mean sum of squares: \"+str(np.mean(regr.predict(X_train) - y_train) ** 2))\nprint('Variance score: %.2f' % regr.score(X_train, y_train))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "R2_mv = r2_score(y_true=y_mv, y_pred=y_pred_mv)\nprint (\"R^2 value = {:,.3f}\".format(R2_mv))\nMSE_mv = mean_squared_error(y_true=y_mv, y_pred=y_pred_mv)\nprint (\"Root Mean Squared Error (RMSE) = {:,.2f}\".format(sqrt(MSE_mv)))\nprint (\"Mean Absolute Error (MAE) = {:,.2f}\".format(mean_absolute_error(y_true=y_mv, y_pred=y_pred_mv)))\nprint (\"Correlation Coefficient from corrcoef = {:,.3f}\".format(np.corrcoef(y_mv, y_pred_mv)[1,0]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "def model_test(X_test, y_test, model):\n  fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n  auc_s = auc(fpr, tpr)\n  return fpr, tpr, auc_s \n", "intent": "Function, used to test one model in particular:\n"}
{"snippet": "def return_mean_score(scores):\n  return np.mean([element[2] for element in scores])\n", "intent": "Function used to calculate mean scores:\n"}
{"snippet": "lm.evaluate(x, y, verbose=0)\n", "intent": "This has now learnt internal weights inside the lm model, which we can use to evaluate the loss function (MSE).\n"}
{"snippet": "y_predicted = classifier.predict(X_test)\n", "intent": "Let's test the trained classifier using the testing data\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_predicted)\nprint \"Classifier accuracy:\", accuracy\n", "intent": "We then compute the **accuracy** of the classifier: The fraction of correct predictions.\n"}
{"snippet": "import utils\nname = \"Christian\"\nvector = utils.get_vector(name, feature_names, X[0])\nprint \"Classifier says\", name, \"is a\", \"female\" if classifier.predict(vector) == 1 else \"male\"\n", "intent": "We extract features, vectorize, and then test!\n"}
{"snippet": "def evaluate(model, y):\n    acc = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n    return acc\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(feats[style_layers[i]]) - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import fbeta_score\nprint(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\nprint(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\nprint(\"F =\", fbeta_score(y_test, clf.predict(X_test), beta=1))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$\n$$Recall = \\frac{TP}{TP + FN}$$\n$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "print r2_score(y_test, y_pred)\nprint mean_squared_error(y_test, y_pred)\n", "intent": "Now we can use the metrics we imported earlier to evaluate our model performance.\n"}
{"snippet": "clf2 = joblib.load('rf_regressor.pkl')\nclf2.predict(X_test)\n", "intent": "And that's it. When you want to load the model again, simply use this function:\n"}
{"snippet": "valid_d, valid_t = to_data_and_target(valid)\nmodel.evaluate(valid_d.values, valid_t.values)\n", "intent": "There is no early stopping, creating the potential for overfitting.\n"}
{"snippet": "valid_d, valid_t = to_data_and_target(valid)\nmodel.evaluate(valid_d.values, valid_t.values)\n", "intent": "That looks pretty good! Let's assess accuracy on our validation data set.\n"}
{"snippet": "xfit = np.linspace(-1, 11)\nXfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "- generate some new data\n- ask what y should be\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Now gauge it's accuracy by comparing the true values to the test set predictions\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n", "intent": "compute the fraction of correctly labeled points\n"}
{"snippet": "yprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n", "intent": "In the above, we see a curved boundary\n- in general, Gaussian Naive Bayes is quadratic\nWe can perform probabilistic classification\n"}
{"snippet": "def predict_category(s, train=train, model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n", "intent": "This simple classifier works well for some, but not between reglious talk\nWe can use this model to predict any string\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Now check how accurate our unsupervized clustering was in finding similar digits within the data\n"}
{"snippet": "predictions = classifier.predict(mytest_features)\nprediction_probabilities = classifier.predict_proba(mytest_features)\nacc = accuracy_score(mytest_outcomes, predictions)\nlos = log_loss( pd.get_dummies( mytest.OutcomeType ).values, prediction_probabilities)\nprint(\"accuracy = {0:1.2f} \\nlog-loss = {1:1.3f}\".format(acc,los) )\n", "intent": "50 percentile on Kaggle has log-loss = 0.84434\n"}
{"snippet": "predicted = model.predict(test_data)\nprint(\"Evaluation report: \\n\\n%s\" % metrics.classification_report(test_labels, predicted))\n", "intent": "Use **test data** to generate an evaluation report to check your **model quality**.\n"}
{"snippet": "test_predictions = loaded_model.predict(test_data[:10])\n", "intent": "Make test predictions to check that the model has been loaded correctly.\n"}
{"snippet": "y_pred = clf.best_estimator_.predict(X_test, ntree_limit= n_trees)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))\n", "intent": "Display the accuracy of best parameter combination on the test set.\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))\n", "intent": "Now you are ready to evaluate accuracy of the model trained on the reduced set of features.\n"}
{"snippet": "y_pred_proba = classifier.predict_proba(X_test)\n", "intent": "Now that we have build the model on the training set, let's see what at what probability the model predicts for each of the test set records.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, layer in enumerate(style_layers):\n        style_loss += style_weights[idx] * 2 * tf.nn.l2_loss(gram_matrix(feats[layer]) - style_targets[idx])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(\"Confusion Matrix: \")\nprint(confusion_matrix(testset_labels,model_results))\nprint(\"\\n\")\nprint(\"Classification report: \")\nprint(classification_report(testset_labels,model_results))\n", "intent": "Finally Lets check how good is our model\n"}
{"snippet": "tp=sum((Y_test==1)&(Y_pred==1))\nfp=sum((Y_test!=1)&(Y_pred==1))\ntn=sum((Y_test!=1)&(Y_pred!=1))\nfn=sum((Y_test==1)&(Y_pred!=1))\nprecision=tp/(tp+fp)\naccuracy=(tp+tn)/(tp+tn+fp+fn)\naccuracy1=sum(Y_test==Y_pred)\nrecall=tp/(tp+fn)\nprint(precision,accuracy,recall, accuracy1)\nprint(accuracy_score(Y_pred, Y_test))\n", "intent": "Don't use sklearn's functions here, compute these yourself to see how it's done\n"}
{"snippet": "print ('rfr_MAE count',  mean_absolute_error(Y_test , rfrc.predict(X_test)+rfrd.predict(X_test)))\nprint ('rfr_MSE count',   mean_squared_error(Y_test , rfrc.predict(X_test) + rfrd.predict(X_test)))\nprint ('rfr_R2 count',  r2_score(Y_test , rfrc.predict(X_test) +rfrd.predict(X_test)))\n", "intent": "old\nrfr_MAE registered 19.1539063784\nrfr_MSE registered 1008.68780969\nrfr_R2 registered 0.955340759827\nThe MSE is again reduced...\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that our function is correct.\n"}
{"snippet": "val_pseudo_out = model.predict(val_inputs)\ncombo_in = [\n            np.concatenate((train_inputs[0], val_inputs[0])),\n            np.concatenate((train_inputs[1], val_inputs[1])),\n            np.concatenate((train_inputs[2], val_inputs[2])),\n            np.concatenate((train_inputs[3], val_inputs[3])),\n            np.concatenate((train_inputs[4], val_inputs[4]))\n]\ncombo_out = np.concatenate((train_cat_out, val_pseudo_out))\ncombo_sample_weights = np.concatenate((train_sample_weights, val_sample_weights))\n", "intent": "Now create pseudo-labels for the validation data and add that to the training set\n"}
{"snippet": "def build_style_loss(a, x):\n    M = a.shape[1] * a.shape[2]    \n    N = a.shape[3]                 \n    A = gram_matrix_val(a, M, N)   \n    G = gram_matrix(x, M, N)\n    loss = (1. / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow((G - A), 2))\n    return loss\n", "intent": "Okay, now we may compute the style loss:\n"}
{"snippet": "knn.predict_proba(new_iris)\n", "intent": "You can also do probabilistic predictions:\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\nprint('Loss', test_loss)\nprint('Accuracy', test_acc)\n", "intent": "Present the test data to classify handwritten digit images.\n"}
{"snippet": "ResNet_model_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis =0))) for feature in test_Resnet50]\ntest_accuracy = 100 * np.sum(np.array(ResNet_model_predictions) == np.argmax(test_targets, axis=1))/len(ResNet_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Train Accuracy: \", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test Accuracy: \", score[1])\nprint(\"Epochs:\", epochs, \" Batch Size:\",batch_size, \" Learning Rate:\", eval(model.optimizer.lr))\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "res_model_predictions = [np.argmax(res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(res_model_predictions)==np.argmax(test_targets, axis=1))/len(res_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "_, validate_features, validate_labels = validate\npredicted_validate_labels =  rough_best[\"classifier\"].predict(validate_features)\nsklearn.metrics.f1_score(validate_labels, predicted_validate_labels, average='macro')\n", "intent": "We will now apply the classifier to the validation dataset.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0)))\n                     for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==\n                           np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'Coefficients: \\n' + str(lin_reg.coef_[0])\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((lin_reg.predict(family_plan.effort.values.astype(float).reshape(np.size(family_plan.effort), 1)) - family_plan.change.values.astype(float) ** 2))\n", "intent": "**Oh my gosh!! What is going on with the fit function!**\nDon't worry I'll explain it all. \n"}
{"snippet": "vnet_model = train_vnet_pipeline.get_model_by_name('vnet')\npredicted_masks = vnet_model.predict(feed_dict={'images': batch.unpack('images')})\n", "intent": "Usually, it's quiet usefull to have a look on predictions:\n"}
{"snippet": "print((r2_score(y_pred__qlty,y_result_qlty__test)))\nprint((r2_score(y_pred__col,y_result_Col__test)))\n", "intent": "Running the model & fitting on scaled variables \n"}
{"snippet": "print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))\n", "intent": "Now we can make predictions on the test set:\n"}
{"snippet": "a = get_network('secbern')\nb = get_network('infosec_truths')\njaccard_similarity_score(a,b)\n", "intent": "The scikit-learn package also has a jaccard_similarity_score function you can use, like this:\n"}
{"snippet": "x_test = np.array(['I am in love with my work'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "def evaluate(model):\n    score = model.evaluate(imgTest, onehotTest, verbose=0)\n    print('Test loss     :', score[0])\n    print('Test accuracy :', score[1])\nevaluate(model)\n", "intent": "Here we tell Keras to evaluate our model and provide the metrics we defined earlier. We can now see the accuracy of our model on the mnist dataset:\n"}
{"snippet": "Y_estim_train = model_lin.predict(train_X, batch_size = batch_size)\nY_estim_test = model_lin.predict(test_X, batch_size = batch_size)\nprint(Y_estim_train.shape, Y_estim_test.shape)\n", "intent": "Now we use the trained model to predict labels of the input images used for training as well as for testing\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:\n"}
{"snippet": "for lr in learning_rates:\n    layers = [50,40]\n    animal_errors = []\n    ws,ws_dash,bs,bs_dash = AutoEncoder(training_data_animal,2000,lr,animal_errors,layers,0.2,True)\n    errors_learning_rates_s.append(calc_test_error(testing_data_animal,ws,bs,ws_dash,bs_dash,sparse=False))\n", "intent": "__Plotting errors for varying learning rates__\n"}
{"snippet": "for sparsity_level in sparsity_levels:\n    errors = []\n    ws,ws_dash,bs,bs_dash = auto_encoder(training_data_animal,1000,0.5,errors_animal,layers,sparsity_level,True)\n    test_error = get_test_error(testing_data_animal,ws,bs,ws_dash,bs_dash,sparse=True)\n    errors_animal_sparse.append(test_error)\n", "intent": "__Sparsity level vs test error__\n"}
{"snippet": "errors_combined = []\nfor lr in learning_rates:\n    layers = [50,40]\n    errors = []\n    ws,ws_dash,bs,bs_dash = auto_encoder(all_training,1000,lr,errors,layers,0.9,True)\n    e = get_test_error(all_testing,ws,bs,ws_dash,bs_dash,sparse=True)\n    errors_combined.append(e)\n", "intent": "__Learning rates vs test error__\n"}
{"snippet": "for sparsity_level in sparsity_levels:\n    ws,ws_dash,bs,bs_dash = auto_encoder(all_training,1000,0.5,errors,layers,sparsity_level,True)\n    test_error = get_test_error(all_testing,ws,bs,ws_dash,bs_dash,sparse=True)\n    errors_all_sparse.append(test_error)\n", "intent": "__Sparsity level vs test error__\n"}
{"snippet": "for hidden_dim in hidden_units:\n    ws,ws_dash,bs,bs_dash = auto_encoder(all_training,1000,0.3,errors,[hidden_dim,60],sparsity_level,True)\n    errors_hidden.append(get_test_error(all_testing,ws,bs,ws_dash,bs_dash,sparse=True))\n", "intent": "__Hidden units vs test error__\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, ShuffleSplit\nresults = cross_val_score(model, X, y, cv=ShuffleSplit(n_splits=5))\nprint(\"Score: %.2f (%.2f)\" % (results.mean(), results.std()))\n", "intent": "Let's train and evaluate the model - see how it does!\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_data, test_labels)\nprint('test_acc:', test_acc)\nprint('test_loss:', test_loss)\n", "intent": "Now we will run model on test data, and with this Conv1D model, we get 75% accuracy.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_data, test_labels)\nprint('test_acc:', test_acc)\nprint('test_loss:', test_loss)\n", "intent": "Now we will run model on test data, and with this simple model, we get 68% accuracy.\n"}
{"snippet": "ypred = clf.predict(xtest)\nscore = f1_score(ytest, ypred, average='micro')\nprint ('Test Score: %0.3f'%(score))\nfpr, tpr, _ = roc_curve(ytest, ypred)\nconfusion_matrix(ytest, ypred)\n", "intent": "Time to apply the best model with optimal values on Test Set!\n"}
{"snippet": "score1 = np.mean(np.sqrt(-cross_val_score(elnet,X_train,y_train,cv=5,scoring='neg_mean_squared_error')))\nscore2 = np.mean(np.sqrt(-cross_val_score(ridge,X_train,y_train,cv=5,scoring='neg_mean_squared_error')))\nscore3 = np.mean(np.sqrt(-cross_val_score(rf,X_train,y_train,cv=5,scoring='neg_mean_squared_error')))\nscore4 = np.mean(np.sqrt(-cross_val_score(ada,X_train,y_train,cv=5,scoring='neg_mean_squared_error')))\nscore5 = np.mean(np.sqrt(-cross_val_score(gbr,X_train,y_train,cv=5,scoring='neg_mean_squared_error')))\nscore6 = np.mean(np.sqrt(-cross_val_score(lasso,X_train,y_train,cv=5,scoring='neg_mean_squared_error')))\nscores = [score1,score2,score3,score4,score5,score6]\nprint(scores)\nprint(np.mean(scores))\n", "intent": "evaluate the expected testscore via a 5-fold-cv\n"}
{"snippet": "i = [0,1,2,3]\nx = X_test[i,:]\nprint x\nyhat = predictor.predict(x)\nprint np.array(zip(yhat,y_test[i]))\n", "intent": "Now that we trained `predictor` we can use it to provide predictions on any example `x`. \n"}
{"snippet": "predicted1 = clf1Final.predict(tt)\n", "intent": "Predicting using trained classifier.\n"}
{"snippet": "predicted2 = clf2Final.predict(tt2)\n", "intent": "Predicting using trained classifier.\n"}
{"snippet": "xgb_scores = cross_val_score(XGB_reg, X_train_dtm_svd, y_train, cv=10)\nprint xgb_scores\nprint np.mean(xgb_scores)\ny_pred = XGB_reg.predict(X_test_dtm_svd)\nr2_score(y_test, y_pred)\n", "intent": "Better score than ElasticNet, but worse than lasso\n"}
{"snippet": "start_time = time.time()\nclusterLabelsAndPred = labelsAndData2.map(lambda x: (clusters2.predict(x[1]), x[0])).collect()\nclusterLabelsAndPred[0:5]\n", "intent": "This method calculate the entropy of each cluster and entropy of each class.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "No the classifier is not much better. The model is still overfit with the training data accuracy being much better than the test data.\n"}
{"snippet": "prediction = dct.predict(X_test)\n", "intent": "**Creating predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rfpredict = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predicting the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,rfpredict))\n", "intent": "**classification report from the results**\n"}
{"snippet": "predict = knn.predict(X_test)\n", "intent": "**Using the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print(classification_report(y_test,predict))\n", "intent": "** Creating a confusion matrix and classification report.**\n"}
{"snippet": "predictions = lgm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "prediction = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Using the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predict = pipe.predict(X_test)\n", "intent": "** using the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predict = svc_model.predict(X_test)\n", "intent": "**predictions from the model and creating a confusion matrix and a classification report.**\n"}
{"snippet": "grid_predicts = grid.predict(X_test)\n", "intent": "** taking the grid model and creating some predictions using the test set and creating classification reports and confusion matrices for them.**\n"}
{"snippet": "print np.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.zeros([])\n    for weight, layer, target in zip(style_weights, style_layers, style_targets):\n        loss += weight * tf.reduce_sum((gram_matrix(feats[layer]) - target) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = ppn.predict(X_test_pca)\ncv_scores['ppn']=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)\n", "intent": "We can now proceed and predict the class labels of the unseen test set. With the model trained with 101 iterations, we obtain an accuracy of 93%.\n"}
{"snippet": "y_pred = lr.predict(X_test_pca)\ncv_scores['lr']=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)\n", "intent": "The accuracy of this model as measured on the test set is higher than that for the perceptron model.\n"}
{"snippet": "seeds_small = seeds[['area', 'length']]\npred_small = km.fit_predict(seeds_small)\n", "intent": "Not let's try clustering for only two variables so that we can visualize it.\n"}
{"snippet": "test_metrics = ffnn.evaluate(x = x_test_vectors, y = y_test_onehot, batch_size = num_test_instances, verbose = 0)\ntest_loss = test_metrics[0]\ntest_accuracy = test_metrics[1]\nprint(\"Test Accuracy = \", str(test_accuracy*100),\"%\")\n", "intent": "Now that our network is trained we would of course like to know how well it does on the test set:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_f1_scores = cross_val_score(clf, X, y, scoring = f1_scorer, cv = 10 )\nprint( \"Average F1 score: \", np.round( np.mean( cv_f1_scores ), 2 ) )\nprint( \"Standard deviation in F1 score: \", np.round( np.std( cv_f1_scores ), 2) )\n", "intent": "- The model behaves fine. \nLet's select it and cross validate it using all the provided data set \n"}
{"snippet": "score = network.evaluate(test_data, test_label, batch_size = 128)\nscore\n", "intent": "Evaluation is made by .evaluate() method. It computes testing sets loss function and accuracity. \n"}
{"snippet": "predictions = model.predict(data)\ny_pred = np.argmax(predictions, axis = 1)\nprint(classification_report(labels, y_pred))\n", "intent": "Metrics are calulate by sklearn library. There are classification report, confusion matrix and possibility to check predictions by text.\n"}
{"snippet": "pred = linReg.predict(X_test[['season']])\n", "intent": "Let's submit our model and check the result on datahack.\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.8f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    counter = 0\n    for layer in style_layers:\n        feature, style_target, w = feats[layer], style_targets[counter], style_weights[counter]\n        g = gram_matrix(feature)\n        mep = (g - style_target) ** 2\n        loss += w * torch.sum(mep)\n        counter += 1\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.1000f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram= gram_matrix(model.extract_features()[5])\n    student_output= sess.run([gram], {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.10f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    counter = 0\n    for layer in style_layers:\n        feature, style_target, w = feats[layer], style_targets[counter], style_weights[counter]\n        g = gram_matrix(feature)\n        mep = (g - style_target) ** 2\n        loss += w * tf.reduce_sum(mep)\n        counter += 1\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n"}
{"snippet": "print('\\nAccuracy on test set: {}'.format(model.evaluate(X_test, y_test)[1]))\n", "intent": "Finally, we can check the performance of our trained network on\nthe test set:\n"}
{"snippet": "accuracy_score(y_test,predicted)\n", "intent": "We will evaluate this model in the following ways:\n* Accuracy Score (% correctly classified)\n* Confusion Matrix: look for model bias\n"}
{"snippet": "model_mlp_dqn.predict(np.array([[110,geohash_index_1]]))\n", "intent": "- COnfirm that MLP DQN  predicts the correct moves\n"}
{"snippet": "model_lstm_dqn.predict(np.array([[[110,geohash_index_1]]]))\n", "intent": "-  Confirm that LSTM can predict the correct move\n- takes longer to train than the MLP model. ~ 700 epochs\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "*** Predictions ***\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Using the predict method, an entry can be passed in and the result returned as true or false\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n", "intent": "Much better way to evaluate the perfromance of a classifier is the confusion matrix\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n", "intent": "Computing the AUC (area under curve) the better the classifier the closer that figure is to 1\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\nnever_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "The classifier can retun an accuracy of 90% since saying every number is not 5 will be right 90% of the time, \n"}
{"snippet": "predictions = model.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nprint(\"RMSE is:\", mean_squared_error(y_test, predictions))\n", "intent": "This means that our features explain approximately 89% of the variance in our target variable\n"}
{"snippet": "img = prepare_image('sloth.jpg')\nout = model.predict(img)\ny_pred = np.argmax(out)\nprint y_pred\nprint synset.loc[y_pred].synset\n", "intent": "<img src='imgs/sloth.jpg'/>\n"}
{"snippet": "from sklearn.metrics import precision_score,recall_score\nprint \"Precision: \",precision_score(Y_test,Y_pred)\nprint \"Recall: \",recall_score(Y_test,Y_pred)\n", "intent": "Precision & Recall are high $\\implies$ more confidence\n"}
{"snippet": "from sklearn.metrics import f1_score\ny_true = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34]\ny_pred = results\nf1_score(y_true, y_pred, average='micro') \n", "intent": "Below we simply compute the f1_score which is 1.0:\n"}
{"snippet": "def get_residual_sum_of_squares(model, data, outcome):\n    model_predictions = model.predict(data)\n    residuals = outcome - model_predictions\n    RSS = sum(residuals*residuals)\n    return(RSS)    \n", "intent": "Now that we can make predictions given the model, let's write a function to compute the RSS of the model.\n"}
{"snippet": "def get_residual_sum_of_squares(model, data, outcome):\n    model_predictions = model.predict(data)\n    residuals = outcome - model_predictions\n    RSS = sum(residuals*residuals)\n    return RSS\n", "intent": "Now, building a function to compute the RSS\n"}
{"snippet": "X_dev = [x['vector'] for x in dev_set]\ny_dev = [x['label'] for x in dev_set]\ny_dev_pred = log_model.predict(X_dev)\n", "intent": "And finally, we apply `log_model.predict()` to test the learned algorithm on the SST dev set:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_dev_pred, y_dev))\n", "intent": "We can now look at how accurate our function was:\n"}
{"snippet": "def predict(theta, X):\n", "intent": "For better computability the same formula can be re-written as,\n"}
{"snippet": "start_time = timeit.default_timer()\nG_Train =  Gausian_Kernel(mX_Train,np.std(mX_Train))\ngalpha = np.dot(np.linalg.inv(G_Train),mY_Train)\ndual_theta = np.dot(mX_Train.transpose(),galpha)\ndual_Y_Cap =  predict(dual_theta,mX_Train)\ndual_Y_Cap_Test = predict(dual_theta,mX_Test)\nend_time = timeit.default_timer() \ngaussian_time = end_time --start_time \n", "intent": "$ \\hat Y = \\theta^T \\cdot X $\n"}
{"snippet": "Titanic_Age_pred = regr.predict(Titanic_Age_test_set)\n", "intent": "We make predictions using the testing set\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % mean_squared_error(Titanic_Age_test_target, Titanic_Age_pred))\n", "intent": "We compute the mean squared error\n"}
{"snippet": "print('Variance score: %.2f' % r2_score(Titanic_Age_test_target, Titanic_Age_pred))\n", "intent": "Here I will run the R2\n"}
{"snippet": "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\nexample_counts = count_vectorizer.transform(examples)\npredictions = classifier.predict(example_counts)\npredictions \n", "intent": "Let's test our classifier.\n"}
{"snippet": "pred_train = lreg.predict(X_train)\npred_test = lreg.predict(X_test)\n", "intent": "* go ahead and try to use training and testing sets to predict house prices\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=ppn,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=1)\nprint('CV accuracy scores: %s' % scores)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "<h1 align=\"center\"> Part 3  Model Evaluation</h1> <br>\n"}
{"snippet": "scores = model.evaluate(x_test, y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "For a better measure of the quality of the model, let's see the model accuracy for the test data. \n"}
{"snippet": "predictions = model.predict(X_test)\nshow_failures(predictions)\n", "intent": "Here are the first 10 test digits the CNN classified to a wrong class:\n"}
{"snippet": "linscores = linmodel.evaluate(X_test, Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (linmodel.metrics_names[1], linscores[1]*100))\n", "intent": "For a better measure of the quality of the model, let's see the model accuracy for the test data. \n"}
{"snippet": "linpredictions = linmodel.predict(X_test)\nshow_failures(linpredictions)\n", "intent": "Here are the first 10 test digits the linear model classified to a wrong class:\n"}
{"snippet": "scores = model.evaluate(X_test, Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Accuracy for test data.  The model should be somewhat better than the linear model. \n"}
{"snippet": "X_test = np.random.rand(64, 100)\nY_test = np_utils.to_categorical(np.random.randint(10, size=64))\nloss, acc = model.evaluate(X_test, Y_test, batch_size=32)\nprint()\nprint('loss:', loss, 'acc:', acc)\n", "intent": "Evaluate your performance on test data with `.evaluate():`\n"}
{"snippet": "predictions = model.predict(X_test)\nshow_failures(predictions)\n", "intent": "Here are the first 10 test digits the RNN classified to a wrong class:\n"}
{"snippet": "test_words = ['if', 'import', 'def', '(']\ntop_n = 5\nfor word in test_words:\n    test_int = reduced_w2n_dict[word]\n    output = model.predict(np.array([test_int]))\n    candidates = output.argsort()[0][::-1]\n    for idx in range(top_n):\n        print(word, \"->\", reduced_n2w_dict[candidates[idx]])\n", "intent": "Let's throw some words at the network and see what it predicts.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\npredictions = cross_val_predict(estimator=pipe_lr,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=1)  \nprint('CV accuracy scores: %d %s' % (len(scores), len(y_train)))\nprint('CV accuracy: %.3f' % accuracy_score(y_train, predictions))\n", "intent": "- Exchange cross_val_score with cross_val_predict\n- Calculate accuracy of all predictions\n- Compare with previous result\n"}
{"snippet": "model.predict(np.array([[0,1]]))\n", "intent": "Onced trained, we can use it!\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    print(sess.run(tf.shape(model.extract_features()[5]), {model.image: style_img_test}))\n    print(sess.run(tf.shape(gram), {model.image: style_img_test}))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for layer, w, t in zip(style_layers, style_weights, style_targets):\n        loss += w * tf.norm(t - gram_matrix(feats[layer])) ** 2\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = my_first_regression.predict(X_test)\n", "intent": "Then, let's make some prdictions: \n"}
{"snippet": "accuracy_score(titanic.Survived, predictions)\n", "intent": "In scikit, all scores work as follows:   \n`some_score.(reality, predictions)`\n"}
{"snippet": "predictions = logit.predict(X_test);\naccuracy_score(y_test, predictions)\n", "intent": "Make predictions, and score your results:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(top_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def get_pipeline_classifier(pipeline):\n    return pipeline.steps[-1][-1].__class__.__name__\ndef evaluate_performance(pipeline, X, Y, metric_list=[accuracy_score]):\n    predictions = pipeline.predict(X)\n    print(get_pipeline_classifier(pipeline), \"results:\")\n    for metric in metric_list:\n        score = metric(Y, predictions)\n        print(\"\\t\", metric.__name__, \": %.4f\" % score)\nmetric_list = [accuracy_score, f1_score, precision_score, recall_score, roc_auc_score]\n", "intent": "The classifiers will be evaluated with the following metrics:\n- accuracy\n- f1-score\n- precision\n- recall\n- AUC\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never3Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "The proportion of $3$s in the dataset is only $10\\%$. Which means a model only returning `False` has already $90\\%$ accuracy.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Measure the model's root mean squared error:\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Compute the scores in the same way for the Linear Regressor:\n"}
{"snippet": "recall_score(y_train_5, y_train_pred)\n", "intent": "Meaning when the predictor claims a number is a 5, it is correct 87% of the time\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "Meaning the predictor 'catches' 76% of the 5s\n"}
{"snippet": "cder = CDER(parsimonious=True)\nresults = cross_val_score(cder, clouds_all, labels_all, scoring='accuracy', cv=5)\nprint(results)\n", "intent": "Cross-Validation\n----------------\nWe should always run a CV, to be sure we aren't fooling ourselves\n"}
{"snippet": "model.evaluate(x_test, one_hot_test_labels)\n", "intent": "Let's evaluate the results on the test set\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nlinreg_predictions = lin_reg.predict(housing_clean)\nlinreg_mse = mean_squared_error(housing_labels, linreg_predictions)\nlinreg_rmse = np.sqrt(linreg_mse)\ntree_predictions = tree_reg.predict(housing_clean)\ntree_mse = mean_squared_error(housing_labels, tree_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(\"linreg: \", linreg_rmse)\nprint(\"tree: \", tree_rmse)\n", "intent": "Now let's calculate the root mean squared error of each on the training set\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_clean, housing_labels, \n                        scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Sd:\", scores.std())\ndisplay_scores(tree_rmse_scores)\n", "intent": "Looks like the regression tree is probably overfitting the training set. Let's use cross-validation to estimate expected prediction error instead.\n"}
{"snippet": "scores = cross_val_score(lin_reg, housing_clean, housing_labels,\n                        scoring=\"neg_mean_squared_error\", cv=10)\nlinreg_rmse_scores = np.sqrt(-scores)\ndisplay_scores(linreg_rmse_scores)\n", "intent": "Clearly, the Regression tree is overfitting the training set. Let's see how linear regression fares:\n"}
{"snippet": "include = [c for c in P.columns if c not in ['mlp-nn']]\nprint('Truncated ensemble ROC-AUC score: %.3f' % roc_auc_score(ytest, P.loc[:, include].mean(axis=1)))\n", "intent": "We can try to improve the ensemble by removing the worst offender, say the Multi-Layer Perceptron (MLP)\n"}
{"snippet": "test_features_norm = (test_features - train_mean) / train_std\nmse, _, _ = model.evaluate(test_features_norm, test_labels)\nrmse = np.sqrt(mse)\nprint('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "print(test_images.shape)\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(feats[layer])-style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\nx = len(dog_breed_predictions)\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/(x * 1.0)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/(len(VGG16_predictions) * 1.0)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\nx = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))\ny = len(InceptionV3_predictions)\ntest_accuracy = x/( y * 1.0)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, X, y, cv=10)\n", "intent": "So, the AUC is pretty good. Let's see how using K-Fold Cross Validation affects it. \n"}
{"snippet": "sents = [\"once upon a time\",\n         \"the quick brown fox jumps over the lazy dog\"]\nload_and_score([s.split() for s in sents])\n", "intent": "Now we can test as:\n"}
{"snippet": "def format_predict(array):\n    reverse_label = {v:k for k,v in label_index.items()}\n    return reverse_label[np.argmax(prediction)]\na = 8000\nfor prediction in model.predict(x_test)[:5]:\n    a+=1\n    print(a,format_predict(prediction))\n", "intent": "Let's print out the predicted results for the first 5 test cases\n"}
{"snippet": "results = []\na = 8000\nfor prediction in model.predict(x_test):\n    a+=1\n    results.append(str(a))\n    results.append('\\t')\n    results.append(format_predict(prediction))\n    results.append('\\n')\n", "intent": "Let's write all this to `result.txt` \n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"india_traffic.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "print(y[0])\nprint(knn_lc.predict(X.head(1)))\n", "intent": "**Heavy prevalence of loans that performed in the dataset but still nice<br>to see that the predict method worked on a sample**\n"}
{"snippet": "print('Confusion Matrix: (note that False is first row and column)')\nprint(metrics.confusion_matrix(valid_dataframe['Quasar'], RFPredictions))\nprint('\\n')\nprint('Classification Report:')\nprint(metrics.classification_report(valid_dataframe['Quasar'], RFPredictions))\nprint('\\n')\nprint('Validation score: %.4f' % RFScore)\n", "intent": "Below is the confusion matrix, precision, recall, and f1-score on the validation set.\n"}
{"snippet": "CNPredictions = 1 - np.argmax(CNPredictions, axis=1)\nTrueValid = 1 - \\\n    np.argmax(valid_dataframe[['EncodedQuasar',\n                               'EncodedNonQuasar']].values, axis=1)\nprint('Confusion Matrix: (note that Non-quasar is first row and column)')\nprint(metrics.confusion_matrix(TrueValid, CNPredictions))\nprint('\\n')\nprint('Classification Report:')\nprint(metrics.classification_report(TrueValid, CNPredictions))\n", "intent": "Below is the confusion matrix, precision, recall, and f1-score on the validation set.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    N = len(style_layers)\n    for i in range(N):\n        G = gram_matrix(feats[style_layers[i]])\n        A = style_targets[i]\n        loss += style_weights[i] * tf.reduce_sum((G - A)**2)\n    return loss \n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Ypred_PR = results_PR1.predict(exog=Xtest)\nYpred_PR = np.exp(Ypred_PR)\n", "intent": "Predict the daily trips for one of the two Poisson models.\n"}
{"snippet": "def predict_trait(word, pipeline):\n    return pipeline.predict_proba([word])[0,1]\npredict_trait(\"I'm an outgoing extravert and i like festivals, travel,\\\nculture, parties, alcohol, singing, and playing the ukulele\", pipeline=pl_e)\n", "intent": "A taste of what's to come: Let's have the model try predicting some text.\n"}
{"snippet": "preds = clf.predict_proba(x)[:, 1]\n", "intent": "So, would I like Margaret Atwood?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(lr, X, y, cv=10, scoring='roc_auc').mean()\n", "intent": "This is close to our average f1-score (~0.93), representing the correct ccuracy for our imbalanced dataset.\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "The measure of the error choosen by the challenge is the Mean Absolute Percentage Error (MAPE).\n"}
{"snippet": "print \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 2 * validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 3 * validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 4 * validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 5 * validation[\"TARGET\"].values)\n", "intent": "It means that the minimum is far from the other values. So we decided to use not the minimum but a value above, but a readjustment of it instead. \n"}
{"snippet": "print \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 3.65 * validation[\"TARGET\"].values)\n", "intent": "Using ground trial, we settled for the following value of minimal readjustment :\n"}
{"snippet": "y_pred_filled = reg.predict(test_filled[numeric_cols].values)\nsubmission_filled = test_filled[[\"ID\", \"product_id\"]].copy()\nsubmission_filled[\"TARGET\"] = y_pred_filled\nsubmission_filled.head()\n", "intent": "We predict the target value for the rows without missing entries.\n"}
{"snippet": "y_pred_filled = reg.predict(test_filled[numeric_cols].values)\nsubmission_filled = test_filled[[\"ID\", \"product_id\"]].copy()\nsubmission_filled[\"TARGET\"] = y_pred_filled\nsubmission_filled.head()\n", "intent": "For all values above the threshold, fill values by interpolation.\n"}
{"snippet": "print \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 2*validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 3*validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 4*validation[\"TARGET\"].values)\nprint \"MAPE :\", mean_absolute_percentage_error(train_missing[\"TARGET\"].values, 5*validation[\"TARGET\"].values)\n", "intent": "It means that the minimum is far from the other values. So we decided to use not the minimum but a value above, we tried many possibilities.\n"}
{"snippet": "transfer_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_transfer]\ntest_accuracy = 100*np.sum(np.array(transfer_predictions)==np.argmax(test_targets, axis=1))/len(transfer_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_test_pred = svm_grid_rbf.predict(X_test)\n", "intent": "Conducting validation in the test dataset. \n"}
{"snippet": "metrics.classification.accuracy_score(y_test,y_pred)\n", "intent": "**Classification accuracy**: percentage of correct predictions\n"}
{"snippet": "np.sqrt(mean_squared_error(target.values, oof_xgb*0.5 + oof*0.5))\n", "intent": "Let's create a quick ensemble by averaging the submissions generated by LightGBM and XGBoost\n"}
{"snippet": "print(\"Linear SVM\")\nprint(linear_svm_clf.score(X_test, y_test))\nprint()\nprint()\nprint(\"Deep Net\")\nprint(dnn_model.evaluate(X_test, y_test_keras)[1])\n", "intent": "Let's test them on the test data we have prepared.\n"}
{"snippet": "example_predictions = example_model.predict(X_train)\nprint(\"example_predictions[0]\", example_predictions[0]) \n", "intent": "Once a model is built we can use the .predict() function to find the predicted values for data we pass. For example using the example model above:\n"}
{"snippet": "mse = mean_squared_error(y_test,example_model.predict(X_test),multioutput=\"uniform_average\")\nmsebyN = mse*y_test.shape[0]\nprint(\"mean_squared_error (scikit-learn):\",round(mse,2))\nprint(\"mean_squared_error * N number of observations:\",round(msebyN,2))\n", "intent": "**Comparing our function with scikit-learn mean_squared_error**\nscikit-learn mean_square_error = RSS/N\nwith N=number of observations\n"}
{"snippet": "y_train.isnull().values.any()\nCounter(y_train.values)\nround(np.count_nonzero(y_train==0)/len(y_train), 2)\npredict_0 = np.zeros(len(y_test)) \nprint(\"If I predict everyone perished, per my y_test set, my accuracy would be: {:.4f}\"\n      .format(accuracy_score(y_test, predict_0)))\n", "intent": "The above is training set of 712 rows, some data sample from training set and the label.\n"}
{"snippet": "print model\nprint model.get_params()\nerror_sum = 0\ntest_size = len(X_test)\nprint 'Prediction sample', model.predict(X_test[0])[0]\nfor i in range(0, test_size):\n    error_sum += abs(model.predict(X_test[i])[0] - y_test[i]) / 2\nprint 'error sum is:', error_sum\naverage_error = float(error_sum) / test_size\nprint 'average_error is:', average_error\n", "intent": "We will now use the cross-validation set to evaluate our model.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, model.predict(X_test))\nprint 'Confusion matrix:'\nprint cm\nfrom sklearn.metrics import classification_report\nprint 'Classification report:'\nprint classification_report(y_test, model.predict(X_test))\n", "intent": "We will now use the cross-validation set to evaluate our model.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_accuracy = accuracy_score(y_test,y_pred)\nprint(\"test accuracy: %.3f\" %test_accuracy)\n", "intent": "Evaluate the model by accuracy score.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "Evaluate the model by precision, recall, f1 score and support.\n"}
{"snippet": "print(classification_report(y_test,pred_lr,target_names=[\"Not in Trouble\",\"In Trouble\"]))\n", "intent": "So we have 4 false negatives, out of a sample of 12. This could indicate a problem with the model, or a lack of adequate data. (See Section 3)\n"}
{"snippet": "p = (list(clf.predict(X_sel_t)))\nlen(p)\n", "intent": "**Random Forest gave better score than KNN on test data**\n* Using Random forest for `Score` Prediction\n"}
{"snippet": "def evaluate(sess, X, Y):\n    predicted = tf.cast(inference(X) > 0.5, tf.float32)\n    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\n", "intent": "**5.** Model Evaulation \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The test accuracy was 76% --> It decreased. It is therefore worse now. Maybe the best alpha is between 0.1 and 1 ?\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_new.predict(np.expand_dims(feature, axis=0))) for \n                        feature in test_Resnet50_new]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "random.seed(123)\npred_prob=rfmodel.predict_proba(x_test)[:, 1]\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfpr, tpr, thresh = roc_curve(y_test, pred_prob)\nroc_auc = auc(fpr, tpr)\n", "intent": "Let's look at the receiving operating characteristic curve for the binary classification\n"}
{"snippet": "print('data:')\nprint(X_test[:10])\nprint('\\npredictions:')\nprint(reg.predict(X_test)[:10])\nprint('\\ntargets:')\nprint(y_test[:10])\n", "intent": "Code here is from Layton too.\n"}
{"snippet": "print(X_test[:10])\nprint('\\npredictions:')\nprint(reg.predict(X_test)[:10])\nprint('\\ntargets:')\nprint(y_test[:10])\n", "intent": "Code here is from Layton too.\n"}
{"snippet": "score, acc = model.evaluate(x_test, y_test,batch_size=128)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\nprint(\"Accuracy: %.2f%%\" % (acc*100))\n", "intent": "When you run the above cell, you will notice that the model trains very fast much faster than above LSTM based.\n"}
{"snippet": "classes = model.predict(x_test[:10], batch_size=128)\nfor i in range (0,10):\n    if(classes[i] > 0.5 and y_test[i] == 1 or (classes[i] <= 0.5 and y_test[i] == 0)):\n        print( classes[i], y_test[i], \" Right prdiction\")\n    else :\n        print( classes[i], y_test[i], \" Wrong prdiction\")\n", "intent": "Check accuracy well around 89% similar to above LSTM with the bonus of faster training.\n"}
{"snippet": "docs_new = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question\",\n            \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n             \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9\",\n            \"hello, thank you\",\n           \"To claim txt DIS to 87121\"]\npredicted = svm_pipeline.predict(docs_new)\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, category))\n", "intent": "In my computer the above processing took about 2.63 seconds. How about yours?\n"}
{"snippet": "print \"mean square error: \", mean_squared_error(y, model1_y)\nprint explained_variance_score(y, model1_y)\n", "intent": "<img src=\"images/low_r_square.png\">\n<img src=\"images/high_r_square.png\">\n"}
{"snippet": "scores = cross_val_score(dt, x, y, scoring='accuracy', cv=10)\nprint scores\nprint scores.mean()\n", "intent": "2nd person (a 55 years old male) is likely to have 2nd heart attack\n"}
{"snippet": "model.summary()\nprint(model.predict_proba(in_data))\nprint(model.get_weights())\n", "intent": "model = Sequential()\nmodel.add(Dense(2, input_dim=2, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\n"}
{"snippet": "ascore = accuracy_score(list(y_test), list(ypred)) * (100)\nprint(\"Accuracy Score: {}\".format(ascore))\n", "intent": "now, we can calculate the accuracy score, its two parameters being the testing values and the pedicted values.\n"}
{"snippet": "print sqft_model.predict(house2)\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "print my_features_model.predict(graphlab.SFrame(bill_gates))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Bill_gates%27_house.jpg/2560px-Bill_gates%27_house.jpg\">\n"}
{"snippet": "test_preds  = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n    source = source.reshape((1,source.shape[0],source.shape[1]))\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "pred = np.exp(res.predict())*df['2017-05-13':].shift(1)\n", "intent": "Now our model can make predictions based on the test set.\n"}
{"snippet": "y_pred = model.predict(np.expand_dims(x_test_scaled, axis=0), 8)\n", "intent": "With our best performing checpoint loaded we make a prediction on our `x_test` set.\n"}
{"snippet": "cmatrix = confusion_matrix(y_test,logregmul.predict(X_test_mul_log))\nprint(\"TP Rate: \",  cmatrix[1][1]/(sum(cmatrix[1])))\nprint(cmatrix)\n", "intent": "We thought it would be import to look at the confusion matrix for Multi Logistic Regression as another metric by which to evalute the model\n"}
{"snippet": "qtest_vec = pd.read_hdf(\"pickles/inferred_test_vectors.h5\")\nprint(qtest_vec.shape)\nstart_time = time.time()\npredictions = xgb_model.predict_proba(qtest_vec)\nprint(time.time()-start_time, \"sec to generate\", predictions.shape[0], \"predictions\")\n", "intent": "This result is much more reasonable. Now we can generate predictions for the test values and submit them:\n"}
{"snippet": "def accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])\ndef F1_score(predictions, labels):\n  return f1_score(np.argmax(labels, 1), np.argmax(predictions, 1))\n", "intent": "Accuracy and F1 Score\n"}
{"snippet": "if test_year_truth_is_known:\n    for model in models:\n        print(\"--> {} classification report {}:\".format(model.estimator_name, test_year))\n        print(\"\")\n        class_names = ['Non-All-Stars (truth)', 'All-Stars (truth)']\n        print(classification_report(model.y_truth, model.y_prediction, target_names=class_names))\nelse:\n    print(\"--> The NBA All-Stars for {} have not been selected yet\".format(test_year))\n", "intent": "Print classification reports for all fitted models:\n"}
{"snippet": "x_p = [[50], [350]]\ny_p = linear_model.predict(x_p)\nplot(x_p, y_p, label = 'Model')\nplot(hours, production, 'r.', label = 'Data')\ntitle('Production by hour')\nxlabel('Hours')\nylabel('Production')\naxis([50, 90, 200, 350])\nlegend(loc=2)\n", "intent": "Let's represent the model of the graphic way:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = grid_search_cv.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "By default, GridSearchCV trains the best model found on the whole training set (you can change this by setting refit=False)\n"}
{"snippet": "propensity_scores = logistic.predict_proba(selectedFeatures)\nDF['propensity_score']=[k[1] for k in logistic.predict_proba(selectedFeatures)]\ntreated = DF.loc[DF['treat'] == 1]\ncontrol = DF.loc[DF['treat'] == 0]\nDF.head()\n", "intent": "Next we calculate the propensity score of each record to be in the treated group:\n"}
{"snippet": "df['cluster'] = ml.predict(df[['lon','lat']])\ndf[['uid','lat','lon','vid','name', 'cluster']].sample(10)\n", "intent": "Add cluster id to each event. Now we can group by cluster id.\n"}
{"snippet": "knn.predict([[5,3,2,4], [3,6,4,3]])\n", "intent": "- Returns a numpy array\n- Needs a numpy array / list as as input\n"}
{"snippet": "trainPredictions = initialMod_lm.predict(featureSet)\ntrainMAE = mae(trainPredictions,targetVar)\n", "intent": "As expected, we are performing extremely badly on this initial submission. Let us check the Mean Absolute Error (MAE) on our training set.\n"}
{"snippet": "y_compare = np.argmax(y_test,axis=1) \nscore = metrics.accuracy_score(y_compare, pred)\nprint(\"Accuracy score: {}\".format(score))\n", "intent": "Now that we have the actual iris flower predicted, we can calculate the percent accuracy (how many were correctly classified).\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\npredicted = [1.1,1.9,3.4,4.2,4.3]\nexpected = [1,2,3,4,5]\nscore_mse = metrics.mean_squared_error(predicted,expected)\nscore_rmse = np.sqrt(score_mse)\nprint(\"Score (MSE): {}\".format(score_mse))\nprint(\"Score (RMSE): {}\".format(score_rmse))\n", "intent": "We will now look at how to calculate RMSE and logloss by hand.  You will need to be able to perform this for the midterm.\n"}
{"snippet": "pred = model.predict([inputs_test, queries_test])\nprint(pred)\n", "intent": "We evaluate the accuracy, using the same technique as previous classification networks.\n"}
{"snippet": "print(\"Remember, I only know these words: {}\".format(vocab))\nprint()\nstory = \"Daniel went to the hallway. Mary went to the bathroom.  Daniel went to the bedroom.\"\nquery = \"Where is Sandra?\"\nadhoc_stories = (tokenize(story), tokenize(query), '?')\nadhoc_train, adhoc_query, adhoc_answer = vectorize_stories([adhoc_stories])\npred = model.predict([adhoc_train, adhoc_query])\nprint(pred[0])\npred = np.argmax(pred,axis=1)\nprint(\"Answer: {}({})\".format(vocab[pred[0]-1],pred))\n", "intent": "You might want to create your own stories and questions.  \n"}
{"snippet": "y = km.predict(data)\nprint y\n", "intent": "Next, we'll predict the clusters for each data point using the <code>predict</code> method:\n"}
{"snippet": "target_predicted = lr.predict(features_test)\n", "intent": "Let's use the <code>predict</code> function and pass through the <code>features_test</code> data.\n"}
{"snippet": "predictions = nb.predict(test_dtm)\npredictions\n", "intent": "Next, we'll make predictions about the test data by using the **<code>predict</code>** function on the NB instance.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy on training data has lowered to 80% from 92% and might be less accurate but we avoid the overfitting problem on training data.   \n"}
{"snippet": "print(classification_report(y_true=y_train,y_pred=model.predict(X_train)))\n", "intent": "Now print the classification report for the training set, using Sklearn's `classification_report` function:\n"}
{"snippet": "print(classification_report(y_true=y_test,y_pred=model.predict(X_test)))\n", "intent": "And the classification report for the test set:\n"}
{"snippet": "print(f1_score(y_true=y_test,y_pred=model.predict(X_test), average='macro'))\n", "intent": "and finally, the f1 score on the test set using `macro` averaging:\n"}
{"snippet": "predicted_probabilities_full = np.max(model.predict_proba(X), axis=1)\nsorted_predicted_probabilities_full_indices = np.argsort(predicted_probabilities_full)\n", "intent": "Next, we'll get the model's confidence for the entire dataset:\n"}
{"snippet": "print(classification_report(y_true=y_train,y_pred=model.predict(X_train)))\n", "intent": "Display the classification report for the training set:\n"}
{"snippet": "print(classification_report(y_true=y_test,y_pred=model.predict(X_test)))\n", "intent": "and for the test set:\n"}
{"snippet": "x_new = bostondf[['LSTAT']]\nbostondf['medv_pred'] = lm.predict(x_new)\nsns.lmplot('medv_pred', 'MEDV', bostondf)\nsns.lmplot('LSTAT', 'MEDV', bostondf)\n", "intent": "** Adding prediction to data frame  ** \n"}
{"snippet": "city_est.predict(data[:5])\n", "intent": "And let's see if it works.\n"}
{"snippet": "city_est.predict([{'city': 'Timbuktu'}])\n", "intent": "There is a problem, however.  What happens if we're asked to estimate the rating of a venue in a city that's not in our training set?\n"}
{"snippet": " def softmax_loss(x, y):\n    probs = np.exp(x - x.max(axis=1)[:, np.newaxis])\n    probs = probs / np.sum(probs, axis=1, keepdims=True)\n    N = probs.shape[0]\n    loss = -np.log(probs[np.arange(N), y]).mean()\n    dx = probs.copy()\n    dx[np.arange(N), y] -= 1\n    dx /= N\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "VGG19_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cv_prediction = cross_val_predict(model, data[feature_labels], data['delta_e'], cv=10)\n", "intent": "Quantify the performance of this model using 10-fold cross-validation\n"}
{"snippet": "predictions = predict(ng_X_test)\nprint \"Accuracy: {:.3f}\".format(accuracy_score(ng_y_test, predictions))\nprint \"Classification report\"\nprint classification_report(ng_y_test, predictions, target_names=newsgroups.target_names)\n", "intent": "Once we are done training the model we print some results\n"}
{"snippet": "def similarity_score(w1, w2):\n    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n    return score\nprint('cat\\tcat\\t', similarity_score('cat', 'cat'))\nprint('cat\\tfeline\\t', similarity_score('cat', 'feline'))\nprint('cat\\tdog\\t', similarity_score('cat', 'dog'))\nprint('cat\\tmoo\\t', similarity_score('cat', 'moo'))\nprint('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\nprint('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\nprint('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))\n", "intent": "Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products.\n"}
{"snippet": "lr.predict_proba(X_test_std)\n", "intent": "Predict the class-membership probability of the samples via the predict_proba method:\n"}
{"snippet": "predicted = model2.predict(x_test)\nprobs = model2.predict_proba(x_test)\n", "intent": "We should predict class labels and generate class probabilities.\n"}
{"snippet": "print (metrics.accuracy_score(y_test, predicted))\nprint (metrics.roc_auc_score(y_test, probs[:, 1]))\n", "intent": "Let's generate evaluation metrices\n"}
{"snippet": "print (metrics.confusion_matrix(y_test, predicted))\nprint (metrics.classification_report(y_test, predicted))\n", "intent": "Let's also look at the confusion matrix and a classification report with other metrics.\n"}
{"snippet": "model.predict_proba(np.array([0,57, 39, 43, 20, 11, 2, 38, 16, 10, 5]))\n", "intent": "So the probability that Benjamin Becker winning a match is %93\n"}
{"snippet": "model.predict_proba(np.array([0,65, 40, 35, 15, 4, 4, 31, 40, 13, 4]))\n", "intent": "Since Murray's probability is higher, we are predicting that Andy Murray had won, and it is true :)\n"}
{"snippet": "model.predict_proba(np.array([0,64, 48, 36, 16, 9, 2, 36, 21, 17, 7]))\n", "intent": "The probability of Novak Djokovic winning a match %99.9910901\n"}
{"snippet": "print(\"Closing Price of Ethereum for last 5 days: \")\nprint(test_predict[-5:])\nprint(\"Ethereum price for tomorrow: \", future_predict)\ntrain_score = math.sqrt(mean_squared_error(y_train[:,0], train_predict[:,0]))\nprint('Train Score: %.2f RMSE' % (train_score))\ntest_score = math.sqrt(mean_squared_error(y_test[:,0], test_predict[:,0]))\nprint('Test Score: %.2f RMSE' % (test_score))\n", "intent": "----\n- Using the model we trained earlier and pass in today's price as the parameter where it would be the last row in the test dataset\n"}
{"snippet": "y_pred = model.predict(X_test)\ny_test_class = np.argmax(y_test, axis=1)\ny_pred_class = np.argmax(y_pred, axis=1)\naccuracy = (y_test_class == y_pred_class).mean()\nprint(\"Raw accuracy: {:.3f}\\n\".format(accuracy))\nnames = encoder.classes_\nprint(\"\\nClassification report:\\n\")\nprint(classification_report(y_test_class, y_pred_class, target_names=names))\nprint(\"\\nConfusion matrix:\")\nplot_confusion_matrix(y_test_class, y_pred_class)\n", "intent": "* We can evaluate the neural network's performance using the same tools we used before\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(df['species'], df['labels'])\nprint(\"K-means clustering accuracy: {:.2f}\".format(acc))\n", "intent": "How many cases did the classifier label correctly?\n"}
{"snippet": "nw_data = np.asarray([[1896, 20],[1922, 20]])\nlinear.predict(nw_data)\n", "intent": "Our model has now been fit to the training data. Let's make up some new cricketers, and see how it classifies them.\n"}
{"snippet": "test = pd.read_pickle(\"../toxic_comment_data/labeled_test_set_with_features.csv\")\npredicted = gs_clf.predict(test)\nnp.mean(predicted == test.Insult)\n", "intent": "See how the new classifier does on accuracy: (noticeable improvement)\n"}
{"snippet": "predicted = gs_clf.predict(comments)\npredicted2 = gs_clf.predict(comments)\nplot_roc(comments.Insult, predicted)\nplot_roc(comments.Insult, predicted2)\n", "intent": "See how the new classifier does on accuracy: (noticeable improvement)\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\nresnet_test_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % resnet_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "sqft_model.predict(house2['sqft_living'])\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "\"our predicted result: {:,.2f}\".format(my_features_model.predict(df_bill_gates)[0])\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Bill_gates%27_house.jpg/2560px-Bill_gates%27_house.jpg\">\n"}
{"snippet": "from gsitk.evaluation.evaluation import Evaluation\ndatasets_evaluation = {\n    'vader': data_ready['vader'],\n    'pl05': data_ready['pl05']\n}\nev = Evaluation(tuples=None,\n                datasets=datasets_evaluation,\n                pipelines=[pipeline, pipeline2])\nev.evaluate()\nev.results\n", "intent": "Let the `Evaluation` do its job: evaluate your pipelines!\n"}
{"snippet": "rs50_predictions = [np.argmax(rs50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_rs50]\ntest_accuracy = 100*np.sum(np.array(rs50_predictions)==np.argmax(test_targets, axis=1))/len(rs50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "I = np.eye(x0.shape[1])\nA = pcr.predict(I) - pcr.predict(I*0)\n", "intent": "Now what about the linear model?\n"}
{"snippet": "def r2_score_peristence(ytrue, ypred, ynull):\n    def ss(x):\n        return np.sum(x**2)\n    return 1 - ss(ytrue-ypred)/ss(ynull-ytrue)\nr2_score_peristence(x1, pcr.predict(x0), x0)\n", "intent": "This is probably worse than the persistence forecast\n"}
{"snippet": "x_test = np.array(['feeling cool'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "logisticRegr.predict(x_test[0].reshape(1,-1))\n", "intent": "* **Step 4.** Predict labels for new data (new images)\n"}
{"snippet": "predictions = logisticRegr.predict(x_test)\n", "intent": "* Make predictions on entire test data\n"}
{"snippet": "trans_model_predictions = [np.argmax(trans_model.predict(np.expand_dims(feature, axis=0))) for feature in test_pretrained]\ntest_accuracy = 100*np.sum(np.array(trans_model_predictions)==np.argmax(test_targets, axis=1))/len(trans_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def print_errors(model, pred_train, pred_test, tar_train, tar_test):\n    train_error = mean_squared_error(tar_train, model.predict(pred_train))\n    test_error = mean_squared_error(tar_test, model.predict(pred_test))\n    print 'Training data MSE:', train_error\n    print 'Test data MSE:', test_error\n    train_error = mean_absolute_error(tar_train, model.predict(pred_train))\n    test_error = mean_absolute_error(tar_test, model.predict(pred_test))\n    print 'Training data MAE:', train_error\n    print 'Test data MAE:', test_error\nprint_errors(model, pred_train, pred_test, tar_train, tar_test)\n", "intent": "How does model do on test data?\n"}
{"snippet": "def getPrediction(in_sentences):\n  labels = [0, 1]\n  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n  predictions = estimator.predict(predict_input_fn)\n  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n", "intent": "Now let's write code to make predictions on new sentences:\n"}
{"snippet": "w_training = logr.train(X_training, Y_training)\npredictions = logr.predict(w_training, X_testing)\naccuracy_training_all = logr.compute_accuracy(predictions, Y_testing)\n", "intent": "Here we simply train logistic regression on half of the data and test it on the other half.\n"}
{"snippet": "    def predict(self, X):\n        N,D = X.shape\n        Y = zeros((N,self.L))\n        Y[:,self.j_max] = 1\n        return Y\n", "intent": "__predict__: Calls the prediction of the node.\n"}
{"snippet": "frag_len = 40\nseqs=[]\ntotal_base = 0\nfor i in range(625):\n    ss = random.randint(-frag_len+kmer_size, len(seq)-kmer_size)\n    if ss < 0:\n        ss = 0\n    seqs.append(sim_error(seq[ss:ss+frag_len], pi=0.05, pd=0.05, ps=0.01))\n    total_base += len(seqs[-1])\nprint(\"average coverage:\", total_base/seq_len)\n", "intent": "Simulate some erroneous sequence fragment of length 40 with insertion error at 5%, deletion errors at 5% and subsitution error at 1%.\n"}
{"snippet": "linkage_matrix_full = scipy.cluster.hierarchy.ward(novelsCoocMat.toarray())                  \nhierarchicalClusters_full = scipy.cluster.hierarchy.fcluster(linkage_matrix_full, 4, 'maxclust')\nprint(\"For our complete clusters:\")                                                          \nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(novelsDF['name'], hierarchicalClusters_full)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(novelsDF['name'], hierarchicalClusters_full)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(novelsDF['name'], hierarchicalClusters_full)))\nprint(\"Adjusted Rand Score: {:0.3f}\".format(sklearn.metrics.adjusted_rand_score(novelsDF['name'], hierarchicalClusters_full)))\n", "intent": "Implement the method on the full data set\n"}
{"snippet": "sklearn.metrics.accuracy_score(test_data_df['category'],\n                               horror_bag.predict(np.stack(test_data_df['vect'], axis = 0)))\n", "intent": "Check the accuracy score:\n"}
{"snippet": "sklearn.metrics.accuracy_score(test_data_df['category'],\n                               horror_knearest.predict(np.stack(test_data_df['vect'], axis = 0)))\n", "intent": "Check acuuracy acore:\n"}
{"snippet": "sklearn.metrics.accuracy_score(test_data_df['category'],\n                               horror_svm.predict(np.stack(test_data_df['vect'], axis = 0)))\n", "intent": "Check accuracy rate from my testing set\n"}
{"snippet": "sklearn.metrics.accuracy_score(test_data_df['category'],\n                               horror_nn.predict(np.stack(test_data_df['vect'], axis = 0)))\n", "intent": "Check accuracy rate from my testing set\n"}
{"snippet": "test_loss = estimator.model.evaluate(X_test, y_test)\nprint(\"test set mse is %.2f\" % test_loss)\n", "intent": "Let's get the MSE for the test set.\n"}
{"snippet": "rate = KNNmodel.predict(X_train) == y_train\nprint('Training Error Rate:', 1 - np.mean(rate))\nrate = KNNmodel.predict(X_test) == y_test\nprint('Testing Error Rate:', 1 - np.mean(rate))\n", "intent": "`knn.predict(X) `\n* Testing error rate\n* Training error rate \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Less accurate than the classifer before, however this one is not overfit, and makes this a more reasonable classifier.\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\n", "intent": "We can see above that we have an accuracy of 98.87% in the training set. Let's evaluate in the test set:\n"}
{"snippet": "Yhat_energy = energy_reg.predict(X)\nYhat_temp = temp_reg.predict(X)\nprint(\"Energy Predictions: %s\" % str(Yhat_energy))\nprint(\"Temperature Predictions: %s\" % str(Yhat_temp))\n", "intent": "Then, we make predictions on using this nd-array\n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint all_predictions\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset. Let's beginby getting all the predictions:\n"}
{"snippet": "def softmax_loss(logits, labels):\n\tcross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels, name='cross_entropy_per_example')\n\tmean_cross_entropy = tf.reduce_mean(cross_entropy, name='cross_entropy')\n\treturn mean_cross_entropy\n", "intent": "A function to calculate the softmax loss of a batch given the true labels\n"}
{"snippet": "def l2_loss(noise_image, l2_weight):\n\treturn l2_weight * tf.nn.l2_loss(noise_image)\n", "intent": "A function to calculate the L2 loss of the noise image\n"}
{"snippet": "softmax_loss = softmax_loss(logits, y)\n", "intent": "Use the helper function we defined above to calculate the softmax loss of the noisy images and the true labels\n"}
{"snippet": "l2_noise_loss = l2_loss(noise_img, l2_weight)\n", "intent": "Use the helper function we defined above to calculate the L2 loss of the adversarial noise image\n"}
{"snippet": "n_classes = 2 \nn_observations = len(Y_test) \nMSE = (((match.predict_proba(X_test) - model.predict_proba(X_test))) ** 2).sum() / n_observations / n_classes\n1 - MSE\n", "intent": "There we go. Now we can construct our probability distribution fitness metric:\n"}
{"snippet": "print('Mean difference in predicted probability value: {:.4f}'\\\n        .format(abs(match.predict_proba(X_test) - model.predict_proba(X_test)).sum()/n_observations/n_classes))\n", "intent": "Maybe our model is doing pretty good, after all. Not so much:\n"}
{"snippet": "def test_data_regression(regressors, X):\n    y_pred = []\n    for r in regressors:\n        y_pred.append( r.predict( np.nan_to_num(np.array(X)) ) )\n    return np.mean(np.array(y_pred), axis=0)\n", "intent": "Now I need to load the test data and run the array of regressors on it. I'll average the results.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, preds1)\n", "intent": "Classification models best measure is the AUC score\n"}
{"snippet": "preds2 = rfc2.predict(X_test)\n", "intent": "Now to get the predictions from our model\n"}
{"snippet": "preds3 = rfc3.predict(X_test)\n", "intent": "Now to get the predictions from our model\n"}
{"snippet": "preds4 = rfc4.predict(X_test)\n", "intent": "Now to get the predictions from our model\n"}
{"snippet": "preds5 = rfc5.predict(X_test)\n", "intent": "Now to get the predictions from our model\n"}
{"snippet": "X_test_spl = X_test[0:10]\ny_test_spl = y_test[0:10]\npredictions = loaded_RF.predict(X_test_spl)\nfor i in range(0,len(predictions)):\n    print(predictions[i], y_test_spl[i])\n", "intent": "We can then predict some values from the model. \n"}
{"snippet": "xx, yy = np.meshgrid(np.arange(x_min, x_max, step),\n                     np.arange(y_min, y_max, step))\nmesh_predict = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "Create a grid and predict the species\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))\n", "intent": "The table displays the precision and the recall\n"}
{"snippet": "predictions = clf.predict(X_example_tfidf)\nfor prediction, doc_example in zip(predictions, doc_examples):\n    print(\"{} - {}\".format(doc_example, newsgroups_train.target_names[prediction]))\n", "intent": "Using the trained classifer predict the newsgroup category\n"}
{"snippet": "prediction = text_clf.predict(newsgroups_test.data)\n", "intent": "Predict the category of the test set\n"}
{"snippet": "print(metrics.classification_report(newsgroups_test.target, predicted,\n    target_names=newsgroups_test.target_names))\n", "intent": "Display the precision and recall of the classifier\n"}
{"snippet": "predicted = gs_clf.predict(newsgroups_test.data)\nprint('Accuracy = {:.2f}'.format(np.mean(predicted == newsgroups_test.target) * 100))\n", "intent": "Using the classifier with the best parameters we calculate the accuracy\n"}
{"snippet": "y_predicted = clf.predict(docs_test)\n", "intent": "Use the classifier on the training data to predict language category\n"}
{"snippet": "sentences = [\n    u'This is a language detection test.',\n    u'Ceci est un test de d\\xe9tection de la langue.',\n    u'Dies ist ein Test, um die Sprache zu erkennen.',\n]\npredicted = clf.predict(sentences)\nfor s, p in zip(sentences, predicted):\n    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))\n", "intent": "Make a prediction on a simple example\n"}
{"snippet": "y_predicted = grid_search.predict(docs_test)\n", "intent": "Predict the sentiment using the training dataset\n"}
{"snippet": "model.load_weights(filepath = '.mdl_wts.hdf5')\nscore = model.evaluate(Xtrain, Ytrain, verbose=1)\nprint('Train score:', score[0])\nprint('Train accuracy:', score[1])\n", "intent": "Load the best weights and check the score on the training data.\n"}
{"snippet": "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', loss)\nprint('Test accuracy:', accuracy)\n", "intent": "We evaluate the model using the test set, obtaining the test loss and accuracy (% examples correctly classified)\n"}
{"snippet": "def include_features(img_obj):\n    img_obj[\"fc2\"] = fc2_model.predict(img_obj[\"x\"])\ninclude_features(elephant1)\ninclude_features(elephant2)\ninclude_features(hippo1)\n", "intent": "Let's use the fc2_model to obtain feature vectors for our example images, and include them in the dictionaries.\n"}
{"snippet": "predictions = [np.argmax(model_net.predict(np.expand_dims(feature, axis=0))) for feature in test_net]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lm.evaluate(x, y, verbose=0)\n", "intent": "**\"Ok model, do it five more times.\"**\n"}
{"snippet": "def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, Y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\nrmse_cv = np.vectorize(rmse_cv)\n", "intent": "First, we define RMSE function to compute loss for cross validation.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        current_im_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((current_im_gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(\"Remember, I only know these words: {}\".format(vocab))\nprint()\nstory = \"Daniel went to the hallway. Mary went to the bathroom.  Daniel went to the bedroom.\"\nquery = \"Where is Daniel?\"\nadhoc_stories = (tokenize(story), tokenize(query), '?')\nadhoc_train, adhoc_query, adhoc_answer = vectorize_stories([adhoc_stories])\npred = model.predict([adhoc_train, adhoc_query])\nprint(pred[0])\npred = np.argmax(pred,axis=1)\nprint(\"Answer: {}({})\".format(vocab[pred[0]-1],pred))\n", "intent": "You might want to create your own stories and questions.  \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This shows further improvement in reducing overfit.\n"}
{"snippet": "print(\"Mean squared error on test data:\")\nprint(ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test}))\nprint(\"Mean absolute error on test data:\")\nprint(ed.evaluate('mean_absolute_error', data={X: X_test, y_post: y_test}))\n", "intent": "With this we can evaluate various quantities using predictions from\nthe model (posterior predictive).\n"}
{"snippet": "y_predict_proba = lr.predict_proba(X_test)[:,1]\n", "intent": "Get probability for being a positive (column 1 of the output of **lr.predict_proba**)\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint (score)\n", "intent": "Finally, we can evaluate our model on the test data:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predicted))\n", "intent": "Classification Report\n"}
{"snippet": "rogue_point = [[6.0, 1.8]]\nclf.predict(rogue_point)\n", "intent": "The rogue point is now classified as class 1. \n"}
{"snippet": "lr.predict_proba(X_test_std[0,:].reshape(1,-1)) \n", "intent": "Furthermore, we can predict the class probabilties for each class using predict_prob method.\n"}
{"snippet": "print( bcancer_model.predict( [bcancer.data[0]] ) )\nprint( bcancer.target[0] )\n", "intent": "Let's evaluate the prediction of this model on two samples: one from class 0 (malign) and one from class 1 (benign).\n"}
{"snippet": "bcancer_predictions = bcancer_model.predict( bcancer.data )\nprint( bcancer_predictions )\n", "intent": "We can also evaluate the prediction of your model on an entire dataset:\n"}
{"snippet": "print(\"Probability for P1: \", clf.predict_proba(P1))\nprint(\"Probability for P2: \", clf.predict_proba(P2))\n", "intent": "**Question:** Why did we use [[ and ]] in the previous cell?\nYou can also check the probability that a given point belongs to a class:\n"}
{"snippet": "score = model.evaluate(X_val, y_val, verbose=0)\nprint('Validation loss:', score[0])\nprint('Validation accuracy:', score[1])\n", "intent": "Let's also show the numeric validation accuracy and loss.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "We calculate the test loss and accuracy.\n"}
{"snippet": "def cifar_loss(logits, targets):\n    targets = tf.squeeze(tf.cast(targets, tf.int32))\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = targets, logits = logits)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    return cross_entropy_mean\n", "intent": "Create the loss function\n"}
{"snippet": "class SklearnPredictor:\n    def fit(self, X, y):\n        return self\n    def predict(X):\n        return predictions\nclass SklearnTransformer:\n    def fit(self, X, y=None):\n        return self\n    def transform(X):\n        return transformed_data\n", "intent": "Notations:\n* `X` is a 2D matrix, rows represents data points, columns contains feature values\n* `y` is a 1D array containing the labels\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_tpred = pipeline.predict(X_train)\ny_pred = pipeline.predict(X_test)\nprint(\"Train accuracy: {}\".format(accuracy_score(y_train, y_tpred)))\nprint(\"Test accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n", "intent": "**Accuracy** is a common metric used in classification evaluation.\n$\\mathrm{Accuracy} = \\frac{\\mathrm{\\\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "$\\mathrm{F\\ score} = 2 \\cdot \\frac{\\mathrm{Precision} \\cdot \\mathrm{Recall}}{ \\mathrm{Precision} + \\mathrm{Recall}}$\n"}
{"snippet": "logodds = logreg.intercept_ + logreg.coef_[0] * 2\nodds = np.exp(logodds)\nprob = odds / (1 + odds)\nprint(prob)\nlogreg.predict_proba(2)[:, 1]\n", "intent": "We can check at the coefficient of the logistic regression does in fact generate the log-odds.\n"}
{"snippet": "reg_loss_W = tf.nn.l2_loss(W)\nreg_loss_b = tf.nn.l2_loss(b)\n", "intent": "$$\nR(W) = \\|W\\|_2^2\\\\\nR(b) = \\|b\\|_2^2\n$$\nHint: You may use the function *tf.nn.l2_loss()*\n"}
{"snippet": "cohen_rand_forest = sk.metrics.cohen_kappa_score(df_y_test,y_pred_best)\nprint(\"Cohen's Kappa\")\nprint(cohen_rand_forest)\n", "intent": "We will now compute Cohen's Kappa of the best model\n"}
{"snippet": "predictions_new = predictions - mean_rmse\nRMSE_new = sqrt(mean_squared_error(Y_test2_inverse, predictions_new))\nprint('Test GRU model RMSE_new: %.3f' % RMSE_new)\n", "intent": "Next, we subtract the mean RMSE from each prediction our model produced. Then, we recalculate the RMSE for the model. \n"}
{"snippet": "score = accuracy_score(truth['after'].tolist(),\n                       results['after'].tolist())\n", "intent": "**Overall Accuracy**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels,housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "We can use sklearn's `mean_squared_error`\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Now for the linear regression model\n"}
{"snippet": "sgd_clf.predict([mnist.data[36000]])\n", "intent": "The 36000-th entry is a five. Check on that\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf,X_train,y_train_5,cv=3,method='decision_function')\n", "intent": "To select the threshold we use the `cross_val_predict` but ask for the `decision_function` scores instead of the actual predictions\n"}
{"snippet": "y_train_pred_forest = cross_val_predict(rf_clf, X_train, y_train_5,cv=10)\nprint 'The AUC is: %s' % roc_auc_score(y_train_5,y_scores_forest)\nprint \"Precision: %s\" % precision_score(y_train_5,y_train_pred_forest)\nprint \"Recall: %s\" % recall_score(y_train_5,y_train_pred_forest)\n", "intent": "Much better!!! Now the AUC, precision, and recall scores. Need the predictions for that!\n"}
{"snippet": "rf_clf.predict_proba([mnist.data[36000]])\n", "intent": "and the list of probabilities that show why 5 was chosen\n"}
{"snippet": "cross_val_score(sgd_clf,X_train,y_train,cv=3,scoring='accuracy')\n", "intent": "Evaluate the classifier using `cross_val_score()`:\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf,X_train_scaled,y_train,cv=3)\nconf_mx = confusion_matrix(y_train,y_train_pred)\nconf_mx\n", "intent": "Start by looking at the confusion matrix just as before\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        l = style_layers[i]\n        feat = gram_matrix(feats[l])\n        loss += style_weights[i]*tf.reduce_sum((feat-style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "accuracy = np.round((np.array(y_test) == logreg.predict(np.array(X_test))).sum() / len(np.array(y_test)) * 100, decimals=5)\n", "intent": "Now we can compute the percentage of samples correctly classified:\n"}
{"snippet": "clf.predict(iris_X_train)\n", "intent": "Print the predicted class of iris\n"}
{"snippet": "def accuracy(x,y):\n    output = []\n    for i,j in zip(x,y):\n        if i == j:\n            output.append(1)\n        else:\n            output.append(0)\n    return np.mean(output)\nprint(\"training accuracy: {}\".format(accuracy(clf.predict(iris_X_train),iris_y_train)))\n", "intent": "Based on the target values in the training set, calculate the training accuracy:\n"}
{"snippet": "print(\"testing accuracy: {}\".format(accuracy(clf.predict(iris_X_test),iris_y_test)))\n", "intent": "And here's the testing accuracy:\n"}
{"snippet": "loss, metrics = model.evaluate(X_test, ohe_y_test, verbose=0)\nprint(\"Accuracy = {:.2f}\".format(metrics))\n", "intent": "We can ``evaluate()`` our accuracy by using that method on the test data; this is equivalent to ``sklearn``'s ``score()``.\n"}
{"snippet": "classes = model.predict_classes(X_test, verbose=0)\nprobs = model.predict_proba(X_test, verbose=0)\nprint('(class) [ probabilities ]')\nprint('-'*40)\nfor x in zip(classes, probs):\n    print('({}) {}'.format(x[0],x[1]))\n", "intent": "Not bad!\nThere are also ``sklearn``-like methods that return class assignment and their probabilities.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(iris_y_test, iris_y_predict)\n", "intent": "Thankfully, ``sklearn`` also has [many built-in ways](http://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "b_y_predict = new_lr.predict(b_X_test)\n", "intent": "And now we can use it to predict the ``target`` for our housing data (remember, we use the \"test\" data for measuring the success of our estimator.\n"}
{"snippet": "y_injuries_pred_rfc= rfc_best_injuries.predict(X_injuries_test)\nprint y_injuries_pred_rfc.shape, y_injuries_test.shape\ncls_rep = classification_report(y_injuries_test, y_injuries_pred_rfc)\nprint cls_rep\n", "intent": "Obviously the model with these three predictors is able to predict a majority of non-accidents, but unable to predict the vast majority of accidents.\n"}
{"snippet": "y_injuries_pred_rfc= rfc_best_injuries.predict(X_injuries_test)\nprint y_injuries_pred_rfc.shape, y_injuries_test.shape\ncls_rep = classification_report(y_injuries_test, y_injuries_pred_rfc)\nprint cls_rep\n", "intent": "When I incorporate the additional features in my model, it improves slightly. I will continue to work on and improve this model.\n"}
{"snippet": "dfscale['KM_PCA_cluster'] = km.fit_predict(dfscale[['PCA0','PCA1']])\n", "intent": "Cluster using just PCA0 and PCA1.  Store the cluster numbers in our dataframe.\n"}
{"snippet": "dfscale['KM_cluster'] = km.fit_predict(dfscale[['sat_scaled', 'cost', 'earn', 'gradrate']])\n", "intent": "For purposes of comparision, cluster will all the original data:\n"}
{"snippet": "probs = rf.predict_proba(test_data)\nprobs\n", "intent": "Query predictive model with all test data, for purposes of testing\nthe effectiveness\n"}
{"snippet": "chosen_model_pred = [np.argmax(chosen_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_bottleneck]\ntest_accuracy = 100*np.sum(np.array(chosen_model_pred)==np.argmax(test_targets, axis=1))/len(chosen_model_pred)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.evaluate(X_test, Y_test, verbose=0)\n", "intent": "Evaluate model on test data\n"}
{"snippet": "print('True Outcome:',list(Y))\nprint('ML Outcome  :',list(clfsvm.predict(X)))\n", "intent": "In fact we can run all the possible plays and see the ML algorithm's performance.\n"}
{"snippet": "predictions = model.predict(X)\nrounded = [round(x[0]) for x in predictions]\nprint(rounded)\n", "intent": "We could use more sophesticated neural nets (with better computational power on our systems)!\n"}
{"snippet": "proba_predictions = logit.predict_proba(X_train) \nprint(proba_predictions[0:10])  \n", "intent": "It can also predict probabilities, instead of predicting just 0 or 1: \n"}
{"snippet": "recall_prob_array = lr_classifier.predict_proba(training_data)[:,1]\nauc = roc_auc_score(recalls_array, recall_prob_array)\nprint auc\n", "intent": "Once we have the coeeficients we can calculate AUC for the training data. \n"}
{"snippet": "from sklearn import grid_search\nshuffle=sklearn.cross_validation.StratifiedShuffleSplit(labels,n_iter=10)\ndef scorer(estimator,features_test,labels_test):\n    labels_pred=estimator.predict(features_test)   \n    pre=sklearn.metrics.precision_score(labels_test,labels_pred)\n    rec=sklearn.metrics.recall_score(labels_test,labels_pred)\n    if pre > 0.3 and r > 0.3:\n        return sklearn.metrics.f1_score(labels_test,labels_pred)\n    return 0\n", "intent": "Tune the algorithms\n------------------\nTry Random Forest, AdaBoost, K Neighbors\n"}
{"snippet": "sgd_test_prob = clf_sgd_cal.predict_proba(df_lr[sgd_feat][te_mask].values)[:, 1]\ntest_predictions['LR_CAL']=sgd_test_prob\n", "intent": "We then make calibrated predictions on the test set that will be used as part of the final prediction using the trained stacked logistic regression.\n"}
{"snippet": "GBT_test_prob = clf_GBT_cal.predict_proba(df_i[features][te_mask].values)[:, 1]\ntest_predictions['GBT_CAL']=GBT_test_prob\n", "intent": "We now make calibrated predictions on the test set that will be used as part of the final prediction using the trained stacked logistic regression.\n"}
{"snippet": "print(\"Prediction score: %.3f\" % accuracy_score(preds, y[75:]))\n", "intent": "To round off, let's see how the ensemble as a whole fared.\n"}
{"snippet": "guesed = [list(zip([tuple((featureSet['word'], featureSet['pos'])) for featureSet in sent], model.predict([sent])[0])) for sent in X_test]\ngold = test_sentences\nfrom nltk.chunk import conlltags2tree, tree2conlltags\nguesed = [conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in guesed]\ngold = [conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in gold]\n", "intent": "convert to conll trees\n"}
{"snippet": "print(svm.predict(X_test))\nprint(y_test)\n", "intent": "3) Apply / evaluate\n"}
{"snippet": "def S_predict(pic_array):\n    y = snetx.propagate(pic_array)\n    if 1 - y[0] < y[0]:\n        return \"the ball is on the right side of the photo\"\n    else:\n        return \"the ball is on the left side of the photo\"\ndef C_predict(pic_array):\n    x = cnetx.propagate(pic_array)\n    return [(x[0] * 2)-1, (x[1] *2)-1]\n", "intent": "Now that we have successfully trained our networks, time to test them with these prediction functions...\n"}
{"snippet": "array = vectorizer(grayscale(pic))/255\nC_newAngles = C_predict(array)\nS_verdict = S_predict(array)\n", "intent": "<img src=\"demoPhoto1.jpeg\" width=\"250\"/>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_squared_error\ny_predict = clf.predict(X_train)\nprint 'valid accuracy =', eval.mse(y_train, y_predict)\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "from sklearn.metrics import mutual_info_score\nsk_mi = mutual_info_score(df.a,df.b)\nnp.round(sk_mi,10) == np.round(my_mi,10) \n", "intent": "<h4> as a final test check to see how output compares to sk learn's MI function\n"}
{"snippet": "print(\"train accuracy for P: \",accuracy_score(model.predict(F1_train),Y_train))\nprint(\"test accuracy for P: \",accuracy_score(model.predict(F1_test),Y_test))\n", "intent": "__Evaluating the model__\n"}
{"snippet": "print(\"train accuracy for E: \",accuracy_score(model.predict(F2_train),Y_train))\nprint(\"test accuracy for E: \",accuracy_score(model.predict(F2_test),Y_test))\n", "intent": "__Evaluating the model__\n"}
{"snippet": "print(\"train accuracy for V: \",accuracy_score(model.predict(F3_train),Y_train))\nprint(\"test accuracy for V: \",accuracy_score(model.predict(F3_test),Y_test))\n", "intent": "__Evaluating the model__\n"}
{"snippet": "print(\"train accuracy for P, E, V : \",accuracy_score(model.predict(F123_train),Y_train))\nprint(\"test accuracy for P, E, V : \",accuracy_score(model.predict(F123_test),Y_test))\n", "intent": "__Evaluating the model__\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\npred=kmeans.labels_\nprint(confusion_matrix(df['Cluster'],pred))\nprint(classification_report(df['Cluster'],pred))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pre = lm.predict(x_test)\n", "intent": "Predicting test data\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pre))\nprint('MSE:', metrics.mean_squared_error(y_test, pre))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pre)))\n", "intent": "Evaluating the model by calculating the Mean Absolute Error, Mean Squared Error and the Root Mean Squared Error.\n"}
{"snippet": "prediction = lm.predict(X_test)\n", "intent": "Predicting test data\n"}
{"snippet": "dog_breed_predictions = [np.argmax(Xception_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_Xcep]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Xception Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test, y_pred))\nprint('The error rate of class 1 is', 1 - 0.867,\n      '\\nThe error rate of class 2 is', 1 - 1,\n      '\\nThe error rate of class 3 is', 1 - 1)\nprint(f'Validation set MSE = {mean_squared_error(y_test, y_pred)}')\n", "intent": "**(2) Report your error rates on the test set**\n"}
{"snippet": "model.predict([(2,2), (3, 4)])\n", "intent": "... and what does this model predict on the input (1,2)? On the input (3,4)?\n"}
{"snippet": "affair_mod.predict(respondent500)\n", "intent": "Here, we added a 0th dimension for the bias, with weight 1. \n"}
{"snippet": "def custom_loss(y_true, y_pred):\n    y_true = tf.reduce_mean(y_true,axis=1)\n    y_pred = tf.reduce_mean(y_pred,axis=1)\n    return K.mean(K.square(y_pred - y_true), axis=-1)\n", "intent": "We have used a custom loss function which will reduce the squared error of the **average** of the horizon or 10 predictions made by the model. \n"}
{"snippet": "row = X[0:1,0:8]\nprint(str(row)+\" is classified as \"+str(Y[0])+\" in training data\")\npredictions = model.predict(row)\nrounded = [round(x[0]) for x in predictions]\nprint(\"predicted as : \"+str(rounded))\n", "intent": "Now we of course start to use the model for predictions on unknown data, which is the whole point of training\n"}
{"snippet": "total = len(lm.predict(X))\nnp.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "preds = model.predict(x_test, verbose=1)\n", "intent": "Let's predict the images.\n"}
{"snippet": "ResNet50_predictions = [\n    np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "valid_pred = np.argmax(model.predict(valid_feats), 1)\nvalid_labels_dense = np.argmax(valid_labels.values, 1)\nconfusion_matrix(valid_labels_dense, valid_pred)\n", "intent": "Great! We have some data about the accuracy of the model but just for fun let's make a confusion matrix\n"}
{"snippet": "print(\"LendingClub's ROC score: {:.3f}\".format(roc_auc_score(y_test.paid_off, results['proba'])))\nprint(\"Logistic Regression ROC score: {:.3f}\".format(roc_auc_score(y_test.paid_off, lr.predict_proba(X_test)[:, 1])))\nprint(\"Random Forest ROC score: {:.3f}\".format(roc_auc_score(y_test.paid_off, rf.predict_proba(X_test)[:, 1])))\n", "intent": "As predicted, the ROC of Random Forest classifier is only slightly better than Logistic Regression\n"}
{"snippet": "lr = ml.linear.linearRegress(Xtr, Ytr); \nxs = np.linspace(0,10,200); \nxs = xs[:,np.newaxis] \nys = lr.predict(xs); \n", "intent": "Training data by linear regression model\n"}
{"snippet": "x_test = np.array(['it is awful'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "class_name = family_index()\ny_true = []\ny_preds = []\nwith open(\"test_actual.txt\",\"r\") as f:\n    for line in f:\n        y_true.append(int(line.strip()))\nwith open(\"test_predictions.txt\",\"r\") as f:\n    for line in f:\n        y_preds.append(int(line.strip()))      \nclassify_utils.plot_classification_report(classification_report(np.array(y_true), np.array(y_preds), target_names=class_name), title='Aircraft Classification Report')\n", "intent": "_matplotlib wrapper code obtained from https://stackoverflow.com/questions/28200786/how-to-plot-scikit-learn-classification-report_\n"}
{"snippet": "wacerp.predict(test_X, return_type=\"prob\")\n", "intent": "**Make predictions on testing set:**  \nThe return type can be either \"prob\" or \"label\"\n"}
{"snippet": "predictions = wacerp.predict(test_X)\naccuracy = np.mean(predictions == test_y)\nprint (\"accuracy on testing set is {}\".format(accuracy))\n", "intent": "**Accuracy on testing set:**\n"}
{"snippet": "print (warf1.predict(test_X))\n", "intent": "**Predictions on testing set:**\n"}
{"snippet": "RESTNET_predictions = [np.argmax(RESTNET_model.predict(np.expand_dims(feature, axis=0))) for feature in test_RESTNET]\ntest_accuracy = 100*np.sum(np.array(RESTNET_predictions)==np.argmax(test_targets, axis=1))/len(RESTNET_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(\n    np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions) == \n                          np.argmax(test_targets, axis=1)) / len(dog_breed_predictions)\nprint(\"Test accuracy: %.4f%%\" % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(\n    np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(\n    np.array(VGG16_predictions) == np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint(\"Test accuracy: %.4f%%\" % test_accuracy)\n", "intent": "Now we can use the CNN to test how well it identifies breed within our test dataset of dog images. We print the test accuracy below.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(\n    np.expand_dims(feature, axis=0))) for feature in test_inception]\ninception_test_accuracy = 100*np.sum(np.array(Inception_predictions)==\n                                    np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint(\"Test accuracy: %.4f%%\" % inception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "index = np.zeros((26)).astype(np.float32)\nindex[10] = 1 \ncoreml_style_index = index[:,np.newaxis,np.newaxis,np.newaxis,np.newaxis]\ncoreml_input = {'input__0': coreml_image_input, 'style_num__0': coreml_style_index}\ncoreml_out = mlmodel.predict(coreml_input, useCPUOnly = True)['Squeeze__0']\ncoreml_out = np.transpose(np.squeeze(coreml_out), (1,2,0))\nimshow(coreml_out/255.0)\n", "intent": "That looks cool! Lets try another style. \n"}
{"snippet": "print(\"test accuracy \", lsvm.score(test_feat, test_label))\ny_pred = lsvm.predict(test_feat)\nprint(classification_report(test_label, y_pred))\n", "intent": "Compute and display test set accuracy, precision, recall, f1-score\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    score = r2_score(y_true, y_predict) \n    return score\n", "intent": "We will improve our best guess from above using grid search to search for trees with the optimal tree depth\n"}
{"snippet": "model.evaluate(x = X_test, y = y_test)\n", "intent": "Let's see how well we did\n"}
{"snippet": "r2_score(y_test, clf.predict(X_test))\n", "intent": "Below is the calculated coefficient of determination for the\nabove linear regression\n"}
{"snippet": "(mean_squared_error(y_test, clf.predict(X_test)))**0.5\n", "intent": "Below is the calculated root mean squared error for the above linear regression\n"}
{"snippet": "response = predictor.predict(data)\nprint('Raw prediction result:')\nprint(response)\nlabeled_predictions = list(zip(range(10), response[0]))\nprint('Labeled predictions: ')\nprint(labeled_predictions)\nlabeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\nprint('Most likely answer: {}'.format(labeled_predictions[0]))\n", "intent": "Now we can use the ``predictor`` object to classify the handwritten digit:\n"}
{"snippet": "input_data = hist.validation_data[0] \npredictions = cnn_out_model.predict(input_data, verbose=1, batch_size=64)\nprint(\"Predictions shape:\", predictions.shape)\n", "intent": "Lets make predictions for the whole training and validation data to see what type of bigrams each kernel has learnt to recognize.\n"}
{"snippet": "Y_pred = model.predict(X)\n", "intent": "32% of the women in the dataset had affairs\n"}
{"snippet": "from nltk.metrics.agreement import AnnotationTask\nfrom sklearn.metrics import cohen_kappa_score\nprint('\\nsklearn kappa:', cohen_kappa_score(a_labels, b_labels))\nt = AnnotationTask(data=[x.split() for x in open(\"artstein_poesio_example.txt\")])\nprint('\\nnltk kappa:', t.kappa())\nprint('nltk alpha:', t.alpha()) \n", "intent": "Scikit-learn includes an implementaion of Cohen's kappa. NLTK also implements Kreppendorf's alpha.\n"}
{"snippet": "predictions = []\nfor t_feature in test_resnet50:\n    expanded_feature = np.expand_dims(t_feature, axis=0)\n    prediction = transfer_learn_model.predict(expanded_feature)\n    predictions.append(np.argmax(prediction))\nsuccessful_predictions = np.array(predictions)==np.argmax(test_targets, axis=1)\ntotal_predictions_size= len(predictions)\nprediction_test_accuracy = np.sum(successful_predictions)/total_predictions_size\nprint(\"Prediction Acc: {} %\".format(prediction_test_accuracy*100))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Confusion matrix for Decision Tree Classifier with criterion gini index')\nprint(confusion_matrix(y_test, Pred))\nprint('Test accuracy: '+str(accuracy_score(y_test, Pred)))\n", "intent": "Finally we can look at the actual results from the Logistic regression.\n"}
{"snippet": "print('Confusion matrix for Decision Tree')\nprint(confusion_matrix(y_test, Pred))\nprint('Test accuracy: '+str(accuracy_score(y_test, Pred)))\n", "intent": "Below are the results from the decision tree.\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(np.exp(y_test), np.exp(predictions)))\nprint('MSE:', metrics.mean_squared_error(np.exp(y_test), np.exp(predictions)))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(np.exp(y_test), np.exp(predictions))))\n", "intent": "Let's evaluate the performance of Linear model using MAE, MSE and RMSE.\n"}
{"snippet": "val_set_index = round(len(df)* validation_split_size) * -1\npreds = model.predict([img_arrs[val_set_index:], df[[\"direction_before_14\", \"direction_before_30\"]].values[val_set_index:]])\npreds = list(map(lambda x: x[0], preds))\nactuals_direction = df[prediction_column].values[val_set_index:]\nactuals_performance = df[\"pct_after_30\"].values[val_set_index:]\ninitial_trade_dates = df[\"Date\"].values[val_set_index:]\ndays_shift = int(prediction_column.split(\"_\")[-1])\nend_trade_dates = list(map(lambda x: pd.to_datetime(x).date()+datetime.timedelta(days=days_shift), initial_trade_dates))\n", "intent": "Now we can predict with our model and see the performance. \n"}
{"snippet": "output = clf_rgf.predict(X_test)\nprint(output[10:20])\nprint()\noutput_prob = clf_rgf.predict_proba(X_test)\nprint(output_prob[10:20])\n", "intent": "Choosing the best classifier and training with all training data:\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint ('MSE ='+ str(mse))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_local_test, y_pred)\nfrom sklearn.metrics import r2_score, classification_report, accuracy_score\nprint('This is R2: {}'.format(r2_score(y_local_test, y_pred)))\nprint('This is the accuracy score: {}'.format(accuracy_score(y_local_test, y_pred)))\nprint('Confusion Matrix')\nprint(cm)\n", "intent": "**Accuracy Summary for local test**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nfrom sklearn.metrics import r2_score, classification_report, accuracy_score\nprint('This is R2: {}'.format(r2_score(y_test, y_pred)))\nprint('This is the accuracy score: {}'.format(accuracy_score(y_test, y_pred)))\nprint('Confusion Matrix')\nprint(cm)\n", "intent": "**Accuracy Summary for local test**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_local_test, y_pred)\nfrom sklearn.metrics import r2_score, classification_report, accuracy_score\nprint('This is R2: {}'.format(r2_score(y_local_test, y_pred)))\nprint('This is the accuracy score: {}'.format(accuracy_score(y_local_test, y_pred)))\nprint('Confusion Matrix: {}'.format(cm))\n", "intent": "**Accuracy Summary for local test**\n"}
{"snippet": "roc_auc_score(data_test.target, pipe_1_cv.predict_proba(data_test.drop(\"target\", axis=1))[:, 1])\n", "intent": "Predict on the test set\n"}
{"snippet": "print(\"Recall: {:.2}%\".format(recall_score(y.map(d_sent), y_pred, average = \"macro\")*100))\nprint(\"F1 score: {:.2}%\".format(f1_score(y.map(d_sent), y_pred, average=\"macro\")*100))\n", "intent": "Always predicting \"neutral\" leads to a 22% macro-average accuracy, and is more penalized by the micro-average which considers evenly each sentiment.\n"}
{"snippet": "pred_train = lreg.predict(X_train) \npred_test = lreg.predict(X_test)\n", "intent": "Now run a prediction on both the X training set and the testing set.\n"}
{"snippet": "max_order = 10\nr2_train = []\nr2_test = []\nfor i in range(max_order):\n    p = np.poly1d(np.polyfit(x, y, i+1))\n    r2_train.append(r2_score(np.array(trainY), p(np.array(trainX))))\n    r2_test.append(r2_score(testy, p(testx)))\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))) \n    source = np.expand_dims(source, axis=0)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "def predict(finalData,inputData)\n    return sum([finalData[i].predict(inputData) for i in finalData.keys()])/float(len(finalData.keys))\n", "intent": "STEP 3:\n* For a new patient, identify the optimal care according to the protocol\n"}
{"snippet": "y_prob = model.predict_proba(X_test, verbose=0)\n", "intent": "we can use the model to predict probabilities on the validation data\n"}
{"snippet": "train_metrics = estimator.evaluate(input_fn=train_input_fn)\neval_metrics = estimator.evaluate(input_fn=eval_input_fn)\nprint(\"train metrics: %r\"% train_metrics)\nprint(\"eval metrics: %r\"% eval_metrics)\n", "intent": "Here we evaluate how well our model did.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def rep(x):\n    return [1] * len(x)\ny_pred_rep = rep(x_test)\ndef rep_acc():\n    return accuracy_score(y_test, y_pred_rep)\nprint \"Accuracy always predicting Republican:\", rep_acc()\n", "intent": "Do the same with predicting 'republican' all the time and measure its accuracy.\n"}
{"snippet": "def runtime(x):\n    return [1] * len(x)\ny_pred_runtime = runtime(x_test3)\ndef runtime_acc():\n    return accuracy_score(y_test3, y_pred_runtime)\nprint \"Baseline predictor accuracy:\", runtime_acc()\n", "intent": "Make a baseline stupid predictor that always predicts the label that is present the most in the data. Calculate its accuracy on a test set.\n"}
{"snippet": "final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))\n", "intent": "We only used a subset of the validation set during training, to save time. Now let's check performance on the whole validation set.\n"}
{"snippet": "labels = predict(hits)\n", "intent": "Utilizing the above functions, a prediction is generated and evaluated based on the truth labels.\n"}
{"snippet": "print(\"Posterior probablity for unsmoothen-ed model:\",naive.predict_proba(xtest)[44])\nprint(\"Posterior probablity for smoothen-ed model:\",naive_smooth.predict_proba(xtest)[44])\n", "intent": "Part e) iii)Estimated Prosterior probability for 45th observations for smoothened and unsmoothened model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The test data is not much different, but the model no longer overfits.\n"}
{"snippet": "cluster_assignments = estimator.predict(image_flattened)\n", "intent": "Next,predicting the cluster assignment for each of the pixels in the original image.\n"}
{"snippet": "print('Micro F1 Score (Bodies)-Training Data: ',f1_score(y_true=labels_train, y_pred = pred_train_nb, average='micro'))\nprint('Macro F1 Score (Bodies)-Training Data: ',f1_score(y_true=labels_train, y_pred = pred_train_nb, average='macro'))\nprint('Accuracy (Bodies)-Training Data: ',accuracy_score(y_true=labels_train, y_pred = pred_train_nb))\nprint('Micro F1 Score (Bodies)-Test Data: ',f1_score(y_true=labels_test, y_pred = pred_test_nb, average='micro'))\nprint('Macro F1 Score (Bodies)-Test Data: ',f1_score(y_true=labels_test, y_pred = pred_test_nb, average='macro'))\nprint('Accuracy (Bodies)-Test Data: ',accuracy_score(y_true=labels_test, y_pred = pred_test_nb))\n", "intent": "<h3>Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score (Bodies)-Training Data: ',f1_score(y_true=labels_train, y_pred = pred_train_knn, average='micro'))\nprint('Macro F1 Score (Bodies)-Training Data: ',f1_score(y_true=labels_train, y_pred = pred_train_knn, average='macro'))\nprint('Accuracy (Bodies)-Training Data: ',clf.score(X = vectors_train,y =labels_train))\nprint('Micro F1 Score (Bodies)-Test Data: ',f1_score(y_true=labels_test, y_pred = pred_test_knn, average='micro'))\nprint('Macro F1 Score (Bodies)-Test Data: ',f1_score(y_true=labels_test, y_pred = pred_test_knn, average='macro'))\nprint('Accuracy (Bodies)-Test Data: ',clf.score(X = vectors_test,y =labels_test))\n", "intent": "<h3>Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score (Bodies)-Training Data: ',f1_score(y_true=labels_train, y_pred = pred_train_roc, average='micro'))\nprint('Macro F1 Score (Bodies)-Training Data: ',f1_score(y_true=labels_train, y_pred = pred_train_roc, average='macro'))\nprint('Accuracy (Bodies)-Training Data: ',accuracy_score(y_true=labels_train, y_pred = pred_train_roc))\nprint('Micro F1 Score (Bodies)-Test Data: ',f1_score(y_true=labels_test, y_pred = pred_test_roc, average='micro'))\nprint('Macro F1 Score (Bodies)-Test Data: ',f1_score(y_true=labels_test, y_pred = pred_test_roc, average='macro'))\nprint('Accuracy (Bodies)-Test Data: ',accuracy_score(y_true=labels_train, y_pred = pred_train_roc))\n", "intent": "<h3>Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score (Subjects)-Training Data: ',f1_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_nb, average='micro'))\nprint('Macro F1 Score (Subjects)-Training Data: ',f1_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_nb, average='macro'))\nprint('Accuracy (Subjects)-Training Data: ',accuracy_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_nb))\nprint('Micro F1 Score (Subjects)-Test Data: ',f1_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_nb, average='micro'))\nprint('Macro F1 Score (Subjects)-Test Data: ',f1_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_nb, average='macro'))\nprint('Accuracy (Subjects)-Test Data: ',accuracy_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_nb))\n", "intent": "<h3>Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score (Subjects)-Training Data: ',f1_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_knn, average='micro'))\nprint('Macro F1 Score (Subjects)-Training Data: ',f1_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_knn, average='macro'))\nprint('Accuracy (Subjects)-Training Data: ',accuracy_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_knn))\nprint('Micro F1 Score (Subjects)-Test Data: ',f1_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_knn, average='micro'))\nprint('Macro F1 Score (Subjects)-Test Data: ',f1_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_knn, average='macro'))\nprint('Accuracy (Subjects)-Test Data: ',accuracy_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_knn))\n", "intent": "<h3>Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score (Subjects)-Training Data: ',f1_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_roc, average='micro'))\nprint('Macro F1 Score (Subjects)-Training Data: ',f1_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_roc, average='macro'))\nprint('Accuracy (Subjects)-Training Data: ',accuracy_score(y_true=labels_train_subjects, y_pred = pred_train_subjects_roc))\nprint('Micro F1 Score (Subjects)-Test Data: ',f1_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_roc, average='micro'))\nprint('Macro F1 Score (Subjects)-Test Data: ',f1_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_roc, average='macro'))\nprint('Accuracy (Subjects)-Test Data: ',accuracy_score(y_true=labels_test_subjects, y_pred = pred_test_subjects_roc))\n", "intent": "<h3>Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_predict, average='micro'))\nprint('Macro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_predict, average='macro'))\nprint('Accuracy : ',accuracy_score(y_true=test_labels, y_pred = legal_predict))\n", "intent": "<h3> Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_roc_predict, average='micro'))\nprint('Macro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_roc_predict, average='macro'))\nprint('Accuracy : ',accuracy_score(y_true=test_labels, y_pred = legal_roc_predict))\n", "intent": "<h3> Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_knn3_predict, average='micro'))\nprint('Macro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_knn3_predict, average='macro'))\nprint('Accuracy : ',accuracy_score(y_true=test_labels, y_pred = legal_knn3_predict))\n", "intent": "<h3> Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_knn5_predict, average='micro'))\nprint('Macro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_knn5_predict, average='macro'))\nprint('Accuracy : ',accuracy_score(y_true=test_labels, y_pred = legal_knn5_predict))\n", "intent": "<h3> Model Evaluation</h3>\n"}
{"snippet": "print('Micro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_knn7_predict, average='micro'))\nprint('Macro F1 Score : ',f1_score(y_true=test_labels, y_pred = legal_knn7_predict, average='macro'))\nprint('Accuracy : ',accuracy_score(y_true=test_labels, y_pred = legal_knn7_predict))\n", "intent": "<h3> Model Evaluation</h3>\n"}
{"snippet": "svc_linear_tuned =\nprint(confusion_matrix(y_test, svc_linear_tuned.predict(X_test)))\nprint(svc_linear_tuned.score(X_test, y_test))\n", "intent": "Now try using the ${\\tt GridSearchCV()}$ function to select an optimal value for ${\\tt c}$. Consider values in the range 0.01 to 10:\n"}
{"snippet": "svc_quadratic_tuned = \nprint(confusion_matrix(y_test, svc_quadratic_tuned.predict(X_test)))\nprint(svc_quadratic_tuned.score(X_test, y_test))\n", "intent": "And now try ${\\tt kernel=\"poly\"}$ with ${\\tt degree=2}$:\n"}
{"snippet": "pred_p = model.predict_proba(X_test)\n", "intent": "We can also get the predicted _probabilities_ using the ${\\tt predict\\_proba()}$ function:\n"}
{"snippet": "base_scores = cross_val_score(rf_clf, X=X_prepared, y=Y, cv=5, scoring='roc_auc')\nprint(\"Base model's score: \", base_scores.mean())\n", "intent": "Choose the best strategy from the following strategies:\n    1. Under sampling\n    2. Over sampling\n    3. Easy Ensemble\n    4. SMOTE algorithm\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "$F1$ (`f1-score`) is the harmonic average of precision and recall.\n"}
{"snippet": "pred_y = all_data_model.predict(train_xs)\nvalid_pred_y = all_data_model.predict(valid_xs)\n[(valid_pred_y[i], valid_y[i]) for i in range(len(valid_y)) if not(np.round(valid_pred_y[i]) == valid_y[i])]\n", "intent": "Saving the output as this is needed for distilling the model.\n"}
{"snippet": "yts_pred = regr.predict(Xts)\nRSS_test = np.mean((yts_pred-yts)**2)/(np.std(yts)**2)\nprint(\"RSS TEST: \", RSS_test)\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, l in enumerate(style_layers):\n        A = style_targets[i]\n        G = gram_matrix(feats[l], normalize=True)\n        loss += 2*style_weights[i]*tf.nn.l2_loss(A - G)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.4f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "import math\ndef model_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.5f MSE (%.2f RMSE)' % (float(testScore[0]), float(math.sqrt(testScore[0]))))\nmodel_score(model, new_X_train, y_train, new_X_test, y_test)\n", "intent": "Now that we have trained our model, let's evaluate it.\n"}
{"snippet": "predDev = bkgEncoder.predict(inputPipe.transform(devData[autoTrainFeatures].values.astype('float32')))\ntargetDev = outputPipe.transform(devData[autoTargetFeatures].values.astype('float32'))\npredVal = bkgEncoder.predict(inputPipe.transform(valData[autoTrainFeatures].values.astype('float32')))\ntargetVal = outputPipe.transform(valData[autoTargetFeatures].values.astype('float32'))\n", "intent": "First run the model over the validation data\n"}
{"snippet": "pred = classifier.predict(testData[classTrainFeatures].values.astype('float32'))\ntestData['pred_class'] = pandas.Series(pred[:,0], index=testData.index) \n", "intent": "Having done all the development of the classifier, we're now ready to do final testing on the withheld testing data.\n"}
{"snippet": "def get_accuracy(models, test_data):\n    for key in models.keys():\n        model = models[key][\"model\"]()\n        filepath = pathlib.Path(\".\").absolute().joinpath('saved_models', f'weights.best.{key}.hdf5')\n        model.load_weights(str(filepath))\n        dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_data]\n        models[key][\"test_accuracy\"] = \\\n            100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n    return models\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "Calculate some basic metrics. \n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "Overall, how often is the classifier correct\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "Overall, how often is the classifier incorrect?\n"}
{"snippet": " print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred_class))\n", "intent": "When the actual value is positive, how often is the prediction correct?\n"}
{"snippet": "print(TP / float(TP + FP))\nprint(metrics.precision_score(y_test, y_pred_class))\n", "intent": "When a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_class))\n", "intent": "The percentage of the ROC plot that is underneath the curve:\n"}
{"snippet": "y_cas = regr.predict(X_train)\ny_reg = regr2.predict(X_train)\ny_cas = np.exp(y_cas)-1\ny_reg = np.exp(y_reg)-1\ny_cas[y_cas<0]=0\ny_reg[y_reg<0]=0\ny_count = y_cas + y_reg\nprint(\"RMSLE: \", rmsle(y_train_count,y_count ))\n", "intent": "**count = casual + registered **\n"}
{"snippet": "dtrain = xgb.DMatrix(X_train,label=y_train_count)\ny_cas = bst1.predict(dtrain,ntree_limit=bst1.best_iteration)\ny_reg = bst2.predict(dtrain,ntree_limit=bst2.best_iteration)\ny_cas[y_cas<0]=0\ny_reg[y_reg<0]=0\ny_count = y_cas + y_reg\nprint(\"RMSLE: \", rmsle(y_train_count,y_count ))\n", "intent": "** count = casual + registered **\n"}
{"snippet": "ypred1 = regr.predict(Xtest)\nypred1 = np.exp(ypred1)-1\nypred1[ypred1<0]= 0\nypred2 = regr2.predict(Xtest)\nypred2 = np.exp(ypred2)-1\nypred2[ypred2<0]= 0\nypred = ypred1 + ypred2\n", "intent": "**prediction - random forest **\n"}
{"snippet": "ypred_cas = bst1.predict(dtest,ntree_limit=bst1.best_iteration)\nypred_cas[ypred_cas<0]=0\nypred_reg = bst2.predict(dtest,ntree_limit=bst2.best_iteration)\nypred_reg[ypred_reg<0]=0\nypred_xgb = ypred_cas + ypred_reg\n", "intent": "**prediction - xgboost**\n"}
{"snippet": "precision_weighted = precision_score(y_test, y_test_pred, average=\"weighted\")\nrecall_weighted = recall_score(y_test, y_test_pred, average=\"weighted\")\nprecision_macro = precision_score(y_test, y_test_pred, average=\"macro\")\nrecall_macro = recall_score(y_test, y_test_pred, average=\"macro\")\nprint(\"Precision (weighted avg): \", precision_weighted)\nprint(\"Recall (weighted avg): \", recall_weighted)\nprint(\"Precision (simple avg): \", precision_macro)\nprint(\"Recall (simple avg): \", recall_macro)\n", "intent": "**TODO:** explanation\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out the model on the test dataset of dog images. I should reach a test accuracy is greater than 1% (hopefully!)\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, I can use the CNN to test how well it identifies breed within our test dataset of dog images.  I print the test accuracy below.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out the model on the test dataset of dog images. It should reach a test accuracy of at least 60%.\n"}
{"snippet": "def calc_scores(classes):\n    p = []\n    for c in classes:\n        p.append(pred_score(c))\n    return p\n", "intent": "A couple of utility functions:\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday I May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_pred_proba_dt = dt_model.predict_proba(X_test)\ny_pred_proba_log_reg = log_reg_model.predict_proba(X_test)\ny_pred_proba_nn = nn_model.predict_proba(X_test)\nroc_index_dt = roc_auc_score(y_test, y_pred_proba_dt[:, 1])\nroc_index_log_reg = roc_auc_score(y_test, y_pred_proba_log_reg[:, 1])\nroc_index_nn = roc_auc_score(y_test, y_pred_proba_nn[:, 1])\nprint(\"ROC index on test for DT:\", roc_index_dt)\nprint(\"ROC index on test for logistic regression:\", roc_index_log_reg)\nprint(\"ROC index on test for NN:\", roc_index_nn)\n", "intent": "With this concept in mind, ROC AUC score aims to find the best model under varied threshold. To compute our ROC AUC score, use the code below.\n"}
{"snippet": "arr = clf.predict(no_gender[features])\narr\n", "intent": "Finally we use this new classifier to attempt to assign a gender to participants who did not provide a gender response. \n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscore = cross_val_score(knn, X, Y, cv=5)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\nscore_w = cross_val_score(knn_w, X, Y, cv=5)\nprint(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))\n", "intent": "Here, I checked for accuracy. I think these are slightly better that the ones before. \n"}
{"snippet": "logisticRegr.predict(np.reshape(x_test[3], (1, -1)))\n", "intent": "**4.** Predict the labels of new data (new images) using the information the model learned during the model training process\n"}
{"snippet": "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\nprint(\"scores:\",scores,\"Mean:\",scores.mean())\n", "intent": "Calculate the score.\n"}
{"snippet": "cross_validation.cross_val_score(f, wine.values, grape.values, cv=5,\n                                 scoring='precision')\n", "intent": "Furthermore, we can customize the scoring method by specifying the `scoring` parameter:\n"}
{"snippet": "evaluator1 = RegressionEvaluator(\n    labelCol=\"relevance\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator1.evaluate(predictions1)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n", "intent": "In order to evaluate the performance of baseline model, I choose Root Mean Squared Error(RMSE), which was used in Kaggle for performance test. \n"}
{"snippet": "print(precision_score(actual, predicted))\n", "intent": "Here we see the model accuracy.\n"}
{"snippet": "print \"Class predictions according to GraphLab Create:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "print \"Class predictions according to GraphLab Create:\"\npredictions = [x[1] for x in sentiment_model.predict_proba(sample_test_matrix)]\nprint predictions\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "accuracy = get_classification_accuracy(sentiment_model, test_matrix, test_data['sentiment'])\nprint('Accuracy by sklearn %s' % accuracy_score(test_data['sentiment'], sentiment_model.predict(test_matrix)))\n", "intent": "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**.\n"}
{"snippet": "print get_classification_accuracy(simple_model, test_matrix_word_subset, test_data['sentiment'])\nprint('Accuracy by sklearn %s' % accuracy_score(test_data['sentiment'], simple_model.predict(test_matrix_word_subset)))\n", "intent": "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier.\n"}
{"snippet": "node_idx_in_graph = G_new.nodes()\nsample_mask = np.in1d(np.arange(N), node_idx_in_graph)\nax = plot_embed(X_thre_fw[sample_mask], labels[sample_mask])\nerror = NN_generalization_error(X_thre_fw[sample_mask], labels[sample_mask])\nax.set_title('threshold=%.1f%%, error=%.3f' %(percentile, error))\nax = plot_embed(X_threknn_fw, labels)\nerror = NN_generalization_error(X_threknn_fw[sample_mask], labels[sample_mask])\nax.set_title('threshold=%.1f%%, k=%d, error=%.3f' %(percentile, k, error))\n", "intent": "To make a fair comparison between thresholding-Firework and t-SNE, re-compute the error by ignoring nodes in the small connected components.\n"}
{"snippet": "from numpy import argmax\npredictions_scores = text_bow_model.predict([test_x])\nprint(predictions_scores.shape)\npredictions_best = argmax(predictions_scores, axis=-1)\nprint(predictions_best.shape)\npredictions_answers = [index2word_y[x] for x in predictions_best]\nprint(len(predictions_answers))\n", "intent": "Given encoded test questions, we use the maximum likelihood principle to withdraw answers.\n"}
{"snippet": "y_test_prediction = model.predict(X_test)\n", "intent": "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(model, X, y, cv=5) \n", "intent": "Instead of doing this process mannualy we can use predifiened method of scikit-learn `cross_val_score` to perform this task:\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6,-14] + [14,18] * rng.rand(2000 , 2)\nynew = model.predict(Xnew)\n", "intent": "Lets generate some new data points and predict them \n"}
{"snippet": "yproba = model.predict_proba(Xnew)\nyproba[-8:].round(2)\n", "intent": "A nice peice of bayesian learning is formation of formalism which can be done with the help of `predict_proba` feature:\n"}
{"snippet": "def predict_category(s,train=train,model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n", "intent": "Lets make a quick utiliy function to predict any string that is been entered to this model \n"}
{"snippet": "n_folds=3\ndef rmsle_cv(model,x,y):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "Here i am calling all the nessery modules that I will use for creating the model.\n"}
{"snippet": "with open(os.path.join(DATA_FOLDER, 'xgboost_logit.pickle'), 'rb') as handle:\n    xgboost_logit_pickle = pickle.load(handle)\ndct_model_comparison.append({'model' : 'XGBoost (Logit)',\n                             'mse': mean_squared_error(Y_Test, xgboost_logit_pickle['xgboost_preds']),\n                             'r2_score': r2_score(Y_Test, xgboost_logit_pickle['xgboost_preds'])})\nr2_score(Y_Test, xgboost_logit_pickle['xgboost_preds'])\n", "intent": "The fit has improved, however, the predicted values are not in [0,1] range. Using a logit transformation,\n"}
{"snippet": "train_score = math.sqrt(mean_squared_error(train_y[0], train_predict[:, 0]))\nprint('Train Score: %.2f RMSE' % train_score)\ntest_score = math.sqrt(mean_squared_error(test_y[0], test_predict[:, 0]))\nprint('Test Score: %.2f RMSE' % test_score)\n", "intent": "Next we will calculate the erroe score that is RMSE value for the model.\n"}
{"snippet": "train_score = math.sqrt(mean_squared_error(train_y[0], train_predict[:, 0]))\nprint('Train Score: %.2f RMSE' % train_score)\ntest_score = math.sqrt(mean_squared_error(test_y[0], test_predict[:, 0]))\nprint('Test Score: %.2f RMSE' % test_score)\n", "intent": "We will calculate the error score for the improved model.\n"}
{"snippet": "train_score_prophet = math.sqrt(mean_squared_error(train_prophet['y'], train_predict_prophet['yhat']))\nprint('Train Score: %.2f RMSE' % train_score_prophet)\ntest_score_prophet = math.sqrt(mean_squared_error(test_prophet['y'], test_predict_prophet['yhat']))\nprint('Test Score: %.2f RMSE' % test_score_prophet)\n", "intent": "Calculate the RMSE for training and testing data.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    k = 0\n    for i in style_layers:        \n        s = torch.sum(torch.sum(torch.sum((style_targets[k] - gram_matrix(feats[i].clone())) ** 2)))\n        style_loss += style_weights[k] * s\n        k += 1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "logit_1.predict([[3], [4]])  \n", "intent": "- Prediction can be made with the `predict` method:\n"}
{"snippet": "logit_1.predict_proba([[3], [4]])\n", "intent": "- We can access the probability in each class predicted by the logistic regression as well:\n"}
{"snippet": "random_divide = ms.KFold(n_splits=5)\nscores = ms.cross_val_score(estimator=logit, X=iris.data, y=iris.target, cv=random_divide)\nscores\n", "intent": "Here is a case that we randomly divide the data set by **KFold**:\n"}
{"snippet": "stratify_divide = ms.StratifiedKFold(n_splits=5, random_state=0)\nscores = ms.cross_val_score(estimator=logit, X=iris.data, y=iris.target, cv=stratify_divide)\nscores\n", "intent": "We can also use **StratifiedKFold** to split the dataset more evenly among different classes\n"}
{"snippet": "scores = ms.cross_val_score(model, X, y ,cv=5)\n1 - scores\n", "intent": "- Use the function **cross_val_score** to implement 5-fold cross validation with a **Linear Discriminant Analysis** model. Report the test errors.\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['Spammer', 'Not spammer']\nprint classification_report(y_te, y_pre, target_names=target_names)\n", "intent": "Get the precision recall and f1-score to analyze the performance of the model.\n"}
{"snippet": "y_pred_nolinear = results_qra4.predict(df_qra_test)\nprint('Nolinear and interaction model MSE:')\nprint(metrics.mean_squared_error(y_test, y_pred_nolinear))\n", "intent": "**Now all p-values are below 0.05, so we can use this model as final model and train it**\n"}
{"snippet": "accuracy_score(y_test,dt1)\n", "intent": "1. We can easily see the score of model on test and train data and score of train dataset imply that model is <b>overfitting</b>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out model on the test dataset of dog images.\n"}
{"snippet": "cv_new_tweets = cv.transform(new_tweets)\ny_sentiment = bnb.predict_proba(cv_new_tweets)\n", "intent": "Now let us use the newly fetched tweets in sentiment analysis.\n"}
{"snippet": "df_train_rfe = selector.transform(df_train)\ndf_test_rfe = selector.transform(df_test)\nevaluate(clf, df_train_rfe, y_train, df_test_rfe, y_test)\n", "intent": "We know which 10 features are the most informative. Let us carry the analysis using these features.\n"}
{"snippet": "predicted_labels = model.predict(X_test,verbose=0)\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "<span style=\"color:red;font-weight:bold\">predict function for model</span> predicts labels or values for the testing data provided \n"}
{"snippet": "stest = dp.load_file('stest2.csv', dir='files/', has_number_line=True)\nX_predict = dp.prepare_predictset(stest, n_times=9, feature='dwt')\nforest.predict(X_predict)\n", "intent": "Load prediction dataset and predict answers:\n"}
{"snippet": "y_test = mlp.predict(test[\"X\"])\n", "intent": "This model is our best performing, so we make test predictions and a submission DataFrame.\n"}
{"snippet": "print \"Class predictions according to GraphLab Create:\" \nprint sentiment_model.predict(sample_test_data, output_type='probability')\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "y_train_pred = model.predict(x_train, batch_size=200).reshape(-1)\n", "intent": "Model predictions can be obtained via the `.predict` method, again as in sklearn:\n"}
{"snippet": "model.predict(x_train, batch_size=200).shape\n", "intent": "Here we used the numpy `.reshape()` method to obtain a column vector:\n"}
{"snippet": "y_pred = model.predict(x_test, batch_size=200).reshape(-1)\n", "intent": "We similarly make predictions on the validation set:\n"}
{"snippet": "y_val_pred = model.predict(x_val)\nx_val.head()\n", "intent": "** Predict  -  Validation Data**\n"}
{"snippet": "predictions = logreg.predict(X_test)\n", "intent": "Predicting values for the testing data from our model.\n"}
{"snippet": "decisiontree.predict(X_demo_test)\n", "intent": "Once the tree is trained we can use it to generate predictions over data. Reusing the same toy data:\n"}
{"snippet": "print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"\\nThe real prices are\")\nprint(y.head())\nprint(\"\\nThe predictions are\")\nmelbourne_model.predict(X.head())\n", "intent": "Let's see if our model predicts the first 5 house prices:\n"}
{"snippet": "ys_tr_pred = np.squeeze(model.predict(xstrain))\nRSStrain = np.mean(((ys_tr_pred-ystrain)**2)/(np.std(ystrain)**2))\nprint(\"Normalized RSS training = {0:f}\".format(RSStrain))\nys_ts_pred = np.squeeze(model.predict(xstest))\nRSStest = np.mean(((ys_ts_pred-ystest)**2)/(np.std(ystest)**2))\nprint(\"Normalized RSS test = {0:f}\".format(RSStest))\n", "intent": "We try to predict the target value on both training data and test data, and evaluate the accurate by Normalized RSS\n"}
{"snippet": "roc_auc_score(y_test,clf.predict_proba(x_test)[:,1])\n", "intent": "let's find out the accuracy of the model\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"carinspace.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def test_sample(model, sample):\n    sample_counts = count_vect.transform([sample])\n    sample_tfidf = tfidf_transformer.transform(sample_counts)\n    result = model.predict(sample_tfidf)[0]\n    prob = model.predict_proba(sample_tfidf)[0]\n    print(\"Sample estimated as %s: negative prob %f, positive prob %f\" % (result.upper(), prob[0], prob[1]))\ntest_sample(logreg, \"The food was delicious, it smelled great and the taste was awesome\")\ntest_sample(logreg, \"The whole experience was horrible. The smell was so bad that it literally made me sick.\")\ntest_sample(logreg, \"The food was ok, I guess. The smell wasn't very good, but the taste was ok.\")\n", "intent": "From the above result, we can see that around 921995 words are deciding factor for positive and Negative reviews.\n"}
{"snippet": "inception_predictions = [np.argmax(dog_inception.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = model.predict(X_test)\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred) \n", "intent": "**R-squared of a model**\n"}
{"snippet": "model.predict(4)\n", "intent": "<b>Prediction using the fitted model</b>\n"}
{"snippet": "y_pred = clf.best_estimator_.predict_proba(X_test)\nlog_loss(y_test, y_pred)\n", "intent": "Finally, we use the best estimator to predict the review scores of the test set.\n"}
{"snippet": "print(\"Neigh23's Accuracy: \"), metrics.accuracy_score(y_testset, pred23)\nprint(\"Neigh90's Accuracy: \"), metrics.accuracy_score(y_testset, pred90)\n", "intent": "Interesting! Let's do the same for the other instances of KNeighborsClassifier.\n"}
{"snippet": "predForest=skullsForest.predict(X_testset)\n", "intent": "Let's now create a variable called <b>predForest</b> using a predict on <b>X_testset</b> with <b>skullsForest</b>.\n"}
{"snippet": "print((np.mean((LinReg.predict(X_testset) - y_testset)**2))**0.5)\n", "intent": "And lastly, <b>RMSE</b>:<br>\n<img src = \"https://ibm.box.com/shared/static/hc55qagftbvl8yb306vcx78a738n2ylc.gif\", height = 100, align = 'left'>\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\n", "intent": "Finally, we need to check how the model performs in the test set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(data[1],kmeans.labels_))\nprint(classification_report(data[1],kmeans.labels_))\n", "intent": "** Since we know labels in advance, we can compare our result with original labels via confusion matrix **\n"}
{"snippet": "y_pred = regr.predict(X_test)\n", "intent": "At this point, we have trained our model. We can now apply the model to data the model was not trained on: our test set.\n"}
{"snippet": "predicted_y_rfc = rfc.predict(X_test)\nprint(result)\n", "intent": "And we can now apply the trained Random Forest model to the test data:\n"}
{"snippet": "sm.accuracy_score(y_test, predicted_y_rfc)\n", "intent": "We obtained the predicted values and can now compare them with the actual values in y_test:\n"}
{"snippet": "sm.accuracy_score(y_test, predicted_y_log_reg)\n", "intent": "And we can now compare predicted labels with the ones in our test dataset.\n"}
{"snippet": "sm.accuracy_score(y_test, predicted_y_svm)\n", "intent": "And we can now compare predicted labels with the ones in our test dataset.\n"}
{"snippet": "mserr= np.mean((Y_test - lm.predict(X_test)) ** 2)\nprint (mserr)\n", "intent": "* using just the test data\n* using just the training data\nAre they pretty similar or very different? What does that mean?\n"}
{"snippet": "def evaluate_clf(classifier=clf, metric='accuracy'):\n    from sklearn.metrics import accuracy_score\n    y_hat = clf.predict(x_test)\n    if metric == 'accuracy':\n        score = accuracy_score(y_test, y_hat)\n        print('Your model has an accuracy of', '{:.2f}%.'.format(100 * score))\n", "intent": "<h1>6. Evaluate classifier</h1>\n"}
{"snippet": "XD = np.matrix(df.drop(\"party\", axis=1))\nyD = ['democrat'] * len(XD)\ny = df.party\nlog_reg_accuracy_d = accuracy_score(y, yD)\nprint log_reg_accuracy_d\n", "intent": "Make a very simple predictor that predicts 'democrat' or \"republican\" for every incoming example.\n"}
{"snippet": "XPG = np.matrix(df.drop([\"Rating\", \"ReleaseDate\",\"Director\",\"Title\"], axis=1))\nyPG = ['PG'] * len(XPG)\ny = df.Rating\nlog_reg_accuracy_pg = accuracy_score(y, yPG)\nprint log_reg_accuracy_pg\n", "intent": "Baseline predictor that guess \"PG\" everytime ~ 15% accuracy\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(target,pred_label))\n", "intent": "Classification report on training data \n"}
{"snippet": "print(classification_report(target,pred_label))\n", "intent": "Classification report on test data . \n"}
{"snippet": "my_model.evaluate(X_train,Y_train,batch_size=32)\n", "intent": "When we evaluate the model on the training data , we get a 100 % accuracy . \n"}
{"snippet": "from sklearn.metrics import classification_report\npred = my_model.predict(X_test)\npred_label = np.argmax(pred,axis=1)\nprint(classification_report(test_y,pred_label))\n", "intent": "Classification Report for the test data .\n"}
{"snippet": "nb_pipeline.predict(X_test)\n", "intent": "Predicting new data is also one line of code:\n"}
{"snippet": "pred = nb_pipeline.predict_proba(X_test)\npred = [p[1] for p in pred]\npred[:10]\n", "intent": "And remember, for classification, <code style=\"color:steelblue\">predict_proba</code> is often more useful:\n"}
{"snippet": "from sklearn.exceptions import NotFittedError\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))\n", "intent": "Let's also make sure that the models have been fitted correctly.\n"}
{"snippet": "from sklearn.exceptions import NotFittedError\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))\nprint(fitted_models.items())     \n", "intent": "Let's also make sure that the models have been fitted correctly.\n"}
{"snippet": "pred = model.predict_proba(augmented_data)\npred[:5]\n", "intent": "**Predict probabilities for <code style=\"color:steelblue\">augmented_data</code> using your model.**\n* Then print the first 5 predictions.\n"}
{"snippet": "from sklearn.exceptions import NotFittedError\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))\nprint(fitted_models.items())        \n", "intent": "<br>\n**Finally, run this code to check that the models have been fitted correctly.**\n"}
{"snippet": "pred_rf = fitted_models['rf'].predict(X_test)\n", "intent": "Predict the test set using the fitted random forest.\n"}
{"snippet": "print(\"R2 is\", r2_score(y_test, pred_rf))\nprint(\"MAE is\", mean_absolute_error(y_test, pred_rf))\nprint(\"RMSE is\", np.sqrt(mean_squared_error(y_test, pred_rf)))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "def predict(parameters, X):\n    X = X.T\n    y_hat, cache = forward_propagation(X, parameters)\n    predictions = (y_hat > 0.5)    \n    return predictions.T\n", "intent": "Lastly lets make a prediction (forward propigate on X) and see recreate the decision boundary function we used in the logistic regression.\n"}
{"snippet": "p = model.predict(x_test)\np[0]\n", "intent": "predict for test data\n"}
{"snippet": "cross_val_score(clf, data, label, cv=2, scoring='accuracy')\n", "intent": "Cross-validate results with full dataset. Because we have only 10 samples, we wil perform 2-fold CV\n"}
{"snippet": "cross_val_score(clf, data_m, label, cv=2, scoring='accuracy')\n", "intent": "Somoe score was obtained, we won't dig into its interpretation\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Grid search will use cross_val_score based on maximizing a utility performance measure. So we maximize(-MSE) here.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n", "intent": "Here we see how generalize on a proxy for the real world via a heldout test set.\n"}
{"snippet": "score = metrics.mean_squared_error(fit.result, Y_test)\nscore\n", "intent": "and the MSE score of the model is:\n"}
{"snippet": "trainAccuracy = 100 * accuracy_score(y_train, model.predict(X_train))\ntestAccuracy = 100 * accuracy_score(y_test, model.predict(X_test))\nprint ('Train & Test Accuracy of Classified Data is: ', round(trainAccuracy, 2), ' & ', round(testAccuracy, 2), '\\n')\nprint(classification_report(y_test,model.predict(X_test)))\nmodelResults = [{\n    'model': 'Logistic Regression',\n    'trainAccuracy': trainAccuracy,\n    'testAccuracy': testAccuracy\n}] \n", "intent": "we will now measure the accuracy of the test and train data\n"}
{"snippet": "try:\n    X = ucTweets.drop(['isLiberal', 'isConservative', 'isDemocratic', 'isGreen', 'politicalParty'], axis=1)\nexcept ValueError:\n    print('error')\nX = modelFeatures(X) \nucTweets['class'] = model.predict(X)\nucTweets.head()\n", "intent": "we will now predict the unclassified data\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(rs, s)\nprint(MSE)\n", "intent": "Calculate the Mean Squared Error (MSE) for the original signal (s) and the recovered signal (rs).\n"}
{"snippet": "MSE = mean_squared_error(s, rns)\nprint(MSE)\n", "intent": "Calculate the MSE for the original signal (s) and the recovered noisy signal (rns).\n"}
{"snippet": "accuracy_score(result['actual'], result['predicted'])\n", "intent": "Calculating the accuracy score of our model\n"}
{"snippet": "predict(model=resnet_model, image_path='train/dogs/dog.613.jpg')\n", "intent": "We can see that original ImageNet dataset contains a few cats categories rather then one category \"cat\".\n"}
{"snippet": "predict(model=resnet_model, url='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQbQR4USIM1PLxSJfSXapS9WMaYMIlv1N-8fZYA90ADWGIhS6Oh', name='husky_dog.jpg')\n", "intent": "Same issue as above.\n"}
{"snippet": "predict(model=resnet_model, url='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQvZlZZ2sHrxhewRJHftyVN_J9m_Rx4_OiADBn3Hg6DCpqydayUcA', name='hairless_cat.jpg')\n", "intent": "Same issue as above.\n"}
{"snippet": "predict(model=resnet_model, url='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTgG8-eb4guR9UsYNn6ANeuFh2AiDEBHffkjpZO-qmcTOK9FYPNEA', name='zebra_dog.jpg')\n", "intent": "Same issue as above.\n"}
{"snippet": "predict(model=model, image_path='train/cats/cat.207.jpg', should_decode=False)\n", "intent": "This is a state of the art accuracy on less than a few minutes training\n"}
{"snippet": "np.mean((fhat2.predict(PhiX)-Y)**2)\n", "intent": "On calcule\n$$\n    \\frac1 N\\sum({\\hat{f}(X_i) = Y_i})^2\n$$\n"}
{"snippet": "pred1 = svmodel.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "cv_scores = cross_val_score(model, X, y, cv=5)\nprint(\"mean: \", cv_scores.mean(), \", scores: \", cv_scores)\n", "intent": "Fit with multiple cross validation sets\n"}
{"snippet": "yhat=poly.predict(x_test_pr )\n", "intent": " We can see the output of our model using the \"predict.\" method and assign the values to \"yhat\".\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\n", "intent": "Feeding all test examples to the model yields all predictions\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        deviation = tf.reduce_sum(tf.square(style_targets[i] - gram))\n        style_loss += style_weights[i] * deviation\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def get_outcomes(gridCV):\n    print \"Best parameters from the grid search: \", gridCV.best_params_\n    clf_gridCV = gridCV.best_estimator_\n    print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n    clf_gridCV_pred = clf_gridCV.predict(features_test)\n    print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n    print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)\n", "intent": "Here I have a output function get_outcomes(). It prints out all important informations from grid_search.\n"}
{"snippet": "mean_squared_error(df1['ViolentCrimesPerPop'], predict)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "ridge_predict = model4.predict(df2)\nmean_squared_error(df1['ViolentCrimesPerPop'], ridge_predict)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "mse = cross_val_score(model5, x_reg_features,  y_reg_labels, cv=10, scoring='neg_mean_squared_error')\nprint(\" Mean MSE: %f\" % cross_val_score(model5 , df2, df1['highCrime'] , cv=10, scoring='neg_mean_squared_error' ).mean())\n", "intent": "i.\tWhat is the estimated MSE of the model under 10-fold CV?\n"}
{"snippet": "mean_squared_error(df1['ViolentCrimesPerPop'], y_predict)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is better since the accuracy difference between train and test are smaller and better model.\n"}
{"snippet": "training_error = model.evaluate(X_train, y_train, verbose=0)\nprint('training error = ' + str(training_error))\ntesting_error = model.evaluate(X_test, y_test, verbose=0)\nprint('testing error = ' + str(testing_error))\n", "intent": "Print out training and testing errors\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "Let's make predictions on both our training and testing sets.\n"}
{"snippet": "preds = classifier.predict(X_test)\nprint(\"prediction:\",list(preds[:10]))\nprint(\"Actual:    \",y_test[:10])\n", "intent": "Now we will predict our data display a small sample of data to look how close our prediction is to the actual rating. \n"}
{"snippet": "user_agent = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1741.89 Safari/537.36\"\npred_family = predict(user_agent)\nprint(\"the predicted family is {} [ground truth: Chrome]\".format(pred_family))\nuser_agent = \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30; .NET CLR 3.0.04506.648; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729\"\npred_family = predict(user_agent)\nprint(\"the predicted family is {} [ground truth: IE]\".format(pred_family))\nuser_agent = \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0; KTXN B631976907A78190T950184P1) like Gecko\"\npred_family = predict(user_agent)\nprint(\"the predicted family is {} [ground truth: IE]\".format(pred_family))\n", "intent": "Now we can use our model to predict the family of sample user agent string showed in `Introduction` part\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i,style_weight in enumerate(style_weights):\n        loss += content_loss(style_weight, gram_matrix(feats[style_layers[i]]), style_targets[i])\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "yhat = np.round(model.predict(X_ints_test + [X_test_num]))\nprint(mt.confusion_matrix(y_test,yhat),mt.recall_score(y_test,yhat))\n", "intent": "Let us see what we have achieved.\n"}
{"snippet": "happiness = swn.senti_synset('happy.a.03')\nprint(happiness.neg_score())\nprint(happiness.pos_score())\nprint(happiness.obj_score())\n", "intent": "For this part sentiwordnet dictionary will be used. And example of its usage below:\n"}
{"snippet": "pos, neg, obj = 0., 0., 0.\nfor h in happy:\n    pos += h.pos_score()\n    neg += h.neg_score()\n    obj += h.obj_score()\npos = pos / len(happy)\nneg = neg / len(happy)\nobj = obj / len(happy)\nprint(pos, neg, obj)\n", "intent": "Then we can count an average of the scores\n"}
{"snippet": "clf.predict(X.loc[5:6])\n", "intent": "Predict for random sample\n"}
{"snippet": "print(\"ROC_AUC score: {:.4f}\".format(roc_auc_score(y_test, y_pred)))\nprint(\"Accuracy score: {:.4f}\".format(accuracy_score(y_test, y_pred)))\n", "intent": "Due to an unbalanced dataset, the best metric is ROC_AUC Curve (it will be much more accurate than accuracy)\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = clf.predict(test_examples)\n", "intent": "Produce predictions on our test examples based on the classifier that we defined above. \n"}
{"snippet": "accuracy_score(test_labels, y_pred) * 100\n", "intent": "Get the accuracy of these predictions based on the test labels that we used above.\n"}
{"snippet": "model_small.evaluate(examples_small, y_labels_small, batch_size=32)\n", "intent": "Is the model behaving how we expect it to? It should overfit to the training examples-- let's feed it some training examples: \n"}
{"snippet": "learn.predict(df.iloc[0])\n", "intent": "As usual, we can use the [`Learner.predict`](/basic_train.html\n"}
{"snippet": "r2_score(y_test, y_pred)\n", "intent": "Very huge error ! :S\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmodel = CatBoostClassifier(iterations=400, loss_function='MultiClass', custom_loss='Accuracy', logging_level='Silent')\nscores = cross_val_score(model, x_train, y_train, scoring='accuracy', n_jobs=-1, fit_params=None)\n", "intent": "Each of the arrays contains loss in each iteration (like `staged_predict`):\n"}
{"snippet": "xmodel_pred = [np.argmax(xmodel.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(xmodel_pred)==np.argmax(test_targets, axis=1))/len(xmodel_pred)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "x_test = np.array(['I love you'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "prediction_kaggle_data = svm_model.predict(data_kaggle_test)\n", "intent": "Now we predict on the Kaggle data and submit to get a score.\n"}
{"snippet": "N_splits = 10\nkfold = model_selection.KFold(n_splits=N_splits)\nscoring = 'accuracy'\nscore = model_selection.cross_val_score(svm_model, X_train, y_train, cv=kfold, n_jobs=1, scoring=scoring)\nprint(\"mean = {}\".format(score.mean()*100)) \nprint(\"variance = {}\".format(score.var()*100)) \n", "intent": "From this we can conclude that a value of C=0.1 is best for a linear kernel SVM.\n"}
{"snippet": "validation_x, validation_y = loadValidationData()\nvalidation_x, validation_y = normalize(validation_x,validation_y)\nvalidation_x = pca.transform(validation_x)\nvalidation_x = validation_x.T\nvalidation_prediction = predict(validation_x, validation_y, parameters)\nvalidation_accuracy = validation_prediction[1]\nprint(\"Validation data accuracy :\", str(validation_accuracy))\n", "intent": "<h2><b> Validating the given parameters </b></h2>\n"}
{"snippet": "test_x, test_y = loadTestData()\ntest_x, test_y = normalize(test_x,test_y)\ntest_x = pca.transform(test_x)\ntest_x = test_x.T\ntest_prediction = predict(test_x, test_y, parameters)\ntest_accuracy = test_prediction[1]\nprint(\"Test data accuracy :\", str(test_accuracy))\n", "intent": "<i> :: predicting result for the test data </i>\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"gendarme.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model4.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "DogResNet50_pred = [np.argmax(DogResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_DogRes50]\ntest_accuracy = 100*np.sum(np.array(DogResNet50_pred)==np.argmax(test_targets, axis=1))/len(DogResNet50_pred)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets,axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for l in range(len(style_layers)):\n        g = gram_matrix(feats[style_layers[l]])\n        a = style_targets[l]\n        w = style_weights[l]\n        sub_gram = (g - a) ** 2\n        sum_gram = tf.reduce_sum(sub_gram)\n        style_loss += w * sum_gram\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "clf.predict(test_data)\n", "intent": "We now feed the test data (predictors) into the decision tree and get back the (predicted) test labels:\n"}
{"snippet": "clf.predict(test_data) == test_target\n", "intent": "An easier way to check is with a conditional statment which shows us that the predicted and original labels are the same:\n"}
{"snippet": "def content_loss(base, combination):\n    return K.sum(K.square(combination - base))\n", "intent": "Content is the straight forward sum of squares between the base and the target image. \n"}
{"snippet": "yPredict = my_DecisionTree.predict(xTest)\nprint(yPredict)\n", "intent": "Testing the model on the testing set and printing out the predictions\n"}
{"snippet": "score = accuracy_score(yTest, yPredict)\nprint(score)\n", "intent": "Getting the accuary score of the model using sklearn function and printing out the score\n"}
{"snippet": "score = accuracy_score(yTest, votedList)\nprint(score)\n", "intent": "Here I get the accuracy score of the Bagging model.\n"}
{"snippet": "score = accuracy_score(yTest, yAdaBoostPredict)\nprint(score)\n", "intent": "Here we evaluate the accuracy of our model and print out our accuracy score.\n"}
{"snippet": "scoreRandomForest = accuracy_score(yTest, yRandomForestPredict)\nprint(scoreRandomForest)\n", "intent": "Here we evaluate the accuracy of our model and print out our accuracy score.\n"}
{"snippet": "y_pred = clf_gini.predict(X_test1)\ny_pred\n", "intent": "Prediction using gini index for test 1\n"}
{"snippet": "x_pred = clf_entropy.predict(X_test1)\nx_pred\n", "intent": "Prediction using entropy for test 1\n"}
{"snippet": "t_pred = clf_gini.predict(X_test2)\nt_pred\n", "intent": "Prediction using gini for test 2\n"}
{"snippet": "z_pred = clf_entropy.predict(X_test2)\nz_pred\n", "intent": "Prediction using entropy for test 2\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\ndef cal_accuracy(y_test, y_pred):\n    print(\"Confusion Matrix: \",\n        confusion_matrix(y_test, y_pred))\n    print (\"Accuracy : \",\n    accuracy_score(y_test,y_pred)*100)\n    print(\"Report : \",\n    classification_report(y_test, y_pred))\n", "intent": "<b>Calculate accuracy </b>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = model.predict(test[columns])\nmean_squared_error(predictions, test[target])\n", "intent": "Our accuracy is very high at 53%,almost as good as the bookies.\n"}
{"snippet": "test_features_1 = [[fruit_data_set[\"weight\"][0], fruit_data_set[\"smooth\"][0]]]\ntest_features_1_fruit = fruit_classifier.predict(test_features_1)\nprint(\"Actual fruiPredicting the fruit type from the trained classifiert type: {act_fruit} , Fruit classifier predicted: {predicted_fruit}\".format(act_fruit=fruit_data_set[\"fruit\"][0], predicted_fruit=test_features_1_fruit))\ntest_features_3 = [[fruit_data_set[\"weight\"][2], fruit_data_set[\"smooth\"][2]]]\ntest_features_3_fruit = fruit_classifier.predict(test_features_3)\nprint(\"Actual fruit type: {act_fruit} , Fruit classifier predicted: {predicted_fruit}\".format(act_fruit=fruit_data_set[\"fruit\"][2], predicted_fruit=test_features_3_fruit))\ntest_features_8 = [[fruit_data_set[\"weight\"][7], fruit_data_set[\"smooth\"][7]]]\ntest_features_8_fruit = fruit_classifier.predict(test_features_8)\nprint(\"Actual fruit type: {act_fruit} , Fruit classifier predicted: {predicted_fruit}\".format(\n    act_fruit=fruit_data_set[\"fruit\"][7], predicted_fruit=test_features_8_fruit))\n", "intent": "Predicting the fruit type from the trained classifier\n"}
{"snippet": "def predict(weight, smoot):\n    if weight <= 157.5:\n        return 0\n    else:\n        return 1\n", "intent": "Decision tree visualization\n"}
{"snippet": "test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n", "intent": "Finally, let's also evaluate your new model and see how it performs!\n"}
{"snippet": "y_pred = np.array(rf.predict(X_test))\ny_true = np.array(y_test)\n", "intent": "The accuracy is not great. But here, I am interested in knowing if any of these have an more impact amongst them on the first trip taken.\n"}
{"snippet": "import numpy as np\ndef predict_class(data_point, classes=['Yellow', 'Blue', 'Red']):\n    data_point = np.array(data_point).reshape(1, -1)\n    result = tree_clf.predict(data_point)\n    print('Predicted class for point {}: {}'.format(data_point,\n                                                    classes[np.asscalar(result)]))\npredict_class([-5, 10])\npredict_class([10, 5])\npredict_class([-10, -10])\n", "intent": "We have grown a tree on our 2-D data set. Classifying new examples is simple:\n"}
{"snippet": "model.evaluate(x_test,y_test)\n", "intent": "This model generalizes pretty well.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * torch.sum((gram_matrix(feats[style_layers[i]].clone()) - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from  sklearn.metrics import mean_squared_error\nmean_squared_error(vmt_monthly, predictions_ARIMA)\n", "intent": "The Mean Squared Error for the ARMIA Forecast\n"}
{"snippet": "def expand_dims(f):\n    return np.expand_dims(f, axis=0)\npredictions = [np.argmax(model.predict(expand_dims(feature))) for feature in test_InceptionV3]\naccuracy = 100 * np.sum(np.array(predictions)==np.argmax(test_targets, axis=1)) / len(predictions)\nprint('Accuracy', accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Inception_predictions = [np.argmax(Incept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Print the predictions.\n"}
{"snippet": "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\nprint('')\nprint('Got %.2f%% accuracy' % (acc * 100.))\n", "intent": "Now it's time to actually test the network. \nGet above **65%**!\n"}
{"snippet": "y_pred = self_training_kNN(Cancer, 3)\nprint(100 * l1_loss(y_pred, Cancer))\ny_pred_mnist = self_training_kNN(Mnist, 3)\nprint(100 * l1_loss(y_pred_mnist, Mnist))\n", "intent": "Let's test self-training on the cancer and mnist datasets\n"}
{"snippet": "x_test = np.array(['not feeling good'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "import sklearn.model_selection as ms\nkf = ms.KFold(10)\nauc = ms.cross_val_score(cl,X,y=Y,cv=kf,scoring='roc_auc').mean()\nauc\n", "intent": "Run 10-fold cross validation and record the AUC\n"}
{"snippet": "y_pred_test = cl.predict(X_test)\n", "intent": "Get binary predictions on test set\n"}
{"snippet": "y_pred_proba_test = cl.predict_proba(X_test)[:,1]\n", "intent": "And probability predictions\n"}
{"snippet": "import sklearn.metrics as metrics\nmetrics.accuracy_score(Y_test,y_pred_test)\n", "intent": "Accuracy and AUC on test set\n"}
{"snippet": "y_pred_test = clf.best_estimator_.predict(X_test)\nmetrics.accuracy_score(Y_test,y_pred_test)\n", "intent": "Accuracy on test set\n"}
{"snippet": "y_test_preds=rand.predict_proba(X_test)[:, 1]\n", "intent": "Let's take a look at some plots of the predictions:\n"}
{"snippet": "predictions_train=rand.predict(X_train)\npredictions_test=rand.predict(X_test)\n", "intent": "First lets check the performance on the train set.\n"}
{"snippet": "y_test_preds=rand.predict_proba(X_test)[:, 1]\n", "intent": "Lets take a look at some plots of the predictions:\n"}
{"snippet": "y_pred= kmeans.predict(X_test)\nprint(y_pred[:100])\nprint(y_test[:100])\ny_predNS= kmeans.predict(X_testNS)\nprint(y_predNS[:100])\nprint(y_testNS[:100])\n", "intent": "Let us visually observe the predicted labels vrsus test labels. Let us also print the confusion matrix and accuracy score using Metrics module\n"}
{"snippet": "y_pred= kmeans_random.predict(X_test)\nprint(y_pred[:100])\nprint(y_test[:100])\ny_predNS= kmeans_random.predict(X_testNS)\nprint(y_predNS[:100])\nprint(y_testNS[:100])\n", "intent": "Once clsuters are created, we will plot the centroids of the clsuters in the form of images\n"}
{"snippet": "print \"F1 Score\", round(f1_score(df_test2.iraq, lr.predict(X2s)),2)\n", "intent": "Basically 99% of our articles are not from our topic, we are predicting the 1%, so we should be aiming for almost 100% .\n"}
{"snippet": "from keras import backend as K\nimport tensorflow as tf\ndef max_margin_loss(y_true, y_pred):\n    positive = y_pred[:,0]\n    negative = y_pred[:,1]\n    return K.sum(K.maximum(0., 1. - positive + negative))\n", "intent": "\\begin{equation*}\nloss = \\sum_i{max(0, 1 -p_i + n_i)}\n\\end{equation*}\n"}
{"snippet": "blob_sentiment = [blob_polarity(text) for text in testFile.text]\nprint accuracy_score(testFile.sentiment, blob_sentiment)\n", "intent": "Finally we perform the function above to the testFile and use the accuracy_score to compare the testFile.sentiment and blob_sentiment\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "It is simply the mean of residual sum of Squares\n"}
{"snippet": "print(accuracy_score(gs.predict(Xtestlr), ytestlr))\n", "intent": "This gives a different value of C as best value for C. \n"}
{"snippet": "clf.predict_proba(vectorizer.transform(['This movie is not remarkable, touching, or superb in any way']))\n", "intent": "It predicts it to be 95% fresh. It does not account for word 'not' besides accounts for words remarkable, touching and superb.\n"}
{"snippet": "def gram_matrix_test(correct):\n    feature = model.extract_features()[5]\n    gram = gram_matrix(feature)\n    student_output = sess.run(gram, feed_dict={model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print(correct.shape, student_output.shape)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for index, style_layer in enumerate(style_layers):\n        gram = gram_matrix(feats[style_layer])\n        loss += style_weights[index] * tf.reduce_sum(tf.square(gram - style_targets[index]))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "bim_p = gabor_vocab.predict(bim_feats)\nbim_l = random.permutation(bim_locs[bim_p==word_nr])\nfor i in range(len(bim_l)):\n    if i < 24:\n        splot(3, 8, i+1)\n        util.imshow(get_patch(bim, bim_l[i], zoom=1))\n", "intent": "And we can see that similar structures are mapped to the same word\n"}
{"snippet": "res = model.evaluate(pwaves[12:], keras_labels[12:], verbose=0)\nprint('Accuracy = {}'.format(res[1]))\n", "intent": "Maybe we can do any good after all.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncvs = cross_val_score(enet, X, y, cv =5)\ncvs\n", "intent": "- The cross_val score is almost one, so our model should be considered robust\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    Ls = 0\n    for i in range(len(style_layers)):\n        l = style_layers[i]\n        G = gram_matrix(feats[l])\n        Ls += style_weights[i] * ((G - style_targets[i])**2).sum()\n    return Ls\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, l in enumerate(style_layers):\n        w_l = style_weights[i]\n        G_l = gram_matrix(feats[l])\n        A_l = style_targets[i]\n        style_loss += w_l * tf.reduce_sum(tf.squared_difference(G_l, A_l))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Y_hat = model.predict(\n    [Xca, Xco, Xnav]\n)\n", "intent": "Generating a prediction is simple.\n"}
{"snippet": "y_train_pred = knn.predict(X_train)\nnp.sum(y_train_pred == y_train) / len(y_train)\n", "intent": "It is important to mention that we can also \"predict\" the training set. Surprisingly, the model does not get the training set 100% correct.\n"}
{"snippet": "print('Performance metric (R2_score) using training data: %.4f' % metrics.r2_score(y, predKaggle))\nprint('Performance metric (RMSE) using training data: %.4f' % np.sqrt(metrics.mean_squared_error(y, predKaggle)))\n", "intent": "Lastly, perform the measurement metrics. We are aiming at somewhere between ~5 RMSE which is not good but also not bad :).\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "The estimated coefficients for j = 1 are -24.011, 1.7, -0.266, 1.224, 0.023. The estimated coefficients for j = 2 are -1.468, -0.333, 0.664, -0.923.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "mseCRIM = np.mean((bos.PRICE - lm.predict(X[['CRIM']])) ** 2)\nprint mseCRIM\n", "intent": "The F-statistics for CRIM: 88.15\nThe $R^{2}$ for CRIM: 0.149\n"}
{"snippet": "mseRM = np.mean((bos.PRICE - lm.predict(X[['RM']])) ** 2)\nprint mseRM\n", "intent": "The F-statistics for RM: 471.8\nThe $R^{2}$ for RM: 0.484\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nMSEtest = mean_squared_error(Y_test, lm3.predict(X_test))\nprint('Mean Squared Error (MSE) for the test data: {}'.format(MSEtest))\n", "intent": "Calculate the mean squared error for the test data:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nMSEtraining = mean_squared_error(Y_train, lm3.predict(X_train))\nprint('Mean Squared Error (MSE) for the test data: {}'.format(MSEtraining))\n", "intent": "Calculate the mean squared error for the training data:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint (confusion_matrix(df['Cluster'], kmeans.labels_))\nprint('\\n')\nprint (classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds = lm.predict(X_test)\n", "intent": "* Predict off test set.\n"}
{"snippet": "print('MAE:', metrics.mean_absolute_error(y_test,preds))\nprint('MSE:', metrics.mean_squared_error(y_test,preds))\nprint('RMSE:', np.sqrt(metrics.mean_absolute_error(y_test,preds)))\nprint('R^2:', metrics.r2_score(y_test, preds))\n", "intent": "* Check model coefficient scores. Relatively good.\n"}
{"snippet": "preds = lm.predict(X_test)\n", "intent": "* Make predictions.\n"}
{"snippet": "evaluate_error(strided_cnn_model)\n", "intent": "To evaluate this model, we are gonna calculate the error rate on test set.\n"}
{"snippet": "evaluate_error(nin_cnn_model)\n", "intent": "The error rate should be a bit higher than the other three since it is simpler.\n"}
{"snippet": "evaluate_error(combine_model)\n", "intent": "Let us print out the error rate to see whether the combination of models will work or not. \n"}
{"snippet": "final_preds = []\nfor pred in predictions:\n    final_preds.append(pred['predictions'])\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test,final_preds)**0.5\n", "intent": "** Prediction error **\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inceptionModel.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(' (0,0)  belongs to cluster:', km.predict((0, 0)))\nprint(' (5,5)  belongs to cluster:', km.predict((5, 5)))\nprint('(10,10) belongs to cluster:', km.predict((10, 10)))\nprint('(15,15) belongs to cluster:', km.predict((15, 15)))\nprint('(20,20) belongs to cluster:', km.predict((20, 20)))\nprint('(25,25) belongs to cluster:', km.predict((25, 25)))\n", "intent": "Once the system is trained, we can use `.predict()` to figure out which cluster a point belongs to.\n"}
{"snippet": "predictions = []\nfor pred in list(dnn_model.predict(input_fn=predict_input_func)):\n    predictions.append(pred['predictions'])\n", "intent": "<b>Evaluating the model</b>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy for Resnet50: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred_test = reg_train[0].predict(test_merged_f[feature_columns])\nprint len(pred_test)\n", "intent": "Use the model to predict the result\n"}
{"snippet": "accuracy_score(y_test, a)\n", "intent": "Testing accuracy using SkLearns accuracy score where we feed it the expected values and compare them to our generated values.\n"}
{"snippet": "def evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, batch_size):\n        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n        valid_acc=sess.run(accuracy,feed_dict={x: batch_x, y: batch_y, keep_prob:1.})\n        total_accuracy += (valid_acc * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "Evaluate how well the loss and accuracy of the model for a given dataset.\n"}
{"snippet": "for i in range(48):\n    clf.upload_image(img[i,:,:].reshape(-1,21,21))\npred, history = clf.predict()\n", "intent": "We upload its 48 epochs one by one and obtain prediction history:\n"}
{"snippet": "clf = online_classifier(model, 'masking')\nclf.reset()\nfor i in range(48):\n    clf.upload_image(img[i,:,:].reshape(-1,21,21))\npred, history = clf.predict()\n", "intent": "Now we can repeat the same procedure for with the `masking` method. The same results are obtained.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(model2.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "semantic.evaluate.evaluate(model, dataset_test)\n", "intent": "Get the Mean IoU for each class in the dataset\n"}
{"snippet": "scores = cross_val_score(lin_reg, Gmi.reshape(-1,1), Pdc, cv=10, n_jobs=-1)\n", "intent": "And we assess its performances with 10-fold cross validation.\n"}
{"snippet": "predicted = cross_val_predict(lin_reg, Gmi.reshape(-1,1), Pdc, cv=10, n_jobs=-1)\n", "intent": "Finally, we show the learnt models in a plot.\n"}
{"snippet": "predicted2 = cross_val_predict(lin_reg2, Gmi2.reshape(-1,1), Pdc2, cv=10, n_jobs=-1)\n", "intent": "The predicted models are shown below.\n"}
{"snippet": "val_score = metrics.roc_auc_score(validate.Churn, \n                              np.array([ pred_proba for pred_proba in classifier.predict_proba(validate[FEATURE_COLUMNS])])[:,1])\nprint(\"AUC: %f\" % val_score)\n", "intent": "A customer churn files are usually biased. For imbalanced data, we need to do\n* ROC AUC\nValidate model with AUC\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis = 0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions) == np.argmax(test_targets, axis = 1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "No, this classifier is not not of an improvement from the default model. The score for test data is actually worst.\n"}
{"snippet": "pred = knn.predict(X_test)\naccuracy = knn.score(X_test, y_test)\nprint('Accuracy: '+str(accuracy*100)+'%')\n", "intent": "Predict and determine accuracy.\n"}
{"snippet": "print('start predicting....')\nY_pred = svc.predict(test_data.iloc[:,1:])\nP_prob = svc.predict_proba(test_data.iloc[:,1:])\nprint('Done!')\n", "intent": "Now we predict the probabilities of the test data using our trained SVM.\n"}
{"snippet": "knn1_cm = metrics.confusion_matrix(test_lab, knn1_lab_pred)\nprint(metrics.classification_report(test_lab, knn1_lab_pred))\n", "intent": "KnN <b> without </b> feature selection:\n"}
{"snippet": "knn2_cm = metrics.confusion_matrix(test_lab_fs, knn2_lab_pred)\nprint(metrics.classification_report(test_lab_fs, knn2_lab_pred))\n", "intent": "KnN <b> with </b> feature selection:\n"}
{"snippet": "def perceptron_predict(features, mparams):\n    linear = np.dot(features, mparams['weights']) + mparams['bias']\n    predicted_labels = np.where(linear.reshape(-1) > 0., 1, 0)\n    return predicted_labels\n", "intent": "- Implement a function for perceptron predictions in NumPy\n"}
{"snippet": "train_errors = np.sum(perceptron_predict(X_train, model_params) != y_train)\ntest_errors = np.sum(perceptron_predict(X_test, model_params) != y_test)\nprint('Number of training errors', train_errors)\nprint('Number of test errors', test_errors)\n", "intent": "- Compute training and test error\n"}
{"snippet": "def predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'movie':\n        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n    return pred\n", "intent": "Next, we make predictions\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        loss += style_weights[i]*torch.sum((gram_matrix(feats[style_layers[i]])-style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "roc = roc_auc_score(test_y, y_prob)\nprint(\"ROC AUC:\", round(roc, 3))\np, r, f, s = precision_recall_fscore_support(y_true, y_pred, average='micro')\nprint(\"F-Score:\", round(f, 2))\n", "intent": "Keras metrics are important, but we can also add our own metrics which are more informative.\n"}
{"snippet": "y_pred_train = fitted_model.predict(X_train)\ny_residual_train = y_train - y_pred_train\ny_pred_test = fitted_model.predict(X_test)\ny_residual_test = y_test - y_pred_test\n", "intent": "Predict on training and test set, and calculate residual values.\n"}
{"snippet": "actual = test_labels\npredicted = rf_cl.predict(test_features)\ncorrect = actual == predicted\nnum_correct = sum(correct)\nfrac_correct = float(num_correct)/len(actual)\nprint 'The classifier got %i right' %num_correct, 'which is equivalent to %0.3f' %frac_correct\n", "intent": "Now we test the classifier on the test data, comparing the known digits to the predicted digits.\n"}
{"snippet": "predicted = rf.predict(X_test)\nprint(accuracy_score(y_test, predicted))\nprint(1 - rf.oob_score_)\nimportances = [(features.keys()[i], rf.feature_importances_[i]) for i in range(len(features.keys()))]\nprint(importances)\n", "intent": "Check scores for classifier\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint('The MSE is:', metrics.mean_squared_error(income['Income'], income['incomePred'] ))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.cpu().numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, l in enumerate(style_layers):\n        gram_current = gram_matrix(feats[l].clone())\n        style_loss_l = style_weights[i] * torch.sum((gram_current - style_targets[i]) ** 2)\n        style_loss += style_loss_l\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def view_error(model):\n    data = list(zip(X_rnn_dev, y_rnn_dev))\n    for _ in range(len(data)):\n        ex, label = random.choice(data)\n        pred = model.predict([ex])[0]\n        if label != pred:\n            print(\" \".join(ex))\n            print(\"Correct label: {}\".format(label))\n            return\n", "intent": "It can be challenging to make sense of the errors that these models are making, but we should try. Here's a function for viewing random errors:\n"}
{"snippet": "def process_new_examples(model):\n    examples = [\n        ['great'], ['excellent'], ['bad'], \n        ['boring'], ['not', 'good']\n    ]    \n    for ex in examples:\n        ex = ['This', 'was'] + ex\n        prediction = model.predict([ex])[0]\n        print(\"{0:<30} {1:}\".format(\" \".join(ex), prediction))\n", "intent": "It can also be informative to invent examples and see how the model deals with them:\n"}
{"snippet": "RBFtrainingMSE = []\nRBFtestMSE = []\nfor c in Cs:\n    rbf_model = svm_fit(trainingPoints,trainingLabels,cValue=c, gammaValue=1)\n    RBFtrainingMSE.append(1-rbf_model[1]['accuracy'])\n    RBFtestMSE.append(1-svm_predict(rbf_model[0],testPoints,testLabels)[1]['accuracy'])\n", "intent": "MSE for RBF kernel, gamma=1\n"}
{"snippet": "linearTrainingMSE = []\nlinearTestMSE = []\nfor c in Cs:\n    linear_model = svm_fit(trainingPoints,trainingLabels,cValue=c, kernelType='linear', gammaValue=1)\n    linearTrainingMSE.append(1-linear_model[1]['accuracy'])\n    linearTestMSE.append(1-svm_predict(linear_model[0],testPoints,testLabels)[1]['accuracy'])\n", "intent": "MSE for linear kernel, gamma=1\n"}
{"snippet": "bestC = Cs[np.argmin(RBFtestMSE)]\ngammaTrainingMSE = []\ngammaTestMSE = []\nfor g in gammas:\n    rbf_model = svm_fit(trainingPoints,trainingLabels,cValue=bestC, gammaValue=g)\n    gammaTrainingMSE.append(1-rbf_model[1]['accuracy'])\n    gammaTestMSE.append(1-svm_predict(rbf_model[0],testPoints,testLabels)[1]['accuracy'])\n", "intent": "MSE for RBF kernel, C=bestC\n"}
{"snippet": "df_run['future_problem'] = clf.predict(features[['startyear_client', 'duration', 'payments']])\n", "intent": "Predict future payment problems:\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "** Calculate the Mean Absolute Error, Mean Squared Error, and the Root Mean Squared Error. Refer to the lecture or to Wikipedia for the formulas**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Grab predictions off our test set and see how well it did!\n"}
{"snippet": "Yhat=lm.predict(27)\nYhat[0:5]   \n", "intent": " We can output a prediction \n"}
{"snippet": "def log_likelihood(clf,x,y):\n    prob = clf.predict_log_proba(x)\n    rotten = y == 0\n    fresh = ~rotten\n    return prob[rotten, 0].sum() + prob[fresh, 1].sum()\n", "intent": "Using clf.predict_logproba, write a function that computes the log-likelihood of a dataset\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "We will test across the classification accuracy metrics\n"}
{"snippet": "def answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    knn = answer_five()\n    pred = knn.predict(X_test)\n    qq = knn.score(X_test, y_test)\n    return qq\nanswer_eight()\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nscore = loaded_model.evaluate(X, Y)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n", "intent": "Getting the same result as before\n"}
{"snippet": "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size = 128),\n                              steps_per_epoch = len(X_train) / 128, epochs = epochs)\nmodel.save_weights(\"/home/teresas/notebooks/deep_learning/files/load_CIFAR10/WRNet.h5\")\nprint(\"Saved model to disk\")    \nscores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "**2: with data augmentation**\n"}
{"snippet": "print(classification_report(y_train,y_pred))\n", "intent": "<img height=\"600\" width=\"750\" src=\"precision_recall.png\">\n"}
{"snippet": "cv_score = cross_val_score(rnd_clf,x_train,y_train,cv=5,scoring='f1')\nprint(\"Average F1 score CV\",cv_score.mean())\n", "intent": "We could improve the precision score by changing the threshold to 0.63\n"}
{"snippet": "pred = model.predict(np.reshape(image[0], [1, 28, 28, 1]))\n", "intent": "Now checking the predictions from the model\n"}
{"snippet": "def evaluate(train_set, test_set, classifier):\n    print (\"Traning Set Accuracy = \" + str(classify.accuracy(classifier, train_set)))\n    print (\"Test Set Accuracy = \" + str(classify.accuracy(classifier, test_set)))    \n    classifier.show_most_informative_features(50)\nevaluate(train_set, test_set, classifier)\n", "intent": "***Evaluating the accuracy of the classifier and extracting the most predictive features/most informative words:***\n"}
{"snippet": "y_pred_linear_reg = linear_reg.predict(X_test)\n", "intent": "Lets now make predictions on the test set using the model built\n"}
{"snippet": "predictions = model.predict(x_val)\n", "intent": "To further analyze the results, we can produce the actual predictions for the validation data.\n"}
{"snippet": "pred_dt = clf_dt.predict(X_test.reshape(-1,28*28))\nprint('Predicted', len(pred_dt), 'digits with accuracy:', accuracy_score(y_test, pred_dt))\n", "intent": "Classifying a new sample with a decision tree is fast, as it consists of following a single path in the tree until a leaf node is found.\n"}
{"snippet": "print(classification_report(y_test, pred_rf, labels=list(range(10))))\n", "intent": "Precision and recall for each class:\n"}
{"snippet": "pred_xgb = clf_xgb.predict(X_test.reshape(-1,28*28))\nprint('Predicted', len(pred_xgb), 'digits with accuracy:', accuracy_score(y_test, pred_xgb))\n", "intent": "At least with only a subset of training data and default hyperparameters values, XGBoost does not reach the performance of random forest.\n"}
{"snippet": "t0 = time()\npredictions = clf.predict(X_test[:100,:,:].reshape(-1,28*28))\nprint('Time elapsed: %.2fs' % (time()-t0))\n", "intent": "And try to classify some test samples with it.\n"}
{"snippet": "t0 = time()\npredictions_reduced = clf_reduced.predict(X_test.reshape(-1,28*28))\nprint('Time elapsed: %.2fs' % (time()-t0))\n", "intent": "Now we can use the classifier created with reduced data to classify our whole test set in a reasonable amount of time.\n"}
{"snippet": "print('Predicted', len(predictions_reduced), 'digits with accuracy:', accuracy_score(y_test, predictions_reduced))\n", "intent": "The classification accuracy is however now not as good:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = torch.nn.MSELoss(size_average=False)\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i]*loss(gram_matrix(feats[style_layers[i]].clone()),style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "ynewpred =  lm.predict(Xnew)\nprint(ynewpred)\n", "intent": "The linear model object that we created has a \"predict\" method which can be used to generate the new  values that we are interested in.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        target_gram = style_targets[i]\n        style_loss += style_weights[i] * torch.sum((gram - target_gram) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "- Returns a Numpy array\n- Can predict for mutliple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "print (10 + 0 + 20 + 10)/4.\nfrom sklearn import metrics\nprint metrics.mean_absolute_error(true, pred)\n", "intent": "$$\\Large \\frac{1}{n} \\sum\\limits_{i=1}^n \\vert y_i - \\hat{y}_i \\vert$$\n"}
{"snippet": "print ((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint metrics.mean_squared_error(true, pred)\n", "intent": "$$\\Large \\frac{1}{n} \\sum\\limits_{i=1}^n ( y_i - \\hat{y}_i )^2$$\n"}
{"snippet": "import numpy as np\nprint np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint np.sqrt(metrics.mean_squared_error(true, pred))\n", "intent": "$$\\Large \\sqrt{\\frac{1}{n} \\sum\\limits_{i=1}^n ( y_i - \\hat{y}_i )^2}$$\n"}
{"snippet": "print TP / float(TP + FP)\nprint metrics.precision_score(y_test, y_pred_class)\n", "intent": "**Precision:** When a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "logreg.predict_proba(X_test)[0:10, 1]\n", "intent": "The instances in column 1 that are **greater than 0.5** is when class 1 is predicted \n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "Area under the curve (AUC) is the **percentage** of the ROC plot that is **underneath the curve:**\n"}
{"snippet": "y_pred = results1.predict(X_test)\nprint(y_pred.head())\n", "intent": "__Prediction with testing data (numerical & categorical)__\n"}
{"snippet": "mse = mean_squared_error(y_test, y_pred)\nprint(mse)\n", "intent": "__Compute mean squared error between y in testing data and prediction__\n"}
{"snippet": "y_pred = results2.predict(X_test)\nprint(y_pred.head())\n", "intent": "__Prediction with testing data (only with numerical)__\n"}
{"snippet": "mse = mean_squared_error(y_test, y_pred)\nprint(mse)\n", "intent": "__Prediction with testing data (only with numerical)__\n"}
{"snippet": "from sklearn import metrics\nscore = metrics.accuracy_score(y_test, stk.predict(X_test))\nprint()\nprint( 'Blended Classifier Accuracy = %s' % (score))\nprint()\nscore0 = metrics.accuracy_score(y_test, stk._base_estimator_predict(stk.estimators_[0], X_test))\nprint( 'Random Forest (10 trees) Accuracy = %s' % (score0))\nscore1 = metrics.accuracy_score(y_test, stk._base_estimator_predict(stk.estimators_[1], X_test))\nprint( 'Extra Trees (10 trees) Accuracy = %s' % (score1))\n", "intent": "Now we can compare the performance of the ensemble of blended models to that of the individual (base) ones:\n"}
{"snippet": "def validation_error(valid_images, prediction):\n    RMSE = 0\n    for fname in valid_images:\n        img_x, img_y = load_train_set([fname])\n        img_pred = prediction(img_x)\n        img_y = img_y.ravel()\n        img_pred = img_pred.ravel() \n        RMSE += np.sqrt(np.mean((img_y - img_pred)**2))\n    RMSE = RMSE / len(valid_images)\n    return RMSE\n", "intent": "We write a function to compute the Root Mean Square Error that our prediction functions achieve on the validation set. \n"}
{"snippet": "from random import randint\npred = [randint(0, 1) for i in range(len(y_test))]\nprint(\"Accuracy =\", accuracy(pred, y_test))\nprint(\"Precision =\", precision(pred, y_test))\nprint(\"Recall =\", recall(pred, y_test))\nprint(\"F1_score =\", f1_score(pred, y_test))\n", "intent": "Let's compute these metrics in the case where we predict randomly $0$ or $1$.\nThis will give us metrics to be outperformed by our strategies.\n"}
{"snippet": "d0_out = intermediate_output = intermediate_layer_model.predict(samp_conv_val_feat)\n", "intent": "Then we can call the function to get our layer activations:\n"}
{"snippet": "probs = lm.predict(val_features, batch_size=batch_size)\nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "probs = model2.predict(val_data, batch_size=batch_size)\npreds = np.array([np.argmax(x) for x in probs])\npreds[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "model2.load_weights(model_path+'finetune2.h5')\nmodel2.evaluate(val_data, val_labels)\n", "intent": "You can always load the weights later and use the model to do whatever you need:\n"}
{"snippet": "predict_results = est_malevsfemale.predict(\n    input_fn=lambda: imgs_input_fn(test_files[:10], \n                                   labels=None, \n                                   perform_shuffle=False,\n                                   batch_size=10))\n", "intent": "To predict we can set the `labels` to None because that is what we will be predicting.\nHere we only predict the first 10 images in the test_files.\n"}
{"snippet": "predict_results = est_kerasvgg.predict(\n    input_fn=lambda: imgs_input_fn(test_files[:10], \n                                   labels=None, \n                                   perform_shuffle=False,\n                                   batch_size=10))\n", "intent": "To predict we can set the `labels` to None because that is what we will be predicting.\nHere we only predict the first 10 images in the test_files.\n"}
{"snippet": "MyRes50_predictions = [np.argmax(MyRes50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(MyRes50_predictions)==np.argmax(test_targets, axis=1))/len(MyRes50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(Y_test,knn.predict(X_test[['ApplicantIncome', 'CoapplicantIncome',\n                             'LoanAmount', 'Loan_Amount_Term', 'Credit_History']]))\n", "intent": "Checking the performance of our model on the testing data set\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": " We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n"}
{"snippet": "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation using a K of 5:\n"}
{"snippet": "confusionMatrix = makeConfusion(x_pred,y_test)\nprint confusionMatrix\nprint '\\nAccuracy: %f' %accuracy_score(x_pred,y_test)\n", "intent": "We can now compute the autoencoder confusion matrix using euclidean distance for kNN and its corresponding accuracy.\n"}
{"snippet": "prediction_frn = model_frn.predict(x_test_subs_vect)\n", "intent": "We then repeat the process using the LSTM trained with French text, and output the resulting conditional probabilities.\n"}
{"snippet": "local_y_pred = clf.predict(X_val)\nlocal_y_pred[0:5]\n", "intent": "Let's try running this model for local_test or local validation.\n"}
{"snippet": "local_y_pred = clf.predict(X_val)\n", "intent": "Let's try running this model for local_test or local validation.\n"}
{"snippet": "local_lin_mae = mean_absolute_error(np.expm1(Y_val), np.expm1(local_y_pred))\nlocal_lin_mae\n", "intent": "Converting log(1+x)'ed scores back into actual loss response variables to check for MAE.\n"}
{"snippet": "accuracy = accuracy_score(y1,y_hat2)\nprint \"Accuracy: \", accuracy\n", "intent": "MODEL EVALUATION USING ROCSCORE\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print('\\n--- '+str(metric)+' ---')\n    cv = StratifiedKFold(n_splits=4, random_state=123, shuffle=True)\n    NNF = NearestNeighborsFeats(n_jobs=1, k_list=k_list, metric=metric)\n    train_knn_feats = cross_val_predict(NNF, X, Y, cv=cv)\n    np.save('data/knn_feats_%s_train.npy' % metric, train_knn_feats)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "test_preds = best_alpha*X_test_level2[:,0] + (1-best_alpha)*X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr_stack.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr_stack.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "print(classification_report(rf_full_pred, y_test))\n", "intent": "4,633 out of 5,995 were correctly identified as dying within 7 days. \n"}
{"snippet": "my_model_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXception]\ntest_accuracy = 100*np.sum(np.array(my_model_predictions)==np.argmax(test_targets, axis=1))/len(my_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nerror = mean_squared_error(test, pred.predicted_mean)\nprint('MSE {}'.format(error))\n", "intent": "The predictions above is not perfect but it is still better compare to other parameters. Let us see the Mean Squared Error of the model.\n"}
{"snippet": "def predict(x, t, x_test, K=1):\n    w_hat = get_w_hat(x, t, K)\n    X_test = get_X(x_test, K)\n    predictions = X_test * w_hat\n    return predictions\n", "intent": "<div class=\"alert alert-info\">\nWrite a function that, when given `x`, `t` and `x_test`, computes `w_hat` and makes predictions at `x_test`.</div>\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        gram_current = gram_matrix(model.extract_features()[style_layers[i]])\n        gram_style = style_targets[i]\n        style_loss += style_weights[i] * tf.reduce_sum((gram_current - gram_style)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.5f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "base_df['cluster'] = kmeans.predict(base_df)\nbase_df.head(10)\n", "intent": "**Let's save the clusters to our dataframe.**\n"}
{"snippet": "threshold_df['cluster'] = kmeans.predict(threshold_df)\nthreshold_df.head(10)\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">threshold_df</code>.**\n"}
{"snippet": "pca_df['cluster'] = kmeans.predict(pca_df)\npca_df.head(10)\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">pca_df</code>.**\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ninception_test_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % inception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preditions = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint( classification_report(y_test, preditions) )\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "pred = knn.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\npreditions = dtree.predict(X_test)\nprint(confusion_matrix(y_test, preditions))\nprint('\\n')\nprint(classification_report(y_test, preditions))\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preditions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(confusion_matrix(y_test, preditions))\nprint('\\n')\nprint(classification_report(y_test, preditions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint (classification_report(messages['label'], all_predictions))\n", "intent": "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />\n"}
{"snippet": "cls.predict(iris.data[0:1], prediction_type=\"Probability\")\n", "intent": "Predict probabilities:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(mln_pred, labels_test, labels=[1,0]))\n", "intent": "* True Positive : **193**\n* False Positive : **43**\n* False Negative: **5**\n* True Negative : **220**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(bln_pred, labels_test, labels=[1,0]))\n", "intent": "* True Positive : **172**\n* False Positive : **15**\n* False Negative: **26**\n* True Negative : **248**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(g_pred, labels_test, labels=[1,0]))\n", "intent": "* True Positive : **195**\n* False Positive : **68**\n* False Negative: **3**\n* True Negative : **195**\n"}
{"snippet": "accuracies = cross_val_score(estimator = clf3, X = X_train, y = y_train, cv = 10)\naccuracies\n", "intent": "<h3>Applying k-Fold Cross Validation</h3>\n"}
{"snippet": "import sklearn.metrics as metrics\nri = metrics.adjusted_rand_score(labels,news_data.target)\nss = metrics.silhouette_score(dtm,kmeans.labels_,metric='euclidean')\nprint('Rand Index is {}'.format(ri))\nprint('Silhouette Score is {}'.format(ss))\n", "intent": "Let's evaluate the clusters.  We'll assume that the newgroup the article came from is the 'ground truth.'\n"}
{"snippet": "y_pred_train = knn.predict(X_train)\nprint('Accuracy on training data: {}'.format(knn.score(X_train, y_train)))\n", "intent": "Just for kicks we can see how the classifier does on the training data, although that's not as important as the test data.\n"}
{"snippet": "clf, y_pred = fit_and_predict(X_train_scaled, y_train, X_test_scaled, 'rbf')\n", "intent": "Now we're ready to actually fit a SVM and use it to make predictions.\n"}
{"snippet": "print(accuracy_score(y_test, y_pred))\n", "intent": "Let's check the accuracy:\n"}
{"snippet": "gs = GridSearchCV(estimator=pipe_lr,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=2)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "Let's use nested k-fold cross-validation to perform compare the logistic regression model to a support vector machine. \n"}
{"snippet": "predictions = dtree.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n", "intent": "Predict the test set labels and evaluate the model. We can see that the model does a good job of predicting the target labels.\n"}
{"snippet": "predictions = forest.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n", "intent": "Looking at our model performance on the test data set, we can see that the model actually correctly predicted all of the target labels. \n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nrep = {1:0, 0:1}\ny_km2 = np.array(pd.Series(y_km).replace(rep))\nprint confusion_matrix(y, y_km2)\nprint '\\n', classification_report(y, y_km2)\n", "intent": "In this particular case, we can use the confusion matrix and the classification report to see how well the clusters predicted the actual targets. \n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint confusion_matrix(y_test, yhat)\nprint '\\n', classification_report(y_test, yhat)\n", "intent": "Evaluating our model:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint confusion_matrix(y_train, yhat)\nprint '\\n', classification_report(y_train, yhat)\n", "intent": "Solely looking at the training data, we see a precision of 94%, which is pretty good. \n"}
{"snippet": "yhat = lr.predict(X_test_lda)\nprint confusion_matrix(y_test, yhat)\nprint '\\n', classification_report(y_test, yhat)\n", "intent": "Now, looking at the test data, we can see that our accuracy goes down to 87%, meaning that our model slighly overfitted to the linear discriminants. \n"}
{"snippet": "yhat = nb.predict(X_test)\nber_pt = nb.predict_proba(X_test)\nprint confusion_matrix(y_test, yhat)\nprint '\\n', classification_report(y_test, yhat)\n", "intent": "Evaluate the model on the test dataset:\n"}
{"snippet": "yhat = gnb.predict(X_test)\ngnb_pt = gnb.predict_proba(X_test)\nprint confusion_matrix(y_test, yhat)\nprint '\\n', classification_report(y_test, yhat)\n", "intent": "Evaluate the model on the test dataset:\n"}
{"snippet": "print confusion_matrix(y_test, yhat)\nprint '\\n', classification_report(y_test, yhat)\n", "intent": "Evaluate the model on the test dataset:\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint(\"Test accuracy:\", round(accuracy_score(output, predictions),3))\nprint(\"Test precision:\", round(precision_score(output, predictions),3))\nprint(\"Test recall:\", round(recall_score(output, predictions),3))\n", "intent": "**Compute accuracy, precision, and recall of the model**\n"}
{"snippet": "print(\"Test Accuracy:\", accuracy_score(test_labels, submission.Label.tolist()))\n", "intent": "**Print test accuracy**\n"}
{"snippet": "creport1 = classification_report(Y_test, RFpredict)\nprint (creport1)\n", "intent": "**Classification Report**\nThe f1-score is equal to the weighted average of the precision and recall. \n"}
{"snippet": "creport2 = classification_report(y_test, y_pred2)\nprint (creport2)\n", "intent": "**Classification Report**\n"}
{"snippet": "np.sum(classifier.predict(X) == Y) / len(Y)\n", "intent": "We compare the predicted values of our classifier with the actual values and compute the accuracy.\n"}
{"snippet": "errors   = np.sum(np.abs(Y - M.predict(X)))\naccuracy = (len(Y) - errors) / len(Y)\naccuracy\n", "intent": "This time, logistic regression is able to predict all but two of the results correctly.\n"}
{"snippet": "cost = tf.losses.mean_squared_error(Y, Y_pred)\n", "intent": "We use the mean squared error as our loss function.\n"}
{"snippet": "preds = [np.argmax(res50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res50]\ntest_accuracy = 100*np.sum(np.array(preds)==np.argmax(test_targets, axis=1))/len(preds)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "if not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\n", "intent": "Evaluating the model on test data\n"}
{"snippet": "predictions = lr.predict(X_test)\nprint(\"Predictions are: \", predictions)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "features_train=model.predict(train_img)\n", "intent": "Below two commands of Feature extraction on Training and Testing dataset can take some time to process on CPU only machines.\n"}
{"snippet": "predictions_valid = model.predict(X_valid, verbose=1)\n", "intent": "After running our model just for 10 epochs, we are able to achieve 100% accuracy on trainig data and 98.3% accuracy on validation data\n"}
{"snippet": "accuracy_score(pred,status_test)\n", "intent": "Calculate and display the accuracy of the testing set:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmodel = CatBoostClassifier(iterations=400, loss_function='MultiClass', custom_loss='Accuracy', logging_level='Silent')\nscores = cross_val_score(model, x_train, y_train, scoring='accuracy', fit_params=None)\n", "intent": "Each of the arrays contains loss in each iteration (like `staged_predict`):\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint 'Simple model validation accuracy: {:.4}'.format(\n    accuracy_score(y_validation, model_simple.predict(X_validation))\n)\nprint 'Early-stopped model validation accuracy: {:.4}'.format(\n    accuracy_score(y_validation, model_with_earlystop.predict(X_validation))\n)\n", "intent": ".. and make predictions for test with best model:\n"}
{"snippet": "clustered = kmeans.fit_predict(X2_pqcode)\nprint(clustered[:100]) \n", "intent": "Let's run the PQk-means over X2_pqcode.\n"}
{"snippet": "print(\"PQk-means:\")\nids_pqkmeans = kmeans_pqkmeans.fit_predict(X_code)\n", "intent": "Run each method and see the computational cost.\n"}
{"snippet": "model.predict([[3,2,2500,2]])\n", "intent": "Three bedrooms, two bathrooms, 2500 sqft, 2 floors\n"}
{"snippet": "model.predict([[3,2,2500,2]])\n", "intent": "* 3 bedrooms, 2 bathrooms, 2500sqft, and 2 floors\n"}
{"snippet": "test['Gradient Boosting Machine Pred'] = GBM.predict_proba(X_test_scaled.values)[:,1]\nnewfig = plotData(test[x.featureNames + ['Response','Gradient Boosting Machine Pred']],\n                  ['Response','Gradient Boosting Machine Pred'])\n", "intent": "Again, I plot average actual and predicted responses by feature using the test dataset: \n"}
{"snippet": "test['Feed Forward Neural Network Pred'] = FFNN.predict(X_test_scaled.values)\nnewfig = plotData(test[x.featureNames + ['Response','Feed Forward Neural Network Pred']],\n                  ['Response','Feed Forward Neural Network Pred'])\n", "intent": "Again I plot the average response and prediction by feature:\n"}
{"snippet": "cnn_predictions = [np.argmax(cnn_model.predict(np.expand_dims(feature, axis=0))) for feature in test_cnn]\ntest_accuracy = 100*np.sum(np.array(cnn_predictions)==np.argmax(test_targets, axis=1))/len(cnn_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score=model.evaluate(data_test_example,data_test_label,batch_size=100)\n", "intent": "print('test score',loss_and_metrics[0])\nprint('test accuracy:',loss_and_metrics[1])\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0005.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "length = 6.0\nwidth = 3.2\ndata = np.array([width,length]).reshape(1,-1)\npred_class = logreg.predict(data)[0]\ntarget_name = iris['target_names'][pred_class]\nprint \"Overall predicted class of the new flower is {0:}.\\n\".format(target_name)\npred_probs = logreg.predict_proba(data)\nfor name,prob in zip(iris['target_names'],pred_probs[0]):\n    print \"\\tProbability of class {0:12} is {1:.2f}%.\".format(name,prob*100.)\n", "intent": "So, now you have a predictor for the class of any future object based on the input data (length and width of sepals). For example:\n"}
{"snippet": "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n    print('%.10s is %.4f'% (metric,cross_val_score(model_logit, X,y, scoring=metric, cv=15).mean()))\nprint('baseline is ', 1-y.mean())\n", "intent": "The odds results are alinged up with results generated from Statsmodel\n??? how to use sklearn to calculate confidence interval?\n"}
{"snippet": "from sklearn.metrics import r2_score\nprint \"For the Simple Regression the R2 Score is: %.4f\" % r2_score(regr_y, regression.predict(regr_X))\n", "intent": "For comparision the R^2 Score of this linear regression will be calculated in the following.\n"}
{"snippet": "def optimize_predict(X):\n", "intent": "I will set up the prediction algorithm in the following:\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nY_pred = model.predict(X_test, verbose=2)\ny_pred = np.argmax(Y_pred, axis=1)\nconf_mx = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\nconf_mx\n", "intent": "Most of the misclassifications occurred between \"cats\" and \"dogs\"\n"}
{"snippet": "VGG19_predictions = [np.argmax(model_VGG19.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "knn.predict_proba([[3, 5, 4, 2],])\n", "intent": "You can also do probabilistic predictions to estimate the probability that of belonging to each class:\n"}
{"snippet": "predictions = logModel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "x_test = np.array(['I hate you'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['May 3rd 29', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)  \ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\n", "intent": "Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n"}
{"snippet": "x_test = np.array(['you are not very happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\ny_pred\n", "intent": "And now we make a prediction by adding the results of each component:\n"}
{"snippet": "np.sum(np.equal([predict(x, w1, b1, w2, b2) for x in dataX_train[:10]], [0, 2, 0, 1, 0, 2, 0, 0, 1, 0]))\n", "intent": "<span style=\"color:green\"> by running the following command you should get number bigger than 8 </span>\n"}
{"snippet": "print(confusion_matrix(y_test, predictions))\nprint(\"\\n\")\nprint(classification_report(y_test, predictions))\n", "intent": "** Using confusion matrix and classification report to evaluate the model **\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.9f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.9f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.zeros([1])\n    for index, style_layer in enumerate(style_layers):\n        inner_sum = tf.reduce_sum(tf.pow(gram_matrix(feats[style_layer]) - style_targets[index], 2))\n        layer_loss = tf.multiply(tf.cast(style_weights[index], tf.float32), inner_sum)\n        style_loss = tf.add(style_loss, layer_loss)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.9f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntestaccuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint(\"Test accuracy:\" +str(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted = knn.predict(X_test)\n", "intent": "__Warning__: 10 minutes runtime on 7 cores\n"}
{"snippet": "output = model.predict(test_data[:,1:])\n", "intent": "We now apply the trained model to the test data (omitting the column PassengerId) to produce an output of predictions.\n"}
{"snippet": "def predict(parameters,X):\n    A2,cache=forward_propagation(X,parameters)\n    predictions=np.array([1 if(i>0.5) else 0 for i in A2[0]])\n    return predictions\n", "intent": "use our model to predict the result.use the forward propagation to predict.\n"}
{"snippet": "cv_results = model_selection.cross_val_score(RFclf,X_train, y_train, cv=2, scoring='accuracy')\n", "intent": "For each classifier we run 10 fold cross validation which will help us narrow down one final model\n"}
{"snippet": "test_predicted = model.predict(test_images)\n", "intent": "Use learned nearest neighbor classifier (model) for predicting test images\n"}
{"snippet": "scores = cross_val_score(DT_fit, features_train, target_train, cv=5)\nprint(\"Cross Validation Score for each K\",scores)\nscores.mean()       \n", "intent": "Cross validating this model shows little variance in its prediction accuracy.\n"}
{"snippet": "scores = cross_val_score(KNN_fit, features_train, target_train, cv=5)\nprint(\"Cross Validation Score for each K\",scores)\nscores.mean() \n", "intent": "***\nThis model had consistent performance with the potential to break 90% accuracy.\n"}
{"snippet": "tic = time.time() \nsvr = Predict(dat,Y,algorithm='svr',subject_id = holdout, \n              output_dir=out_folder, cv_dict = {'loso':holdout}, \n              **{'kernel':\"linear\"})\nsvr.predict()\nprint 'Elapsed: %.2f seconds' % (time.time() - tic) \n", "intent": "<p>Run Linear Support Vector Regression with leave one subject out cross-validation (LOSO)</p>\n"}
{"snippet": "tic = time.time() \nridge = Predict(dat, Y, algorithm='ridgeCV',subject_id = holdout, \n                output_dir=out_folder, cv_dict = {'kfolds':5},\n                **{'alphas':np.linspace(.1, 10, 5)})\nridge.predict()\nprint 'Total Elapsed: %.2f seconds' % (time.time() - tic) \n", "intent": "<p>Run Ridge Regression with 5 fold Cross-Validation and a nested cross-validation to estimate shrinkage parameter</p>\n"}
{"snippet": "tic = time.time() \nmask = nb.load(os.path.join(out_folder,'Masks','subcortex_drewUpdated.nii'))\nsvr = Predict(dat,Y,algorithm='svr',subject_id = holdout, \n              output_dir=out_folder, cv_dict = {'kfolds':5}, \n              mask = mask, **{'kernel':\"linear\"})\nsvr.predict()\nprint 'Elapsed: %.2f seconds' % (time.time() - tic) \n", "intent": "<p>You might be interested in only training a pattern on a subset of the brain using an anatomical mask.  Here we use a mask of subcortex.</p>\n"}
{"snippet": "predicted = text_clf.predict(docs_new)\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, twenty_train.target_names[category]))\n", "intent": "Retest on the previous example\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,target_names=twenty_test.target_names))\n", "intent": "scikit-learn further provides utilities for more detailed performance analysis of the results:\n"}
{"snippet": "twenty_train.target_names[gs_clf.predict(['God is loved by Michael'])[0]]\n", "intent": "The result of calling fit on a GridSearchCV object is a classifier that we can use to predict:\n"}
{"snippet": "def predict(row, coefficients):\n\tyhat = coefficients[0]\n\tfor i in range(len(row)-1):\n\t\tyhat += coefficients[i + 1] * row[i]\n\treturn yhat\ndataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6,6]]\ncoef = [0.5, 0.8]\nfor row in dataset:\n\tyhat = predict(row, coef)\n\tprint(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))\n", "intent": "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6,6]]\ncoef = [0.5, 0.8]\n"}
{"snippet": "def predict(row, coefficients):\n\tyhat = coefficients[0] + coefficients[1] * row[0]\n\treturn yhat\ndataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6, 6]]\ncoef = [0.5, 0.8]\nfor row in dataset:\n\tyhat = predict(row, coef)\n\tprint(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))\n", "intent": "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6,6]]\ncoef = [0.5, 0.8]\n"}
{"snippet": "y_subset_test_pred_prob = svc_poly_clf.predict_proba(X_subset_test)\ny_subset_test_pred = svc_poly_clf.predict(X_subset_test)\ny_subset_test_pred = pd.Series(y_subset_test_pred, index=X_subset_test.index)\n", "intent": "Predicting the test set with prediction probabilities:\n"}
{"snippet": "(svm_classifier.predict(X_train) == y_train.values).mean()\n", "intent": "10% accuracy...This seems wrong. \n"}
{"snippet": "path = saver.logdir + '/' + 'runlog_0.pkl'\nrunlog = load_runlog(path)\nplot_loss(runlog)\n", "intent": "Using these functions we can view the training loss over time:\n"}
{"snippet": "predictions = model.predict(data, verbose=1)\nprint(\"The first prediction is: {}\".format(predictions[0]))\n", "intent": "Then perform the prediction!\n"}
{"snippet": "output_test = iris_df.iloc[test_ix].copy()\noutput_test['prediction'] = output_test.apply(lambda x: rpart2.predict(x.to_dict()), axis = 1)\nconfusion_matrix(output_test.species.values, output_test.prediction.values)\n", "intent": "We can quickly predict on our holdout samples, and calculate the confusion matrix using from sklearn.metrics.confusion_matrix\n"}
{"snippet": "y_pred = sv.predict(Xtest)\nprint(y_pred)\nnp.shape(y_pred)\n", "intent": "**Classify (predict) the U.S. Atlantic Coast test data set**\n"}
{"snippet": "y_pred = sv.predict(Xtest)\nprint(y_pred)\nnp.shape(y_pred)\n", "intent": "**Classify the test data set**\n"}
{"snippet": "score = model.evaluate(X_test, Ytest, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Calculating the score and accuracy of cross-valication set:\n"}
{"snippet": "from sklearn.metrics import classification_report\nypred=model.predict(X_test)\nY=np.zeros((len(ypred),1))\nfor i in range(len(ypred)):\n    Y[i]=np.where(ypred[i,:] == np.max(ypred[i,:]))\nprint(classification_report(ytest, Y))\n", "intent": "Calulate the presecion, recal and f-score:\n"}
{"snippet": "ypred=model.predict(X_Test)\nY=np.zeros((len(ypred),1))\nfor i in range(len(ypred)):\n    Y[i]=np.where(ypred[i,:] == np.max(ypred[i,:]))\nnp.savetxt('submission6.csv',Y,delimiter=',',fmt='%.2f')\n", "intent": "Calculate the labels on the test set and save the result as CSV file:\n"}
{"snippet": "print(confusion_matrix(ytest, ypred))\nprint(classification_report(ytest, ypred))\n", "intent": "calculate the confusion matrix and classification report for the trained svm:\n"}
{"snippet": "Ypred=classifier.predict(XTest)\nnp.savetxt('submission.csv',Ypred,delimiter=',',fmt='%.2f')\n", "intent": "Predict the Kaggle competition Test set and save it in a CSV file:\n"}
{"snippet": "EXAMPLES = ['20 Aug 1991', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "df2[\"lm_pred\"] = (res21.predict()>0.5).astype(int)\n", "intent": "Which round up to 1, so this is bad prediction, 0 was expected ! But how do we behave on the entire dataset ?\n"}
{"snippet": "y_pred = grid.predict(X_test)\ngrid.score(X_test, y_test)\n", "intent": "<H3>Test score</H3>\n"}
{"snippet": "    y_pred[0] = (y_pred[0] > 0.5)*1\n    for n in range(4):\n        print (round(fbeta_score(y_true[0].iloc[:,n], y_pred[0][:,n], average='binary', beta=2),3))\n    print ('*'*20)\n    y_pred[1] = (y_pred[1] > 0.5)*1\n    for n in range(13):\n        print (round(fbeta_score(y_true[1].iloc[:,n], y_pred[1][:,n], average='binary', beta=2),3))\nprint ('F2 score ')        \nevaluation(y_pred_test,y_test)        \n", "intent": "- show confusion matrix for every label\n- calculate f2 score\n- tuning threshold \n- tho using sigmoid, you do not need \n"}
{"snippet": "score = VGG19_model.evaluate(test_VGG19, test_targets, verbose=0)\nprint('Test accuracy: {:.4f}%'.format(score[1]*100))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred_test = model.predict(emb_test)\nnp.savetxt(fname = os.path.join(PATH_TO_DATA, \"logreg_bov_y_test_sst.txt\"), X = pred_test)\n", "intent": "     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n     You will be evaluated on the results of the test set.\n"}
{"snippet": "sj_predictions = sj_df[['total_cases']].copy()\npredictions = LR_Pipeline.predict(sj_X_train_df.iloc[208:,:]).tolist() + \\\n              LR_Pipeline.predict(sj_X_test_df).tolist()\nsj_predictions['model_predictions'] = np.nan\nsj_predictions.loc['1994-04-30':,'model_predictions'] = [prediction[0] for prediction in predictions]\nplot_actual_vs_pred_line(sj_predictions, 'San Juan', \n                         'Multi Linear Regression model with 4-year dengue lag (no regularisation)',\n                        'upper right')\n", "intent": "Including the 4-year lagged dengue case counts has decreased the accuracy of the model on the test set.\n"}
{"snippet": "sj_predictions = sj_df[['total_cases']].copy()\npredictions = ETR_gs.predict(sj_X_train_df.iloc[:,:-1]).tolist() + \\\n              ETR_gs.predict(sj_X_test_df.iloc[:,:-1]).tolist()\nsj_predictions['model_predictions'] = predictions\nplot_actual_vs_pred_line(sj_predictions, 'San Juan', 'Extra Trees Regressor (with hyperparameters)', 'upper right')\n", "intent": "I'll now plot the predictions vs actuals to visualise the model performance.\n"}
{"snippet": "iq_predictions = iq_df[['total_cases']].copy()\npredictions = RFR_Pipeline.predict(iq_X_train_df).tolist() + \\\n              RFR_Pipeline.predict(iq_X_test_df).tolist()\niq_predictions['model_predictions'] = predictions\nplot_actual_vs_pred_line(iq_predictions, 'Iquitos', 'Random Forest Regressor (default)', 'upper left')\n", "intent": "I'll visualise the predictions to see if I can understand what's happeing here.\n"}
{"snippet": "iq_predictions = iq_df[['total_cases']].copy()\npredictions = ETR_Pipeline.predict(iq_X_train_df).tolist() + \\\n              ETR_Pipeline.predict(iq_X_test_df).tolist()\niq_predictions['model_predictions'] = predictions\nplot_actual_vs_pred_line(iq_predictions, 'Iquitos', 'Extra Trees Regressor (default)', 'upper left')\n", "intent": "I'll visualise the predictions.\n"}
{"snippet": "iq_predictions = iq_df[['total_cases']].copy()\npredictions = KNR_gs.predict(iq_X_train_df).tolist() + \\\n              KNR_gs.predict(iq_X_test_df).tolist()\niq_predictions['model_predictions'] = predictions\nplot_actual_vs_pred_line(iq_predictions, 'Iquitos', 'KNeighbors Regressor (with hyperparameters)', \n                        'upper left')\n", "intent": "Poor model performance\n"}
{"snippet": "print \"%.3f\"%accuracy_score(predictions, y)\n", "intent": "accuracy of predicions\n"}
{"snippet": "s = cross_val_score(est, X, y, cv=KFold(10, shuffle=True), scoring=make_scorer(accuracy_score))\nprint s\nprint \"accuracy %.3f (+/- %.5f)\"%(np.mean(s), np.std(s))\n", "intent": "let's use cross validation and build more confidence in our results\n"}
{"snippet": "def tpr(est,X,y):\n    p = est.predict(X)\n    return np.mean (p[y==1] == y[y==1])\ndef tnr(est,X,y):\n    p = est.predict(X)\n    return np.mean(p[y==0] == y[y==0])\n", "intent": "let's understand better how this estimator is performing.\n- TPR: true positive rate\n- TNR: true negative rate\n"}
{"snippet": "s = cross_val_score(p, X, y, cv=KFold(10, shuffle=True), scoring=make_scorer(accuracy_score))\nprint \"accuracy %.3f (+/- %.5f)\"%(np.mean(s), np.std(s))\n", "intent": "now with cross validation\n"}
{"snippet": "cross_val_score(XGBRegressor(), train, y, cv = 3).mean()\n", "intent": "Create a baseline using the default paramters of XGBoost.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint ('Test Accuracy : %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "new_cust = np.array([100,0,50,0,0,0]).reshape(1,-1)\nnew_cust_clv = clf.predict(new_cust)\nprint 'The CLV for this customer is:', new_cust_clv[0],'$'\n", "intent": "Let us say we have a new customer who in his first 3 months have spend 100,0,50 on your website. Let us use the model to predict his CLV.\n"}
{"snippet": "results = m.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))\n", "intent": "After the model is trained, we can evaluate how good our model is at predicting the labels of the holdout data:\n"}
{"snippet": "base_df['cluster'] = k_means.predict(base_df)\nbase_df.head()\n", "intent": "Finally, getting the clusters is as easy as calling `.predict()` on the dataset.\n**Let's save the clusters to our dataframe.**\n"}
{"snippet": "threshold_df['cluster'] = k_means.predict(threshold_df)\nthreshold_df.head()\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">threshold_df</code>.**\n"}
{"snippet": "pca_df['cluster'] = k_means.predict(pca_df)\npca_df.head()\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">pca_df</code>.**\n"}
{"snippet": "adjusted_rand_score(threshold_df.cluster, base_df.cluster)\n", "intent": "Furthermore, the adjusted Rand index is **symmetric**, which means you can pass in the clusters in any order.\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n    source = source[np.newaxis, :]\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_scores = cross_val_score(model, X, y, cv=5)  \ncv_scores.mean()\n", "intent": "Let's see what a 5 fold CV score mean looks like\n"}
{"snippet": "y_pred = model.predict(X) \n", "intent": "**3) Generate the predicted y values given the fitted model. Save these as y_pred**\n(model.predict)\n"}
{"snippet": "cross_val_dtree = cross_val_score(tree, wine_data, wine_labels, cv=4)\ncross_val_dtree\n", "intent": "And let's cross-validate again. Let's only run it four times.\n"}
{"snippet": "cross_val_nb = cross_val_score(gnb, wine_data, wine_labels, cv=4)\ncross_val_nb\n", "intent": "And of course, let's cross-validate to see how well we did. Let's only run it four times.\n"}
{"snippet": "adjusted_rand_score(base_df_cluster, threshold_cluster)\n", "intent": "** Compare Clusters **\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(valid_y, pred))\nprint(rms)\n", "intent": "We can now calculate the RMSE on the predictions.\n"}
{"snippet": "preds = happyModel.evaluate(x = X_test, y = Y_test)\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Note that if you run `fit()` again, the `model` will continue to train with the parameters it has already learnt instead of reinitializing them.\n"}
{"snippet": "print classification_report(y_test, gs.predict(X_test))\n", "intent": "The multinomial naive bayer model was evaluated using the classification report. The model performance is pretty good.\n"}
{"snippet": "print\"confusion matrix\\n\",confusion_matrix(y_test_exam,y_pred),\"\\n\"\nprint\"classification report\\n\",classification_report(y_test_exam,y_pred)\n", "intent": "Now, we evaluate the classification results by several metrics, first, we use confusion matrix and classification report\n"}
{"snippet": "test = pd.concat([X_test, y_test], axis=1)\ny_pred = test.apply(lambda x: predict(model, x.userId, x.itemId), axis=1)\nMSE = mean_squared_error(y_true = y_test.values, y_pred = y_pred)\nMSE**(0.5)\n", "intent": "Test the classifier on the test data\n"}
{"snippet": "output = model.predict(test_data[:,1:])\n", "intent": "test the test data on the model(omitting the appid from the test)\n"}
{"snippet": "tl_predictions = [np.argmax(tl_model.predict(np.expand_dims(feature, axis=0))) for feature in test_tl]\ntest_accuracy = 100*np.sum(np.array(tl_predictions)==np.argmax(test_targets, axis=1))/len(tl_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(model.predict(np.array([[0, 0]])))\n", "intent": "The trained model can be use to classify new samples \nusing `predict`:\n"}
{"snippet": "def pred_fun(x1, x2):\n        xval = np.array([[x1, x2]])\n        return model.predict(xval)[0, 0]\npl.figure(figsize = (8,16/3))    \nplot_decision_region(x_train, pred_fun)\nplot_data(x_train, y_train)\n", "intent": "This is the decision region of the trained model:\n"}
{"snippet": "preds = model.predict(x)\nprint('Predicted:', decode_predictions(preds, top=5)[0])\n", "intent": "The top choice is indeed Granny_Smith with probability 0.9996773.\n"}
{"snippet": "pred = model.predict_proba(augmented_data)\nprint(pred[:5])\n", "intent": "**Predict probabilities for <code style=\"color:steelblue\">augmented_data</code> using your model.**\n* Then print the first 5 predictions.\n"}
{"snippet": "base_df['cluster'] = k_means.predict(base_df)\nbase_df.head()\n", "intent": "**Let's save the clusters to our dataframe.**\n"}
{"snippet": "prediction = model.predict(test_images)\nprediction = np.array(prediction)\n", "intent": "Now we can take some time to analyse the accuracy of the model, in particular where the errors in the model are and why.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet_50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def class_metrics(y_test, y_preds):\n    print('Accuracy score: ', format(accuracy_score(y_test,y_preds)))\n    print('Precision score: ', format(precision_score(y_test, y_preds)))\n    print('Recall score: ', format(recall_score(y_test, y_preds)))\n    print('F1 score: ', format(f1_score(y_test,y_preds)))\n", "intent": "These are convenience functions to output the class metrics and ROC curve: \n"}
{"snippet": "Inception_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100 * np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "jmodel_predictions = [np.argmax(jmodel.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(jmodel_predictions)==np.argmax(test_targets, axis=1))/len(jmodel_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def content_loss(current, computed):\n    _, height, width, number = computed.shape\n    size = height * width * number\n    return tf.sqrt(tf.nn.l2_loss(current - computed) / size)\n", "intent": "Difference between content layers of result image and content layers of content image.\n"}
{"snippet": "resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0)))\n                       for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions) ==\n                          np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('\\nTest Accuracy: %4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = clf.predict(test_features_scaled)\n", "intent": "Now we're going to test the model by using it to predict the class labels of the test data!\n"}
{"snippet": "predictions = clf3.predict(test_features)\n", "intent": "Now we're going to test the model by using it to predict the class labels of the test data!\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n", "intent": "Finall,y we compute the fraction of correctly labeled points\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n", "intent": "Finally, we compoute the fraction of correctly labeled points:\n"}
{"snippet": "false_pos = sum(model_5.predict(validation_data.drop(\"safe_loans\",1)) > validation_data[\"safe_loans\"])\nfalse_pos\n", "intent": "Calculate the number of false positives made by the model on the validation_data.\n"}
{"snippet": "false_neg = sum(model_5.predict(validation_data.drop(\"safe_loans\",1)) < validation_data[\"safe_loans\"])\nfalse_neg\n", "intent": "Calculate the number of false negatives made by the model on the validation_data.\n"}
{"snippet": "test_loss = model.evaluate(X_test, y_test, batch_size = 64, verbose=1)\nprint 'Test Loss: ', test_loss\n", "intent": "<H4>Verify the accuracy of the model on test set</H4>\n"}
{"snippet": "test_loss = model1.evaluate(X_test, y_test, batch_size = 64, verbose=1)\nprint 'Test Loss:', test_loss\nclasses = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nY_test = np.dot(y_test, classes).astype(int)\ny_predict1 = model1.predict_classes(X_test, verbose = 1)\ntest_accuracy1 = accuracy_score(Y_test, y_predict1)\nprint 'Test Accuracy:', test_accuracy1 \n", "intent": "<H4>Verify the accuracy of the model on test set</H4>\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted))\n", "intent": "To get all the above metrics at one go, use the following function:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    total_loss = tf.constant(0.0)\n    for i, layer_idx in enumerate(style_layers):\n        total_loss += style_weights[i] * tf.reduce_sum( (gram_matrix(feats[layer_idx]) - style_targets[i]) ** 2 )\n    return total_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    total_loss = tf.constant(0.0)\n    for l in style_layers:\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "nn.predict(sess, images)\n", "intent": "Now the prediction should give better results\n"}
{"snippet": "def score_func(y_test, y_pred):\n    return np.sqrt(mean_squared_error(y_test, y_pred))\nscorer = make_scorer(score_func, greater_is_better = False)\n", "intent": "We should define some useful functions, which will help us in models fitting.\n"}
{"snippet": "test_x = pad(preproc_english_sentences_test, max_french_sequence_length_test)\ntest_x = test_x.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\npreproc_french_sentences_test = preproc_french_sentences_test.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\nsimple_rnn_model_score = simple_rnn_model_split.evaluate(test_x, preproc_french_sentences_test, verbose=0)\nprint(\"Simple model accuracy on unseen test dataset: {0:.2f}%\".format(simple_rnn_model_score[1]*100))\n", "intent": "See how the model trained on the training set data performs on the unseen test data.\n"}
{"snippet": "test_x = pad(preproc_english_sentences_test, max_french_sequence_length_test)\ntest_x = test_x.reshape((-1, preproc_french_sentences_train.shape[-2]))\nembedding_model_score = embedding_model_split_train.evaluate(test_x, preproc_french_sentences_test, verbose=0)\nprint(\"Embedding model accuracy on unseen test data: {0:.2f}%\".format(embedding_model_score[1]*100))\n", "intent": "See how the model trained on the training set data performs on the unseen test data.\n"}
{"snippet": "test_x = pad(preproc_english_sentences_test, max_french_sequence_length_test)\ntest_x = test_x.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\nbidirectional_model_score = bidirectional_model_train.evaluate(test_x, preproc_french_sentences_test, verbose=0)\nprint(\"Bidirectional model accuracy on test data: {0:.2f}%\".format(bidirectional_model_score[1]*100))\n", "intent": "See how the model trained on the training set data performs on the unseen test data.\n"}
{"snippet": "test_x = pad(preproc_english_sentences_test, max_french_sequence_length_test)\ntest_x = test_x.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\npreproc_french_sentences_test = preproc_french_sentences_test.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\nprint(test_x.shape)\nprint(preproc_french_sentences_train.shape)\nprint(preproc_french_sentences_test.shape)\nencoder_decoder_model_score = encoder_decoder_model_train.evaluate(test_x, preproc_french_sentences_test, verbose=0)\nprint(\"Encoder-decoder model accuracy on the unseen test data: {0:.2f}%\".format(encoder_decoder_model_score[1]*100))\n", "intent": "Use the unseen test data set to evaluate the enc-dec model trained on the training data.\n"}
{"snippet": "test_x = pad(preproc_english_sentences_test, max_french_sequence_length_test)\ntest_x = test_x.reshape((-1, preproc_french_sentences_train.shape[-2]))\npreproc_french_sentences_test = preproc_french_sentences_test.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\nprint(test_x.shape)\nprint(preproc_french_sentences_train.shape)\nprint(preproc_french_sentences_test.shape)\nfinal_model_test_score = final_model_train.evaluate(test_x, preproc_french_sentences_test, verbose=0)\nprint(\"Final model accuracy on test dataset: {0:.2f}%\".format(final_model_test_score[1]*100))\n", "intent": "See how the model trained on the training set data performs on the unseen test data.\n"}
{"snippet": "mu_mse = np.zeros_like(mse_pars)\nstd_mse = np.zeros_like(mse_pars)\nfor i, g in enumerate(g_vals):\n    for j, k in enumerate(k_vals):\n        instance = np.log10([[g, k]])\n        mu, sigma = gaussian_process.predict(instance, return_std=True)\n        mu_mse[i, j] = mu[:]\n        std_mse[i, j] = sigma[:]\n", "intent": "Explore whole parameter space.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_predict = clf.predict(X_train)                   \nprint ('Training accuracy = ', accuracy_score(y_train, y_predict))\nclass_labels = clf.get_classes()     \nprint ('Class labels=', class_labels)\nprint ('Confusion matrix [known in lines, predicted in columns]=\\n',confusion_matrix(y_train, y_predict, class_labels))\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "import libscores\nY_train, C = libscores.onehot(y_train)                                   \nprint ('Dimensions Y_train=', Y_train.shape, 'Class labels=', C)\nfrom libscores import bac_metric \nfrom libscores import pac_metric \nfrom libscores import log_loss\ny_predict_proba = clf.predict_proba(X_train)      \nprint ('Training balanced accuracy = ', bac_metric(Y_train, y_predict_proba, task='multiclass.classification'))\nprint ('Training probabilistic accuracy = ', pac_metric(Y_train, y_predict_proba, task='multiclass.classification'))\nprint ('Training log loss = ', log_loss(Y_train, y_predict_proba, task='multiclass.classification'))\n", "intent": "ADVANCED: Sklearn does not have multi-class metrics, this shows how libscore metrics work.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        current_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((current_gram - style_targets[i])**2)\n    return style_loss  \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred=gbm_clf.predict(X_test)\n", "intent": "The next step is to get the predictions from our trained model. We will get the predictions as well as the probabilities:\n"}
{"snippet": "myclass.predict(X)\n", "intent": "And then you can proceed to predicting the target responses:\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "Let's evaluate our model performance by calculating the residual sum of squares and the explained variance score (R^2).\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "** Now take that grid model and create some predictions using the test set and create classification reports and confusion matrices for them.**\n"}
{"snippet": "keypoints = predict(img)\n", "intent": "The output in this case is a matrix with the heatmaps for all the 17 keypoints. Each heatmap is a mask with the resolution of W=96 and H=72\n"}
{"snippet": "y_predict = gmm.predict(x)\n", "intent": "The trained model is used to predict anomalous data points,\n"}
{"snippet": "predictions = svc.predict(X_test)\nprint(predictions)\n", "intent": "Here are the predictions:\n"}
{"snippet": "x_test = np.c_[4, 2.3].T\ny_test = lr.predict(x_test)\nprint x_test.T\nprint y_test\n", "intent": "We can use the method `predict()` to predict `y` for a new `x`\n"}
{"snippet": "yhat_test = lr.predict(test_X)\n", "intent": "Now that we have obtained the model parameters, we can use the model to predict for unseen data:\n"}
{"snippet": "from sklearn import metrics\nprint(\"DecisionTrees's Accuracy: \"), metrics.accuracy_score(y_testset, predTree)\n", "intent": "Next, let's import metrics from sklearn and check the accuracy of our model.\n"}
{"snippet": "predForest = skullsForest.predict(X_testset)\n", "intent": "Let's now create a variable called <b>predForest</b> using a predict on <b>X_testset</b> with <b>skullsForest</b>.\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\naccuracy_score(y_true, y_pred)\naccuracy_score(y_true, y_pred, normalize=False)\n", "intent": "sklearn.metrics.accuracy_score\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\ny_predict = slr.predict(X_test) \nmean_squared_error = mean_squared_error(y_test,y_predict) \nr2_score = r2_score(y_test,y_predict) \nprint ('mean square error:',mean_squared_error )\nprint ('r square:',r2_score )\n", "intent": "Let's calculate the [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n"}
{"snippet": "network_predictions = [np.argmax(network_model.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\nnetwork_test_accuracy = 100*np.sum(np.array(network_predictions)==np.argmax(test_targets, axis=1))/len(network_predictions)\nprint('Test accuracy with', network,': %.4f%%' % network_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "grader.set_answer(\"i8bgs\", s.run(loss, {input_X: X_val_flat, input_y: y_val_oh}))\ngrader.set_answer(\"rE763\", accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})))\n", "intent": "Run these cells after training a 2-layer MLP\n"}
{"snippet": "grader.set_answer(\"i8bgs\", s.run(loss, {input_X: X_val_flat, input_y: y_val_oh}))\ngrader.set_answer(\"rE763\", accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})))\n", "intent": "Run these cells after training the MLP with 2 hidden layers\n"}
{"snippet": "dnn = DNNClassifier(num_classes=10)\ncheckpoint = tf.train.Checkpoint(dnn=dnn)\ncheckpoint.restore(save_path='../graphs/lecture05/applied_example_wkde/dnn-epoch-10-counter-2')\nyhat = np.argmax(dnn.predict(x=x_tst), axis=-1)\nprint('test acc: {:.2%}'.format(np.mean(yhat == y_tst)))\n", "intent": "Restore my model at epoch 10\n"}
{"snippet": "metrics.evaluate('accuracy')\n", "intent": "Now we can easiliy calculate any metrics we need\n"}
{"snippet": "def loss_poisson(target, predictions):\n    print(target, predictions)\n    loss = tf.reduce_mean(tf.nn.log_poisson_loss(target, predictions))\n    tf.losses.add_loss(loss)\n    return loss\n", "intent": "We have to create our own loss function:\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - Y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(X_test, Y_test))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "net_predictions = [np.argmax(net_model.predict(np.expand_dims(feature, axis=0))) for feature in test_net]\ntest_accuracy = 100*np.sum(np.array(net_predictions)==np.argmax(test_targets, axis=1))/len(net_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = logitm_model.predict(x_test)\nfalse_negative_indices = [i for i, x in enumerate(y_test) if ((y_pred[i] == 0) and (y_test[i] == 1))]\nfalse_positive_indices = [i for i, x in enumerate(y_test) if ((y_pred[i] == 1) and (y_test[i] == 0))]\nprint 'Accuracy of the model', logitm_model.score(x_test, y_test)\nprint 'Number of false positives', len(false_positive_indices)\nprint 'Number of false negatives', len(false_negative_indices)\n", "intent": "After the model parameters have been determined, the next step is to make predictions on the testing set. \n"}
{"snippet": "y_pred_log = logitm_model.predict(x_poly_test)\ny_pred_lda = lda_model.predict(x_test)\ny_pred_qda = qda_model.predict(x_test)\n", "intent": "After the model parameters have been determined, the next step is to make predictions on the testing set. \n"}
{"snippet": "predictions=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.pipeline import Pipeline   \nfrom sklearn.metrics import classification_report,confusion_matrix\npredictions=n.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(classification_report(y_test,predictions))\nprint('                              ')\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "vgg.predict(imgs, details=True)\n", "intent": "Data is loaded, model is prepared. Let's try predict\n"}
{"snippet": "y_score = model.predict(x_test)\nloss, acc = model.evaluate(x_test, y_test, verbose=0)\nprint('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n", "intent": "Next, evaluate our model\n"}
{"snippet": "print(topics[np.argmax(model.predict([get_tf_record(\"US warships sail near disputed islands in South China Sea in a move likely to anger Beijing\")]))])\n", "intent": "Here the Accuracy of the model is pretty low due to the training dataset size\nSome of the predictions are wrong\n"}
{"snippet": "for x_split in X_test:\n    print(x_split[0])\n    print(topic[np.argmax(model.predict([get_tf_record(x_split[0])]))])\n    print(\"=\"*85)\n", "intent": "Here the Accuracy of the model is pretty low due to the training dataset size\nSome of the predictions are wrong\n"}
{"snippet": "dt_scores = cross_val_score(decision_tree_classifier, all_inputs, all_classes, cv = 10)\nsb.boxplot(dt_scores)\nsb.stripplot(dt_scores, jitter=True, color='white')\n", "intent": "Now that we have our classfier ready, let's visualize its performance.\n"}
{"snippet": "import json\ny_est_test = dTree.predict(X_test)\ntest_accu = accuracy_score(y_est_test, y_test)\nprint('test_accu', test_accu)\n", "intent": "- do prediction on test dataset.\n"}
{"snippet": "y_est_test = dTree.predict(X_test)\ntest_accu = accuracy_score(y_est_test, y_test)\nprint('test_accu', test_accu)\n", "intent": "Test your prediction accuracy on validation dataset after pruning.\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "* Showing a sample of 36 test images and the corresponding results\n* T:   Ground Truth\n* P:   Predicted Result\n"}
{"snippet": "print(r2_score(y_test, y_pred))\nprint(mean_squared_error(y_test, y_pred))\n", "intent": "Now we can use the metrics we imported earlier to evaluate our model performance.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Question 5: Evaluate your model** \n"}
{"snippet": "model.predict(test_set.X)\n", "intent": "We can also retrieve individual trial predictions as such:\n"}
{"snippet": "list(model.predict(X_test)[0:5])\n", "intent": "This can easily be seen by simply running the code:\n"}
{"snippet": "np.mean((model.predict(X_test) - y_test) **2)\n", "intent": "Note: You can also use Mean Squared Error, which is RSS divided by the degrees of freedom. But I find it helpful to think in terms of RSS.\n"}
{"snippet": "def custom_metric(y_test, y_pred):\n    r2 = r2_score(y_test, y_pred)\n    return r2\n", "intent": "For this example we are just calculating the r-squared score, but we can see that any calculation can be used.\n"}
{"snippet": "trained_model.predict_proba(x_test)\n", "intent": "We can even look at the probabilities the learner assigned to each class:\n"}
{"snippet": "preds = iris.target_names[clf.predict(test[features])]\n", "intent": "Now that we have predicted the species of all plants in the test data, we can compare our predicted species with the that plant's actual species.\n"}
{"snippet": "y_true = y_test\ny_preds = []\nfor clf in clfs:\n    y_preds.append(clf.predict(X_test_final))\n", "intent": "Visualize confusion matrix and show classification report\n"}
{"snippet": "for y_pred, clf in zip(y_preds, clfs):\n    print type(clf).__name__\n    print classification_report(y_true, y_pred, target_names=None)\n", "intent": "Show scoring like precision, recall, f1 and their average for each label\n"}
{"snippet": "y_pred = clf.predict(X_new_rfe)\nCounter(y_pred)\n", "intent": "Predict using trained models\n"}
{"snippet": "probs = clf.predict_proba(X_plagiarized)\ndef get_predicted_labels(probs, k):\n    return probs.argsort(axis=1)[:,-k:]\n", "intent": "We are finding whether or not any predicted class is within the true labels\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        G = style_targets[i]\n        A = gram_matrix(model.extract_features()[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((G-A)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "some_messages = ['You make money!!! ', \"hello can we talk later\"]\nexample_counts = vectorizer.transform(some_messages)\npredictions = classifier.predict(example_counts)\npredictions\n", "intent": "now let's try it with some messages \n"}
{"snippet": "exple_message= [str(input(\"give us a message to classify :\" ))]\nexple_count = vectorizer.transform(exple_message)\npre = classifier.predict(exple_count)\npre[0]\n", "intent": "run this code and give your own message so the model predict if it's spam or not \n"}
{"snippet": "logreg = LogReg(C = 1, class_weight='balanced')\nlogreg.fit (x_train, y_train)\ny_predlog = logreg.predict_proba(x_test)\nR2_log = logreg.score(x_test,y_test) \nprint \"Accuracy of the test set for log. reg. is: \", np.round(R2_log,2)\n", "intent": "The first model we will try is logistic regression. \n"}
{"snippet": "reconstructed_text = model.predict(x_test)\nreconstructed_text[0]\n", "intent": "Let's take a look at one of the reconstructed responses:\n"}
{"snippet": "score = model.evaluate(x_test, y_test)\nprint(\"Test score:\", score[0])\nprint(\"Test accuracy:\", score[1])\n", "intent": "Let's see how well our model performed.\n"}
{"snippet": "score = model.evaluate(x_test, y_test)\nprint(\"Test score:\", score[0])\nprint(\"Test accuracy:\", score[1])\n", "intent": "Let's see how well our model ended up doing.\n"}
{"snippet": "score = model.evaluate(x_test, y_test)\nprint(\"Test score:\", score[0])\nprint(\"Test accuracy:\", score[1])\nresults = model.predict(x_test)\n", "intent": "Let's see how well our model did.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now we are going to evaluate our model by comparing our predicted data vs test data\n"}
{"snippet": "predictions = logm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predict = avg_predict(df.iloc[:,[6,7,11]], ols_fit, knn_fit, rmse_ols, rmse_knn)\nrmse_2nd = np.sqrt(np.sum((predict - df[\"2ndHalfK%\"])**2) / len(df[\"2ndHalfK%\"]))\nprint('RMSE: '+str(rmse_2nd))\ndf['predict'] = predict\ndf.iloc[:,[0,15,17]]\n", "intent": "Now that we've settled on a prediction model, let's see how it does against the second-half K%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nimport numpy as np\naccuracy_score(df_test.passenger_count, \n               np.ones_like(df_test.passenger_count))\n", "intent": "OK, 65% accuracy isn't bad.  \nBut really, always guessing a single passenger wouldn't be that much worse.\n"}
{"snippet": "def predict(est, X):\n    return est.predict(X[columns])\npredictions = [e.submit(predict, est, test) for est in estimators]\nprogress(predictions, complete=False)\n", "intent": "We'll use `e.submit(function, *args)` in a loop to submit more tasks\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_val_score\nprint('Accuracy: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='accuracy', cv=3)))\nprint('Precision: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='precision', cv=3)))\nprint('Recall: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='recall', cv=3)))\nprint('F1: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='f1', cv=3)))\n", "intent": "<b>144</b> were predicted right, while <b>19</b> were predicted wrong. 144/163 = <b>0.88</b>\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_val_score\nprint('Accuracy: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='accuracy', cv=3)))\nprint('Precision: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='precision', cv=3)))\nprint('Recall: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='recall', cv=3)))\nprint('F1: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='f1', cv=3)))\n", "intent": "<b>136</b> were predicted right, while <b>27</b> were predicted wrong. 136/163 = <b>0.83</b>\n"}
{"snippet": "accuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint('Accuracy: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='accuracy', cv=3)))\nprint('Precision: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='precision', cv=3)))\nprint('Recall: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='recall', cv=3)))\nprint('F1: '+ str(cross_val_score(log_reg, X_train, y_train.ravel(), scoring='f1', cv=3)))\n", "intent": "<b>Cross validation: precision, accuracy, recall and f1<b>\n"}
{"snippet": "model_name.append(\"Backward/Mat\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/Mat\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end\n"}
{"snippet": "mse_ols = mean_squared_error(y_true=y_test, y_pred=y_pred)\nprint ('MSE value for OLS : ', mse_ols)\nr2 = r2_score(y_true = y_test, y_pred = y_pred)\nprint ('R-square score for OLS : ', r2)\n", "intent": "LinearRegression() from the sklearn.LinearModel already uses the least square method. So we can directly use that to define our model.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions:', lin_reg.predict(some_data_prepared))\nprint(\"------------------\")\nprint(\"Labels:\", list(some_labels))\n", "intent": "We can now test out the performance of the regression model on some data\n"}
{"snippet": "housing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(lin_rmse)\n", "intent": "We can measure the regression model's RMSE on the whole training set using Scikit-Learn's `mean_squared_error` function\n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(tree_rmse)\n", "intent": "Evaluate the model on the training set\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Let's compare to the score for the linear regression model\n"}
{"snippet": "cv_score = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')\nprint(\"The cross-validation scores on each of the folds are:\")\nfor i, score in enumerate(cv_score, start=1):\n    print(\"Fold {}: {}\".format(i, score))\n", "intent": "We first compute the accuracy (ratio of correct predictions) using a 3-fold cross-validation.\n"}
{"snippet": "print(\"The precision score of our classifier is\", precision_score(y_train_5, y_train_pred))\nprint(\"------------------\")\nprint(\"The recall score of our classifier is\", recall_score(y_train_5, y_train_pred))\n", "intent": "We can compute the precision and recall directly using Scikit-Learn's built-in `precision_score()` and `recall_score()` functions.\n"}
{"snippet": "print(\"The area under the ROC curve for the Random Forest classifier is\", roc_auc_score(y_train_5, y_scores_forest))\n", "intent": "The ROC curve for the random forest classifier looks much better! We can compare the two classifiers using the AUC score\n"}
{"snippet": "some_digit_probas = forest_clf.predict_proba([some_digit])[0]\nprint(some_digit_probas)\nprint(\"------------------\")\nprint(\"The classifier predicts that the instance is in class 5 with probability\", some_digit_probas[5])\n", "intent": "We can access the probabilities which the Random Forest classifier assigned to the instance for each class using the `predict_proba()` method.\n"}
{"snippet": "print(cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy'))\n", "intent": "We again use cross-validation to evaluate the performance of the multiclass classifiers. For the SGD classifier we find\n"}
{"snippet": "print(\"The instance belongs to class\", svm_clf.predict([[5.5, 1.7]])[0])\n", "intent": "Let's predict the class of an insance with petal length 5.5cm and petal width 1.7cm.\n"}
{"snippet": "print(tree_clf.predict_proba([[5, 1.5]]))\n", "intent": "We can predict the probabilities that a new instance belongs to each class by calling the classifier's `predict_proba` method\n"}
{"snippet": "print(\"The classifier predicts that the instance belongs to class\", tree_clf.predict([[5, 1.5]])[0])\n", "intent": "The predicted class is the one with the highest probability\n"}
{"snippet": "y_pred = bag_clf.predict(X_test)\nprint(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))\n", "intent": "We can compare this to the actual accuracy on the test set\n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\nprint(\"Prediction\", y_pred[0])\n", "intent": "The ensemble can now make a prediction for a new instance by adding up the predictions made by each of the three trees in the ensemble.\n"}
{"snippet": "print(\"The reconstruction pre-image error is\", mean_squared_error(X, X_preimage))\n", "intent": "We can then compute the reconstruction pre-image error\n"}
{"snippet": "reg.predict([[1, 0.0]])\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "clf.predict([[2.0, 2.0]])\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "print(accuracy_score(lr.predict(x_test), y_test))\n", "intent": "* Next I re-print the test accuracy from the testing data using auc\n"}
{"snippet": "xgmat = xgb.DMatrix( toy_test_x, missing = -999.0 )\npred = bst.predict( xgmat )\n", "intent": "With a fitted model, we can make predictions for the test data. Again, we need to provide the data as data matrix.\n"}
{"snippet": "(sample_validation_data['healthy'] == decision_tree_model.predict(sample_validation_data)).sum()/float(len(sample_validation_data))\n", "intent": "What percentage of the predictions on sample_validation_data did decision_tree_model get correct?\n"}
{"snippet": "decision_tree_model.predict(sample_validation_data, output_type='probability')\n", "intent": "For each row in the sample_validation_data, what is the probability (according decision_tree_model) of a cuisine being classified as healthy?\n"}
{"snippet": "small_model.predict(sample_validation_data[1])\n", "intent": "Now, let's verify your prediction by examining the prediction made using GraphLab Create\n"}
{"snippet": "print big_model.evaluate(train_data)['accuracy']\nprint big_model.evaluate(validation_data)['accuracy']\n", "intent": "Now, let us evaluate big_model on the training set and validation set.\n"}
{"snippet": "len(sample_validation_data[sample_validation_data['healthy'] == model_5.predict(sample_validation_data)])/float(len(sample_validation_data))\n", "intent": "What percentage of the predictions on sample_validation_data did model_5 get correct?\n"}
{"snippet": "model_5.predict(sample_validation_data, output_type='probability')\n", "intent": "For each row in the sample_validation_data, what is the probability (according model_5) of a cuisine being classified as healthy?\n"}
{"snippet": "y_pred = clf3.predict(X_test3)\nnumpy.sqrt(mean_squared_error(y_test, y_pred))\n", "intent": "The final RMSE score of the model is:\n"}
{"snippet": "cv_score = cross_validation.cross_val_score(model, X, y, scoring='roc_auc', cv=5)\n", "intent": "Computing the model's AUC using 5 fold cross validation\n"}
{"snippet": "from sklearn.metrics import roc_curve\ny_pred = model.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred)\n", "intent": "Based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n"}
{"snippet": "r2_score(Y_test,lg.predict(X_test))\n", "intent": "**Thus we obtain an accuracy score of 96% in the Dataset during Training. Lets check the testing scores.**\n"}
{"snippet": "rmse = (sum((regressor.predict(test[[\"value\"]]) - test[\"next_day\"])**2) / float(predictions.shape[0])) ** .5\nmae = (sum(abs(regressor.predict(test[[\"value\"]]) - test[\"next_day\"])) / float(predictions.shape[0]))\n", "intent": "wo other commonly used error metrics are root mean squared error, or RMSE, and mean absolute error, or MAE.\nRMSE is just the square root of MSE.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=0\n    for i in range(len(style_layers)):\n        idx=style_layers[i]\n        src=gram_matrix(feats[idx])\n        dst=style_targets[i]\n        style_loss+=style_weights[i]*tf.reduce_sum(tf.squared_difference(src,dst))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "acc = accuracy_score(y_test,y_pred)\nprint('Accuracy Score of Classifier: ',acc)\n", "intent": "So, our Classifier is 95% confident of its predictions. That is what we saw above where only 6 values were misclassified from 140. Pretty good.\n"}
{"snippet": "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\nprint (cross_val_score(ExponentialCrossVal(), X_train, y, scoring = 'neg_mean_squared_error', cv = cv))\n", "intent": "We will consider the metric `neg_mean_squared_error`. For k cross validation, we will shuffle the input data using `ShuffleSplit`.\n"}
{"snippet": "def answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    knn = answer_five()\n    prediction = knn.predict(X_test)\n    key = np.array(y_test.reset_index(drop=True))\n    accuracy = sum(key == prediction) / len(key)\n    return accuracy\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_avg = silhouette_score(pick_drop_df[['Pickup Latitude','Pickup Longitude']].values, pick_drop_df['label_pick'])\nprint(\"For n_clusters =\", 2,\"The average silhouette_score is :\", silhouette_avg)\n", "intent": "**Silhouette score is used to determine the K value in the K means algorithm and here it is 2 because all the other K have low scores**\n"}
{"snippet": "predicted = lrm.predict(X_test.values.reshape(-1, 1))\n", "intent": "Now we use the fitted model to perform predictions on our test set. \n"}
{"snippet": "np.array_equal(eclf1.named_estimators_.lr.predict(X),\n               eclf1.named_estimators_['lr'].predict(X))\n", "intent": "We can use dot notation or JSON notation to access the classification steps inside our `VotingClassifier`\n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Accuracy = {:.2f}\".format(accuracy))\n", "intent": "Run the code below.\n1. What is meant by loss?\n2. What is meant by accuracy?\n"}
{"snippet": "loss, acc = model.evaluate(X_dev.reshape(22,8267,101), Y_dev.reshape(22,2064,1))\nprint(\"Dev set accuracy = \", acc)\n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "y_pred = trn.predict(x_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_vgg19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "[_describe how you are going to evaluate the results of your classifier and what it means_]\n"}
{"snippet": "experiment_data = event_data_for_comparison(\"gs://kubeflow-rl-checkpoints/comparisons/cs-pt*\")\nshow_experiment_loss(experiment_data)\n", "intent": "Taking another look at the loss plots:\n"}
{"snippet": "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n", "intent": "We can easily check by evaluating on the test set:\n"}
{"snippet": "prices = model.predict(x_test, batch_size=128)\n", "intent": "In addition to this, you can always use your model to make predictions in the future:\n"}
{"snippet": "scores = cross_val_score(\n     clf, iris.data, iris.target, cv=5, scoring='f1_macro')\nscores\n", "intent": "By looking at the mean of the scores and their standard deviation, I expect to at least do better than 93% (97% - 4%) on the actual test data!\n"}
{"snippet": "scores = cross_val_score(mlp, X, y, cv=10)    \n", "intent": "It performs 5-fold cross validation, returning five test scores.\n"}
{"snippet": "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))\n", "intent": "We now compute some commonly used measures of prediction \"goodness\".\nFor more detail on these measures see [6],[7],[8],[9]\n"}
{"snippet": "C = 5\ny_test_oh = np.eye(C)[Y_test.reshape(-1)]\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\npred = model.predict(X_test_indices)\nfor i in range(len(X_test)):\n    num = np.argmax(pred[i])\n    if(num != Y_test[i]):\n        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())\n", "intent": "You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nknn_confusion_matrix = confusion_matrix(y_true = y_test, y_pred = loan_knn.predict(X_test))\nprint(\"The Confusion matrix:\\n\", knn_confusion_matrix)\n", "intent": "**Confusion Matrix**\n"}
{"snippet": "prediction_rfc = rfc.predict(X_test) \n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,prediction_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "forecast_set = clf.predict(X_test)\nnum_samples = df.shape[0]\ndf['Forecast'] = np.nan\ndf['Forecast'][int(0.9*num_samples):num_samples]=forecast_set\n", "intent": "the first varible is negative because the model can be arbitrarily worse\n"}
{"snippet": "import numpy as np\nSSreg = np.mean((regr.predict(X_test) - y_test) ** 2)\nSStot =  np.mean((y_test - np.mean(y_test)) ** 2)\nprint(\"R^2: %.2f\" % (1 - SSreg/SStot))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.4f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n      style_loss += style_weights[i] * torch.sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    content_image = 'styles/tubingen.jpg'\n    image_size =  192\n    tv_weight = 2e-2\n    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Error is {:.4f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.4f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.0001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.4f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n      style_loss += style_weights[i] * tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.4f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "from sklearn.metrics import r2_score\nrf_predicted_train=rf.predict(Xrf_train)\nrf_predicted_test=rf.predict(Xrf_test)\nrf_test_score=r2_score(yrf_test, rf_predicted_test)\nprint('Test Data R^2 Score:', rf_test_score)\n", "intent": "~80% accuracy looks pretty good! \n"}
{"snippet": "shap_values = model.predict(X.as_matrix(), pred_contrib=True)\n", "intent": "Here we use the Tree SHAP implementation integrated into Light GBM to explain the entire dataset (32561 samples).\n"}
{"snippet": "y_pred = clf.predict_proba(X_test)[:,1]\n", "intent": "** Test the model**\n"}
{"snippet": "y_pred_bin = clf.predict(X_test)\n", "intent": "The random forest model has achieved an AUC of 80.4%.\n"}
{"snippet": "y_pred_bin = logit.predict(X_test)\n", "intent": "The logistic regression model has achieved an AUC of 72.3%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint(test_accuracy)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "def predict(theta, X):\n    h_theta = h_of_theta(theta, X)\n    return np.round(h_theta)\np = predict(optimal_theta,X)\nprint '\\nTraining Accuracy: ', np.mean((p == y) * 100)\n", "intent": "Let's compute the accuracy on the training set:\n"}
{"snippet": "final_model = grid_search.best_estimator_\nprint('Final Model Parameters:\\n')\npprint(final_model.get_params())\nprint('\\n')\ngrid_final_accuracy = evaluate(final_model, test_features, test_labels)\n", "intent": "The final model from hyperparameter tuning is as follows.\n"}
{"snippet": "print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "The model needs more work.  The median income error probably has less impact at the higher income ranges than the lower ones.\n"}
{"snippet": "predictions = logr.predict(X_test)\n", "intent": "Our newly trained model can now be used to make predictions on our test data.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import StratifiedKFold\nk_fold = KFold(n_splits=5) \nprint 'logistric regression:', np.mean(cross_val_score(logreg, X_train, y_train, cv=k_fold, scoring='roc_auc'))\nprint 'decision tree:', np.mean(cross_val_score(dtree, X_train, y_train, cv=k_fold, scoring='roc_auc'))\nprint 'random forest:', np.mean(cross_val_score(rf, X_train, y_train, cv=k_fold, scoring='roc_auc'))\nprint 'gradient boosting:', np.mean(cross_val_score(gradbclass, X_train, y_train, cv=k_fold, scoring='roc_auc'))\n", "intent": "**Cross-Validation**\n"}
{"snippet": "print(X_test_pad.shape,y_test.shape)\nscores = model.evaluate(X_test_pad, y_test, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "def rmsle(y, yhat):\n    return np.sqrt(mean_squared_log_error(y, yhat))\nmean_error = []\nfor ym in df2['yearmonth'].unique().tolist():\n    r = df2[df2['yearmonth'] == ym]\n    p = r['last_sales'].values\n    err = rmsle(r['sales'].values, p)\n    print('%d : %.5f' % (ym, err))\n    mean_error.append(err)\nprint(np.mean(mean_error))\n", "intent": "Use Root Mean Squared Log Error as a benchmark. \n"}
{"snippet": "train_MSE2= np.mean((y_train - regr.predict(x_train))**2)\ntest_MSE2= np.mean((y_test - regr.predict(x_test))**2)\nprint(\"The training MSE is %2f, the testing MSE is %2f\" %(train_MSE2, test_MSE2))\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_train, regr.predict(x_train)))\nprint(mean_squared_error(y_test, regr.predict(x_test)))\n", "intent": "We can now estimate the mean square errors for the training and test set to see how well we fit the data with our model in each case.\n"}
{"snippet": "train_loss, train_acc = network.evaluate(train_images, train_labels)\n", "intent": "Just for comparison, let's explicitly evaluate the model performance information on the training data:\n"}
{"snippet": "y = model.predict(x)\nprint(np.argmax(y, axis=1))\n", "intent": "In this notebook, it's not really trained to predict\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model\n"}
{"snippet": "from sklearn import metrics\nprint('MAE: ', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE: ', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "Let's evaluate our model performance by calculating the residual sum of squares and the explained variance score (R^2).\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "We can check precision,recall,f1-score using classification report!\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nmens_xgb_model = XGBClassifier(n_estimators=100, random_state= 1, learning_rate= 0.1, \n                               subsample= 0.9, colsample_bytree= 1.0, \n                               eval_metric= 'mlogloss', max_depth= 5, \n                               min_child_weight= 1, gamma= 0)\ny_pred = cross_val_predict(mens_xgb_model, mens_train_X, mens_train_y,cv=4)\nconfusion_matrix(mens_train_y,y_pred)\n", "intent": "---\nUsing the XGBoost model, we perform cross validation (4x) and output the confusion matrix to see which examples the model is misclassifying.\n"}
{"snippet": "y_pred_consonants = Best_top_classifier_consonants.predict(extract_bottleneck_features(X_test_consonants))\nprint(\"Accuracy on test set for consonants: {}\".format(accuracy_score([np.argmax(i) for i in y_test_consonants], y_pred_consonants)))\ny_pred_vowels = Best_top_classifier_vowels.predict(extract_bottleneck_features(X_test_vowels))\nprint(\"Accuracy on test set for vowels: {}\".format(accuracy_score([np.argmax(i) for i in y_test_vowels], y_pred_vowels)))\ny_pred_numerals = Best_top_classifier_numerals.predict(extract_bottleneck_features(X_test_numerals))\nprint(\"Accuracy on test set for numerals: {}\".format(accuracy_score([np.argmax(i) for i in y_test_numerals], y_pred_numerals)))\n", "intent": "We can now assess the accuracy of our three specialized classifiers on their respective testing sets:\n"}
{"snippet": "y_pred = lr.predict(X_test)\ny_pred\n", "intent": "Then, to make predictions on new data (here: predicting the brain weight based on the head size), we use the `predict` method:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n", "intent": "Note that other performance metrics are available from sckit-learn's `metrics` module, http://scikit-learn.org/stable/modules/classes.html\n"}
{"snippet": "used_feature_names = bag_feature_names + numerical_feature_names\nfeatures = get_features_by_names(name_to_feature_dict, used_feature_names, scale_feature_names=numerical_feature_names)\nprint 'feature shape:', features.shape\nmodel = XGBClassifier(**{'n_estimators': 200, 'max_depth': 13, 'gamma': 13, 'min_child_weight': 6})\ntrain_and_evaluate(model, features, data, train_indexes, valid_indexes, test_indexes)\n", "intent": "Trained on training set\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0022.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "predictions = classifier.predict(X_test)\n", "intent": "Let's use our trained model to predict whether the customers from the test set will default or not and evaluate the accuracy of our model.\n"}
{"snippet": "[mean_absolute_error(tree.predict(X_test), y_test) for tree in rf.estimators_]\n", "intent": "Check MAE of each single tree\n"}
{"snippet": "input_text = \"Which movie's plot would change if you removed a letter from its title?\"\nencoded_text = encode_text(input_text, maxlen)\nmodel.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])\n", "intent": "So can see if we can to perform better than 0.67 probability. Make minor changes without avertly changing meaning.\n"}
{"snippet": "input_text = \"Which movie's plot would change if you remove a letter from its title?\"\nencoded_text = encode_text(input_text, maxlen)\nmodel.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])\n", "intent": "Removing \"drastically\" makes things better.\nHow about fixing the tense of \"removed\"?\n"}
{"snippet": "input_text = \"What movie's plot would change if you remove a letter from its title?\"\nencoded_text = encode_text(input_text, maxlen)\nmodel.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])\n", "intent": "Probability increased. From that headline, how about changing \"Which\" to \"What?\"\n"}
{"snippet": "input_text = \"What movie's plot would change if you remove a single letter from its title?\"\nencoded_text = encode_text(input_text, maxlen)\nmodel.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])\n", "intent": "What about using more emphasis: add \"single\" to letter.\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "- Uses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": "- Returns a NumPy array\n- Compute classification accuracy for the model\n"}
{"snippet": "weights = tf.ones([batch_size, sequence_length])\nseq_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nloss = tf.reduce_mean(seq_loss)\noptm  = tf.train.AdamOptimizer(learning_rate=lrate).minimize(loss)\nprint (\"Tensorflow Functions Defined.\")\n", "intent": "This stuff defines optimization algorithm, loss function and weights initialization.\n"}
{"snippet": "sim_preds_gmm = sklearn.mixture.GMM(n_components=3).fit_predict(sim_points)\nplot_simulated(sim_points, sim_preds_gmm, sim_labels, 'EM on mixture of Gaussians')\n", "intent": "Now that we understand we can cluster data using expectation maximization on a mixture of Gaussians, let's try running it on a test dataset.\n"}
{"snippet": "import sklearn.metrics\ndef print_metrics(data, classes, clusters):\n  homogeneity, completeness, v_measure = sklearn.metrics.homogeneity_completeness_v_measure(classes, clusters)\n  print('Homogeneity, completeness, v_measure:', (homogeneity, completeness, v_measure))\n  print('Mean silhouette score:', sklearn.metrics.silhouette_score(data, clusters))\n", "intent": "Let's first define a `print_metrics` helper function to calculate and print our various measures of cluster quality.\n"}
{"snippet": "def predict(alpha, beta, x_i):\n    return beta * x_i + alpha\n", "intent": "First lets define the function which will abstractely represent the modeled data, in the form of a linear function, with the form y=beta*x+alpha\n"}
{"snippet": "accuracy_toxic_n = accuracy_score(test_offensive_df['toxic'].values, predicted_labels_df['toxic'].values)\naccuracy_severe_toxic_n = accuracy_score(test_offensive_df['severe_toxic'].values, predicted_labels_df['severe_toxic'].values)\naccuracy_obscene_n = accuracy_score(test_offensive_df['obscene'].values, predicted_labels_df['obscene'].values)\naccuracy_threat_n = accuracy_score(test_offensive_df['threat'].values, predicted_labels_df['threat'].values)\naccuracy_insult_n = accuracy_score(test_offensive_df['insult'].values, predicted_labels_df['insult'].values)\naccuracy_identity_hate_n = accuracy_score(test_offensive_df['identity_hate'].values, predicted_labels_df['identity_hate'].values)\n", "intent": "Calculate new predictions for toxic comments. But I already anticipate the result...\n"}
{"snippet": "import keras\nyp = clf.predict(X_dev)\nner.save_predictions(yp, \"dev.predicted\")\n", "intent": "Evaluate the model on the dev set using your `predict` function, and compute performance metrics below!\n"}
{"snippet": "test_ID, drop_lst = getID([\"Look What You Made Me Do\"])\ntest = sp.audio_features(test_ID)[0]\nX = pd.Series(test)[['acousticness', 'danceability', 'duration_ms', 'energy', \n                'instrumentalness', 'liveness', 'loudness', 'mode',\n                'speechiness', 'tempo', 'valence', 'key']]\ngbdt.predict(X)\n", "intent": "You can test any song you like here.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    ls = 0\n    for i in range(len(style_layers)):\n        F = feats[style_layers[i]]\n        G = gram_matrix(F)\n        wc = style_weights[i]\n        ls += content_loss(wc,style_targets[i],G)\n    return ls\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "dense_preds = dense_model.predict(norm_in_data[test_indices])[:, 0]\nconv_preds = conv_model.predict(norm_in_data[test_indices])[:, 0]\n", "intent": "Make predictions with each model.\n"}
{"snippet": "gs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=2,\n                  n_jobs=-1)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\nprint('CV accuracy: %.f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "> Algorithm selection with nested cross-validation\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n", "intent": "> Precision and recall\n"}
{"snippet": "for model_name in [\"Logistic\", \"LinearSVC\"]:\n    print(\"Confusion matrix for %s\" % model_name)\n    print(metrics.classification_report(y_test, prediction[model_name], target_names = [\"positive\", \"negative\"]))\n    print()\n", "intent": "I looks like the best are LogisticRegression and LinearSVC. Let's see the accuracy, recall and confusion matrix for these models:\n"}
{"snippet": "socres,acc = model.evaluate(x_test,y_test,\n                            batch_size = batch_size,\n                            verbose=1)\nprint('Test Accuracy: %.2f%%' % (acc*100))\n", "intent": "This will give you the accuracy and scores of the model, as evaluated on the testing set. \n"}
{"snippet": "pred = list(classifier.predict(x=X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "target_prob = classifier.predict_proba(test_X)\n", "intent": "In this part, we want to use ROC curve to test whether our classification is good or not\n"}
{"snippet": "print \"MSE = \"+ str(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "result = model.predict(df_test)\n", "intent": "predict the Loan_Status according to our model.\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall_score(y_test, y_predict, average=\"micro\")\n", "intent": "same to recall_score\n"}
{"snippet": "Y_predict = classifier.predict(X_test)\n", "intent": "Use trained neural network to predict test dataset\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    score = r2_score(y_true, y_predict) \n    return score\n", "intent": "Use `r2_score` from `sklearn.metrics` to perform a performance calculation between `y_true` and `y_predict`.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, s in enumerate(style_layers):\n        layer_gram = gram_matrix(feats[s])\n        target_gram = style_targets[i]\n        layer_loss = style_weights[i] * tf.reduce_sum(tf.square(layer_gram - target_gram))\n        style_loss += layer_loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "scores = model.evaluate(X_test, Y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Now that we've trained the model above, it's time to see how the model performs on data it hasn't trained on:\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nnp_array_prob = prob.as_data_frame().as_matrix()\nnp_array_test = test[response].as_data_frame().as_matrix()\nprobInPy = np_array_prob\nlabeInPy = np_array_test\nroc_auc_score(labeInPy, probInPy)\n", "intent": "We can bring those ensemble predictions to our Python session's memory space and use other Python packages.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nresults_predicted=clf.predict(X_val_pca)\ncnf_matrix=confusion_matrix(testY, results_predicted, labels=[0,1,2,3,4,5,6,7,8,9])\n", "intent": "References - http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n"}
{"snippet": "y_pred = regr.predict(xts)\n", "intent": "After fitting the linear model, we predict it using test data.\n"}
{"snippet": "logisticRegr.predict(x_test[0].reshape(1,-1))\n", "intent": "Uses the information the model learned during the model training process\n"}
{"snippet": "logisticRegr.predict(test_img_PCA[0].reshape(1,-1))\n", "intent": "Uses the information the model learned during the model training process\n"}
{"snippet": "from sklearn import metrics\nsample_size = 50\nX_train = train_dataset[:sample_size].reshape(sample_size, 784)\ny_train = train_labels[:sample_size]\npredicted = regr.predict(X_test)\ncm = metrics.confusion_matrix(y_test, predicted)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "intent": "<h3 align='Left'>50 Training Samples</h3>\n"}
{"snippet": "import numpy as np\nfrom sklearn import metrics\nprediction = classifier.predict(features_test_transformed)\nfscore = metrics.f1_score(labels_test, prediction, average='macro')\nprint(\"F score {:.2f}\".format(fscore))\n", "intent": "Calcular el Score F1 https://en.wikipedia.org/wiki/F1_score\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Inception V3 Test accuracy: %.4f%%' % test_accuracy)\nresnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Resnet 50 Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import r2_score\nxi=np.array([90,106,105,115,113])\nyi=np.array([103,131,85,99,144])\ny_pred=B2+B1*xi\nRSS=sum(pow((y_pred-yi),2))\nTSS=sum(pow((yi-np.mean(yi)),2))\nR2=1-RSS/TSS\nprint(R2)\nprint(r2_score(yi,y_pred))\n", "intent": "|Xi|Yi|\n|:--:|:-------------------------------:|\n|90|103|\n|106|131|\n|105|85|\n|115|99|\n|113|144|\n"}
{"snippet": "ypred = clf.predict(Xtest)\nprint(confusion_matrix(ytest, ypred))\n", "intent": "This single number doesn't tell us **where** we've gone wrong: one nice way to do this is to use the confusion matrix\n"}
{"snippet": "pred = ens.predict(xtest)\n", "intent": "Predictions are generated as usual:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.cast(0, tf.float32)\n    for i in range(len(style_layers)):\n        s_target = style_targets[i]\n        s = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(tf.subtract(s_target, s)))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred_grid = ridge_grid.predict(X)\nprint('MSE (whole training set) = {:.4f}'.format(mean_squared_error(y, y_pred_grid)))\nprint('RMSE (whole training set) = {:.4f}'.format(np.sqrt(mean_squared_error(y, y_pred_grid))))\n", "intent": "Entire training set results\n"}
{"snippet": "y_pred_grid = tree_reg_grid.predict(X)\nprint('MSE (whole training set) = {:.4f}'.format(mean_squared_error(y, y_pred_grid)))\nprint('RMSE (whole training set) = {:.4f}'.format(np.sqrt(mean_squared_error(y, y_pred_grid))))\n", "intent": "Performance on entire training set\n"}
{"snippet": "y_pred_grid = rfr_grid.predict(X)\nprint('MSE (whole training set) = {:.4f}'.format(mean_squared_error(y, y_pred_grid)))\nprint('RMSE (whole training set) = {:.4f}'.format(np.sqrt(mean_squared_error(y, y_pred_grid))))\n", "intent": "Performance on entire training set\n"}
{"snippet": "gbrt_pred = gbrt_stack.predict(X_2)\nridge_pred = ridge_stack.predict(X_2)\nX_blender = np.stack((gbrt_pred, ridge_pred), axis=1)\n", "intent": "Make predictions on 2nd half\n"}
{"snippet": "y_pred = model_blr_grid.predict(X_blender_poly)\nprint('\\nMSE (whole training set) = {:.4f}'.format(mean_squared_error(y_2, y_pred)))\nprint('RMSE (whole training set) = {:.4f}'.format(np.sqrt(mean_squared_error(y_2, y_pred))))\n", "intent": "Performance on entire training set\n"}
{"snippet": "-cross_val_score(model, X, y, cv=4, scoring='log_loss').mean()\n", "intent": "**Accuracy: 79.7% **\n"}
{"snippet": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nX = df_pima.iloc[:,0:8]\ny = df_pima.iloc[:,8]\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=3, random_state=7)\nresult = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(result.mean())\n", "intent": "Since the values in the dataset have NANs, the model can be implemented efficiently\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.):\n    residuals = tf.abs(predictions - labels)\n    condition = tf.less(residuals, delta)\n    small_res = 0.5*tf.square(residuals)\n    large_res = delta*residuals - 0.5*tf.square(delta)\n    return tf.where(condition, small_res, large_res)\n", "intent": "$$L_\\delta(y, f(x)) = \\frac{1}{2}(y - f(x))^2\\ \\ for\\ \\ |y - f(x)| < \\delta$$\n$$L_\\delta(y, f(x)) = \\delta|y - f(x)| - \\frac{1}{2}\\delta^2 $$\n"}
{"snippet": "loss, acc=model.evaluate(word_seq_test,test['final_status'],verbose=0)\n", "intent": "Make predictions on the test data\n"}
{"snippet": "network_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(network_predictions)==np.argmax(test_targets, axis=1))/len(network_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,log_test))\n", "intent": "To optimize our classifer, we implemented a grid search cross-validation, which resulted in an 'C' parameter of 0.01 and a best score of 0.523.\n"}
{"snippet": "print(classification_report(y_test,svm_test))\n", "intent": "To optimize our classifer, we implemented a grid search cross-validation, which resulted in an 'C' parameter of 1 and a best score of 0.526\n"}
{"snippet": "print(classification_report(y_test,rf_test))\n", "intent": "The accuracy of our classifer is [(740 + 3942) / 6000] = 78%, which infers a misclassifed percentage of 22%.\n"}
{"snippet": "print(classification_report(y_test,NN_test))\n", "intent": "Based on the confusion matrix, the accuracy of our classifer is [(770 + 3839) / 6000] = 72%, which infers a misclassifed percentage of 28%.\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>Incorrectly Classified point</h5>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>Inorrectly Classified point</h4>\n"}
{"snippet": "prefix = \"I have lots of\".split()\nnoun = \"toys\"\nadjectives = [\"round\", \"green\", \"plastic\"]\ninputs = []\nfor adjs in itertools.permutations(adjectives):\n  words = prefix + list(adjs) + [noun]\n  inputs.append(words)\nload_and_score(inputs, sort=True)\n", "intent": "The model now picks the correct order.\n"}
{"snippet": "prefix = \"I have lots of\".split()\nnoun = \"toys\"\nadjectives = [\"small\", \"green\", \"plastic\"]\ninputs = []\nfor adjs in itertools.permutations(adjectives):\n  words = prefix + list(adjs) + [noun]\n  inputs.append(words)\nprint inputs\nload_and_score(inputs, sort=True)\n", "intent": "Same thing here for 'round'.\n"}
{"snippet": "print \"Predicted: \"\nprint model.predict(X_test[[0]])\nprint \"Actual: \"\nprint Y_test[0]\n", "intent": "Let's test our model on one of the test data that we set aside earlier:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = model.predict(X_test)\nprint accuracy_score(Y_test, predictions)\n", "intent": "Awesome! Our model got it right! Let's try this on all of the test data and calculate the accuracy:\n"}
{"snippet": "data =   [\n          [   25,       2,       2,     15,          0,          0,          1,            0,           0 ], \n          [   76,       1,       0,     65,          1,          0,          0,            1,           0 ], \n          [    5,       4,       2,      0,          0,          1,          0,            0,           1 ]  \n] \npredictions = model.predict(np.array(data))\nprint([\"survives\" if x == 1 else \"dies\" for x in predictions])\n", "intent": "we can use the trained model to make predictions about new or even hypothetical passengers\n"}
{"snippet": "printCM(cvy, clf.predict(cvx))\n", "intent": "Training accuracy is a respectable 73%, but both precision and recall are lower than that number.\n"}
{"snippet": "def regScore(trainy, predy):\n    print(\"R^2: \" + str(m.r2_score(trainy, predy)))\n    print(\"MSE: \" + str(m.mean_squared_error(trainy, predy)))\n    print(\"MAE: \" + str(m.mean_absolute_error(trainy, predy)))\n", "intent": "Let's define a function to analyze metrics for our regressions.\n"}
{"snippet": "y_pred = regressor.predict(X_test)\ny_pred\n", "intent": "Now we will be predicting the values of y based on the X_test which will result in a vector y containing the profits predicted by the regressor\n"}
{"snippet": "y_pred = regressor.predict(X_test)\nprint(y_pred)\n", "intent": "Now we will be predicting the values of y based on the X_test which will result in a vector y containing the salaries predicted by the regressor \n"}
{"snippet": "df_Senators['resultsNN']=clf2.predict(df_Senators.loc[:,vecIndex])\n", "intent": "Sentiment analysis predictions\n"}
{"snippet": "def object_predictions(model, obj):\n    print('Probability of class 1 = {:.4f}'.format(model.predict_proba([obj])[0][1]))\n    print('Formula raw prediction = {:.4f}'.format(model.predict([obj], prediction_type='RawFormulaVal')[0]))\n", "intent": "https://github.com/slundberg/shap\n"}
{"snippet": "ESS_PRICE_PTRATIO=np.sum((m_PRICE_PTRATIO.predict(bos.PTRATIO) - np.mean(bos.PRICE)) ** 2)\nprint('ESS_PRICE_PTRATIO:', ESS_PRICE_PTRATIO) \nRSS_PRICE_PTRATIO=np.sum((bos.PRICE - m_PRICE_PTRATIO.predict(bos.PTRATIO)) ** 2)\nprint('RSS_PRICE_PTRATIO:', RSS_PRICE_PTRATIO) \nTSS_PRICE_PTRATIO=ESS_PRICE_PTRATIO + RSS_PRICE_PTRATIO\nprint('TSS_PRICE_PTRATIO:', TSS_PRICE_PTRATIO) \nR_sqr_PRICE_PTRATIO=ESS_PRICE_PTRATIO/TSS_PRICE_PTRATIO\nprint('\\n')\nprint('R_sqr_PRICE_PTRATIO: %.3f' %(R_sqr_PRICE_PTRATIO))\n", "intent": "<p><b>(b) Exercise:</b> Calculate (or extract) the $R^2$ value. What does it tell you?</p>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncf=confusion_matrix(ytest, clf.predict(xtest))\nprint(cf)\n", "intent": "- With an accuracy score of 0.73, this classifier in not better than the above.\n"}
{"snippet": "pred = gbc.predict_proba(X_test)\npred = [p[1] for p in pred]\nprint('Area under the curve: %.2f' %(roc_auc_score(y_test, pred)))\n", "intent": "- The above graph shows that the most important feature is phone_Android, followed by avgdist, as well as weekday_pct. \n"}
{"snippet": "print(\"Evaluating the model on validation data...\")\nY_validate_preds_rnn = rnn_model.predict(X_validate, batch_size=BATCH_SIZE)\nprint(\" RMSLE error:\", RMSLE(Y_validate, Y_validate_preds_rnn))\n", "intent": "<a id=\"Model_Evaluation\"></a>\n"}
{"snippet": "rnn_preds = rnn_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\nrnn_preds = np.expm1(rnn_preds)\n", "intent": "<a id=\"Model_Output\"></a>\n"}
{"snippet": "Y_dev_preds_ridge = ridge_model.predict(X_dev)\nY_dev_preds_ridge = Y_validation_preds_ridge.reshape(-1, 1)\nprint(\"RMSL error on validation set:\", rmsle(Y_dev, Y_dev_preds_ridge))\n", "intent": "<a id=\"Model_Evaluation1\"></a>\n"}
{"snippet": "ridge_preds = ridge_model.predict(X_test)\nridge_preds = np.expm1(ridge_preds)\nridgeCV_preds = ridge_modelCV.predict(X_test)\nridgeCV_preds = np.expm1(ridgeCV_preds)\n", "intent": "<a id=\"Model_Output1\"></a>\n"}
{"snippet": "def generate_term_vector(sift_picture, k, vocab_classifier):\n    tv = [0 for i in range(k)]\n    features = np.hsplit(sift_picture, sift_picture.size/128)\n    for feature in features:\n        result = vocab_classifier.predict(feature.reshape(1,-1))\n        tv[int(result)] += 1\n    return tv\n    raise NotImplementedError\n", "intent": "Now you will create a term vector for one of the training images. \n"}
{"snippet": "y_predict = clf.predict(X_test)\ny_predict\n", "intent": "Now we can predict the production of hydro run-of-river by using the fitted model:\n"}
{"snippet": "model.compile_loss()\nn_sentences = 100\nin_sentences = np.array([[np.random.randint(n_vocab) \n                              for i in range(n_words_in_sent)]\n                                                for j in range(n_sentences)])\nout_sentences = np.array([[np.random.randint(n_vocab) \n                              for i in range(n_words_in_sent)]\n                                                for j in range(n_sentences)])\nmodel.loss(in_sentences,out_sentences)\n", "intent": "Let's make some fake data and evaluate the loss:\n"}
{"snippet": "y2_pred = best_model.predict(X2_test)\npredictions = [round(value) for value in y2_pred]\naccuracy = accuracy_score(y2_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))t\n", "intent": "We have achieved perfect score on train test area.\n"}
{"snippet": "Inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.5f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred = rf.predict(X_test)\nprecision = precision_score(y_pred=pred, y_true=y_test)\nrecall = recall_score(y_pred=pred, y_true=y_test)\nf1 = f1_score(y_pred=pred, y_true=y_test)\naccuracy = accuracy_score(y_pred=pred, y_true=y_test)\nconfusion = confusion_matrix(y_pred=pred, y_true=y_test)\nprint('precision: {0:1.2f}, recall: {1:1.2f}, f1: {2:1.2f}, accuracy: {3:1.2f}'.format(precision, recall, f1, accuracy))\nprint('Confusion Matrix')\nprint(confusion)\n", "intent": "The expected results for a 10 days prediction according to the paper in table 15 for Apple stock should be around 92%\n"}
{"snippet": "def loss(x, y, input_shape):\n    out1 = forward(x, input_shape)\n    return cross_entropy_loss(y, out1)\n    pass\n", "intent": "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network.\n"}
{"snippet": "X_tst = scale(X_test[features])\nX_test['prediction_1'] = clf_1.predict(X_tst)\nX_test['prediction_2'] = clf_2.predict(X_tst)\nX_test['prediction_3'] = clf_3.predict(X_tst)\nprint (\"The deed is done.\")\n", "intent": "<h2>Final Prediction on Train/Test</h2>\n"}
{"snippet": "scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=5)\nprint scores\nprint scores.mean()\n", "intent": "Our model has the greatest accuracy when the threshold is set at 0.6, if we make it higher or lower the accuracy decreases.\n"}
{"snippet": "print metrics.classification_report(y_test,dfTest['ThreshSurvive'])\n", "intent": "The cross validated model has a lower score than the model we were already using. \n"}
{"snippet": "predictions = np.where(y_pred>=0.3,1,0)\nprint_conf(confusion_matrix(y_train, predictions))\nprint(\"F1:\",f1_score(y_train, predictions))\n", "intent": "This could be easily improved (in terms of F1) by manipulating the cut-off threshold. For exemple:\n"}
{"snippet": "threshold = 0.29\npredictions = np.where(y_pred>threshold,1,0)\nprint_conf(confusion_matrix(y_train, predictions))\nprint(\"F1:\",f1_score(y_train, predictions))\n", "intent": "Let's see confusion matrix for the chosen model:\n"}
{"snippet": "print(clf.predict(scaler.transform([[4.7, 3.1]]))[0])\n", "intent": "8\\. Predict results using test data\n"}
{"snippet": "from sklearn import metrics\ny_train_pred = clf.predict(X_train)\nmetrics.accuracy_score(y_train,y_train_pred)\n", "intent": "10\\. Evaluating results\n - accuracy = proportion of correct\n"}
{"snippet": "y_pred = clf.predict(X_test)\nmetrics.accuracy_score(y_test,y_pred)\n", "intent": "80% are correct for the training set...overfits, now try test\n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n    return(\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores))\nprint(mean_score(scores))\n", "intent": "These are the k=5 scores, we can calc mean, stderr\n"}
{"snippet": "keras_model_predictions = keras_model.predict({'sequence':one_hot_data}, batch_size=20)['output']\n", "intent": "Let's obtain the corresponding predictions from the original keras model\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for style_layer, style_target, style_weight in zip(style_layers, style_targets, style_weights):\n        gram_features = gram_matrix(feats[style_layer])\n        style_loss += style_weight * tf.reduce_sum((gram_features - style_target) * (gram_features - style_target))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "train_conv_out = conv_model.predict(train_data)\nval_conv_out = conv_model.predict(val_data)\n", "intent": "<span style=\"color:red\"> Warning: These two following operations are computationally costly. Run on server. </span>\n"}
{"snippet": "train_proba = train_fit.predict_proba(train_set)\ntrain_proba[:3]\n", "intent": "We can also predict the probabilities that are assigned to each institution using the `predict.proba()` method.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, college_prep, college_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Computing the the K-fold cross validation scores for the linear regression model yields similar results as our first linear regression model.\n"}
{"snippet": "decisiontree_model_predictions = tree_reg.predict(x_test_prepared)\ntree_mse = mean_squared_error(y_test, decisiontree_model_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "As we can see from our linear regression model, the RMSE is higher than our random forest model on our test set.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5]) \n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0 \n    idx = 0\n    for l in style_layers:\n        style_gram = gram_matrix(feats[l])\n        diff_matrix = style_gram - style_targets[idx] \n        style_loss += style_weights[idx] * l2_norm_sqr(diff_matrix)\n        idx += 1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    student_output = tv_loss_for_loop(content_img_test, tv_weight)\n    error = rel_error(correct, student_output)\n    print('For Loop Error is {:.3f}'.format(error))\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Vectorization Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "y_pred = finalmod.predict_proba(X_test)\ny_pred = y_pred[:, 1]\n", "intent": "To generate the final csv for the submission\n"}
{"snippet": "print(\"Training error: \", np.mean((regr.predict(X_train) - y_train) ** 2))\nprint(\"Test     error: \", np.mean((regr.predict(X_test) - y_test) ** 2))\n", "intent": "Now we calculate the mean square error on the test set\n"}
{"snippet": "y_pred_train = logreg.predict(X_train)\n", "intent": "First, we have a look at how logistic regression did on the training set.\nWe do this by now setting colors using the predicted class.\n"}
{"snippet": "y_pred_test = logreg.predict(X_test)\nprint(\"Accuracy on test set:\", logreg.score(X_test, y_test))\n", "intent": "1.0 means an accuracy of 100%, just as it looked like.\nNow let us have a look at how the algorithm generalizes to the test data.\n"}
{"snippet": "np.mean(((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def classification_metrics(y_true, y_pred_proba, threshold=0.5):\n    y_pred = np.where(y_pred_proba > threshold, 1, 0)\n    cm_dict = {}\n    cm_dict['Accuracy'] = accuracy_score(y_true, y_pred)\n    cm_dict['Precision'] =  precision_score(y_true, y_pred)\n    cm_dict['Recall'] =  recall_score(y_true, y_pred)\n    cm_dict['F1'] =  f1_score(y_true, y_pred) \n    cm_dict['AUC'] = roc_auc_score(y_true, y_pred_proba)\n    cm_dict['Confusion Matrix'] = confusion_matrix(y_true, y_pred).tolist()\n    return cm_dict\n", "intent": "You will use a helper function called classification_metircs to calculate the evaluation metrics for each fold. \n"}
{"snippet": "for train_index, test_index in tqdm(skf.split(features, labels)):\n    X_train, X_test = features[train_index], features[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]\n    lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n    clf = lgb.train(params, lgb_train, num_boost_round=500)\n    y_pred_proba = clf.predict(X_test)\n    cm_dict = classification_metrics(y_test, y_pred_proba)\n    print(cm_dict)\n    cv_results = cv_results.append(classification_metrics(y_test, y_pred_proba),ignore_index=True)\n", "intent": "Below, you will train your classifer for each fold and print the metrics.\n"}
{"snippet": "vgg2.predict(imgs, True)\n", "intent": "Without finetuning, the model vgg2 outputs the probability of 1000 classes. As you can see below, they are sometimes totally irrelevant.\n"}
{"snippet": "print(\"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2))\nprint(\"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2))\n", "intent": "They are roughly similar. The training set got a better MSE however. Perhaps there is a better model for a better test accuracy.\n"}
{"snippet": "k1pred = knn1.predict(Xtest)\n", "intent": "Now we have our classifer built from the training set, we will attempt a prediction on the test/validation set.\n"}
{"snippet": "print metrics.classification_report(Ytest, k1pred)\n", "intent": "Here is the output from this first attempt at a classifer.\nA combined f1-score of 78 is not bad, but lets see if we can improve on this...\n"}
{"snippet": "sklearn.metrics.mean_squared_error(bos.PRICE,lm.predict(X))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def generate_z(n_samples, z_dim=100):\n    return np.random.uniform(0, 1, size=[n_samples, z_dim])\ndef generate_fake_images(generator, n_samples, z_dim=100):\n    return generator.predict(\n        generate_z(n_samples, z_dim),\n        batch_size=250,\n        verbose=1,\n    )\n", "intent": "- Function to create noise input for the generator.\n- Function to create fake_images from the generator.\n"}
{"snippet": "def compute_square_loss(X, y, theta):\n    res1 = np.dot(X, theta) - y\n    res2 = np.dot(np.transpose(res1), res1)\n    loss = (1/(2 * X.shape[0])) * res2 \n    return loss\n", "intent": "5.Modify the function `compute_square_loss`, to compute $J(\\theta)$ for a given $\\theta$.\n"}
{"snippet": "svc.predict([q])\n", "intent": "Let's check the numerical values of the prediction for point $q$:\n"}
{"snippet": "evaluation = selected_model.evaluate(testing_data_padded_tokens, outputs_for_testing)\n", "intent": "After training the model, its accuracy on the test set is calculated.\n"}
{"snippet": "predicted_test_set = selected_model.predict(x=testing_data_padded_tokens[0:1000])\npredicted_test_set = predicted_test_set.T[0]\n", "intent": "To show a Mis-Classified Text example, the predicted sentiment for the first 1000 texts in the testing set is calculated.\n"}
{"snippet": "selected_model.predict(tokens_padded_NewText)\n", "intent": "Prediction on New Texts Using the Trained Model\n"}
{"snippet": "Predict_EN = np.expm1(favoriteClf_EN.predict(X_test_scaler))\n", "intent": "Lets transform the predictions back to the real values by applying the exponential function\n"}
{"snippet": "Predict_RFR = np.expm1(clf.predict(X_test))\n", "intent": "Using random forest we are going to predict the house prices and store it in another submission file\n"}
{"snippet": "pred_train=model_logit.predict(X_train)\n", "intent": "**Performing Predictions**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,clf_rf.predict(X_test)))\n", "intent": "Percision and Recall scores for random forest 2\n"}
{"snippet": "y_test_predictions = regressor_lm.predict(x_test)\ny_test_predictions\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Linear Regression Model\n<br><br></p>\n"}
{"snippet": "y_test_predictions_tree = regressor_t.predict(x_test)\ny_test_predictions_tree\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Decision Tree Regressor\n<br><br></p>\n"}
{"snippet": "def rel_error(x, y):\n", "intent": "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n"}
{"snippet": "new_Xcept_predictions = [np.argmax(new_Xcept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(new_Xcept_predictions)==np.argmax(test_targets, axis=1))/len(new_Xcept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test_flat)\ndecoded_imgs = decoder.predict(encoded_imgs)\nshow_comparison(x_test_flat, decoded_imgs, 20)\nshow_latent(encoded_imgs, y_test, 1000)\n", "intent": "Encode and decode some test samples\n"}
{"snippet": "y_pred = model.predict(X_test)\ncls_pred = np.argmax(y_pred, axis=1)\ncls_true = np.argmax(y_test, axis=1)\n", "intent": "Great! The performance is now much better, but let's see what this new model is having trouble with...\n"}
{"snippet": "U_images = X_test[:9]\nV_images = resampler.predict(U_images)\n", "intent": "below we'll plot some good predictions to see how it performed there.\n"}
{"snippet": "y_pred=rand_search_exTree.predict(X_test)\nprint(rand_search_exTree.score(X_test, y_test))\nmse_scores= mean_squared_error(y_test, y_pred)\nrmse_scores=np.sqrt(mse_scores)\nprint(rmse_scores.mean())\n", "intent": "The Extra Trees Regressor  model performed better than the RF model but slighlt worse than XGB regression model.\n"}
{"snippet": "predictions = lm.predict( X_test)\n", "intent": "**Now that we have fit our model, let's evaluate its performance by predicting off the test values**\n"}
{"snippet": "sampleData = dfCheck[2:3]\nsampleDataFeatures = np.asarray(sampleData.drop('insuranceclaim',1))\nsampleDataFeatures = (sampleDataFeatures - means)/stds\npredictionProbability = insuranceLoadedModel.predict_proba(sampleDataFeatures)\nprediction = insuranceLoadedModel.predict(sampleDataFeatures)\nprint('Insurance Claim Probability:', predictionProbability)\nprint('Insurance Claim Prediction:', prediction)\n", "intent": "Now use the third record to make our insurance claim prediction.\n"}
{"snippet": "sample_preds2 = clusterer2.predict(data2.iloc[indices])\nfor i, pred in enumerate(sample_preds2):\n    print \"Sample point\", i, \"predicted to be in Cluster\", pred\n", "intent": "Interestingly, our clusterer again only finds 2 clusters but a slightly better silhouette score.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_XceptionModel]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "target = y\ndata = X\ny_pred = clf.predict(data)\nscore = clf.score(data, target) \ncross_val = cross_val_score(clf, data, target, cv=5)\n", "intent": "This is overfiting b/c the test set has better results than the Training set   \n"}
{"snippet": "target = y\ndata = X\ny_pred = clf.predict(data)\nscore = clf.score(data, target) \ncross_val = cross_val_score(clf, data, target, cv=5)\n", "intent": "This is overfiting b/c the test set has better results than the Training set  \n"}
{"snippet": "target = y\ndata = X\ny_pred = clf.predict(data)\nscore = clf.score(data, target) \ncross_val = cross_val_score(clf, data, target, cv=5)\n", "intent": "This is performing about the same on the Test set as it does on the Training set.  \nWe can conclude that the model predicts well on itself.   \n"}
{"snippet": "target = y\ndata = X\ny_pred = clf.predict(data)\nscore = clf.score(data, target) \ncross_val = cross_val_score(clf, data, target, cv=5)\n", "intent": "This is overfiting b/c the test set has better results than the Training set \n"}
{"snippet": "print(classification_report(Y_train, Y_train_pred_sgdc))\n", "intent": "This is actually perfoming worst than the one that didn't use lsa.     \nThe accuracy is lowered when using applying lsa to the training set.   \n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "To evaluate accuracy of the model 5-fold cross validation method will be used to get output in the form of score out of 1.\n"}
{"snippet": "print(\"Train accuracy:\", model.score(X_train, y_train))\nprint(\"Test accuracy:\", model.score(X_test, y_test))\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "After the model is trained, evaluate the performance of the model on both training and test dataset.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_pred_proba_dt = dt_model.predict_proba(X_test)\ny_pred_proba_log_reg = log_reg_model.predict_proba(X_test)\ny_pred_proba_nn = nn_model.predict_proba(X_test)\nroc_index_dt = roc_auc_score(y_test, y_pred_proba_dt[:, 1])\nroc_index_log_reg = roc_auc_score(y_test, y_pred_proba_log_reg[:, 1])\nroc_index_nn = roc_auc_score(y_test, y_pred_proba_nn[:, 1])\nprint(\"ROC index on test for DT:\", roc_index_dt)\nprint(\"ROC index on test for logistic regression:\", roc_index_log_reg)\nprint(\"ROC index on test for NN:\", roc_index_nn)\n", "intent": "The ROC AUC score aims to find the best model under the varied threshold values. To compute the ROC AUC score, use the code below.\n"}
{"snippet": "def get_error(y_true, y_pred, type = \"rmsle\"):\n    assert len(y_true) == len(y_pred), \"Length of prediction and test do not match!\"\n    list_types = [\"rmsle\",\"rmse\"]\n    assert type in list_types, \"Error type you requested is not an option. Choose 'rmse' or 'rmsle' please.\"\n    if type == \"rmse\":\n        return np.square(y_pred - y_true).mean() ** 0.5\n    if type == \"rmsle\":\n        return np.square(np.log(y_pred) - np.log(y_true)).mean() ** 0.5\n", "intent": "Below is an error function that will return either rmse or rmsle. Both are sometimes useful.\n"}
{"snippet": "train_metrics = estimator.evaluate(input_fn=train_input_fn)\neval_metrics = estimator.evaluate(input_fn=eval_input_fn)\nprint(\"train metrics: %r\"% train_metrics)\nprint(\"eval metrics: %r\"% eval_metrics)\n", "intent": "**Step 5: evaluate performance.**\n"}
{"snippet": "train_metrics = estimator.evaluate(input_fn=train_input_fn)\neval_metrics = estimator.evaluate(input_fn=eval_input_fn)\nprint(\"train metrics: %r\"% train_metrics)\nprint(\"eval metrics: %r\"% eval_metrics)\n", "intent": "**Step 5: evaluate performance** (same as above)\n"}
{"snippet": "from sklearn import metrics\ny_estimado = clf1.predict(income['Age'].values.reshape(-1, 1))\nMSE = metrics.mean_squared_error(income['Income'].values.reshape(-1, 1), y_estimado)\nprint('MSE : ',MSE)\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "print(\"MSE: %.2f\"% mean_squared_error(D1_y_test, y_pred_D1))\nprint('R Cuadrado: %.2f' % r2_score(D1_y_test, y_pred_D1))\n", "intent": "$D_1$:  $X = \\textit{'RM'}$, $y = \\textit{'MEDV'}$\n"}
{"snippet": "print(\"MSE: %.2f\"% mean_squared_error(D2_y_test, y_pred_D2))\nprint('R Cuadrado: %.2f' % r2_score(D2_y_test, y_pred_D2))\n", "intent": "$D_2$:  $X = \\textit{'LSTAT'}$, $y = \\textit{'MEDV'}$\n"}
{"snippet": "model.predict(X_pad)\n", "intent": "Predecimos sobre la red neuronal calulada anteriormente\n"}
{"snippet": "for network in networks:\n    exec(network + \"_predictions = [np.argmax(\" + network + \"_model.predict(np.expand_dims(feature, axis=0))) \\\n                                    for feature in test_\" + network + \"]\")\n    exec(\"test_accuracy = 100*np.sum(np.array(\" + network + \"_predictions)==np.argmax(test_targets, axis=1))/ \\\n                                    len(\" + network + \"_predictions)\")\n    print('Test accuracy - ' + network + ': %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resNet_predictions = [np.argmax(resNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resNet]\ntest_accuracy = 100*np.sum(np.array(resNet_predictions)==np.argmax(test_targets, axis=1))/len(resNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "error_all = []\nfor n in range(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(train_data[target], predictions)\n    error_all.append(error)\n    print (\"Iteration %s, training error = %s\" % (n, error_all[n-1]))\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "clf.predict([[0, 0.45, 0.35, 0.115,0.4415, 0.186, 0.0935, 0.13]])\n", "intent": "I use the 25% of each feature to create a instance:\n"}
{"snippet": "reg_tree.predict([[0, 0.45, 0.35, 0.115,0.4415, 0.186, 0.0935, 0.13]])\n", "intent": "The image seems similar to the DecisionTreeClassifier. Notice this time my labels are 29 classes. I try to use the same instance to predict:\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision_score(y_test, y_pred, average='micro')\n", "intent": "**Precision Value**\n"}
{"snippet": "print(accuracy_score(y_test, y_pred))\n", "intent": "Seems to be doing prettey good. Let's have a look at the accuracy\n"}
{"snippet": "import sklearn\nsklearn.metrics.r2_score(y_test, predictions_test)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "tests = [\n    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 6, 37.764641, -122.426930, 1], \n    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 37.787546, -122.421760,0], \n    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,12, 37.774153, -122.445216,0], \n    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 17, 37.764620, -122.492383,1], \n    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 37.793516, -122.407931,1], \n    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 17, 37.764641, -122.426929,1] \n]\nregress_tree.predict(tests)\n", "intent": "______________________\nNow let's test the regression tree by making a few predictions...\n"}
{"snippet": "print(classification_report(X_prediction, y_test))\n", "intent": "The confusion matrix informs us that there are 5 False Negative, 0 False Positive, 68 True Positives and 98 True Negatives.\n"}
{"snippet": "X_prediction = svm.predict(X_test)\nprint(X_prediction - y_test)\nprint()\nprint(f1_score(X_prediction, y_test))\n", "intent": "Now, we can notice that as long as we have standardized data, the SVM is a very good model for making predictions.\n"}
{"snippet": "liste_of_f1_scores.append(f1_score(X_prediction, y_test))\nmodel_names.append(\"Polynomial SVM\")\n", "intent": "Then, the Polynomial SVM Model seems to be less efficient than the Radial SVM Model.\n"}
{"snippet": "def getPrediction(in_sentences):\n  labels = [\"Negative\", \"Positive\"]\n  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n  predictions = estimator.predict(predict_input_fn)\n  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]\n", "intent": "Now let's write code to make predictions on new sentences:\n"}
{"snippet": "SSE = np.sum((regr.predict(X_test) - y_test) ** 2)\nSST = np.sum((y_test - np.mean(y_test)) ** 2)\nR_squared = 1-SSE/SST\nR_squared\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "fit_and_evaluate(dataset, LogisticRegression, \"Phishing Logistic Regression Classifier\")\n", "intent": "Logistic Regression\n"}
{"snippet": "y_val_pred = model.predict(df_val)\n", "intent": "Now lets compare to Linear Regression and Ridge Regression\n"}
{"snippet": "y_pred = model.predict(x_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram,fn = gram_matrix(model.extract_features()[5])\n    student_output, fnn = sess.run([gram,fn], {model.image: style_img_test})\n    print(student_output.shape)\n    print (fnn.shape)\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for i in range(len(style_layers)):\n        current_gram_matrix = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i]*tf.reduce_sum(tf.squared_difference(current_gram_matrix, style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "LR_predictions = LR_model.predict(X_test_vectorized)\nprint(type(LR_predictions))\nprint(LR_predictions.shape)\n", "intent": "We'll use the obtained Logistic Regression model to predict sentiment categories (classes) of test documents.\n"}
{"snippet": "LR_predictions_1 = LR_model_1.predict(X_test_vectorized_1)\nprint(type(LR_predictions_1))\nprint(LR_predictions_1.shape)\n", "intent": "We'll use the obtained Logistic Regression model to predict sentiment categories (classes) of test documents.\n"}
{"snippet": "LR_predictions = LR_model.predict(X_test_vectorized)\nprint(type(LR_predictions))\nprint(LR_predictions.shape)\n", "intent": "We'll use the obtained LR model to predict sentiment categories (classes) of test documents.\n"}
{"snippet": "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\nY_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\npred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\nprint_predictions(X_my_sentences, pred)\n", "intent": "Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 127 examples. \n"}
{"snippet": "xgb_proba = reviewPredictions(le,gbm,X_test,y_test,0.6)\ny_pred = gbm.predict(X_test)\ncreateConfusionMatrix(le,y_pred,y_test)\nprint \"\\n\\nApplying confidence threshold and classifying all below as unknown_activity\"\ncreateConfusionMatrix(le,y_pred,y_test,cmType = 'modified', probDF = xgb_proba)\n", "intent": "Review results of model and confusion matrices\n"}
{"snippet": "master_proba = reviewPredictions(le,newXGB,X_test,y_test,0.6)\ny_pred = newXGB.predict(X_test)\ncreateConfusionMatrix(le,y_pred,y_test)\nprint \"\\n\\nApplying confidence threshold and classifying all below as unknown_activity\"\ncreateConfusionMatrix(le,y_pred,y_test,cmType = 'modified', probDF = master_proba)\n", "intent": "Review results of model and confusion matrices\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))\n", "intent": "Classification report for the model\n"}
{"snippet": "print(classification_report(y_test, y_pred_test))\n", "intent": "The classification report and confusion matrix are shown below:\n"}
{"snippet": "from sklearn import metrics\nprint ( \"Accuracy = %.3f\" % (metrics.accuracy_score(decision_tree.predict(mpg_df[predictor_cols]), mpg_df[\"is_high_mpg\"])) )\n", "intent": "How good is our model? Let's compute accuracy, the percent of times where we correctly identified that a car was high MPG.\n"}
{"snippet": "print accuracy_score(y_train_data, big_model.predict(X_train_data))\nprint accuracy_score(y_validation_data, big_model.predict(X_validation_data))\n", "intent": "Now, let us evaluate **big_model** on the training set and validation set.\n"}
{"snippet": "pred = model_5.predict(X_validation_data)\nfalse_positives = np.count_nonzero(pred > y_validation_data)\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_test,\n                                        predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_test,\n                                        predictions_with_default_threshold)\nprecision_with_high_threshold =  precision_score(y_test,\n                                        predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_test,\n                                        predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "for j, (train_idx, val_idx) in enumerate(folds):\n    print('\\nFOLD =',j)\n    X_train_cv = X_train[train_idx]\n    y_train_cv = y_train[train_idx]\n    X_valid_cv = X_train[val_idx]\n    y_valid_cv= y_train[val_idx]\n    name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n    model.load_weights(name_weights)\n    print(model.evaluate(X_valid_cv, y_valid_cv))\n", "intent": "Now let's see how the model performs on each fold, i. e. each of the k validation sets.\n"}
{"snippet": "model_angle.evaluate([X_valid, angle_valid], y_valid)\n", "intent": "Let's see how the model performs on the validation set:\n"}
{"snippet": "x_test = np.array(['why god why we had a deal'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['Saturday 9 May 2018', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "print(classification_report(y_train_new, clf_tuned.best_estimator_.predict(X_train)))\n", "intent": "Let's look at the results of tuned estimator\n"}
{"snippet": "mat_data = sio.loadmat('spamTest.mat')\nX_test = mat_data['Xtest']\ny_test = mat_data['ytest'].ravel()\nprint 'Evaluating the trained Linear SVM on a test set...'\np = clf.predict(X_test)\nprint 'Test Accuracy:', np.mean(p == y_test) * 100\n", "intent": "Evaluate the trained Linear SVM on a test set:\n"}
{"snippet": "labels = model.predict(patches_hog)\nlabels.sum()\n", "intent": "Finally we can take this test hog to predict wether image has a face patch or not\n"}
{"snippet": "eval_result = classifier.evaluate(\n    input_fn=lambda:eval_input_fn(test_feature, test_label, batch_size))\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n", "intent": "Evaluate Model with test Data\n"}
{"snippet": "yhat = LR.predict(X_test)\nyhat\n", "intent": "Now we can predict using our test set:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_test, yhat, average='weighted') \n", "intent": "You can also easily use the __f1_score__ from sklearn library:\n"}
{"snippet": "y_hat_gb = gb_.predict_proba(X = X_test)[:, 1]\npreds_gb = gb_.predict(X = X_test)\nCF = confusion_matrix(y_test,preds_gb)\nCF = CF / len(X_test)\nprint(CF)\nprint('\\nAccuracy: {0:2.3f}'.format(accuracy_score(y_test, preds_gb,normalize = True)))\n", "intent": "Apply model to predict test dataset.\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "* Great! We've fit a model. Now, let's try to predict values on the testing set so we can gain an idea for how well we did.\n"}
{"snippet": "def mean_squared_error_(ground_truth, predictions):\n    return mean_squared_error(ground_truth, predictions) ** 0.5\nRMSE = make_scorer(mean_squared_error_, greater_is_better=False)\n", "intent": "<h2>Functions for prediction</h2>\n"}
{"snippet": "acc_base = accuracy_score(y_test,y_base_pred)\nprint(\"Accuracy: \", acc_base)\n", "intent": "Below we can find the accuracy of our classifier :\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "        - Returns a NumPy array\n        - Can predict for multiple observations at once \n"}
{"snippet": "from sklearn import metrics \nprint metrics.accuracy_score(y, y_pred)\n", "intent": "- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "cross_val_score(clf,X,y,cv=20)\n", "intent": "     Como era de preveer el accuracy obtenido es sensiblemente inferior al obtenido en el test.\n"}
{"snippet": "y_predict = rf.predict(X_test)\nmetrics.confusion_matrix(y_test, y_predict)\n", "intent": "* ** Testing the model with test data **\n"}
{"snippet": "Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n", "intent": "Now we pull the prediction method on our data:\n"}
{"snippet": "train_accuracy = clf.evaluate(X_train, y_train_k, batch_size=200)[1]\ntest_accuracy = clf.evaluate(X_test, y_test_k, batch_size=200)[1]\ncritical_accuracy = clf.evaluate(data[critical], y_critical_k, batch_size=200)[1]\nprint (\"Accuracy on train data: {0}\".format(train_accuracy))\nprint (\"Accuracy on test data: {0}\".format(test_accuracy))\nprint (\"Accuracy on critical data: {0}\".format(critical_accuracy))\n", "intent": "We now evaluate the model.\n"}
{"snippet": "y_hat = clf.predict(X_test)\nr_test = r_squared(y_test, y_hat)\nsk_r_test = clf.score(X_test, y_test)\nassert abs(r_test - sk_r_test) < 1e-2\n", "intent": "This is the same metric used by Scikit-learn for their regression models when scoring.\n"}
{"snippet": "always_spam_acc_train = metrics.accuracy_score(\n    trec_TRAIN_df['label'], \n    ['spam']*len(trec_TRAIN_df) \n)\nalways_spam_acc_test = metrics.accuracy_score(\n    trec_TEST_df['label'], \n    ['spam']*len(trec_TEST_df) \n)\nprint('TRAIN accuracy if we call everything spam: %.2f%%' % (always_spam_acc_train*100))\nprint('TEST accuracy if we call everything spam: %.2f%%' % (always_spam_acc_test*100))\n", "intent": "There's more spam than ham here. We can beat 50% accuracy just by saying everything is spam!\n"}
{"snippet": "def get_accuracy_metrics(dataset, labels):\n    predictions = classify(dataset)\n    predicted_labels = np.array([x['label'] for x in predictions])\n    accuracy = accuracy_score(labels, predicted_labels)\n    precision = precision_score(labels, predicted_labels, average=None)\n    recall = recall_score(labels, predicted_labels, average=None)\n    f1 = f1_score(labels, predicted_labels, average=None)\n    return accuracy, precision, recall, f1\n", "intent": "Evenutally, after having picked network architecture and dataset, we can run evaluation on the test dataset.\n"}
{"snippet": "print (np.mean((bos.PRICE-lm.predict(X)) **2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "score = model.predict(X_test)\n", "intent": "We can now predict the fraction RUL values for the testset:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(RUL, xpredictedRUL)\nprint mse\n", "intent": "\\begin{equation*}\nMSE \\ RUL = \\frac{1}{N} \\sum_{i=1}^n (Predicted - True)^2\n\\end{equation*}\n"}
{"snippet": "txtscript.evaluate('I', 20, temperature=0.6)\n", "intent": "Testing our **text_script**\n"}
{"snippet": "Y_pred_np = random_forests.predict(X_test_np)\nrandom_forests.score(X_train_np, Y_train_np)\n", "intent": "So, how well have we done? \n"}
{"snippet": "track_id = 4504011944230912.0\ntrack_hits_coords, hits_xyz, track_mod_onehot = mh.get_track_hit_NNinput_realxyz_coords_and_module(track_id)\npredictions = model.predict(np.array([track_hits_coords]))[0]\nmodule_ids, top_predictions = mh.get_top_k_module_id_and_pred(predictions, k=10)\n", "intent": "Get the top 10 predictions for every hit in the track.\n"}
{"snippet": "y_pred = model.predict(x)\nnhit = 10\nfirst_track_no = np.searchsorted(lengths, nhit-1, side='right')\nfail_tracks = list_k_fails(y_true[first_track_no:,nhit-2], y_pred[first_track_no:,nhit-2], track_id_list[first_track_no:], k=100)\n", "intent": "Find the tracks where the correct answer for the 10th hit is not even in the top 100 predictions.\n"}
{"snippet": "track_id = fail_tracks[5]\ntrack_hits_coords, hits_xyz, track_mod_onehot = mh.get_track_hit_NNinput_realxyz_coords_and_module(track_id)\npredictions = model.predict(np.array([track_hits_coords]))[0]\nmodule_ids, top_predictions = mh.get_top_k_module_id_and_pred(predictions, k=10)\n", "intent": "Absolutely no excuse. It was a very nice track. A lot of nearby modules lit up, just not this one.\n"}
{"snippet": "accuracy_score(target_val, rf_predictions)\n", "intent": "What about the accuracy of the model ? Here it is :\n"}
{"snippet": "accuracy_score(target_val, svm_predictions)\n", "intent": "Let us see how accurate our model is :\n"}
{"snippet": "xi = np.linspace(0,10,15) \nxi = xi.reshape((-1,1)) \nyp = lr.predict(xi)\n", "intent": " Now, that we have this fitted, we can evaluate\nthe fit using the `predict` method,\n"}
{"snippet": "walmart[['Weekly_Sales']].assign(\n    pred_numeric=simple_classifier.predict(X),\n    pred_both=clf.predict(encoded_X)\n)\n", "intent": "We can compare a few of the predictions from both classifiers to see whether there's a large difference between the two.\n"}
{"snippet": "pred_ten = clf_ten.predict(X_ten)\nprint(f'MSE cost for linear reg:      {mse_cost(pred_linear, y):.3f}')\nprint(f'MSE cost for deg 2 poly reg:  {mse_cost(pred_quad, y):.3f}')\nprint(f'MSE cost for deg 5 poly reg:  {mse_cost(pred_five, y):.3f}')\nprint(f'MSE cost for deg 10 poly reg: {mse_cost(pred_ten, y):.3f}')\n", "intent": "Here are the mean squared costs for the regression models we've seen thus far:\n"}
{"snippet": "tree_clf.predict([[5, 1.5]])\n", "intent": "Here is the following probabilities: $(0/54) = 0\\%, (49/54) = 90\\%, (5/54) = 9.3\\%$\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')\n", "intent": "Lets here evaluate different multiclass classification algorithms\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "We can also measure __RMSE__ on the whole training set using Scikit-Learn's **mean_squared_error**\n"}
{"snippet": "auc_score = roc_auc_score(y, y_scores)\nprint(\"AUC-Score: {}\".format(round(auc_score,4)))\n", "intent": "AUC score calculates the area under the ROC curve. An AUC score of 1 signifies a perfect classfier.\n"}
{"snippet": "predictions = [1 if x>threshold else 0 for x in y_scores]\nprint('Precision: {}'.format(round(precision_score(y, predictions), 4)))\nprint('Recall: {}'.format(round(recall_score(y, predictions), 4)))\nprint('F1-Score: {}'.format(round(f1_score(y, predictions), 4)))\nprint('Matthews Correlation Coefficient: {}'.format(round(matthews_corrcoef(y, predictions), 4)))\n", "intent": "We can now re-calculate evaluation metrics with the updated threshold.\n"}
{"snippet": "accuracy_score(y_test.values, y_predict)\n", "intent": "```python\naccuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n```\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test005.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "file_name = 'data/test_oligos.txt'\ndf = read_data(file_name)\nX = process_features(df)\nforest.predict(X)\n", "intent": "Read from a CSV file containing these columns:\n- mutation_size\n- distance_from_pam\n- full_guide_sequence\n- ss_oligo\n"}
{"snippet": "scores = model.evaluate(X, Y, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "So how did we do? Of course it's easy to find out.\nJust call .evaluate() on your model and pass the input and the labels.\n"}
{"snippet": "test_loss, test_accuracy = softmax_classifier.evaluate(testX, testy_onehot)\nprint 'Accuracy on test set:', test_accuracy\n", "intent": "Evaluate this trained model on the test data, which produces a tuple consisting of the loss on the test set and the accuracy.\n"}
{"snippet": "test_loss, test_accuracy = hidden1_classifier.evaluate(testX, testy_onehot)\nprint 'Accuracy on test set:', test_accuracy\n", "intent": "Evaluate this new model on the test data...\n"}
{"snippet": "test_loss, test_accuracy = hidden2_classifier.evaluate(testX, testy_onehot)\nprint 'Accuracy on test set:', test_accuracy\n", "intent": "Evaluate this new model on the test data...\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n kfold = model_selection.KFold(n_splits=10, random_state=7)\n cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)\n", "intent": "Now you loop through the list **model** and calculate the accuracy for individual algorithms.\n"}
{"snippet": "predict_mlp = model_mlp.predict(test_data)\n", "intent": "Just like any other model in CivisML, we can use hyperband-tuned models to make predictions using `.predict()` on the `ModelPipeline`.\n"}
{"snippet": "Res_predictions = [np.argmax(Res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resNet]\ntest_accuracy = 100*np.sum(np.array(Res_predictions)==np.argmax(test_targets, axis=1))/len(Res_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import completeness_score, homogeneity_score\ncompleteness = completeness_score(class_data['Class Code'], kmeans.labels_)\nhomogeneity = homogeneity_score(class_data['Class Code'], kmeans.labels_)\nprint('Completeness of Cluster: '+  str(completeness))\nprint('Homogeneity of Clusters: '+ str(homogeneity))\n", "intent": "So we can see the cluster centroids for each of the features for all the 7 clusters.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, (layer, weight) in enumerate(zip(style_layers, style_weights)):\n        gram_current = gram_matrix(feats[layer])\n        loss += weight * tf.reduce_sum(tf.squared_difference(gram_current, style_targets[i]))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn import metrics\nprint('The accuracy of the svm',metrics.accuracy_score(y_pred,y_test))\n", "intent": "Now print the error metrics\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\n", "intent": "Now apply k-fold cross validation with k = 10 and determine the mean accuracy\n"}
{"snippet": "def predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n    return pred\n", "intent": "Lets write a predit function based on above, as follows\n"}
{"snippet": "from sklearn import metrics  \npreds_train = classifier.predict(X_train) \nprint(\"accuracy: \", metrics.accuracy_score(y_train, preds_train))  \nprint(\"precision: \", metrics.precision_score(y_train, preds_train))  \nprint(\"recall: \", metrics.recall_score(y_train, preds_train))  \nprint(\"f1: \", metrics.f1_score(y_train, preds_train))  \nprint(\"area under curve (auc): \", metrics.roc_auc_score(y_train, preds_train))  \n", "intent": "Now evaluate the model error parameters\n"}
{"snippet": "preds_test = classifier.predict(X_test)  \nprint(\"accuracy: \", metrics.accuracy_score(y_test, preds_test))  \nprint(\"precision: \", metrics.precision_score(y_test, preds_test))  \nprint(\"recall: \", metrics.recall_score(y_test, preds_test))  \nprint(\"f1: \", metrics.f1_score(y_test, preds_test))  \nprint(\"area under curve (auc): \", metrics.roc_auc_score(y_test, preds_test))  \n", "intent": "Observe that the oneclassSVM classifier has a very good accuracy on the training set. Lets do the same for test set.\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, cate_test_labels)\nprint('test_acc:', test_acc)\n", "intent": "* model.evaluate()\n  * x\n  * y\n  * batch_size\n  * verbose: 0 / 1\n"}
{"snippet": "first_x = x_values['x'].iloc[0]\nfirst_y = y_values['y'].iloc[0]\nfirst_y_pred = demo_reg.predict(first_x)\nfirst_y_delta = pred_first_y - first_y\nprint(\"first x: \" + str(first_x))\nprint(\"first y: \" + str(first_y))\nprint(\"first y predicted: \" + str(first_y_pred))\nprint(\"first y delta: \" + str(first_y_delta))\n", "intent": "It's straightforward to compare the actual y-value versus predicted y-value for a single x-value.\n"}
{"snippet": "set_x = x_values['x'].iloc[:2].values.reshape(-1,1)\nset_y = y_values['y'].iloc[:2].values.reshape(-1,1)\nset_y_pred = demo_reg.predict(set_x)\nset_y_delta = set_y_pred - set_y\nprint(\"set x: \" + str(set_x))\nprint(\"set y: \" + str(set_y))\nprint(\"set y predicted: \" + str(set_y_pred))\nprint(\"set y delta: \" + str(set_y_delta))\n", "intent": "We can also compare actual versus predicted y-values for a set of x-values.\n"}
{"snippet": "y_predicted_log = model_log.predict(x_test)\ny_prob_predicted_log = model_log.predict_proba(x_test)\nprint (y_predicted_log)\nprint(y_prob_predicted_log)\n", "intent": "The model fits quite well on the train data\n"}
{"snippet": "local_results = pipeline.predict(test_features)\nlocal = pd.Series(local_results, name='local')\npd.crosstab(actual,local)\n", "intent": "Use a confusion matrix create a visualization of the predicted results from the local model. These results should be identical to the results above.\n"}
{"snippet": "deepiep.predict(0)\n", "intent": "No particular error handling has been programmed, if you feed it something it doesn't expect, it will fail\n"}
{"snippet": "processed_images = preprocess_input(images.copy()) \npredictions = model.predict(processed_images) \nlabel = decode_predictions(predictions) \nlabel\n", "intent": "Now, predict the image's class.\n"}
{"snippet": "preprocessed_perturbed = preprocess_input(image_perturbed.copy())\npredictions = model.predict(preprocessed_perturbed)\nlabel = decode_predictions(predictions)\nlabel\n", "intent": "Let's see what the model now predicts.\n"}
{"snippet": "y_pred_rf = clf_rf.predict(X_test)\ny_pred_rf = np.where(y_pred_rf==0,-1,1)\n", "intent": "** RF fits the data better. Sill, our aim is to compare the decision boundaries. **\n"}
{"snippet": "print \"MSRE Value For Linear Regression: \",metrics.mean_squared_error(ytest_log,ols_preds)\n", "intent": "Mean Squared Error:\n"}
{"snippet": "print \"R squared Value For Linear Regression: \",metrics.r2_score(ytest_log, ridge_preds)\nprint \"RMSE Value For Linear Regression: \",metrics.mean_squared_error(ytest_log,ridge_preds)\n", "intent": "The optimal parameter is 10,000\n"}
{"snippet": "print classification_report(ytest_c, rbf_c_x_pred)\n", "intent": "Classification Report:\n"}
{"snippet": "print classification_report(ytest_c, svm_pred)\n", "intent": "Classification Report:\n"}
{"snippet": "print (tagtools.bieso_classification_report(bib.dev_y, bib.dev_predictions))\n", "intent": "Summarize performance of the classifier\n"}
{"snippet": "print (tagtools.bieso_classification_report(bib.dev_y, bib.dev_decoded))\n", "intent": "Summarize performance of the decoder\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, style_layer in enumerate(style_layers):\n        style_loss += style_weights[i]*(gram_matrix(feats[style_layer]) - style_targets[i]).norm()**2\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "import sys\ndef style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in np.arange(len(style_layers)):\n        layer_var = gram_matrix(feats[style_layers[i]])\n        loss_i = tf.reduce_mean((layer_var - style_targets[i])**2) * style_weights[i] \n        style_loss = tf.add(style_loss, loss_i)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "import sys\ndef style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in np.arange(len(style_layers)):\n        layer_var = gram_matrix(feats[style_layers[i]])\n        loss_i = tf.reduce_sum((layer_var - style_targets[i])**2 * style_weights[i])\n        style_loss = tf.add(style_loss, loss_i)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(predictions, target)\nprint mse\n", "intent": "In terms of metrics, the [sklearn.metrics](http://scikit-learn.org/stable/modules/classes\n"}
{"snippet": "p, err = nn_bgd.predict(x_test, y_test)\nprint(err)\n", "intent": "Now we calculate the test error\n"}
{"snippet": "p, err = nn_mbgd.predict(x_train, y_train)\nprint(err)\n", "intent": "We calculate training error rate\n"}
{"snippet": "p, err = nn_mbgd.predict(x_test, y_test)\nprint(err)\n", "intent": "Now we calculate the test error rate\n"}
{"snippet": "def predict_one():\n    image_batch, mask_batch = next(b)\n    predicted_mask_batch = model.predict(image_batch)\n    image = image_batch[0]\n    predicted_mask = predicted_mask_batch[0].reshape(256, 256)\n    return image, predicted_mask\n", "intent": "model.load_weights('weights/best_weights.hdf5')\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = knn.predict(X_test)\nprint(\"Accuracy of k-means model: \" + str(accuracy_score(y_pred, y_test)))\n", "intent": "We can now test our model accuracy:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"Mean squared error of linear regression: \" + str(mean_squared_error(lr.predict(X_test), y_test)))\nprint(\"Mean squared error of lasso regression: \" + str(mean_squared_error(lr2.predict(X_test), y_test)))\n", "intent": "We evaluate the two models against each other:\n"}
{"snippet": "test_preds = gbr.predict(testx)\n", "intent": "Compared with the first method, the second method's error can be ignored. So I use the gradient boosting regressor to predict the test data.\n"}
{"snippet": "T_test_baseline = baseline_probabilities(y_test)\nT_test_lr_best = lr_best.predict_proba(X_test_sparse)\ntest_loss_baseline = log_loss(y_test, T_test_baseline)\ntest_loss_lr_best = log_loss(y_test, T_test_lr_best)\nprint \"OHE features test set log loss\"\nprint \"\\tBaseline log loss: {0}\".format(test_loss_baseline)\nprint \"\\tlr_best log loss: {0}\".format(test_loss_lr_best)\nprint \"\\nFor comparison, the cv log loss with lr_best:\"\nprint \"\\t{}\".format(cv_loss_lr_best)\n", "intent": "Now we finally evaluate our best model on our test dataset\n"}
{"snippet": "train['mean'] = train['saleprice'].mean()\ny_test = train['saleprice'].values\nprediction_mean = train['mean'].values\nbaseline = np.sqrt(mean_squared_error(y_test, prediction_mean))\nprint(baseline)\n", "intent": "<a id='baseline'></a>\n"}
{"snippet": "pred = pipeline.predict(X_test)\ncm = confusion_matrix(y_test, pred)\nprint(classification_report(y_test, pred))\n", "intent": "By looking at classification report and confusion-matrix below, we can see that our model did well.\n"}
{"snippet": "predicted = rf_clf.predict(df_num.iloc[train.shape[0]:, :])\npredicted_series = pd.Series(predicted, index = df_num.iloc[train.shape[0]:, :].index, name = 'Survived')\n", "intent": "After fitting to the numeric columns, we can generate predictions on the test set.\n"}
{"snippet": "svm_score = accuracy_score(y_test, y_hat)\nprint(svm_score)\nprint(confusion_matrix(y_test, y_hat))\nprint(classification_report(y_test, y_hat))\n", "intent": "Use accuracy_score, confusion_matrix, classification_report functions.\n"}
{"snippet": "metrics.mean_squared_error(a,ytest) \n", "intent": "This is because the independent variables are not standardized\n"}
{"snippet": "predictions = nb.predict(x_test)\n", "intent": "** Now make predictions on new data **\n"}
{"snippet": "strengths = gmm.predict_proba(neighborhood[['median_price']])\n", "intent": "Further, if you want the probabilities around the assignments, you can use the `predict_proba` method:\n"}
{"snippet": "y_pred=classifier.predict(X_test)\n", "intent": "Make predictions on the hold out test set.\n"}
{"snippet": "y_pred1=classifier1.predict(X_test)\n", "intent": "Make predictions on the hold out test set.\n"}
{"snippet": "ypred = bst.predict(dtest)\n", "intent": "Predict the test set\n"}
{"snippet": "expected = digits.target[n_samples / 2:]\npredicted = classifier.predict(data[n_samples / 2:])\nprint(\"Classification report for classifier {}:\\n{}\\n\".format(\n        classifier, metrics.classification_report(expected, predicted)))\nprint(\"Confusion matrix:{}\".format(metrics.confusion_matrix(expected, predicted)))\n", "intent": "Now predict the value of the digit on the second half\n"}
{"snippet": "y_pred = ann.predict(X_test)\ny_pred\n", "intent": "__ Predictin got the test set - we get probabilities which can be used to rank customers on basis of their chances of leaving__\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=1)\nprint(\"Elman RNN Test Accuracy: %.3f%%\" % (scores[1]*100))\n", "intent": "Lastly, let's get the final test error...\n"}
{"snippet": "plot_error(nn)\n", "intent": "First, let's have a look at the error change as the number of training epochs increase:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, labels)\n", "intent": "Let's check the accuracy now:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Let's see how accurate our K-Means classifier is with no label information:\n"}
{"snippet": "preds = pipe.predict(testTexts)\n", "intent": "To test the model, we can make predictions based on the texts in the test set:\n"}
{"snippet": "predictions = dxgb.predict(client, bst, data_test)\npredictions = client.persist(predictions)\npredictions\n", "intent": "Let's get the predictions. The is the same as usual, except we're using `dxgb`.\n"}
{"snippet": "predictions = model1.predict(test_data[features])\n", "intent": "Evaluating the accuracy\n"}
{"snippet": "predictions = model2.predict(feature_vector_test)\n", "intent": "Evaluating the accuracy\n"}
{"snippet": "preds = model.predict(X_test)\naccuracy = accuracy_score(y_test, preds)\nprint(\"Accuracy: \", accuracy)\n", "intent": "**Compute accuracy of the model**\n"}
{"snippet": "r2_score(data.y, model.predict(feature_matrix))\n", "intent": "Printing the r square score of the knn model\n"}
{"snippet": "predictions = model.predict(X)\n", "intent": "**Compute the predictions using the feature matrix**\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\nreported_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Reported accuracy: %.4f%%' % reported_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = dtree.predict(x_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predictions = rforest.predict(x_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(college_data['Cluster'], kmeans.labels_))\nprint(classification_report(college_data['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = model.predict(x_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "predictions = lm.predict(x_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = svc.predict(x_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "estimator.evaluate(eval_input_fn, steps=None)\n", "intent": "For evaluation, if you set epochs to be 1, you can ignore the steps param (set it you None).\nBy default, the latest checkpoint is evaluated\n"}
{"snippet": "dog_breed_predictions = [np.argmax(scratch_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "predictions = [np.argmax(inception_bneck.predict(np.expand_dims(feature, axis=0)))\n              for feature in test_incp_bn]\ntest_accuracy = 100. * np.sum(np.array(predictions) == np.argmax(test_targets, axis=1)) / len(predictions)\nprint('Test accuracy: {:.4f}%'.format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = cross_val_score(logreg, features_array, target, cv=10,\n                         scoring='roc_auc')\nscores.min(), scores.mean(), scores.max()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(model_lrc,\n                            vector_train,\n                            target_train,\n                            cv = 5,\n                            scoring=\"accuracy\")\ncv_scores\n", "intent": "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)\n"}
{"snippet": "from sklearn import metrics\npredicted = clf.predict(pca.transform(X[train_size:]))\nactual = y[train_size:]\nprint(metrics.classification_report(actual, predicted))\nprint(metrics.confusion_matrix(actual, predicted))\n", "intent": "Test the trained network against the remaining 25% of the dataset and print the classification report and confusion matrix\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf.predict(X_test)\nprint (metrics.classification_report(y_test_setosa, y_pred, target_names=['setosa','other']))\n", "intent": "Note that we first scaled the new instance, then applied the `predict` method, and used the result to lookup into the iris target names arrays. \n"}
{"snippet": "raw_flower = [[4.7, 3.1]] \nflower = scaler.transform(raw_flower)\nprint(\"setosa score\",clf2.decision_function(flower)[0][0])\nprint(\"versicolor score\", clf2.decision_function(flower)[0][1])\nprint(\"virginica score\", clf2.decision_function(flower)[0][2])\npred = clf2.predict(flower)\nprint(pred)\n", "intent": "Let us evaluate on the previous instance to find the three-class prediction. Scikit-learn tries the three classifiers. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    print(\"underway...\")\n    k_cv = KFold(n_splits=K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=k_cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores)))\n", "intent": "This function will serve to perform and evaluate a cross validation:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))\n", "intent": "Perform 5-fold cross-validation\n"}
{"snippet": "train_and_evaluate(svc_1, X_train, X_test, y_train, y_test)\ntrain_and_evaluate(linclf, X_train, X_test, y_train, y_test)\n", "intent": "Let's measure precision and recall on the evaluation set, for _each class_. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(n_splits=K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores)))\n", "intent": "If we evaluate our algorithm with a three-fold cross-validation, we obtain a mean score of around 0.81.\n"}
{"snippet": "from sklearn import grid_search\nfrom sklearn.tree import DecisionTreeClassifier\ncv = sklearn.cross_validation.StratifiedShuffleSplit(labels, n_iter=10)\ndef scoring(estimator, features_test, labels_test):\n     labels_pred = estimator.predict(features_test)\n     p = sklearn.metrics.precision_score(labels_test, labels_pred, average='micro')\n     r = sklearn.metrics.recall_score(labels_test, labels_pred, average='micro')\n     if p > 0.3 and r > 0.3:\n            return sklearn.metrics.f1_score(labels_test, labels_pred, average='macro')\n     return 0\n", "intent": "First we'll define cv and scoring for the process.\n"}
{"snippet": "print (sqft_model.predict(house1[features]))\n", "intent": "This house was originally priced at $620,000.\nLets see how much our models predict.\n"}
{"snippet": "train_predict_loaded_model = model.predict(trainX)\ntest_predict_loaded_model = model.predict(testX)\nprint(train_predict_loaded_model.shape, test_predict_loaded_model.shape)\n", "intent": "Re-predict using the loaded model\n"}
{"snippet": "forecast, inp, predict = np.array([]), .0, .0\nadding = time_prediction*7\nfor i in range(adding):\n    if i > 0: inp = np.concatenate([inp[-1, 0, 1:], predict[0] ]).reshape((1, 1,time_prediction-1) )\n    else: inp = np.concatenate([trainX[-1, 0, 1:], train_predict[-1]]).reshape((1, 1, time_prediction-1) )\n    predict = lstm_model.predict(inp)\n    forecast = np.append(forecast, predict)\nforecast = forecast.reshape(int(forecast.shape[0]/14),14)\nforecast_result = regression_to_class_forecast(forecast[:,-1]).reshape(-1,1)\nprint (forecast_result[:,-1])\n", "intent": "Forecasting the CPC for n-days\n"}
{"snippet": "forecast, inp, predict = np.array([]), .0, .0\nadding = 7\nfor i in range(time_prediction+adding):\n    if i > 0: inp = np.concatenate([inp[-1, 0, 1:], predict[0] ]).reshape((1, 1,time_prediction-1) )\n    else: inp = np.concatenate([trainX[-1, 0, 1:], train_predict[-1]]).reshape((1, 1, time_prediction-1) )\n    predict = lstm_model.predict(inp)\n    forecast = np.append(forecast, predict)\n", "intent": "Forecasting the CPC for n-days\n"}
{"snippet": "accuracy_score(clf.predict(xtrain),ytrain)\n", "intent": "As we can see the accuray is much high for training set. Let us check the accuracy of SVM for training set\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        loss += style_weights[i] * tf.reduce_sum(tf.pow(style_targets[i] - gram_matrix(feats[style_layers[i]]), 2))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn import metrics\nincome['IncomePred'] = linreg_AGE_INCOME.predict(income[['Age']])\nprint('MSE:', metrics.mean_squared_error(income['Income'],income['IncomePred'] ))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "income['isUS_predict_prob'] = logreg.predict_proba(X)[:, 1]\n", "intent": "We calculate the predicted probabilities to understand how confident we are in a given prediction\n"}
{"snippet": "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\nfor train_index, test_index in kf:\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\nresults.append(metrics.roc_auc_score(y_test, y_pred_prob))\npd.Series(results)\n", "intent": "Now which is the best set of features selected by AUC\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred_test = model_fit.predict(X_test)\n", "intent": "Let's test our model on our test set.\n"}
{"snippet": "y_pred_test = model2.predict(X_test)\nprint(f'root mean squared error on test data: {np.sqrt(mean_squared_error(y_test,y_pred_test)):.4}')\n", "intent": "Great! This is comparable to the cross validation R2 so we know we are not overfitting! \n"}
{"snippet": "plot_y_yhat_scatter(y_train, trained_regressor.predict(X_train),train_test=\"training\")\n", "intent": "Compare this Random Forest to the result from the Decision Tree.\nWhich one is better?\nWhy?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        F = gram_matrix(feats[style_layers[i]])\n        style_loss += tf.reduce_sum(tf.pow(style_targets[i]-F,2)) * style_weights[i]\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntrainer = BackpropTrainer(network, ds_train)\nfor i in range(10):\n    trainer.trainEpochs(50)\n    print(\"Training Accuracy:\", accuracy_score(target_train, network.activateOnDataset(ds_train).argmax(1)))\n    print(\"Testing Accuracy:\", accuracy_score(target_test, network.activateOnDataset(ds_test).argmax(1)))\n", "intent": "Now we can train as we did before.  We'll need to train this network for longer because the model is more complicated as is the function.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "It is not really better, I have 97% in the training data and 72% in the testing data\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return f1_score(target.values, y_pred, pos_label='yes')\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "The code cell below is run to initialize three helper functions which are used for training and testing the models.\n"}
{"snippet": "inception_predictions = [np.argmax(model_inception.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(College_Data['Cluster'],kmeans.labels_))\nprint(classification_report(College_Data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "salary_train_pred = ridge.predict(X)\nz = sns.jointplot(y, salary_train_pred)\nz.set_axis_labels('sqrt(salary)', 'Predicted Sqrt(Salary)', fontsize=16)\n", "intent": "**Plotting actual salary vs. predicted salary using training and testing data**\n"}
{"snippet": "print(\"OHE label: {}\".format(y_train[0]))\nprint(\"So, the 0-9 label would be: {}\".format(numpy.argmax(y_train[0])))\nprint()\nprint(\"Prediction of first image: {}\".format(model.predict(X_train[0:1])))\nprint(\"So, the 0-9 prediction would be: {}\".format(numpy.argmax(model.predict(X_train[0:1]))))\nprint()\n", "intent": "Now that we have a model, how do we use it?\nIt is as simple as follows:\n"}
{"snippet": "img, img_th, rects = process_img(\"test.jpg\")\nout = predict(model, img, img_th, rects, \"output.jpg\")\nprint(\"Prediction: {}\".format(out))\n", "intent": "<br>\n<center>\n    <img src=\"test.jpg\" width=\"50%\">\n</center>\n"}
{"snippet": "Y_pred_own=predict(count,X_test)\n", "intent": "PREDICTING USING OWN NAIVE_BAYES PREDICT FUNCTION\n"}
{"snippet": "from sklearn.metrics import  classification_report,confusion_matrix\nprint(classification_report(Y_test,Y_pred_own))\nprint(confusion_matrix(Y_test,Y_pred_own))\n", "intent": "REPORT OF SELF-MADE CLASSIFIER\n"}
{"snippet": "print(classification_report(Y_test,Y_pred))\nprint(confusion_matrix(Y_test,Y_pred))\n", "intent": "REPORT OF INBUILT CLASSIFIER\n"}
{"snippet": "predict(X_test[:32])\n", "intent": "These examples are from a subset of the test set and are just a sanity test.\n"}
{"snippet": "def cv_score(model):\n    mse=-cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n    rmse=np.sqrt(mse)\n    return rmse\n", "intent": "We will use Root-Mean-Sqaured-Error(RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\n"}
{"snippet": "average_models=AveragingModels(models=(ridge, lasso, svr, gbdt))\nscore = cv_score(average_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n", "intent": "According to the result of cross validation, we will choose some base models that behave very well as our input models.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = weekly_logit.predict(weekly).round().astype('int')\ncm = confusion_matrix(weekly['Direction_Up'].values, y_pred)\ncm\n", "intent": "For this model, the only predictor which appears to be statistically significant is Lag2.  \n**10 (c)**:\n"}
{"snippet": "train = weekly[weekly['Year'] <= 2008]\ntest = weekly[weekly['Year'] > 2008]\nmodel_accs = {}\ndef test_performance(model, X_test, y_test, name):\n    y_pred = model.predict(X_test).round().astype('int')\n    cm = confusion_matrix(y_test, y_pred)\n    acc = (cm[0, 0] + cm[1, 1])/cm.sum()*100\n    return acc\n", "intent": "**10 (h)**:  \nLDA has the lowest test error rate of 37.50%.  \n**10 (i)**:  \n"}
{"snippet": "estimator = xgboost.XGBClassifier()\nkfold = KFold(n_splits=10)\npred = cross_val_predict(estimator, data, labels, cv=kfold)\nxg_conf_mat = confusion_matrix(labels, pred)\nweak_pred = cross_val_predict(estimator, small_data, labels, cv=kfold)\nxg_conf_mat_small = confusion_matrix(labels, weak_pred)\n", "intent": "XGBOOST\n-------\nFitting an XGBoost classifier an all the data, using 10-fold validation to compute a confusion matrix.\n"}
{"snippet": "Y = (clf.predict(XX))\nT = (YY)\n", "intent": "For each classifier, we'll use the confusion_matrix() function the test labels (XX) and test inputs (YY)\n"}
{"snippet": "test_input = np.array([3.7, 1.26, 70.1, 1.0])\ntest_output = result.predict(test_input)\nprint('Probability of Female: {0:.2f}%'.format(test_output[0]*100.))\n", "intent": "Here is how we can make predictions with our model:\n"}
{"snippet": "X, _ = compress_features(new_restaurant, unique_categs)\nprint(\"Engineered Features:\")\nprint(X)\nprint(\"Predicted Rating:\", best_classifier.predict(X)[0])\n", "intent": "We get the compressed and engineered features using our 'compress_features' function and make the prediction.\n"}
{"snippet": "predictions = nb.predict(x_test)\n", "intent": "** Use the predict method off of nb to predict labels from x_test.**\n"}
{"snippet": "predictions = pipeline.predict(x_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix.**\n"}
{"snippet": "scores = model.evaluate(X, Y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n"}
{"snippet": "def sample(model, base, size):\n    for i in range(size):\n        d = np.argmax(autoencoder.predict(np.array([base[-30:]]))[0])\n        r = np.zeros(nb_index)\n        r[d] = 1\n        base = np.vstack((base, r))\n    return decode_string(base) \ndef sample_string(model, base, size):\n    e = encode_string(base, seg_size, nb_index)\n    return sample(model, e, size)          \n", "intent": "Here we define some utility methods to help monitor the progress of the training process\n"}
{"snippet": "predict_results = est_catvsdog.predict(\n    input_fn=lambda: imgs_input_fn(test_files[:10], \n                                   labels=None, \n                                   perform_shuffle=False,\n                                   batch_size=10))\n", "intent": "To predict we can set the `labels` to None because that is what we will be predicting.\nHere we only predict the first 10 images in the test_files.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint(f'Test accuracy: {test_accuracy:.4f}%')\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint(f'Test Accuracy: {test_accuracy:.4f}%')\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model2.predict(np.expand_dims(feature, \n                                                                        axis=0))) \n                        for feature in test_Resnet50]\nn = len(Resnet50_predictions)\ntest_accuracy = np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1)) / n\nprint(f'Test Accuracy: {test_accuracy:.4%}')\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def predict(model, features):\n    t, fi, reverse = model\n    if reverse:\n        return features[:, fi] <= t\n    else:\n        return features[:, fi] > t\n", "intent": "Next, we'll createa handful of funtions that can be resued to help fit our model to data, as well as predict its accuracy\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nknnAccuracyRate = round(accuracy_score(articlesTestSetLabels, knnPredictionValues)*100, 3)\nprint(\"Accuracy Rate for \"+knnModelName+\" classifier before cross-validation: \"+str(knnAccuracyRate)+'%\\n')\n", "intent": "<b>Next, let us calculate the accuracy rate of this 5NN model.</b>\n"}
{"snippet": "svmAccuracyRate = round(accuracy_score(articlesTestSetLabels, svmPredictionValues)*100, 3)\nprint(\"Accuracy Rate for \"+svmModelName+\" classifier before cross-validation: \"+str(svmAccuracyRate)+'%\\n')\n", "intent": "<b>Next, let us calculate the accuracy rate of the Linear SVM classifier model.</b>\n"}
{"snippet": "predicted = cross_val_predict(logreg, X_test, y_test, cv=10)\npredicted\n", "intent": "Ahora se realiza la prediccion\n"}
{"snippet": "res= cross_val_score(logreg, X_train, y_train, cv=10, scoring='roc_auc')\nres\n", "intent": "Now which is the best set of features selected by AUC\n"}
{"snippet": "clf.predict([[1,0,1,0],[0,1,0,0]])\n", "intent": "Se dan nuevos ejemplos al modelo para que aprenda\n"}
{"snippet": "indices = nyc_data_model.index[~nyc_data_model.index.isin(sample_reg.index)]\ntest_set = nyc_data_model.loc[np.random.choice(indices,size = 500000,replace=False)]\nregs_predictors = ['RateCodeID','Trip_distance','Total_amount','Payment_type','Trip_duration']\nypred = gs_rgs.best_estimator_.predict(test_set[regs_predictors])\nprint \"final mean_squared_error:\", metrics.mean_squared_error(ypred,test_set.Tip_percentage)\nprint \"final r2_score:\", metrics.r2_score(ypred,test_set.Tip_percentage)\n", "intent": "** (5) Final Model and Validation **\n"}
{"snippet": "from sklearn.metrics import r2_score\nprint(\"R^2: \"),r2_score(ytest, pred)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "def evaluate(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    errors = np.sqrt(mean_squared_error(y_test, predictions))\n    print('Model Performance')\n    print('MSE of: ', errors)\n    return errors\n", "intent": "Set up the evaluate function\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        layer, gram_target, weight = style_layers[i], style_targets[i], style_weights[i]\n        gram = gram_matrix(feats[layer], normalize=True)\n        loss += weight * (torch.norm(gram - gram_target) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        layer, target, weight = style_layers[i], style_targets[i], style_weights[i]\n        gram = gram_matrix(feats[layer], normalize=True)\n        loss += weight * (tf.norm(gram - target) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(own_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "predictions = [np.argmax(inceptionV3_bneck_model.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy with transfer learning: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "** Now predicting values for the testing data.**\n"}
{"snippet": "metrics.accuracy_score(ypred1, ytest)\n", "intent": "We can check the accuracy of these classifiers:\n"}
{"snippet": "y_pred=model.predict(X_test)\nX_test.head()\n", "intent": "We use the model above to predict the survive of our test data. \n**The model is fitted with the entire training data.**\n"}
{"snippet": "clf.predict(X_test)\n", "intent": "make a prediction from test data\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature,axis=0))) \n                        for feature in test_Resnet50]\nResnet50_test_accuracy = 100*np.sum(np.array(Resnet50_predictions) == np.argmax(test_targets,axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%'% Resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def reg_loss(M):\n    Ws = [M.W2, M.W4, M.W6, M.W8, M.W9]\n    loss = 0\n    for W in Ws:\n        loss += 0.5 * M.lmbda * np.sum(W*W) \n    return loss\ndef train_step(M, b, f):\n", "intent": "continued from primary forward and backward pass functions\n"}
{"snippet": "parameters = np.array(df_beta_coefs[0].T).tolist()\ncoeffs = []\nfor p in parameters: \n    coeffs.append(p[0])\nprediction = coeffs[0] + predict(preds1[0:6], coeffs[1:])\nprint 'Stepwise Regression'\nprint '\\nbeta coefficients\\n', coeffs, '\\n\\nwine quality taste prediction\\n', prediction\ntaste_prediction.append(prediction)\n", "intent": "Run Sample Predictions\n"}
{"snippet": "diff = 0 \npredictions = clf.predict(x_test)\nfor idx in range(y_test.size):\n    test = y_test.iloc[idx][0]\n    predicted = predictions[idx]\n    print \"test data value: \", test\n    print \"predicted value: \", predicted\n    diff = diff + abs(test-predicted)\nprint \"average difference \", diff / (y_test.size)\n", "intent": "Now to test on the test dataset.\n"}
{"snippet": "actual = list()\nexpected = list()\ndifference = list()\nfor idx in range(len(x_test)):\n    actual.append(y_test.iloc[idx][0])\n    expected.append(clf.predict(x_test[idx:idx+1])[0])\n    difference.append(clf.predict(x_test[idx:idx+1])[0]-y_test.iloc[idx][0])\n    print \"actual: \", y_test.iloc[idx][0], \"expected: \", clf.predict(x_test[idx:idx+1])[0]\n    print \"prediction was \", (clf.predict(x_test[idx:idx+1])[0]-y_test.iloc[idx][0]), \" off\"\n", "intent": "Now to test on the test dataset.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nprint('Test set AUC score:'+str(roc_auc_score(y_test,clf2.predict(X_test))))\nprint('Test set Accuracy:'+str(accuracy_score(y_test,clf2.predict(X_test))))\n", "intent": "Now check performance on the test set.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * 2 * tf.nn.l2_loss(gram - style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = pipeline.predict(heart_test.values)\n", "intent": "We can now make predictions on test data and evaluate the model.\n"}
{"snippet": "y_pred = clf.predict(heart_test.values)\naccuracy_opt = accuracy_score(target_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy_opt * 100.0))\n", "intent": "We can see the accuracy of the best parameter combination on test set.\n"}
{"snippet": "cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)\n", "intent": "The cross-validation score can be directly calculated using the `cross_val_score` helper.\n"}
{"snippet": "cross_val_score(svc, X_digits, y_digits, cv=kfold, scoring='precision_macro')\n", "intent": "Alternatively, the `scoring` arguments can be provided to specify an alternative scoring method.\n"}
{"snippet": "def evaluate(x, y, expr, x_value, y_value):\n    pass\nx = tt.iscalar()\ny = tt.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint(\"SUCCESS!\")\n", "intent": "Now try compiling and running a simple function:\n"}
{"snippet": "preds = baseball_clf.predict(X_test)\n", "intent": "Generate predictions\n"}
{"snippet": "X, Y = X_test, np.array(Y_test_labels)\npredictions = model.predict(x = X.reshape(shapeX))\nconfidence = predictionConfidence(predictions)\nprediction_barcode = predictionGetBarcodeLabel(predictions)\nselection =  (Y != prediction_barcode)\n", "intent": "--- \nManually inspect the mistakes that QuipuNet makes\n"}
{"snippet": "print(\"mse, this fit: %.2f \" % mean_squared_error(dfLogOCx.chl.values, chlMdlFreq))\nprint(r\"r^2, this fit: %.2f\" % r2_score(dfLogOCx.chl.values, chlMdlFreq))\nprint(\"mse, OC4v6: %.2f \" % mean_squared_error(dfLogOCx.chl.values, chlMdlOc4v6))\nprint(r\"r^2, OC4v6: %.2f\" % r2_score(dfLogOCx.chl.values, chlMdlOc4v6))\n", "intent": "Metrics for my freq. model\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\nprint('mean accuracy:',accuracies.mean())\nprint('accuracy stddev:',accuracies.std())\n", "intent": "Looking at the resulting confusion matrix, there is not a significant improvement.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(Resnet50model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = logistic.predict_proba(X)\npatients_score_df = patients_df.copy()\npatients_score_df[\"score\"] = score[:,1]\npatients_score_df.head(3)\n", "intent": "Then add the results in a new dataframe\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet50Model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = svc.predict(X_test)\nprint(predictions)\n", "intent": "Now we have a fit model, which can be used to make predictions about new data. We'll use the `predict` method to accomplish this.\n"}
{"snippet": "prediction = svc.predict(fmri_masked)\nprint(prediction)\n", "intent": "We can then predict the labels from the data\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_score = cross_val_score(svc, fmri_masked, targets)\nprint(cv_score)\n", "intent": "Also note that `sklearn` has tools to perform cross-validation more easily:\n"}
{"snippet": "train_pickup = pd.concat((train['pickup_latitude'], train['pickup_longitude']), axis=1)\ntrain_drop = pd.concat((train['dropoff_latitude'], train['dropoff_longitude']), axis=1)\ntest_pickup = pd.concat((test['pickup_latitude'], test['pickup_longitude']), axis=1)\ntest_drop = pd.concat((test['dropoff_latitude'], test['dropoff_longitude']), axis=1)\ntrain_plabels = kmeans.predict(train_pickup)\ntrain_dlabels = kmeans.predict(train_drop)\ntest_plabels = kmeans.predict(test_pickup)\ntest_dlabels = kmeans.predict(test_drop)\n", "intent": "Great! It seems that our model performs pretty well.\nNext, we create two columns to store the labels as features.\n"}
{"snippet": "Test_Predictions_1=best_est_logreg.predict(S_X_test)\nmetrics_logreg=metrics.classification_report(S_Y_test,Test_Predictions_1,target_names=['No Fraud','Fraud'])\nprint(metrics_logreg)\nconfusion_logreg_smote= metrics.confusion_matrix(S_Y_test,Test_Predictions_1)\nlog_fpr, log_tpr, log_thresold = metrics.roc_curve(S_Y_test,Test_Predictions_1)\n", "intent": "__Classification Report__\n"}
{"snippet": "predictions = logreg.predict(X_test)\n", "intent": "The model is performing quite well on the train set, with an accuracy score greater than the baseline of 0.5.\n"}
{"snippet": "predictions3 = lr3.predict(X_test_wp_ss)\nprint(f'The accuracy of the model on the test set is: \\\n{round(accuracy_score(y_test_w, predictions3),2)}.')\n", "intent": "This is a significant improvement on the accuracy from the previous models.\n"}
{"snippet": "print(f'Recall score/true positive rate is: {round(recall_score(y_test_w, predictions_w),2)}')\nprint(f'Specicity score/true negative rate is: {round(recall_score(y_test_w==0, predictions_w==0),2)}')\nprint(f'False positive rate is: {round(confusion[0][1] / (confusion[0][0] + confusion[0][1]),2)}')\n", "intent": "Again, this calculation cross checks with the accuracy score calculated by sklearn.\n"}
{"snippet": "print('\\nClassification report for model using 0.5 probability threshold...')\nprint(classification_report(y_test_w, predictions_w))\nprint('\\n\\nClassification report for model using 0.6 probability threshold...')\nprint(classification_report(y_test_w, predictions_w_60))\n", "intent": "The model has a greater precision in predicting high salaries, which is aligned with my boss' request.\n"}
{"snippet": "zip(y_test, lr.predict(X_test))\n", "intent": "Making a prediction. \nCombining the real value (y_test) with the value our regressor predicts (lr.predict(X_test))\n"}
{"snippet": "y_predt = lr.predict(X_test)\n", "intent": "Implementing R^2 and MSE functions to measure the performance of Linear regression. \n"}
{"snippet": "zip(y_test,ridge.predict(X_test))\n", "intent": " Combining the real values to the predicted values by the Ridge Regression Model\n"}
{"snippet": "y_predictedRg=ridge.predict(X_test)\n", "intent": " Measuring R^2 and MSE of our Ridge Regression Model\n"}
{"snippet": "preds = lr.predict(full_dev)\nprint(classification_report(dev_labels, preds))\n", "intent": "Full Classification Report:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(dog_breed_model.predict(np.expand_dims(test, axis=0))) for test in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = logistic.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Prediction accuracy: %.2f%%\" % (100*acc))\n", "intent": "Now that we have a trained model we can make a prediction on the test data and measure the prediction accuracy.\n"}
{"snippet": "logisticRegr.predict(test_images[0:28000])\n", "intent": "I predict with logistic regression the test images up to 280000\n"}
{"snippet": "predictions = model.predict_classes(scaled_x_test)\nprint(classification_report(y_test, predictions))\n", "intent": "Let's check how our model performs on new data.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, index in enumerate(style_layers):\n        G = gram_matrix(feats[index])\n        style_loss += style_weights[i] * tf.reduce_sum((G - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "score = silhouette_score(dfX, predicted, metric='euclidean')\nscore\n", "intent": "high Silhouette Score = clusters are well separated\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfrom statistics import mean\ndef compute_score(clf, X, y):\n    xval = cross_val_score(clf, X, y, cv = 5)\n    return mean(xval)\n", "intent": "<h1>5. Logistic regression score cross validation</h1>\nAverage of 5 cross-validations\n"}
{"snippet": "def parse_model_1(X):\n    target = X.Survived \n    class_dummies = pd.get_dummies(X['Pclass'], prefix='split_class')\n    X = X.join(class_dummies)\n    to_del = ['Name','Age','Cabin','Embarked','Survived','Ticket','Pclass','Sex']\n    for col in to_del : del X[col]\n    return X, target\nX, y = parse_model_1(train.copy())\ncompute_score(lr, X, y)\n", "intent": "<h1>7. Parse PClass</h1>\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "<a id=\"step10\"></a>\n[to the top](\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy {}: {:.4f}%'.format(modelname, test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "sgd.predict([digit])\n", "intent": "Now let us test it on the above considered random digit:\n"}
{"snippet": "lm.predict(x_new)    \n", "intent": "The input of predict () must have features that were used for training in the form of an array, such as a DataFrame.\n"}
{"snippet": "model = k_fold_CV(C=100, n_splits=5)\nmodel.fit_predict(X_gram, Y_class)\nmodel.results()\n", "intent": "We test the SVM algorithm on the full dataset (aka No-Zero-Active) with 5-fold cross-validation:\n"}
{"snippet": "test_embeddings_mean = encoder.predict(test_array)\ntest_embeddings_hull = ConvexHull(test_embeddings_mean)\n", "intent": "Let's compute the convex hull of the `test_embedding` points, so that it can be laid over the `training_embedding` points.\n"}
{"snippet": "y_pred_ridge_test = best_r.predict(X_test_s)\nprint 'Ridge r^2 score: ', np.round(r2_score(y_test, y_pred_ridge_test), 6)\nprint 'Ridge mean error: ', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge_test)), 4)\n", "intent": "The r^2 score was slightly higher using Ridge than Lasso, therefore this is the model that i'll choose to use on the test data.\n"}
{"snippet": "accuracy_score(ytest, pred_y)\n", "intent": "** Evaluating the Logistic Regression model performance **\n"}
{"snippet": "pred5 = bc_gs.predict(Xtest)\nbc_gs_accuracy_score = accuracy_score(ytest, pred5)\nbc_gs_accuracy_score\n", "intent": "Creating a bagging classifier model with the optimal parameters and comparing it's performance to the other two models.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_dog.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions) == np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.ERROR)\nsample_flower = np.array( [[5.0,3.0,4.0,2.0]], dtype=float)\npred = list(classifier.predict(sample_flower, as_iterable=True))\nprint(\"Predict that {} is: {}\".format(sample_flower,species[pred]))\nsample_flower = np.array( [[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]], dtype=float)\npred = list(classifier.predict(sample_flower, as_iterable=True))\nprint(\"Predict that {} is: {}\".format(sample_flower,species[pred]))\n", "intent": "Now that you have a neural network trained, we would like to be able to use it.  The following code makes use of our neural network.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,pred))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(College_Data['Cluster'], kmeans.labels_))\nprint('\\n')\nprint (classification_report(College_Data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = grid_search.predict(X_test)\nprint 'Accuracy:', accuracy_score(y_test, predictions)\nprint 'Confusion Matrix:', confusion_matrix(y_test, predictions)\nprint 'Classification Report:', classification_report(y_test, predictions)\nprint df['Sentiment'].as_matrix()\n", "intent": "- confusion matrices\n- Accuracy Precision, recall, and F1 score can be computed for each of the classes\n"}
{"snippet": "class_predict = log_model2.predict(X_test)\nprint(metrics.accuracy_score(Y_test,class_predict))\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "print(accuracy_score(expected, predicted))\n", "intent": "Finally we can see the metrics for performance:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredicted = model.predict(X_test)\nexpected = Y_test\nprint(accuracy_score(expected,predicted))\n", "intent": "Now we'll go ahead and see how well our model did!\n"}
{"snippet": "dog_breed_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_res]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def predict(X,w):\n    z = X.dot(w[1:]) + w[0]\n    yhat = (z > 0)\n    return yhat\nyhat = predict(Xts,w)\nacc = np.mean(yhat == yts)\nprint(\"Test accuracy = %f\" % acc)\n", "intent": "measure accuracy\n--\n"}
{"snippet": "import numpy as np\nfrom scipy.optimize import check_grad\nfrom gradient_check import eval_numerical_gradient_array\ndef rel_error(x, y):\n      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n", "intent": "<h2 align=\"center\">BackProp and Optimizers</h2>\n<img src=\"img/bp.png\" width=\"600\">\n"}
{"snippet": " def softmax_loss(x, y):\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"./img/loss.png\" width=\"300\">\n<img src=\"./img/log.png\" width=\"600\">\n"}
{"snippet": "from plot_helpers import custom_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\ny_true = y\ny_pred = rfc.predict(X)\ncm = confusion_matrix(y_true, y_pred)\ncustom_confusion_matrix(cm, classes=['white','black'], title='Confusion matrix, without normalization')\n", "intent": "Visualize confusion matrix\n"}
{"snippet": "prediction_input_fn = lambda: input_fn(my_feature, targets, num_epochs=1, shuffle=False)\npredictions = linear_regressor.predict(input_fn=prediction_input_fn)\npredictions = np.array([item['predictions'][0] for item in predictions])\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nroot_mean_squared_error = math.sqrt(mean_squared_error)\nprint(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\nprint(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)\n", "intent": "Let's predict how well our model fit training_data during training\n"}
{"snippet": "print('Accuracy = {}%'.format((y_test == pred).mean() * 100))\nprint('ROC-AUC = {}'.format(roc_auc_score(y_test,pred)))\n", "intent": "Let's see some meassures of accuracy. We do the usual mean of absolute value of differences but we also compute ROC-AUC\n"}
{"snippet": "cross_val_score(randforest,Xtrain[~arr],ytrain[~arr],cv=5,scoring=scorer)\n", "intent": "<h4>Our random forests regressor fits the data very well. Let's see how it performs when we generalize it with Cross Validation</h4>\n"}
{"snippet": "ypred = randforest.predict(Xtest)\nprint('Random Forst R^2 Score: {:.4f}'.format(r2_score(ytest,ypred)))\n", "intent": "<h3> Indeed, the model has generalized the data very well too. </h3>\n<h2> Finally, let's see how the random forests performs on the holdout data</h2>\n"}
{"snippet": "to_test_df_with_text = sparse.hstack([to_test_df[X_vars], to_test_df_features, to_test_df_desc]).tocsr()\nto_test_df_text_combined = sparse.hstack([to_test_df_features, to_test_df_desc]).tocsr()\nto_test_y_pred_gbc = gbc.predict_proba(to_test_df[X_vars])\nto_test_y_pred_sgd = sgd.predict_proba(to_test_df_text_combined)\nto_test_y = gbc_semi.predict_proba(sparse.hstack([to_test_df_with_text, to_test_y_pred_gbc, to_test_y_pred_sgd]).tocsr())\n", "intent": "Hmm. So using one single model is better?\n"}
{"snippet": "y_train_pred_cv_l1_avg = y_train_pred_cv_base_lbgm + y_train_pred_cv_text_sgd + y_train_pred_cv_image_lbgm\ny_train_pred_cv_l1_avg = y_train_pred_cv_l1_avg/y_train_pred_cv_l1_avg.sum(axis=1)[:, np.newaxis]\nlog_loss(X_train.interest_level, y_train_pred_cv_l1_avg)\n", "intent": "Try plain averaging.\n"}
{"snippet": "l1_lbgm = GBMClassifier(application='multiclass', metric='multi_logloss', learning_rate=0.1, num_iterations=100, num_class=3, early_stopping_round=10, verbose=False)\ncross_val_score(l1_lbgm, X_train_feature_l1, X_train.interest_level, scoring='neg_log_loss', cv=ps, n_jobs=-1)\n", "intent": "Try `GBMClassifier`.\n"}
{"snippet": "yts_hat = regr.predict(Xts)\nRSS = np.mean((yts_hat - yts)**2)/(np.std(yts)**2)\nprint(RSS)\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "poly1_data[\"predicted\"] = regfit.predict(poly1_data.iloc[:,0].to_frame())\n", "intent": "Now we will visualise our data against the fitted line from the model we built.\n"}
{"snippet": "poly2_data[\"predicted\"] = regfit2.predict(poly2_data[features2])\n", "intent": "Now we will visualise our data against the fitted line from the model we built.\n"}
{"snippet": "residule_test = model_with_best_rss.predict(test[all_features]) - test['price']\nprint 'RSS from our best model with test data',(residule_test * residule_test).sum()\n", "intent": "Now we will compute RSS from our test set.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nprint(LCV.score(X_test,y_test))\nprediction = LCV.predict(X_test)\nprint(prediction[:20],\"\\n\",y_test[:20].values)\nmae = mean_absolute_error(y_test,prediction)\nmse = mean_squared_error(y_test,prediction)\nprint('Mean Absolute Error: {0:.3f}, Mean Squared Error: {1:.3f}'.format(mae,mse))\n", "intent": "* the trained model is applied to the test data\n* Model scoring is done using MAE and MSE\n"}
{"snippet": "preds = model.predict(X_test)\ncls = model.predict_classes(X_test)\n", "intent": "<img src=\"loss_and_accuracy_graph.png\">\n"}
{"snippet": "evaluate(6)\n", "intent": "Based on weighted_counter, my KNN classifier results in a close weight between class 4 (Coat) with weight .304 and 6 (Shirt) with weight .286.\n"}
{"snippet": "tb = RandomForest(X_train, y_train, n_trees=20, sample_sz=150,min_leaf=4,max_features=1)\ntb_train_pred = tb.predict(X_train)\ntb_val_pred = tb.predict(X_val)\nprint(f'MSE train score: {metrics.mean_squared_error(y_train,tb_train_pred)}')\nprint(f'MSE val score: {metrics.mean_squared_error(y_val,tb_val_pred)}')\nprint(f'R2 train score: {metrics.r2_score(y_train,tb_train_pred)}')\nprint(f'R2 val score: {metrics.r2_score(y_val,tb_val_pred)}')\n", "intent": "One possible thing I can tune here but not in sklearn Random Forest is the size of data subset for each tree (sample_sz)\n"}
{"snippet": "linreg.predict(25)\n", "intent": "We can also use the \"predict\" function for the linreg object:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This is not a better classifier, as the accuract on the test data is 74%, down from 77%\n"}
{"snippet": "prediction = knn.predict(X_new)\nprint('Prediction: {}'.format(prediction))\nprint('Predicted target name: {}'.format(iris_dataset['target_names'][prediction]))\n", "intent": "To make a prediction we call the `predict` method of the `knn` object.\n"}
{"snippet": "preds = perceptron.predict(X)\nprint(mean_squared_error(preds, y))\n", "intent": "Let us find out how large the loss is for our current model, which has randomly initialized parameters:\n"}
{"snippet": "tempResults = tree.predict(X_test)\ntempY = y_test.tolist()\nanalysis = perf_measure(tempY, tempResults)\nprint(\"The TP rate is: \", analysis[0])\nprint(\"The FP rate is: \", analysis[1])\nprint(\"The TN rate is: \", analysis[2])\nprint(\"The FN rate is: \", analysis[3])\n", "intent": "Get the confusion matrix information (actual matrix made in presentation) \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    _sum = torch.zeros(1)\n    for index, layer in enumerate(style_layers):\n        _sum += style_weights[index] * torch.sum((gram_matrix(feats[layer]) - style_targets[index]) ** 2)\n    return _sum\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "if 'cluster' in cm_kmeans_df.columns.tolist():\n    del cm_kmeans_df['cluster']\ncm_algorithm = cm_k_algs[4] \ncm_clustering = cm_k_res[4]\ncm_cls_list = cm_algorithm.predict(cm_kmeans_df.drop(['type', 'name'], axis = 1))\ncm_kmeans_df['cluster'] = cm_cls_list\n", "intent": "The elbow method again suggests that 6 is a sensible choice for $k$. Lets' see...\n"}
{"snippet": "test_sample = choices(X_train, k=10)\nreconstructions = autoencoder.predict(test_sample)\nshow_side_by_side(test_sample, reconstructions)\n", "intent": "Now that we have trained the autoencoder, lets randomly sample images from the test set and see how well it can reconstruct them.\n"}
{"snippet": "def predict_cluster(test_data, test_np_matrix, kmeans_model):\n    predicted_labels = kmeans_model.predict(test_np_matrix)\n    output_data = {}\n    for index, artist_id in test_data['ArtistID'].iteritems():\n        output_data[artist_id] = predicted_labels[index]\n    return output_data\nkmeans_5_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_5)\nkmeans_25_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_25)\nkmeans_50_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_50)\n", "intent": "**b.** For each artist in the test set, call **[predict](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "preds = knnc.predict(test);preds\n", "intent": "Now lets try predicting. This will actually take some time, but hopefully only a few tens of seconds...\n"}
{"snippet": "preds = mnb.predict(test);preds\n", "intent": "...and predict the category labels for cat6:\n"}
{"snippet": "print \"Accuracy: \", metrics.accuracy_score(y_test, y_pred)\n", "intent": "~ 86.9%, there is defintely a lot of room for improvement\n"}
{"snippet": "error = mean_absolute_error(y_test, y_pred)\nprint(\"Mean absolute error: {:.1f} eur/sqm\".format(error))\nmean_difference = np.mean(y_pred - y_test)\nprint(\"Average prediction error: {:.1f} eur/sqm\".format(mean_difference))\n", "intent": "Compute the mean error of prediction at the test whole partition.\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))) \n    source = np.expand_dims(source,axis=0) \n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = model.predict(test[columns])\nmean_squared_error(predictions, test[target])\n", "intent": "<h3>Predicting error</h3>\n"}
{"snippet": "y_preds = final_model.predict(X_test)\n", "intent": "* Fit the model to the data\n"}
{"snippet": "import numpy as np\ndef style_loss(feats, style_layers, style_targets, style_weights):\n    loss = torch.zeros(1)\n    i = 0\n    for layer in style_layers:\n        cur_layer_gram = gram_matrix(feats[layer])\n        cur_loss = torch.sum((cur_layer_gram - style_targets[i]) ** 2) * style_weights[i]\n        i += 1\n        loss += cur_loss\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "cross_val_score(pipeline, X, y, cv=5, scoring='f1_weighted')\n", "intent": "Cross-validating result to check variance across multiple train folds\n"}
{"snippet": "model_predictions = model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, model_predictions)\nprint (\"Accuracy of the model is :\" , \n       accuracy , \"\\nApproximately : \", \n       round(accuracy*100) , \"%\\n\")\n", "intent": "Let us see its performance on the test dataset. \n"}
{"snippet": "originals = evaluate(predictions_Org, test_Y)\ntransforms1 = evaluate(predictions_LDA, test_Y)\ntransforms2 = evaluate(predictions_PCA, test_Y)\nplot_CRR(originals, transforms1, transforms2)\n", "intent": "** Below is the implementation of the above function to draw the desired CRR table**\n"}
{"snippet": "true = []\npred = []\nfor i in range(6):\n    for j in range(1,101):\n        s = predict(folders[i]+str(j).zfill(3)+\".jpg\")\n        if s in ten:\n            pred.append(s)\n        else:\n            pred.append(\"other\")\n        true.append(birds[i])\n", "intent": "Let's going to predice some images with the imageNet Network without being modified.\n"}
{"snippet": "VGG19_predictions=[np.argmax(VGG19_model.predict(np.expand_dims(feature,axis=0))) for feature in test_VGG19]\ndef test_accuracy(prediction_result):\n    TP=np.sum(np.array(VGG19_predictions)==np.argmax(test_targets,axis=1))\n    return TP/len(VGG19_predictions)*100\naccuracy=test_accuracy(VGG19_predictions)\nprint('Test accuracy: %.4f%%'%accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "viya_score = model1.predict(my_images, buffer_size=batch_size, n_threads=1)\nprint(viya_score['ScoreInfo'])\n", "intent": "Now use the SAS Viya classification model to score the test images.\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method=\"decision_function\")\ny_scores\n", "intent": "So raising the threshold will decrease the recall. \n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "sklearn did not have to detect/use OvA or OvO since Random Forests can directly classify instances into multiple classes.\n"}
{"snippet": "log_reg.predict([[1.7], [1.5]])\n", "intent": "Where the lines cross is the decision boundary.\n"}
{"snippet": "y_test = regressor.predict(X_test_scaled)\n", "intent": "We can now predict the test data and write the results to a txt file. \n"}
{"snippet": "x_test=movie_seq[len(movie_seq)-B:]\nx_test=one_hot(x_test)\nx_test=np.array(x_one_hot,dtype=float).reshape((1,x_test.shape[0],x_test.shape[1]))\npredict_new=model.predict(x_test)\ntop10=np.argsort(predict_new[0,-1])\nk=0\nfor i in range(len(top10)-1,0,-1):\n    if top10[i]not in movie_seq and k<10:\n        print idx2movie[top10[i]]\n        k+=1\n", "intent": "The predicted match the ground truth. Next we can list the recommended movies from the final batch. Only unseen movies are recommended for user 196.\n"}
{"snippet": "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\nprint(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))\nprediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\nprint(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\nprint(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\nmodel.save(\"iris_nn.h5\")\n", "intent": "Use the testing set to test your model, clearly calculating and displaying the error rate.\n"}
{"snippet": "predictOutput = model.predict(testX)\n", "intent": "In this section we compute Precision and Recall metrics for each of the emotion classes.\n"}
{"snippet": "pred=clf.predict(test[features].values)\n", "intent": "Predictions for test set\n"}
{"snippet": "conf_mat=sklearn.metrics.confusion_matrix(np.ravel(train[target].values),clf.predict(train[features].values))\nprint conf_mat\n", "intent": "Let's see the confusion matrix for predicted values of training set\n"}
{"snippet": "user_prediction_error_eval = sqrt(mean_squared_error(user_ratings_prediction, user_testing_ratings_prediction))\nprint('User-based CF RMSE')\nprint(user_prediction_error_eval)\nprint(\"\")\nitem_prediction_error_eval = sqrt(mean_squared_error(item_ratings_prediction, item_testing_ratings_prediction))\nprint('User-based CF RMSE')\nprint(item_prediction_error_eval)\n", "intent": "I used the popular metric Root Mean Squared Error (RMSE) to evaluate the accuracy of the predicted ratings. \n"}
{"snippet": "X_transformed = vect_2.transform(X)\ndf['Predicted'] = model_final.predict(X_transformed)\ndf.head(10)\n", "intent": "Add the predictions to the dataframe, save the results for safe keeping.\n"}
{"snippet": "ols.predict(5000)\n", "intent": "Suppose that we want to predict the credit card balance of a customer that has a limit of 5000 dollars.\n"}
{"snippet": "y_pred = lasso.predict(X_test)\ny_pred = y_pred*np.mean(np.exp(y_train-y_fit)) \n", "intent": "The code for computing the test predictions using this method is the following.  \n"}
{"snippet": "y_pred = test['tokens'].apply(lambda tokens: (feature in tokens))\nerror  = 1 - accuracy_score(test['positive'], y_pred)\nprint(error.round(3))\n", "intent": "The appearance of \"thank\" would therefore lead to us to classify a tweet as positive. Let's evaluate the performance of the model for the test data. \n"}
{"snippet": "clf2 = pickle.load(open(ROBOT_POSE_CLF))\nclf2.predict(all_data[-1]), all_target[-1]\n", "intent": "Then, in the application we can load the trained classifier again.\n"}
{"snippet": "r2_score_lr = r2_score(y_test, y_pred_lr)\nprint('R squared for linear regression: %f' %r2_score_lr)\n", "intent": "Measuring R^2 and MSE:\n"}
{"snippet": "rsme=math.sqrt(mean_squared_error(y_test, y_pred_lasso))\nprint('RSME for Lasso: %f' %rsme)\n", "intent": " R squared is slightly worse for Lasso than LinearRegression\n"}
{"snippet": "def generate_text(model, length, vocab_size, ix_to_char):\n    ix = [np.random.randint(vocab_size)]\n    y_char = [ix_to_char[ix[-1]]]\n    X = np.zeros((1, length, vocab_size))\n    for i in range(length):\n        X[0, i, :][ix[-1]] = 1\n        print(ix_to_char[ix[-1]], end=\"\")\n        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n        y_char.append(ix_to_char[ix[-1]])\n    return ('').join(y_char)\n", "intent": "Method to generate the text\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy: ', test_acc)\n", "intent": "next, compare how the model performs on the test dataset:\n"}
{"snippet": "print(\"velocity test error: \", 1-np.mean(rf_v.predict(X_test)==Vel_test))\nprint(\"horizontal acceleration test error: \", 1-np.mean(rf_ax.predict(X_test)==Ax_test))\nprint(\"vertical acceleration test error: \", 1-np.mean(rf_az.predict(X_test)==Az_test))\n", "intent": "I am getting seemingly high error across all levels. I'll now calculate the test error for each response.\n"}
{"snippet": "tn, fp, fn, tp = confusion_matrix(y_test, rf_fit.predict(X_test)).ravel()\nprecision = tp/(tp+fp)\nrecall = tp/(tp+fn)\nfscore = 2*(precision*recall)/(precision+recall)\nprint('Precision: ', precision)\nprint('Recall: ', recall)\nprint('F-score: ', fscore)\n", "intent": "I will also calculate the precision and recall of this model, along with the F-score.\n"}
{"snippet": "score = metrics.accuracy_score(y_test, model.predict(X_test))\nscore\n", "intent": "[for the overall model]\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy VGG16: %.4f%%' % test_accuracy)\nInceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy InceptionV3: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_file = readconll('eng.testa')\nto_predict_test = vectorizer.transform(featurizer(test_file)) \npredicted_classes_test = classifier.predict(to_predict_test) \ni = 0 \nfor sentence in test_file:\n    for token in sentence:\n        token.append(predicted_classes_test[i])\n        i += 1\n", "intent": "**Running classifier on test data**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_predict = clf.predict(X_train)                   \nprint 'Training accuracy = ', accuracy_score(y_train, y_predict)\nclass_labels = clf.get_classes()     \nprint 'Class labels=', class_labels\nprint 'Confusion matrix [known in lines, predicted in columns]=\\n',confusion_matrix(y_train, y_predict, class_labels)\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "import libscores\nY_train, C = libscores.onehot(y_train)                                   \nprint 'Dimensions Y_train=', Y_train.shape, 'Class labels=', C\nassert((class_labels==C).all()) \nfrom libscores import bac_metric \nfrom libscores import pac_metric \ny_predict_proba = clf.predict_proba(X_train)      \nprint 'Training balanced accuracy = ', bac_metric(Y_train, y_predict_proba, task='multiclass.classification')\nprint 'Training probabilistic accuracy = ', pac_metric(Y_train, y_predict_proba, task='multiclass.classification')\n", "intent": "ADVANCED: Sklearn does not have multi-class metrics, this shows how libscore metrics work.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_eval)\n", "intent": "$$ accuracy = \\frac{1}{m} \\sum_{i=1}^{m_{test}}{\\mathbb{1}{\\{y^{(i)} = \\hat{y}^{(i)}\\}}} $$\n"}
{"snippet": "X=mix_wav_mag[1:].reshape(1, 512, 128, 1)\ny=model.predict(X, batch_size=32)\ntarget_pred_mag = np.vstack((np.zeros((128)), y.reshape(512, 128)))\n", "intent": "1. Ignore the first row from magnitude of mix sound track.\n2. Feed the magnitude into UNet.\n3. Convert model output to target magnitude.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy2 = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy2)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "k = 1\ny_pred = find_k_closest(dist_m, y_train, k=k)\nprint(\"Parameters:\")\nprint(\"k = {}\".format(k))\nprint(\"\\n\")\nprint(\"Test set report\")\nprint(classification_report(y_test, y_pred, target_names=label_names))\n", "intent": "Now we find the k closest neighbors with k = 1...\n"}
{"snippet": "def find_k_best(dist_m, y_train, y_test, k_range=np.arange(1,22)):\n    k_range = np.arange(1,22) \n    f1_scores = np.empty(k_range.shape) \n    for k in k_range:\n        y_pred = find_k_closest(dist_m, y_train, k=k)\n        f1_scores[k-1] = f1_score(y_test, y_pred, average='macro')\n    return k_range[np.argmax(f1_scores)]\n", "intent": "Now if instead of keeping k=1 we try to optimize this parameter to achieve the best f1 score:\n"}
{"snippet": "k = 1\ny_pred = find_k_closest(dist_m_1, y_train, k=k)\nprint(\"Parameters:\")\nprint(\"k = {}\".format(k))\nprint(\"\\n\")\nprint(\"Test set report\")\nprint(classification_report(y_test, y_pred, target_names=label_names))\n", "intent": "And now we find the 1 nearest neighbor\n"}
{"snippet": "k_best = find_k_best(dist_m_1, y_train, y_test, k_range=np.arange(1,22))\ny_pred = find_k_closest(dist_m_1, y_train, k=k_best)\nprint(\"Parameters:\")\nprint(\"k = {}\".format(k_best))\nprint(\"\\n\")\nprint(\"Test set report\")\nprint(classification_report(y_test, y_pred, target_names=label_names))\n", "intent": "Now like we did before, let's try to optimize k...\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The new classifier didn't do better but it was less of an overfit.\n"}
{"snippet": "GENERATED_LENGTH = 1000\ntest_text = initial_text\ngenerated_text = []\nfor i in range(1000):\n    X = np.reshape(test_text, (1, SEQ_LENGTH, 1))\n    next_character = model.predict(X/float(VOCABULARY))\n    index = np.argmax(next_character)\n    generated_text.append(int_to_char[index])\n    test_text.append(index)\n    test_text = test_text[1:]\n", "intent": "Starting with the initial seed next 1000 characters are generated by shifting the 100 character input window for generating the next character.\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    residual = tf.abs(labels - predictions)\n    condition = tf.less(residual, delta)\n    small_res = 0.5 * tf.square(residual)\n    large_res = delta * residual - 0.5 * tf.square(delta)\n    return tf.where(condition, small_res, large_res)\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "def compute_loss(X, y, w):\n    X = expand(X)\n    z = 1 - y * tf.squeeze((X @ tf.reshape(w, [-1, 1])))\n    return tf.reduce_mean(tf.where(z >= 0, z, tf.zeros_like(z)))\ndef compute_grad(X, y, w):\n    X = expand(X)\n    z = 1 - y * tf.squeeze((X @ tf.reshape(w, [-1, 1])))\n    mask = tf.where(z >= 0, tf.ones_like(z), tf.zeros_like(z))\n    return tf.reduce_mean(tf.reshape(-mask * y, [-1, 1]) * X, axis=0)\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "X_in = tf.placeholder(tf.float32, shape=[None, X.shape[1]], name=\"X\")\ny_in = tf.placeholder(tf.float32, shape=[None], name=\"y\")\nw_in = tf.placeholder(tf.float32, shape=[6], name=\"w\")\nloss_out = compute_loss(X_in, y_in, w_in)\ngrad_out = compute_grad(X_in, y_in, w_in)\n", "intent": "Common things for all algos:\n"}
{"snippet": "metrics.accuracy_score(df_train[5] ^ df_train[95], df_train[100])\n", "intent": "We might notice that target is just a result of XORing those two features. Let's check:\n"}
{"snippet": "print('Coefficients: %.2f' % regr.coef_)\nprint('Intercept: %.2f' % regr.intercept_)\nprint('Residual variance estimate: %.2f'\n      % (np.sum((regr.predict(x) - y) ** 2)/(n-2)))\n", "intent": "Print results related to the regression\n"}
{"snippet": "varhat=np.sum((regr.predict(x) - y) ** 2)/(n-2) \nSSTx=np.sum((x - np.mean(x)) ** 2)\nh=1/n+((x-np.mean(x))**2/SSTx)\nresiduals=(regr.predict(x) - y)\nT=np.divide(residuals,np.sqrt(1-h))/np.sqrt(varhat) \n", "intent": "**2.3.6. and 2.3.7.** Estimate of $\\sigma^2$ and residual analysis\n"}
{"snippet": "MSE = np.sum((regr.predict(x1) - y) ** 2)/(n-2)\nprint('Residual variance estimate: %.2f'% MSE)\n", "intent": "Mean squared error of predicted outputs - residual variance estimate\n"}
{"snippet": "varhat=np.sum((regr.predict(x1) - y) ** 2)/(n-2)\nSSTx=np.sum((x1 - np.mean(x1)) ** 2)\nh=1/n+((x1-np.mean(x1))**2/SSTx)\nresiduals=(regr.predict(x1) - y)\nT=np.divide(residuals,np.sqrt(1-h))/np.sqrt(varhat) \n", "intent": "Standard deviations and standardized residuals\n"}
{"snippet": "MSE = np.sum((regr2.predict(X) - y) ** 2)/(n-(1+p))\nprint('Residual variance estimate: %.2f'% MSE)\n", "intent": "Mean squared error of predicted outputs - residual variance estimate\n"}
{"snippet": "mean_y = np.mean(y)\nMSR = np.sum((regr2.predict(X) - mean_y) ** 2)/p\nF = MSR/MSE\nF_thres = scipy.stats.f.ppf(1-alpha,p,n-(p+1))\nprint('Decision statistics : F = %.2f' %F)\nprint('Decision threshold = %.2f' % F_thres)\n", "intent": "**3.3.6.** Hypothesis testing on the overall model: F-test\n"}
{"snippet": "predictions = logistic.predict(X_test)\npredictions\n", "intent": "Ok so this is printing out the probabilities for each tag\n"}
{"snippet": "inception_predictions = [np.argmax(model_inception.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_acc = 100 * np.sum(np.array(inception_predictions) == np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_acc)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "As before, we need to coerce these x values into a `[n_samples, n_features]` features matrix, after which we can feed it to the model:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Finally, we can use the `accuracy_score` utility to see the fraction of predicted labels that match their true value:\n"}
{"snippet": "from sklearn.metrics import accuracy_score \naccuracy_score(y, y_model)\n", "intent": "Finally, we compute the fraction of correctly labeled points:\n"}
{"snippet": "x_test = test.drop(['label'], axis=1)\ny_test = test['label']\nresults = model.evaluate(x_test, y_test, verbose = 0)\nprint(results)\n", "intent": "Let's see how our model performs against our test data.\n"}
{"snippet": "activations = activation_model.predict(img_tensor)\n", "intent": "When fed an image input, this model returns the values of the layer activations in the original model. \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit, target_names=faces.target_names))\n", "intent": "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test_pandu1.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "my_prediction = my_tree_one.predict(test_features)\n", "intent": "Make the prediction applying our model \"my_tree_one\" to the test set:\n"}
{"snippet": "z_bar = ocsvm.predict(X_test) > 0\nres = res.append(score_result(z_test, z_bar, \"One-class SVM\"))\ndisplay(HTML(res.to_html()))\n", "intent": "Now say we suddenly have access to our query set, we can use this SVM to *also classify* if the new point is in the support of our training data!\n"}
{"snippet": "print(classification_report(np.argmax(Y_val_cat,axis=1),np.argmax(val_preds,axis=1)))\n", "intent": "Now let's look at f1-score precision and recall to see how our model is performing on each digit.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", list(lin_reg.predict(some_data_prepared).astype(int)))\n", "intent": "Done!! You now have a working Linear Regression model. Let's try it out on a few instances from the training set:\n"}
{"snippet": "clf = apply_cluster(x_cols, k=9)[0]\nclusters = clf.predict(x_cols)\n", "intent": "Based on the elbow at 9, I think 9 is a good value for K.\n"}
{"snippet": "confusion_matrix(y_train, (model.predict(X_train)>0.5)).ravel()\n", "intent": "total test dataset size is\n"}
{"snippet": "def regularized_loss(betas, lambda_l1, lambda_l2, X, y):\n    return mse(betas, X, y) + \\\n           elastic_net(betas, lambda_l1, lambda_l2)\n", "intent": "Define the regularised loss that will be minimised as the sum of `mse` and `elastic_net`.\n"}
{"snippet": "def loss(parameters, X, y):\n    return -np.sum(y * predict(parameters, X))\n", "intent": "Define the loss function to minimise.\n"}
{"snippet": "def accuracy(parameters, X, y):\n    y = np.argmax(y, axis=1)\n    y_hat = np.argmax(predict(parameters, X), axis=1)\n    return np.mean(y_hat == y)\n", "intent": "Define a function to measure the accuracy of predictions made using `parameters`.\n"}
{"snippet": "predict(parameters, X_train[:5, :])\n", "intent": "Check initial predictions and loss for the first batch.\n"}
{"snippet": "predictions = model_5.predict(validation_data[features])\nnum_fp = sum((validation_data[target] == -1) & (predictions == +1))\nprint num_fp\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "yhat2 = logreg2.predict(Xs)\nacc = np.mean(yhat2==y2)\nprint(\"Accuracy on training data = %f\" % acc)\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    result = 0\n    for i, idx in enumerate(style_layers):\n        diff = gram_matrix(feats[idx]) - style_targets[i]\n        result += tf.reduce_sum(diff * diff) * style_weights[i]\n    return result\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "ypred = classifier.predict(xtest)\n", "intent": "With the classifier trained, we are ready to make predictions on the test set!\n"}
{"snippet": "print(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This high training accuracy (97.6%) and much lower test accuracy (71.1%) suggests that the model might be overfitting.\n"}
{"snippet": "RN50_model_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(RN50_model_predictions)==np.argmax(test_targets, axis=1))/len(RN50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_ridge_pred = ridge_regr.predict(x_test)\ny_lasso_pred = lasso_regr.predict(x_test)\ny_elnet_pred = elnet_regr.predict(x_test)\ny_rf_pred = rf_regr.predict(x_test)\ny_svm_pred = svm_regr.predict(x_test)\ny_xgb_pred = xgb_regr.predict(x_test)\n", "intent": "First, the individual model predictions:\n"}
{"snippet": "tl_own_predictions = [np.argmax(cnn_tl_own.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(tl_own_predictions)==np.argmax(test_targets, axis=1))/len(tl_own_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_preds = best_alpha*X_test_level2[:, 0] + (1 - best_alpha)*X_test_level2[:, 1] \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nrss\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report\ny_pred_gini = DTgini.predict(X_test)\ny_pred_gini_insample = DTgini.predict(X_train)\nprint('DT_Accuracy (out-of-sample): %.2f' % accuracy_score(y_test, y_pred_gini))\nprint('DT_Accuracy (in-sample): %.2f' % accuracy_score(y_train, y_pred_gini_insample))\nprint('DT_F1 score (out-of-sample): ', f1_score(y_test, y_pred_gini, average='macro'))\nprint('DT_F1 score (in-sample)    : ', f1_score(y_train, y_pred_gini_insample, average='macro'))\nprint('DT_Kappa score (out-of-sample): ', cohen_kappa_score(y_test, y_pred_gini))\nprint('DT_Kappa score (in-sample)    : ', cohen_kappa_score(y_train, y_pred_gini_insample))\nprint(classification_report(y_test, y_pred_gini, target_names=None))\n", "intent": "4.predictive accuracy, precision, recall, f-measure\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report\ny_pred_knn3 = knn3.predict(X_test_std)\ny_pred_knn3_insample = knn3.predict(X_train_std)\nprint('knn3_Accuracy (out-of-sample): %.2f' % accuracy_score(y_test, y_pred_knn3))\nprint('knn3_Accuracy (in-sample): %.2f' % accuracy_score(y_train, y_pred_knn3_insample))\nprint('knn3_F1 score (out-of-sample): ', f1_score(y_test, y_pred_knn3, average='macro'))\nprint('knn3_F1 score (in-sample)    : ', f1_score(y_train, y_pred_knn3_insample, average='macro'))\nprint('knn3_Kappa score (out-of-sample): ', cohen_kappa_score(y_test, y_pred_knn3))\nprint('knn3_Kappa score (in-sample)    : ', cohen_kappa_score(y_train, y_pred_knn3_insample))\nprint(classification_report(y_test, y_pred_knn3, target_names=None))\n", "intent": "3.1.Evaluate performance (k=3)\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report\ny_pred_knn7 = knn7.predict(X_test_std)\ny_pred_knn7_insample = knn7.predict(X_train_std)\nprint('knn7_Accuracy (out-of-sample): %.2f' % accuracy_score(y_test, y_pred_knn7))\nprint('knn7_Accuracy (in-sample): %.2f' % accuracy_score(y_train, y_pred_knn7_insample))\nprint('knn7_F1 score (out-of-sample): ', f1_score(y_test, y_pred_knn7, average='macro'))\nprint('knn7_F1 score (in-sample)    : ', f1_score(y_train, y_pred_knn7_insample, average='macro'))\nprint('knn7_Kappa score (out-of-sample): ', cohen_kappa_score(y_test, y_pred_knn7))\nprint('knn7_Kappa score (in-sample)    : ', cohen_kappa_score(y_train, y_pred_knn7_insample))\nprint(classification_report(y_test, y_pred_knn7, target_names=None))\n", "intent": "3.1.Evaluate performance (k=7)\n"}
{"snippet": "print('The first two instances are predicted to belong to class:', LR.predict(X.iloc[:2, :]))\nprint('The probabilities of belonging to each one of the classes are estimated as:', LR.predict_proba(X.iloc[:2, :]))\n", "intent": "3.Apply the logistic regression model\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 45\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "test_point_index = 4\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 0\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 4\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "MyResNet50_predictions = [np.argmax(MyResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(MyResNet50_predictions)==np.argmax(test_targets, axis=1))/len(MyResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(confusion_matrix(y_test, predictions_rfc))\nprint(classification_report(y_test, predictions_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = regr.predict(xtrain)\n", "intent": "Train the model on the training data using the `regr.fit(...)` method.\n"}
{"snippet": "xtest = df_test[['q2','dq2','eps21', 'eps22', 'eps31', 'eps32','ddq2']]\nytest = df_test['I2']\npredictions = regr.predict(xtest)\n", "intent": "Measure the normalized RSS on the test data.  Is it substantially higher than the training data?\n"}
{"snippet": "def measure_performance(X,y,clf,show_accuracy=True,show_classification_report=True,show_confusion_matrix=True):\n    y_pred = clf.predict(X)\n    if show_accuracy:\n        print \"Accuracy: {0:.3f}\".format(metrics.accuracy_score(y,y_pred)),\"\\n\"\n    if show_classification_report:\n        print \"Classification Report\"\n        print metrics.classification_report(y,y_pred),\"\\n\"\n    if show_confusion_matrix:\n        print \"Confusion Matrix\"\n        print metrics.confusion_matrix(y,y_pred),\"\\n\"\n", "intent": "Create function to evaluate model\n"}
{"snippet": "print np.mean( (bos.PRICE - lm.predict(X))**2 )\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\nclass_names = data.target_names\nplot_confusion_matrix(cnf_matrix, class_names)\n", "intent": "We plot the confusion matrix:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(predictions, target)\nprint(mse)\n", "intent": "In terms of metrics, the [sklearn.metrics](http://scikit-learn.org/stable/modules/classes\n"}
{"snippet": "S = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nS\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lm, X, bos.PRICE, cv=4, scoring= 'neg_mean_squared_error')\nscores\n", "intent": "**Cross_val_score**\n"}
{"snippet": "y_pred = clf.predict(x.reshape(-1, 1))\n", "intent": "We can now see how our SVR models the data.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Let's see how accurate our $k$-means classifier is **with no label information:**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Now I have trained my model it's time to make the predictions\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test.values,predictions_one))\nprint('\\n')\nprint(classification_report(y_test.values,predictions_one))\nprint('\\n')\nprint(\"Accuracy: {:.4%}\".format(np.mean(predictions_one == y_test.values)))\n", "intent": "Let's see how our model has performed.\n"}
{"snippet": "predictions_three = pipeline.predict(x_test.values)\n", "intent": "Now to see if my change of classifier will improve our accuracy results.\n"}
{"snippet": "test_data_predictions = pipeline.predict(test.values)\n", "intent": "Now that I have created my model I will apply it to my test dataset and export the results as a CSV.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,X = X_train, y = y_train,cv = 10)\nprint(\"Average  = \",accuracies.mean())\nprint(\"Variance = \",accuracies.std())\n", "intent": "- Use different fold to test the model performance. more robust\n- Evaluate both average and standard deviation\n"}
{"snippet": "y_pred = lr.predict(X)\ny_pred\n", "intent": "To see how it works out more vividly, let's plot the true targets and predicted targets using `lr`:\n"}
{"snippet": "scores = model_selection.cross_val_score(lr, X_test, y_test, cv=k_folds, scoring='r2')\nAvg_Score=np.mean(scores)\nAvg_Score\n", "intent": "* Model performance is stable. \n"}
{"snippet": "svc_predict = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = logmodel.predict(X_test)\npredictions\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nmse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint('Mean squared error: %f' % (mse))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = model_nb.predict(X_test_count)\n", "intent": "Create the predicted tags\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "Calculate the accuracy score\n"}
{"snippet": "mse = ((train.target - lm.predict(train))** 2).mean()\nprint(\"\\nMean Squared Error:\\n\", mse)\n", "intent": "Mean Squared Error of the model \n"}
{"snippet": "mse = ((test.target - lm.predict(test))** 2).mean()\nprint(\"\\nMean Squared Error:\\n\", mse)\n", "intent": "Print the Mean Squared Error of the model on the training set\n"}
{"snippet": "def get_next(text,token,model,fullmtx,fullText):\n    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n    tmp = token.texts_to_matrix(tmp, mode='binary')\n    p = model.predict(tmp)\n    x= np.random.random_sample()*1000\n    return fullText[int(x)]\n", "intent": "We'll create a function thet gets the word metrix and randommaly select a word from it in order to create a new sentence.\n"}
{"snippet": "pred_forest = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate the model.\n"}
{"snippet": "print(classification_report(y_test,pred_forest))\n", "intent": "**Below is classification report from the results.**\n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "Here I evaluate the KNN model:\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "** Time to evaluate the performance of the model using the test values**\n"}
{"snippet": "vecfeatures = get_features(6000)\nprint cross_val_score(logreg, vecfeatures, labels, cv=kfold).mean()\n", "intent": "It would appear, that a feature-set of the first 6000 TF/IDF vectors performs quite well.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier as KNN\nknn = KNN()\nvecfeatures = get_features(6000)\nrKnnCrossVal = cross_val_score(knn,vecfeatures,labels,cv=kfold)\nprint rKnnCrossVal.mean()\n", "intent": "Ok, with two distinct sets such as this, maybe KNN is a viable alternative\n"}
{"snippet": "def calc_crossval_score_knn(neighbours, features, vals, kfold):\n    _knn = KNN(int(neighbours))\n    return cross_val_score(_knn,features,vals,cv=kfold).mean()\n", "intent": "Ok, looks promising\n"}
{"snippet": "from res.fnc.utils.score import report_score, LABELS, score_submission\npred_rel = relationModel.predict(X_holdout)\npred_neu = neutralModel.predict(X_holdout)\npred_val = valenceModel.predict(X_holdout)\npred = [3 if pred_rel[i] == 3 else (2 if pred_neu[i] == 2 else (1 if pred_val[i] == 1 else 0)) for i in range(0, len(X_holdout))]\npredicted = [LABELS[int(a)] for a in pred]\nactual = [LABELS[int(a)] for a in y_holdout]\nreport_score(actual,predicted)\n", "intent": "Now let's combine all three models together and see how we did!\n"}
{"snippet": "pred_rel = relationModel.predict(X_test)\npred_neu = neutralModel.predict(X_test)\npred_val = valenceModel.predict(X_test)\npred = [3 if pred_rel[i] == 3 else (2 if pred_neu[i] == 2 else (1 if pred_val[i] == 1 else 0)) for i in range(0, len(X_test))]\npredicted = [LABELS[int(a)] for a in pred]\nactual = [LABELS[int(a)] for a in y_test]\nreport_score(actual,predicted)\n", "intent": "Now let's see our final competition score!\n"}
{"snippet": "probs = HLM1.predict_proba(X, cats)\n", "intent": "<h2> Predict and score the model </h2>\n"}
{"snippet": "printStats(test_y, pred_y)\nplotTargetVsPredict(test_y, pred_y)\n", "intent": "Now we can calculate the error statistics for the 2012 test data, and plot the scatter distribution of the predicted and target values:\n"}
{"snippet": "print (\"\\nCLASSIFICATION REPORT for k= 1 \\n\")\nprint(classification_report(y_test,predictions))\n", "intent": "As you can see, we still have 6 False Postives. Can we do better. We'll try in the next step. But first, lets get some more detail. \n"}
{"snippet": "opt_model = pickle.load(open(\"model.xgb\", \"rb\"))\nxgdmat_test = xgb.DMatrix(Xp_test, y_test)\ny_pred = opt_model.predict(xgdmat_test)\ny_pred[y_pred > 0.5] = 1\ny_pred[y_pred <= 0.5] = 0\ny_pred = y_pred.astype(int)\naccuracy = accuracy_score(y_test, y_pred)*100.0\nprint('Test Accuracy: %.1f%%' % accuracy)\n", "intent": "And let's also load that saved model before applying predictions and testing the accuracy of our predictions\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytest,\n                            y_pred,\n                            target_names=faces.target_names\n                           ))\n", "intent": "We wil use the following methods\n1. accuracy_score\n2. classification_score\n3. confusion_matrix\n"}
{"snippet": "def predict_label(text, model=model, test=test):\n    label = test.target_names[model.predict([text])[0]]\n    print( \"Text: {} \\n\\nPredicted Label: {}\" .\\\n            format(text, label))\n", "intent": "Now lets predict the label by model.\n"}
{"snippet": "def calc_loss(predictions, labels):\n    return np.mean(np.square(predictions - labels))\n", "intent": "Calculates loss based on model predictions.\n"}
{"snippet": "score = network.evaluate(data_test, targets_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Question 5: Evaluate your model** \n"}
{"snippet": "for fraction_of_positives, mean_predicted_value in calibration_data:\n    ece_pre_calibration = expected_calibration_error(prediction_probabilities=all_test_predictions[0], \n                                                     accuracy=fraction_of_positives, \n                                                     confidence=mean_predicted_value)\n    print('ECE: (before calibration) {:.4f}'.format(ece_pre_calibration))\n", "intent": "Let' calculate the expected calibration error on the test set before calibration.\n"}
{"snippet": "y_pred = np.zeros(len(test_predictions))\nif use_platt:\n    test_predictions = np.mean(np.array(all_test_probabilistic_predictions), axis=0)\ny_pred[test_predictions.reshape(-1)>0.5] = 1\nprint('Accuracy of model before calibration: {:.2f}'.format(accuracy_score(y_pred=y_pred, \n                                                                           y_true=edge_labels_test)))\n", "intent": "As a final test, check if the accuracy of the model changes after calibration.\n"}
{"snippet": "ece = []\nfor i in range(test_pred.shape[1]):\n    fraction_of_positives, mean_predicted_value = calibration_data[i]\n    ece.append(expected_calibration_error(prediction_probabilities=test_pred[:, i], \n                                          accuracy=fraction_of_positives, \n                                          confidence=mean_predicted_value))\n", "intent": "Also calculate Expected Calibration Error (ECE) for each class. See reference [2] for the definition of ECE.\n"}
{"snippet": "labels = db.labels_\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(node_embeddings, labels))\n", "intent": "Calculating the clustering statistics:\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Predict the hold out test set.\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "Calculate the accuracy of the classifier on the test set.\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\nprint(rfc.feature_importances_)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predictions = lm_6.predict(test)\nerrors = predictions - test.ElantraSales\nsse = sum(errors**2)\nprint(sse)\n", "intent": "Using the model from Problem 6.1, make predictions on the test set. What is the sum of squared errors of the model on the test set?\n"}
{"snippet": "predicted = text_clf.predict(X_test.title_ingred.values)\nprint(np.mean(predicted == cat_test))\n", "intent": "Evaluation of performance on the test set\n"}
{"snippet": "X_train.categories_clean[gs_clf.predict(['ground_beef ketchup',\n                                         'ground_beef green_bell_peppers rice',\n                                         'ziti ground_beef',\n                                         'brown_sugar semisweet_chocolate_chips',\n                                         'chicken soup',\n                                         'steak',\n                                         'black_beans tomato_sauce olives'])]\n", "intent": "Use Case:  I have several ingredients in the pantry, give me ideas for recipes\n"}
{"snippet": "y_test = model.predict([X_te], batch_size=64, verbose=1)\n", "intent": "And finally, get predictions for the test set and prepare a submission CSV:\n"}
{"snippet": "lda_pred_class_coded = lda_clf.predict(test_df[predictors])\nlda_pred_class_coded\n", "intent": "The predict() function returns LDA's predictions about the movement of the market.\n"}
{"snippet": "knn1_pred = knn_1.predict(test_df[predictors])\nknn1_pred\n", "intent": "Now the knn() function can be used to predict the market's movement for the dates in 2005.\n"}
{"snippet": "mses = np.array([])\nfor model in models:\n    features = list(model.params.index[1:])\n    X_test = sm.add_constant(df_test[features])\n    salary_pred = model.predict(X_test)\n    mses = np.append(mses, np.mean((salary_pred - df_test.Salary.values)**2))\nprint('MSEs =', mses)\n", "intent": "We now compute the validation set error for the best model of each model size.\n"}
{"snippet": "X_test_prepared = prepPipeline.transform(X_test)\ny_test_predictions = classifiers[\"adaboost\"].gridSearch.predict(X_test_prepared)\nprint(accuracy_score(y_test, y_test_predictions))\n", "intent": "As the final step, we'll now see how our model has performed on the test set that we set aside earlier.\n"}
{"snippet": "prediction = knn_classifier_2.predict(iris_sample)\nprediction[0]\n", "intent": "With this KNN classifier in place, we can now do some prediction of the *Iris Sample* we declared previously:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npred = knn.predict(X_test)\nscore = accuracy_score(y_true = y_test, y_pred = pred, normalize = True)\nprint(\"%.2f\" % score)\n", "intent": "<img src=\"images/accuracy_score.png\"/>\n"}
{"snippet": "y_pred = est_2.predict(data_test)\n", "intent": "So, how does the model perform overall against out of sample data?\n"}
{"snippet": "forest100_pred = forest100.predict(random_forest_feature_matrix_test.toarray())\nnp.save('forest100pred', forest100_pred)\n", "intent": "Make predictions with random forest set at 100 learners\n"}
{"snippet": "to_impute['predicted_occupation'] = model.predict(to_impute)\n", "intent": "We add our predictions back to our original DataFrame, using the predict function.\n"}
{"snippet": "print(X_train_mybag.shape)\nprint(X_val_mybag.shape)\nprint(X_train_tfidf.shape)\nprint(X_val_tfidf.shape)\ny_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)  \ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\nprint(X_val_tfidf[0])\nprint(y_val_predicted_labels_tfidf[0])\n", "intent": "Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\ntest_acc\n", "intent": "Let's evaluate the model on the test data:\n"}
{"snippet": "model.load_weights('pre_trained_glove_model.h5')\nprint(model.evaluate(x_test, y_test))\nprint(np.unique(y_test, return_counts=True))\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "probab, prediction, prediction_labels = vgg.predict(test_imgs, details = True)\n", "intent": "Here, we make the predictions using our trained model\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "* Returns a NumPy array\n* Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "Classification accuracy:\n* ***Proportion*** of correct predictions\n* Common ***evaluation metric*** for classification problems\n"}
{"snippet": "xgbtest = xgb.DMatrix(test)\nXGBpreds = xgbModel.predict(xgbtest)\n", "intent": "Use our model to make the predictions\n"}
{"snippet": "print \"Accuracy XGBoost\",accuracy_score(Yval, predictions)\nprint \"Kappa XGBoost\",cohen_kappa_score(Yval, predictions)\nprint confusion_matrix(Yval, predictions)\n", "intent": "We see that in this case, XGBoost performs on par (or slightly better than) the best baseline models we have tried\n"}
{"snippet": "accuracy_score(y_test, y_pred_binary)\n", "intent": "So it predicted 69k wontsee and 31k willsee.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, style in enumerate(style_targets):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(\n                    feats[style_layers[i]]) - style))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint 'Linear Regression:',median(cross_val_score(lr,X_norm,y,cv=20))\nprint 'Random Forest:',median(cross_val_score(rf,X_norm,y,cv=20))\n", "intent": "We can compare the two functions using their **mean square error** (MSE) to better quantify which one works better (and by how much).\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for \n                        feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "if best_accuracy > 0.57887:  \n    vec_test_data = vectorize(test_data)  \n    final_preds = best_clf.predict(vec_test_data)  \n    with open(OUTPUT_FILE, 'w') as output:\n        output.write(\"Id,Target\")\n        for id, pred in enumerate(final_preds):\n            output.write(\"\\n%s,%s\" % (id + 1, pred))\n    print(\"Results from %s written to '%s'\" % (best_clf_name, OUTPUT_FILE))\n", "intent": "After finding the best classifier, use it on the test data:\n"}
{"snippet": "pred_Y = model_w.predict(test_normalized)\n", "intent": "The improvement is made on RMSE from 44320 to 38858. Let's use this trained network to predict the house price!\n"}
{"snippet": "model.load_weights('data/saved_models/weights_resnet50.hdf5')\npredictions = model.predict(X)\n", "intent": "Predictions are made using the above neural network. The weights, which have been calculated in this [notebook](nn.ipynb), are loaded.\n"}
{"snippet": "predictions = resnet50.predict(test_resnet50)\n", "intent": "Predictions are made on the test data.\n"}
{"snippet": "yhat_unique = (combine >= 0.5).astype(int)\nscore_unique = f1_score(label, yhat_unique)\nprint('F1 score: %.5f' % score_unique)\n", "intent": "We calculate the $F_1$ score using a unique threshold of 0.5 for all 9 labels.\n"}
{"snippet": "vgg16_predictions = (vgg16.predict(test_vgg16) >= 0.5).astype(int)\n", "intent": "Predictions are made on the test data and the $F_1$ score is then calculated.\n"}
{"snippet": "xception_predictions = (xception.predict(test_xception) >= 0.5).astype(int)\n", "intent": "Predictions are made on the test data and the $F_1$ score is then calculated.\n"}
{"snippet": "resnet50_predictions = (resnet50.predict(test_resnet50) >= 0.5).astype(int)\n", "intent": "Predictions are made on the test data and the $F_1$ score is then calculated.\n"}
{"snippet": "inception_predictions = (inception.predict(test_inception) >= 0.5).astype(int)\n", "intent": "Predictions are made on the test data and the $F_1$ score is then calculated.\n"}
{"snippet": "yhat_unique = (y_test_ovr >= 0.5).astype(int)\nf1_score_ovr = f1_score(y_test_averaged, yhat_unique)\nprint('F1 score: %.5f' % f1_score_ovr)\n", "intent": "We calculate the $F_1$ score using a unique threshold of 0.5 for all 9 labels.\n"}
{"snippet": "predicted = tree_model.predict(X_test)\nexpected = y_test\nprint(predicted[0:50])\nprint(expected[0:50])\n", "intent": "Applying the model on the test data\n"}
{"snippet": "predicted = model.predict(X_test)\nexpected = y_test\nprint(predicted[0:50])\nprint(expected[0:50])\n", "intent": "Applying the model on the test data\n"}
{"snippet": "predicted = center_model.predict(X_test)\nexpected = y_test\nprint(predicted[0:50])\nprint(expected[0:50])\n", "intent": "Applying the model on the test data\n"}
{"snippet": "predicted = clf.predict(X_test)\nexpected = y_test\nprint(predicted[0:50])\nprint(expected[0:50])\n", "intent": "Applying the model on the test data\n"}
{"snippet": "print('prediction:',sklearn.metrics.precision_score(test_target, labels, average = 'weighted')) \nprint('recall:',sklearn.metrics.recall_score(test_target, labels, average = 'weighted')) \nprint('F measure:', sklearn.metrics.f1_score(test_target, labels, average = 'weighted')) \n", "intent": "Let's get the precision, recall, and F-measure.\n"}
{"snippet": "print('precision:', sklearn.metrics.precision_score(test_target, labels, average = 'weighted')) \nprint('recall:', sklearn.metrics.recall_score(test_target, labels, average = 'weighted')) \nprint('F measure:', sklearn.metrics.f1_score(test_target, labels, average = 'weighted')) \n", "intent": "The precision, recall, and F-measure.\n"}
{"snippet": "print(clf.predict(TFVects[82]))\nprint(test_target[82])\n", "intent": "And a prediction from the testing set\n"}
{"snippet": "tic = time.time() \nsvr = Predict(dat,Y,algorithm='svr', output_dir=out_folder, \n              cv_dict = {'type':'loso','subject_id':holdout}, \n              **{'kernel':\"linear\"})\nsvr.predict()\nprint 'Elapsed: %.2f seconds' % (time.time() - tic) \n", "intent": "<p>Run Linear Support Vector Regression with leave one subject out cross-validation (LOSO)</p>\n"}
{"snippet": "tic = time.time() \nridge = Predict(dat, Y, algorithm='ridgeCV',output_dir=out_folder, \n                cv_dict = {'type':'kfolds','n_folds':5,'subject_id':holdout},\n                **{'alphas':np.linspace(.1, 10, 5)})\nridge.predict()\nprint 'Total Elapsed: %.2f seconds' % (time.time() - tic) \n", "intent": "<p>Run Ridge Regression with 5 fold Cross-Validation and a nested cross-validation to estimate shrinkage parameter</p>\n"}
{"snippet": "tic = time.time() \nmask = nb.load(os.path.join(get_resource_path(),'gray_matter_mask.nii.gz'))\nridge = Predict(dat,Y,algorithm='ridge', output_dir=out_folder, \n              cv_dict = {'type':'kfolds','n_folds':5, 'subject_id':holdout}, \n              mask = mask)\nridge.predict()\nprint 'Elapsed: %.2f seconds' % (time.time() - tic) \n", "intent": "<p>You might be interested in only training a pattern on a subset of the brain using an anatomical mask.  Here we use a mask of subcortex.</p>\n"}
{"snippet": "y_hat = tunned_model_q_normal['model'].predict(save_X[normal_and_2010])\nprint('Model predictions of 2010 sale prices:\\n{}'.format('_'*_chars_per_line))\nprint('R2 score:                ',round(r2_score(save_y[normal_and_2010],y_hat),5))\nprint('Root Mean Squared Error: ',round(np.sqrt(mean_squared_error(save_y[normal_and_2010], y_hat))))\ntunned_model_q_normal['report']\n", "intent": "From this we can get the mean R2 value seen in our cross validation, its standard deviation and the ultimate score of the model on our 2010 data.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    N = len(style_weights)\n    style_loss = tf.constant(0.0)\n    for i in xrange(N):\n        current = gram_matrix(feats[style_layers[i]])\n        style_loss += (style_weights[i] * tf.reduce_sum(tf.square(current - style_targets[i])))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3Data]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def measure_score(model,x,y):\n    return model.score(x,y)\n", "intent": "What happens if we tried this again using a function besides the log-likelihood -- for example, the classification accuracy?  Interpret these results\n"}
{"snippet": "y_pred = df.a*df.f_a + df.b*df.f_b + df.c*df.f_c + 3*df.d\nr2_score(y, y_pred)\n", "intent": "To calibrate our expectations, even if we have perfect insight, there's still some noise involved. How well could we do, in the best case?\n"}
{"snippet": "pl.plot(history.history['loss'], label='Training')\npl.plot(history.history['val_loss'], label='Testing')\npl.legend()\npl.grid()\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Show the performance of the model\n"}
{"snippet": "pl.plot(history.history['loss'], label='Training')\npl.plot(history.history['val_loss'], label='Testing')\npl.legend()\npl.grid()\nscore = model.evaluate(X_test_hog, Y_test, verbose=0)\nprint 'Test score:', score[0]\nprint 'Test accuracy:', score[1]\n", "intent": "Show the performance of the model\n"}
{"snippet": "pl.plot(history.history['loss'], label='Training')\npl.plot(history.history['val_loss'], label='Testing')\npl.legend()\npl.grid()\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint 'Test score:', score[0]\nprint 'Test accuracy:', score[1]\n", "intent": "Show the performance of the model\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint('Decision Tree confusion matrix\\n\\n', confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n", "intent": "Generate the confusion matrix and classification report\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X, y, cv=10)\nprint(scores)\nprint('Decision Tree Cross Validation score: ', '{:.2%}'.format(scores.mean()))\n", "intent": "Use K-Fold Cross Validation to get a better measure of the model's accuracy\n"}
{"snippet": "X_test_r = generate_a_rectangle()\nX_test_d = generate_a_disk()\nX_test_t = generate_a_triangle()[0]\nX_test_r = X_test_r.reshape(1, X_test_r.shape[0])\nX_test_d = X_test_d.reshape(1, X_test_d.shape[0])\nX_test_t = X_test_t.reshape(1, X_test_t.shape[0])\nprint(model1.predict(X_test_r))\nprint(model1.predict(X_test_d))\nprint(model1.predict(X_test_t))\n", "intent": "We can check our classifier for all 3 classes\n"}
{"snippet": "def mean_squared_error(y_true, y_pred):\n    mse = np.mean((y_pred-y_true)**2)\n    return mse\n", "intent": "We will be using mean squared error to evaluate accuracy of our model:\n$$\nMSE=\\frac{1}{m}\\sum_{i=1}^{m}{(h^{(i)} - y^{(i)})^2} \n$$\n"}
{"snippet": "mse = mean_squared_error(test_set_y, y_predictions)\n", "intent": "Let's calculate mean squred error(MSE):\n"}
{"snippet": "y_pred = model.predict(X_test)\ny_pred\n", "intent": "Predict the labels for the test set.\n"}
{"snippet": "def predict(features):\n    if features['gender'] == 'female':\n        return 1\n    else:\n        return 0\n", "intent": "We see that gender is a significant factor. So what would happen if we just used gender to predict if an individual survives?\n"}
{"snippet": "def predict(features):\n", "intent": "For men, there seems to be a much higher rate of survival if they were younger than 15. We can add that to our model.\n"}
{"snippet": "y_pred = [ predict(row) for _, row in X_test.iterrows() ]\naccuracy_score(y_test, y_pred)\n", "intent": "How accurate is our predict function for our test data?\n"}
{"snippet": "def predict(features):\n", "intent": "We see that most women with 3rd class tickets who were above 40 did not survive. Let's use that information in our model.\n"}
{"snippet": "y_pred = [ predict(row) for _, row in X_test.iterrows() ]\naccuracy_score(y_test, y_pred)\n", "intent": "How accurate are our predictions now?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(bnb, data, target, cv=10)\n", "intent": "These scores look really consistent! It doesn't seem like our model is overfitting. \n"}
{"snippet": "print(\"MSE: %.2f\" % metrics.mean_squared_error(y_test,y_pred))\nprint('R^2 Score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "topographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\n", "intent": "How good is the fit?\n"}
{"snippet": "from sklearn import metrics\nprint (metrics.accuracy_score(y,logreg.predict(X)))\n", "intent": "   * **proportion** of correct predictions\n   * common **evaluation metric** for classification problems\n"}
{"snippet": "knn.predict([[3,5,4,2]])\n", "intent": "* New sample is reffered to as \"out-of-sample\" data\n"}
{"snippet": "x_new = ([3,5,4,2],[5,4,3,2])\nknn.predict(x_new)\n", "intent": "* can predict multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nconf_matrix = confusion_matrix(y_true = df_predictions[\"Label\"], y_pred = df_predictions[\"PredictedLabel\"])\nmicro = sum(conf_matrix[i][i] for i in range(conf_matrix.shape[0]))/df_predictions.shape[0]\nprint(\"Micro Average Accuracy is {}\".format(micro))\nmacro = sum(conf_matrix[i][i]/(sum(conf_matrix[i][j] for j in range(conf_matrix.shape[0]))) for i in range(conf_matrix.shape[0]))/conf_matrix.shape[0]\nprint(\"Macro Average Accuracy is {}\".format(macro))\nresults = classification_report(y_true = df_predictions[\"Label\"], y_pred = df_predictions[\"PredictedLabel\"])\nprint(results)\n", "intent": "Below we show confusion matrix calculated from our predictions on the test data, versus the actual values.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cv = ShuffleSplit(n_samples, n_iter=5, test_size=.2)\nscores = cross_val_score(classifier, X, y, cv=cv)\nprint(scores)\nprint(np.mean(scores))\n", "intent": "Pass ```cv``` to cross_val_score to get an aggregate score of iterations\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "** Calculate the accuracy of the model on the test dataset.**\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake)\n    g_loss = ls_generator_loss(score_fake)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss.cpu().numpy()))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss.cpu().numpy()))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, layer in enumerate(style_layers):\n        gram_layer = gram_matrix(feats[layer])\n        loss = loss + torch.mul((gram_layer - style_targets[i]).norm()**2, style_weights[i])\n    return loss.cpu()\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "No, this classifier is not better because the accuracy with alpha = 1 is lower than the initial model without tuning for alpha.\n"}
{"snippet": "from mlxtend.evaluate import lift_score\nlift_score(y_test, final_pred.round())\n", "intent": "*Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions.*\n"}
{"snippet": "for i in range(5, 16, 1):\n    print \"Depth:\", i\n    print \"Decision Tree Score:\", fit_tree_score(x_train, y_train, x_test, y_test, i)\n", "intent": "We can optimize the depth of the decision tree model to get a better score. \n"}
{"snippet": "predictions = estimator.predict(X_test)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPredict on test set \n<br><br></p>\n"}
{"snippet": "accuracy_score(y_true = y_test, y_pred = predictions)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nMeasure accuracy of the classifier\n<br><br></p>\n"}
{"snippet": "y_prediction = regressor.predict(X_test)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction\n<br><br></p>\n"}
{"snippet": "score = model.evaluate(imgTest, onehotTest, verbose=0)\nprint('Test loss     :', score[0])\nprint('Test accuracy :', score[1])\n", "intent": "Let see the result of training:\n"}
{"snippet": "xc_predictions = [np.argmax(xc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xc_predictions)==np.argmax(test_targets, axis=1))/len(xc_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\navg_ar_score =  adjusted_rand_score(iris.target, avg_pred)\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "ResNet50_predictions = [np.argmax(\n    my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(\n    np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "evaluate(lambda x,y: [model_orig.predict([x[[i]], y]) for i in range(feature_data.shape[0])], 20)\n", "intent": "Run the evauation for the trained Siamese Network:\n"}
{"snippet": "evaluate(lambda x,y: [model_sim.predict([x[[i]], y]) for i in range(feature_data.shape[0])], 20)\n", "intent": "General model evaluation:\n"}
{"snippet": "evaluate(lambda x,y: [model_sim_id.predict([x[[i]], y]) for i in range(feature_data.shape[0])], 20)\n", "intent": "General model evaluation:\n"}
{"snippet": "kmeans_score = accuracy_score(y, kmeans_pred)\nfuzzy_score = accuracy_score(y, fuzzy_pred)\nprint('K-means score: {}\\nFuzzy c-means score: {:.2f}'.format(kmeans_score, \n                                                              fuzzy_score))\n", "intent": "Comparing the cluster labels assigned by each algorithm to the input data points with the generated sample data labels.\n"}
{"snippet": "scores = cross_val_score(clf_lg, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 19.7 percentage from its mean the model is overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_knn, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 5.9 percentage from mean and this means the model does overfit.\n"}
{"snippet": "scores = cross_val_score(clf_dt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 10.9 percentage from mean and this means the model is overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_rf, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 3.4 percentage from its mean the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_SVC, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 4.5% from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_bag, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 3.8 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_nb, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 14.6 percentage from mean and this means the model is overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_boost, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 3.5% from mean and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_xdt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 4.5% from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_GBC, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 4.5% from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_xgboost, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 9.1% from mean and this means the model is overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_NN, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 0.5 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_sgd, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 8.9 percentage from mean and this means the model is overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_dt, features_train, target_train, cv=10)\nprint(\"Cross Validation Score for each K\",scores)\nscores.mean()     \n", "intent": "All 10-folds results within 2 percentage from 94% to 96%.\n"}
{"snippet": "scores = cross_val_score(clf_knn, features_train, target_train, cv=10)\nprint(\"Cross Validation Score for each K\", scores)\nscores.mean()   \n", "intent": "All 10-folds results within 2 percentage from 87% to 89% and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_nb, features_train, target_train, cv=10)\nprint(\"Cross Validation Score for each K\", scores)\nscores.mean()   \n", "intent": "All 10-folds results more than 2 percentage from 52% to 62% and this means the model overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_rf, features_train, target_train, cv=10)\nprint(\"Cross Validation Score for each K\", scores)\nscores.mean() \n", "intent": "All 10-folds results within 2 percentage from 88% to 91% and this means the model is even better than Decision Tree on overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_knn, X_train, y_train, cv=10)\n", "intent": "All 10-folds results within 1.2 percentage from mean and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_knn, X_train, y_train, cv=10)\n", "intent": "All 10-folds results within 1.5 percentage from mean and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_knn, X_train, y_train, cv=10)\n", "intent": "All 10-folds results within 1 percentage from mean and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_dt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.5 percentage from mean and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_dt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1 percentage from mean and this means the model does not overfit.\n"}
{"snippet": "scores = cross_val_score(clf_dt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2 percentage from mean and this means this model is not as good (on overfitting) as previous decision model.\n"}
{"snippet": "scores = cross_val_score(clf_rf, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2 percentage from its mean the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_rf, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.5% from its mean the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_rf, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.1 percentage from its mean the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_linSVC, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.5% from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_rbf, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2% from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_bag, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.5 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_bag, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.3 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_boost, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_xdt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_xdt, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_GBC, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_NN, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.7 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_NN, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 2.5 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_NN, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 1.4 percentage from mean and this means the model is not overfitting.\n"}
{"snippet": "scores = cross_val_score(clf_sgd, X_train, y_train, cv=10)\nprint(\"Cross Validation Score for each K: \", scores)\nprint(\"Cross Validation Score mean: \", scores.mean())\nprint(\"Cross Validation Max and Min Score difference from mean score: \", scores.mean()-min(scores), max(scores)-scores.mean())\n", "intent": "All 10-folds results within 4 percentage from mean and this means the model is worse than other models on overfitting.\n"}
{"snippet": "log_preds = learn.predict()\nlog_preds.shape\n", "intent": "This means that cats are label 0 and dogs are label 1\n"}
{"snippet": "y_predict_proba=logic_r_model.predict_proba(X_test)[:,1]\n", "intent": "* This is the final Model. \n* Threshold of .35\n* Precision of .42\n* Recall of .86\n"}
{"snippet": "from sklearn.metrics import r2_score\nfrom scipy.stats import spearmanr, pearsonr\npredicted_train = rf.predict(SG_instances)\npredicted_test = rf.predict(SG_test[['net_sale', 'pos', 'labor','installed']])\ntest_score = r2_score(SG_test['game'], predicted_test)\nspearman = spearmanr(SG_test['game'], predicted_test)\npearson = pearsonr(SG_test['game'], predicted_test)\nprint(f'Test data R-2 score: {test_score:>5.3}')\nprint(f'Test data Spearman correlation: {spearman[0]:.3}')\nprint(f'Test data Pearson correlation: {pearson[0]:.3}')\n", "intent": "Again seems that only the POS check number is important feature\n"}
{"snippet": "y_predict = bestForest.predict(X_test)\nprint('Testing data size:', y_test.shape)\nprint('Classification report of test and predicted')\nprint(metrics.classification_report(y_test, y_predict))\nprint(20*'____')\nprint('Confusion Matrix')\nprint(metrics.confusion_matrix(y_test, y_predict))\n", "intent": "And let's measure the entire model performance\n"}
{"snippet": "Y_pred=clf.predict(X_test)\nnum2print = 25\nprint(Y_pred[:num2print])\nprint(Y_test[:num2print])\nseparator_line()\nprint('Cluster Centers Shape:')\nclf.cluster_centers_.shape\n", "intent": "Predict the labels of the test data\n"}
{"snippet": "from sklearn.metrics import homogeneity_score, completeness_score,\\\nv_measure_score,adjusted_rand_score, adjusted_mutual_info_score,silhouette_score\nprint('% 9s' % 'inertia\\thomo\\tcomple\\tv-means\\tARI\\tAMI\\tsilhouette')\nprint('%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f' %(clf.inertia_,\n                                                homogeneity_score(Y_test, Y_pred),\n                                                completeness_score(Y_test, Y_pred),\n                                                v_measure_score(Y_test, Y_pred),\n                                                adjusted_rand_score(Y_test, Y_pred),\n                                                adjusted_mutual_info_score(Y_test, Y_pred),\n                                                silhouette_score(X_test, Y_pred, metric='euclidean')))\n", "intent": "... crap. Only 5 was predicted correctly 41 times.<br>\nLet's look into some *cluster cuality metics*.\n"}
{"snippet": "Y_predict = svc_model_linear.predict(X_test)\nprint(Y_predict[:num2print])\nprint(Y_test[:num2print])\n", "intent": "Now let's check a few predictions to see how good we are doing.\n"}
{"snippet": "print('Classification report of y_test and predicted')\nprint(metrics.classification_report(Y_test, Y_predict))\nseparator_line()\nprint('Confusion Matrix')\nprint(metrics.confusion_matrix(Y_test, Y_predict))\n", "intent": "And how did our model perform?\n"}
{"snippet": "y_pred = cvp.predict(X_test)\n", "intent": "Predict the test data using the pipeline\n"}
{"snippet": "display(r2_score(y_test, y_pred))\nprint(\"MSE (Mean Square Error)\")\ndisplay(mean_squared_error(y_test, y_pred))\n", "intent": "Using the metics we previously imported evaluate the model performance\n\\begin{equation*}\nR^2 score\n\\end{equation*}\n"}
{"snippet": "pred = list(svm_classifier.predict(input_fn=predict_fn))\npredicted_class = map(lambda x:x['classes'], pred)\nannotations = map(lambda x: '%.2f' % x['logits'][0], pred)\n", "intent": "Predict and plot predictions\n"}
{"snippet": "y_hat = uni_result.predict()\n", "intent": "Add a regression line:\n"}
{"snippet": "trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n", "intent": "Now that the model is trained, let's examine its performance. We can compute the predicted values on the training and testing sets. \n"}
{"snippet": "trainScore = np.sqrt(mean_squared_error(trY[0], trP[:,0]))\ntestScore = np.sqrt(mean_squared_error(teY[0], teP[:,0]))\n", "intent": "We can examine the train/test performance by simply evaluating the mean squared error.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model_Xception_predictions = [np.argmax(model_Xception.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_Xception_accuracy = 100*np.sum(np.array(model_Xception_predictions)==np.argmax(test_targets, axis=1))/len(model_Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_Xception_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "fowlkes_mallows_score(df['class'],df['labels_pca'])\n", "intent": "By checking our metrices for new model we can say that there is an improvement in avarage. Now we have better speration of clusters.\n"}
{"snippet": "def rmse(x, y):\n    return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train),y_train),\n         rmse(m.predict(X_valid), y_valid),\n         m.score(X_train, y_train),\n         m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): \n        res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0] \n", "intent": "We'll grab the predictions for each individual tree, and look at one example.\n"}
{"snippet": "test_preds = X_test_level2[:,0] * best_alpha + X_test_level2[:,1] * (1-best_alpha)\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = pred_lr_train\nr2_train_stacking = r2_score(y_test_temp, pred_lr_train)\ntest_preds = pred_lr_test\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "sample_validation_data = sample_validation_data.remove_column(target).to_numpy()\nmodel_5.predict(sample_validation_data)\n", "intent": "For each row in the **sample_validation_data**, write code to make **model_5** predict whether or not the loan is classified as a **safe loan**.\n"}
{"snippet": "validation_label_array = validation_data[target].to_numpy()\nvalidation_data_raw = validation_data.remove_column(target).to_numpy()\npredictions = model_5.predict(validation_data_raw)\nfalse_positives = 0\nfor row_index in xrange(len(predictions)):\n    if predictions[row_index] == 1 and validation_label_array[row_index] == -1:\n        false_positives += 1\nfalse_positives\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "error_all = []\nfor n in xrange(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(y_true=train_data[target].to_numpy(), y_pred=predictions.to_numpy())\n    error_all.append(error)\n    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n                                                   y_pred=predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(),\n                                             y_pred=predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n                                                   y_pred=predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n                                                   y_pred=predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "topK = 10\ninv_lst = np.unique(test[:, 0]).astype(int)\nrec_list = {}\nfor inv in inv_lst:\n    if rec_list.get(inv, None) is None:\n        rec_list[inv] = np.argsort(pmf_mvl.predict(inv))[-topK:]\n", "intent": "We first measure the precision@10 and recall@10\n"}
{"snippet": "ResNet50_predictions = [np.argmax(transfer_ResNet50_model.predict(np.expand_dims(feature,axis=0))) for feature in test_ResNet50]\ntest_accuracy=100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets,axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictedLR = logreg.predict(X_test)\nprint (metrics.accuracy_score(y_test, predictedLR))\n", "intent": "Now we can estimate model accuracy on the class labels for the test set. You can use the function *predict*\n"}
{"snippet": "vgg_bottleneck = VGG16(weights='imagenet', include_top=False, input_shape = (224, 224, 3))\ntrain_vgg16 = vgg_bottleneck.predict(train_tensors, batch_size = 32, verbose = 1)\n", "intent": "***Bottleneck Feature Extraction***\n"}
{"snippet": "vgg_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_vgg16]\ntest_accuracy = 100*np.sum(np.array(vgg_predictions)==np.argmax(test_targets, axis=1))/len(vgg_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "***Test the Model***\n"}
{"snippet": "xception_bottleneck = xception.Xception(weights='imagenet', include_top=False, input_shape = (224, 224, 3))\ntrain_xception = xception_bottleneck.predict(train_tensors, batch_size = 32, verbose = 1)\nvalid_xception = xception_bottleneck.predict(valid_tensors, batch_size = 32, verbose = 1)\ntest_xception = xception_bottleneck.predict(test_tensors, batch_size = 32, verbose = 1)\n", "intent": "***Bottleneck Feature Extraction***\n"}
{"snippet": "xception_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "***Test the Model***\n"}
{"snippet": "inception_bottleneck = inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape = (224, 224, 3))\ntrain_inception = inception_bottleneck.predict(train_tensors, batch_size = 32, verbose = 1)\nvalid_inception = inception_bottleneck.predict(valid_tensors, batch_size = 32, verbose = 1)\ntest_inception = inception_bottleneck.predict(test_tensors, batch_size = 32, verbose = 1)\n", "intent": "***Bottleneck Feature Extraction***\n"}
{"snippet": "inception_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "***Test the Model***\n"}
{"snippet": "nb.predict(X.iloc[0:2])\n", "intent": "Now we can predict!\n"}
{"snippet": "nb.predict_proba(X[0:2])\n", "intent": "Now let's look at how we're predicting probability wise.\n"}
{"snippet": "from sklearn.cross_validation import LeaveOneOut\nscores = cross_val_score(model, X_iris, y_iris, cv = LeaveOneOut(len(X_iris)))\nscores.mean()\n", "intent": "Special case of cross validation using all possible data splits (`cv = n_samples`)\n"}
{"snippet": "def predict_category(s, train=train, model=model): \n    pred = model.predict([s])\n    return train.target_names[pred[0]]\npredict_category('sending a payload to the ISS')\n", "intent": "The very cool thing here is that we now have the tools to determine the category for any string, using the `predict()` method of this pipeline. \n"}
{"snippet": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel = LinearDiscriminantAnalysis()\nresults = cross_validation.cross_val_score(model, X, y, cv=kfold)\nresults.mean()\n", "intent": "For binary and multiclass classification. Assumes Gaussian distribution.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\nXception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('InceptionV3: Test accuracy: %.4f%%' % test_accuracy)\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Xception: Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "accuracy_score(y_test, predicted)\n", "intent": "**Accuracy** measures a fraction of the classifier prediction that are correct.   \n"}
{"snippet": "results = pd.Series(data=np.exp(model.predict(df_test))).round(0)\nresults\n", "intent": "Time to wrap things up.\n"}
{"snippet": "y_prediction = artificial_neural_network_classifier.predict(X_test)  \ny_prediction = (y_prediction > 0.5)\ncm = confusion_matrix(y_test, y_prediction)\nprint('Model Prediction:')\nprint(y_prediction)\nprint('Confusion Matrix')\nprint(cm)\n", "intent": "As the title of this section suggests we will evaluate the model's preformance. \n"}
{"snippet": "new_customer = np.array(sc.transform([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))\nnew_customer_prediction = artificial_neural_network_classifier.predict(new_customer)\ny_prediction = new_customer_prediction[0][0]\nprint y_prediction\nif y_prediction >= 0.5:\n    print('Time to let go of this customer.')\nelse:\n    print('Keep this customer by giving him positive feed back.')\n", "intent": "Now that we have evulated the model we can make predictions from our test set.\n"}
{"snippet": "rmse = math.sqrt(mean_squared_error(real_stock_price, predicted_stock))\nn = len(real_stock_price) \noverall_preformence = rmse / float(n)\nprint \"Overall Preformace of RMSE: \", overall_preformence\n", "intent": "Holy cow look at that prediction man!\n"}
{"snippet": "Xception_predictions = [np.argmax(model_cnn.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "target_svr_sentiments = svr_model.predict(target_dataset)\nsentiment_scatterplot(X, \n                      y = target_svr_sentiments, \n                      title = 'Sentiments Determined by Support Vector Regression');\n", "intent": "Let's use our trained models to discern the sentiment spread of our target tweet dataset.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: {:.2f}'.format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = dnn_clf.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "Let's check that we do indeed get 98.9% accuracy on the test set:\n"}
{"snippet": "y_pred = dnn_clf_bn.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The best params are reached during epoch 20, that's actually a slower convergence than earlier. Let's check the accuracy:\n"}
{"snippet": "y_pred = dnn_clf.predict(X_train1)\naccuracy_score(y_train1, y_pred)\n", "intent": "Let's go back to the model we trained earlier and see how it performs on the training set:\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "**dumb classifier which classify every answer False**\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(precision_score(y_train_5, y_train_pred)) \nprint(recall_score(y_train_5, y_train_pred))\n", "intent": "Precision and Recall\n"}
{"snippet": "roc_auc_score(y_train_5, y_scores_forest)\n", "intent": "ROC curve looks much better than the SGDClassifier's\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "it menas that random forest classifier predict this sample 9 with 90% probability\n"}
{"snippet": "import warnings\nsample = pd.Series((70, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3.7, 225, 18, 1.7, -1, 2))\nprint(clf.predict([sample]))\nprint(clf.predict_proba([sample]))\n", "intent": "Our new patient instance is in pretty bad shape, so he might die, but we're not sure:\n"}
{"snippet": "km.predict(breed_tfidf[4])\n", "intent": "KMeans stops when it stops making progress on it's clustering.\nNow we have a working machine learning classifier on which we can predict our feature.\n"}
{"snippet": "pred = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "yts_pred = regr.predict(Xts)\nRSS = np.mean((yts_pred-yts)**2)/(np.std(yts)**2)\nprint(\"Normalized RSS={0:f}\".format(RSS))\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "The support is the number of occurrences of each class in y_test.\n"}
{"snippet": "predictions = model.predict([x_test, x_char_test], batch_size=b)\neval = get_conll_scores(predictions, y_test, {v: k for k, v in dataset.y_labels.items()})\nprint('Precision {}'.format(eval[0][0]))\nprint('Recall {}'.format(eval[0][1]))\nprint('F1 {}'.format(eval[0][2]))\n", "intent": "Once the model has trained. Run CONLLEVAL to see how well it performs.\n"}
{"snippet": "embedded = np.zeros((metadata.shape[0], 128))\nfor i, m in enumerate(metadata):\n    img = load_image(m.image_path())\n    img = align_image(img)\n    img = (img / 255.).astype(np.float32)\n    embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n", "intent": "Embedding vectors can now be calculated by feeding the aligned and scaled images into the pre-trained network.\n"}
{"snippet": "pred = classifier_rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "X_test = np.linspace(0,10,100).reshape(100,1)\ny_test = reg.predict(X_test)\n", "intent": "We are now in a good position to test the model that we've just built.\n"}
{"snippet": "X = np.linspace(0,1,10)\nY = reg.predict(X.reshape(10,1))\n", "intent": "**Predict** by making a new array of x values and using our model's predict method to compute new y values.\n"}
{"snippet": "X = np.linspace(0,1,100)\nY = clf.predict(X.reshape(100,1)) \n", "intent": "I wonder what these default settings are. Should I even care?\n"}
{"snippet": "sklearn_model.predict(X[:5,:])\n", "intent": "Surprising low time to train\n"}
{"snippet": "wide_deep_preds = wide_deep.predict(x_val)\n", "intent": "Still not so good as  Multi-size CNN, but it looks more stable (less overfitted).\n"}
{"snippet": "print metrics.accuracy_score(y_test, predicted)\nprint metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "Now let's generate some evaluation metrics.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(own_classes, pred)\n", "intent": "The accuracy on these images:\n"}
{"snippet": "score = model.evaluate(X_test, y_test, batch_size=batch_size)\n", "intent": "run the model on the test data for evaluation\n"}
{"snippet": "print model.predict([X_test[0:1]])\n", "intent": "this uses the model to predict the output for the first element in the test set\n"}
{"snippet": "predictions = neural_net.predict(X_test)\n", "intent": "After fitting the neural net with the data, we can now use it to predict the classification on the test set.\n"}
{"snippet": "predictions = neural_net.predict(X_test).reshape(-1, 1)\n", "intent": "After fitting the neural net with the data, we can now use it to predict the classification on the test set.\n"}
{"snippet": "res2 = ncf.evaluate(val_rdd,2800,[MAE()])\n", "intent": "* output the final MAE on validation dataset\n"}
{"snippet": "train_predictions = gs_classifier.predict(X = train_word_counts)\naccuracy_score(train_predictions, train_Y)\n", "intent": "Determining the training set accuracy\n"}
{"snippet": "avg_frac_correct_data = {'values': np.mean(frac_correct_folds['total'], axis=0), 'errors': np.std(frac_correct_folds['total'], axis=0)}\navg_frac_correct, avg_frac_correct_err = comp.analysis.averaging_error(**avg_frac_correct_data)\n", "intent": "Determine the mean and standard deviation of the fraction correctly classified for each energy bin\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscore = cross_val_score(estimator=pipe_lr,\n                        X = X_train,\n                        y = y_train,\n                        cv = 10,\n                        n_jobs=1)\nprint(\"CV Accuracy Score: %s\" % score)\n", "intent": "Using sklearn build in k-fold cross validation scorer.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=5)\nscores\n", "intent": "Scikit-learn provides a nice utility for this:\n"}
{"snippet": "from sklearn.model_selection import LeaveOneOut\nscores = cross_val_score(model, X, y, cv=LeaveOneOut())\nscores\n", "intent": "Bringing this to the extreme, where K is the number of objects in the training set, is known as \"leave one out\" cross-validation:\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=1)\nprint(\"loss: \", score[0])\nprint(\"accuracy: \", score[1])\n", "intent": "Calculate the quality of our neural network\n"}
{"snippet": "predictions = dtree.predict(x)\nprint(classification_report(y,predictions))\n", "intent": "Printing classification report\n"}
{"snippet": "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_val_score(gnb,x,y,scoring = 'accuracy',cv = 10).mean(), cross_val_score(gnb,x,y,scoring = 'accuracy',cv = 10).std() * 2))\nprint(\"Precision: %0.2f (+/- %0.2f)\" % (cross_val_score(gnb,x,y,scoring = 'precision',cv = 10).mean(), cross_val_score(gnb,x,y,scoring = 'precision',cv = 10).std() * 2))\nprint(\"Recall: %0.2f (+/- %0.2f)\" % (cross_val_score(gnb,x,y,scoring = 'recall',cv = 10).mean(), cross_val_score(gnb,x,y,scoring = 'recall',cv = 10).std() * 2))\n", "intent": "Printing Accuracy, Precision, Recall\n"}
{"snippet": "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_val_score(lvc,x,y,scoring = 'accuracy',cv = 10).mean(), cross_val_score(lvc,x,y,scoring = 'accuracy',cv = 10).std() * 2))\nprint(\"Precision: %0.2f (+/- %0.2f)\" % (cross_val_score(lvc,x,y,scoring = 'precision',cv = 10).mean(), cross_val_score(lvc,x,y,scoring = 'precision',cv = 10).std() * 2))\nprint(\"Recall: %0.2f (+/- %0.2f)\" % (cross_val_score(lvc,x,y,scoring = 'recall',cv = 10).mean(), cross_val_score(lvc,x,y,scoring = 'recall',cv = 10).std() * 2))\n", "intent": "Printing Accuracy, Precision, Recall\n"}
{"snippet": "batch = train.next()\nimages = batch[0][0:5]\npredictions = model.predict(images)\npredictions_class_ids = np.argmax(predictions, axis=1)\npredicted_classes = [imagenet_id_to_class(id) for id in predictions_class_ids]\nplots(images, titles=predicted_classes)\n", "intent": "Predict some images\n"}
{"snippet": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\ntested_df = p_model.transform(test_df)\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))\n", "intent": "Let's see how well the model does:\n"}
{"snippet": "predictions={}\npredictions['baseline'] = clf.predict(Xmatrix_test)\n", "intent": "We chose to treat this model as our baseline prediction.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(valid.Count, y_hat.naive))\nprint(rms)\n", "intent": "We will now calculate RMSE to check the accuracy of our model on validation data set.\n"}
{"snippet": "predict=fit1.predict(start=\"2014-9-26\", end=\"2015-4-26\", dynamic=True)\n", "intent": "Now we will forecast the time series for Test data which starts from 2014-9-26 and ends at 2015-4-26.\n"}
{"snippet": "TL_predictions = [np.argmax(TL_model.predict(np.expand_dims(feature, axis=0))) for feature in test_TL]\ntest_accuracy = 100*np.sum(np.array(TL_predictions)==np.argmax(test_targets, axis=1))/len(TL_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def ROC_AUC(probs, real_values):\n    probs_numpy = probs.cpu().numpy()\n    real_values_numpy = real_values.cpu().numpy()\n    value = roc_auc_score(real_values_numpy, probs_numpy)\n    return value\n", "intent": "Train the model for 3 epochs with the chosen learning rate.\n"}
{"snippet": "loaded_model = load(open(filename,'rb'))\nresult = loaded_model.predict(X)\nprint(result)\n", "intent": "Load the model and see the predictions\n"}
{"snippet": "eightball_model.evaluate(scoring='accuracy')\neightball_model.eval_results.print_scores()\n", "intent": "Running `evaluate` with these new model params shows a significant improvement in the model's accuracy.\n"}
{"snippet": "df_pred = eightball_model.predict(df_test)\ndf_pred.head()\n", "intent": "Predictions are made by passing a feature set to the model's `predict` method\n"}
{"snippet": "df_pred = eightball_model.predict(df_test)\ndf_pred.head()\n", "intent": "Predictions are obtained in the same way as above. Preprocessing and imputation are performed on any data before predictions are made.\n"}
{"snippet": "pred = predictions_iq.append(predictions_sj)\ntrue = Y_iq_test.append(Y_sj_test)\nprint (\"cv error:\", mean_absolute_error(pred, true))\ntrain_pred = pred_train_iq.append(pred_train_sj)\ntrain_true = Y_iq_train.append(Y_sj_train)\nprint (\"train error:\", mean_absolute_error(train_pred, train_true))\n", "intent": "Now the combined score for both the cities\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nY_pred = rgr.predict(X_test)\nprint(\"MAE\", mean_absolute_error(Y_test,Y_pred))\n", "intent": "We make predictions for the trainig set and then compare them with original target.\nFor that we use Mean Absolute Error from sci-kit learn\n"}
{"snippet": "Y_pred = clf.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test, Y_pred))\n", "intent": "We make predictions for our training data set and then compare with original target\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(iris.target, pred)\n", "intent": "Or use the built-in functions in the `metrics` part of scikit-learn\n"}
{"snippet": "testpred = clf.predict(testX)\nmetrics.accuracy_score(testpred, testy)\n", "intent": "And then test the fit\n"}
{"snippet": "ResNet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "grid_y = grid_logit.predict(Xtestlr)\naccuracy_score(grid_y, ytestlr)\n", "intent": "The grid search CV also returned C = 0.001 as the best hyperparameter for the data.\n"}
{"snippet": "test_preds_nas = np.load('predictions/raw_test_preds_nas82.npy')\npred_y = test_preds_nas[:,0,:,:].squeeze().argmax(1) + 1\nprint('Accuracy of the initial model predictions: {0:.2f}%'.format(100*accuracy_score(test_labels, pred_y)))\n", "intent": ">  I sure hope so. Let's find out!\n"}
{"snippet": "y_train_pred = predict(images_train,Weights,Bias)\nacc = accuracy(labels_train,y_train_pred)\nprint(\"Train set accuracy:\", acc)\ny_dev_pred = predict(images_dev,Weights,Bias)\nacc = accuracy(labels_dev,y_dev_pred)\nprint(\"Dev set accuracy:\",acc)\n", "intent": "We can now assess the model's accuracy on both sets.\n"}
{"snippet": "y_train_pred = predict(images_train,W1,b1,W2,b2)\nacc = accuracy(labels_train,y_train_pred)\nprint(\"Train set accuracy:\", acc)\ny_dev_pred = predict(images_dev,W1,b1,W2,b2)\nacc = accuracy(labels_dev,y_dev_pred)\nprint(\"Dev set accuracy:\",acc)\n", "intent": "Then, predict the class labels using the previously computed model paramaters.\n"}
{"snippet": "rape_predictions = para.predict('rape has increased in Sweden', [x for x in reddit_comments['body']])\n", "intent": "Not to bad for ~50k comments. Let's see what it considers paraphrases.\n"}
{"snippet": "acc = model.evaluate([Xoh, s0, c0], outputs)\nprint(acc)\n", "intent": "Evaluating the model \n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "This is our accuracy . \n"}
{"snippet": "x_var = 0.2\ny_var = model.predict(x_var)\ny_var\n", "intent": "* Prediction of y value from the independant variable\n"}
{"snippet": "my_predictions = np.where(y_test == logistic.predict(X_test_std), 1, 0)\n", "intent": "<br><br/> \nNow we can predict the labels for the test data, and assign numbers 1 and 0 to correct and incorrect predictions respectively. \n<p><p/> \n"}
{"snippet": "preds1 = clf_lr.predict(test_vectors)\npreds2 = sig_clf_lr.predict(test_vectors)\nprint np.unique(preds1)\nprint np.unique(preds2)\nprint classification_report(test_labels,preds1)\nprint classification_report(test_labels,preds2)\nprint accuracy_score(test_labels,preds1)\nprint accuracy_score(test_labels,preds2)\n", "intent": "  Test Data actually have 5 class labels  \n  Try with training data of 4 labels first\n"}
{"snippet": "preds1 = clf_svc.predict(test_vectors)\npreds2 = sig_clf_svc.predict(test_vectors)\nprint np.unique(preds1)\nprint np.unique(preds2)\nprint classification_report(test_labels,preds1)\nprint classification_report(test_labels,preds2)\nprint accuracy_score(test_labels,preds1)\nprint accuracy_score(test_labels,preds2)\n", "intent": "* Test Data actually have 5 class labels\n* Try with training data of 4 labels first\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\nclf_baseline =  DummyClassifier()\ntrain_predict(clf_baseline, X_train_25, y_train_25, X_test, y_test)\ntrain_predict(clf_baseline, X_train_50, y_train_50, X_test, y_test)\ntrain_predict(clf_baseline, X_train_75, y_train_75, X_test, y_test)\ntrain_predict(clf_baseline, X_train, y_train, X_test, y_test)\n", "intent": "Training sklearn DummyClassifier to be used as baseline to compare with other classifiers. DummyClassifier makes predictions using simple rules\n"}
{"snippet": "ResNet_50_predictions = [np.argmax(ResNet_50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet_50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": " model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "To evaluate our model we'll be using a 5-fold cross validation with the Accuracy metric.\nTo do that, we'll define a small scoring function.\n"}
{"snippet": "print confusion_matrix(y_test, best_rfc.predict(X_test))\n", "intent": "recall means the people who are identified as maligiant, 96% are actualliy maligiant\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, df, y, cv=10)\n", "intent": "K-fold Cross validation\n"}
{"snippet": "print r2_score(df6[label_in], df6['mean_in'])\n", "intent": "We got 0.64 r2. It's not bad. Let's compare this result with base model, where we will use mean input for 6 months like predicted value.\n"}
{"snippet": "yTrue = yTest\npredictions = []\nfor fitModel in fitModels:\n    yPred = fitModel.predict(XTest_pca)\n    predictions.append(list(yPred))\n", "intent": "**Non-standardized dataset**\n"}
{"snippet": "yTrue = yTest\npredictions_std = []\nfor fitModel in fitModels_std:\n    yPred_std = fitModel.predict(XTest_std_pca)\n    predictions_std.append(list(yPred_std))\n", "intent": "**Standardized dataset**\n"}
{"snippet": "i = 0\nfor prediction in predictions:\n    print(names[i])\n    print(classification_report(yTrue, prediction))\n    i = i+1\n", "intent": "**Non-standardized data**\n"}
{"snippet": "i = 0\nfor prediction in predictions_std:\n    print(names[i])\n    print(classification_report(yTrue, prediction))\n    i = i+1\n", "intent": "**Standardized data**\n"}
{"snippet": "print(\"Accuracy score: \" + str(accuracy_score(yTrue, yPred)))\n", "intent": "***- Accuracy score***\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Accuracy score: \" + str(accuracy_score(yTrue, yPred)))\n", "intent": "**- Accuracy score**\n"}
{"snippet": "b=(bos.PRICE - lm.predict(X)) ** 2\nprint (np.mean(b))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def pred_batch(imgs):\n    predictions = model.predict(imgs)\n    idxs = np.argmax(predictions, axis=1) \n    print('Shape: {}'.format(predictions.shape))\n    print('First 5 classes: {}'.format(classes[:5]))\n    print('First 5 probabilities: {}\\n'.format(predictions[0, :5]))\n    print('Predictions prob/class: ')\n    for i in range(len(idxs)):\n        idx = idxs[i]\n        print ('  {:.4f}/{}'.format(predictions[i, idx], classes[idx])) \n", "intent": "Model will give out 1000 probablities for each input image. We can find the maximum of these probabilities to find the prediction of the model.\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nmse\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "deep_classifier_trainer.evaluate(num_evals=1, dataset=train_dataset)\n", "intent": "The details of the evaluation routing will be covered in a later lesson. For now, we will just use it to evaluate the performance of our model.\n"}
{"snippet": "QDA_model = QDA()\nkfold = model_selection.KFold(n_splits=5, random_state=0)\nresults = model_selection.cross_val_score(QDA_model, data_train[cols], data_train.result, cv=kfold, scoring='accuracy')\nprint('Cross Validation Accuracy: ', results.mean())\n", "intent": "<h2>Quadratic Discriminant Analysis</h2>\n"}
{"snippet": "ranks = -predict(U,P)\nfor i, pred in enumerate(ranks):\n    pred = pred.argsort()\n    ranks[i,pred] = np.arange(len(pred))\nmean_rank = np.sum(ranks * V) / len(ranks) + 1\nprint mean_rank\n", "intent": "** Weighted mean rank**\n"}
{"snippet": "iris_predictions = clf.predict(iris_test[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']])\nfrom sklearn.metrics import accuracy_score\naccuracy_score(iris_test['Class'], iris_predictions)\n", "intent": "Ok. that is cool.\nLet's try to classify the test set.\n"}
{"snippet": "knn.predict(athletes_test_features)\n", "intent": "and now we can make predictions on everyone in our test set:\n"}
{"snippet": "predictor = lambda x:  predict(x, aprioriPr, conditionalPr)[0][0]\n", "intent": "now the code to run this test data through our prediction algorithm\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(quad_test['Fly Quad?'], predictions)\n", "intent": "We can just eyeball the accuracy, but let's go ahead and calculate it.\n"}
{"snippet": "knn.predict(others)\n", "intent": "Now I can classify those people:\n"}
{"snippet": "predicted =k1nn.predict(diabetes_features)\nactual = np.array(diabetes_labels)\ntotal = len(actual)\ncorrect = 0\nfor i in range(len(actual)):\n    if actual[i] == predicted[i]:\n        correct += 1\nprint(correct / total)\n", "intent": "Now I am going to test how accurate the classifier is:\n"}
{"snippet": "def evaluate(X_train, X_test, y_train, y_test):\n", "intent": "We'll train and test several machine learning models for comparison with the Bayesian inference model.\n"}
{"snippet": "print(classification_report(y_test, knb_testc_pred, target_names=[\"legit\", \"fraud\"]))\n", "intent": "With this undersampling the score on the test score actually worsens.\n"}
{"snippet": "knn.train(X_fold1, cv2.ml.ROW_SAMPLE, y_fold1)\n_, y_hat_fold2 = knn.predict(X_fold2)\n", "intent": "Train the classifier on the first fold, then predict the labels of the second fold:\n"}
{"snippet": "knn.train(X_fold2, cv2.ml.ROW_SAMPLE, y_fold2)\n_, y_hat_fold1 = knn.predict(X_fold1)\n", "intent": "Train the classifier on the second fold, then predict the labels of the first fold:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_fold1, y_hat_fold1)\n", "intent": "Compute accuracy scores for both folds:\n"}
{"snippet": "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n", "intent": "This object can be passed directly to the `cross_val_score` function in the following way:\n"}
{"snippet": "_, y_hat = knn.predict(X_oob)\naccuracy_score(y_oob, y_hat)\n", "intent": "Test the classifier on the out-of-bag samples:\n"}
{"snippet": "evaluate(final_model, X_test, y_test)\n", "intent": "My final results. MAE of 91 is within 1 standard deviation (within +-276.74) for this sample.\n"}
{"snippet": "score = model.evaluate(x_test_private, y_test_private, verbose=0)\nscore\n", "intent": "**Use trained model to predict test samples**\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "Use the trained model to predict the target values for the test set. \n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "We use the `predict` method to make predictions on the test data.\n"}
{"snippet": "y_pred = model.predict(X_test)\ny_pred\n", "intent": "Let's take a look at the model predictions. Recall that our model is estimating a probability for each class.\n"}
{"snippet": "crossvalscorelr = cross_val_score(lr, countysalesmodelq12015, countysalesmodel2015, n_jobs=1, cv=5)\ncrossvalscorelasso = cross_val_score(lasso, countysalesmodelq12015, countysalesmodel2015, n_jobs=1, cv=5)\nprint \"CrossValScore LR:    %f\"%(crossvalscorelr.mean())\nprint \"CrossValScore Lasso: %f\"%(crossvalscorelasso.mean())\n", "intent": "The lasso model above predicted Q1 2016 sales within 1%.\n"}
{"snippet": "crossvalscorelr = cross_val_score(lr, X1, Y1, n_jobs=1, cv=5)\ncrossvalscorelasso = cross_val_score(lasso, X1, Y1, n_jobs=1, cv=5)\nprint \"CrossValScore LR:    %f\"%(crossvalscorelr.mean())\nprint \"CrossValScore Lasso: %f\"%(crossvalscorelasso.mean())\n", "intent": "The lasso model above predicted Q1 2016 sales within 1%.\n"}
{"snippet": "xception_predictions = [np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint(\"Accuracy:\",  test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_preds = np.where(test_probs > 0.5, 1.0, 0.0).astype(np.int64)\ntest_labels = test_input[labels]\naccuracy_score(test_labels, test_preds)\n", "intent": "Finally, calculate accuracy of the predictions on the test set.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(eclf, X, y, cv=5).mean()\n", "intent": "Voting Classifier Score\n"}
{"snippet": "cross_val_score(clf1, X, y, cv=5).mean()\n", "intent": "Logistic Regression Score\n"}
{"snippet": "cross_val_score(clf2, X, y, cv=5).mean()\n", "intent": "Decision Tree Score : hmm..\n"}
{"snippet": "cross_val_score(clf3, X, y, cv=5).mean()\n", "intent": "Gaussian NB Score : Terrible.. It's better to keep out Gaussian NB\n"}
{"snippet": "model = models[(0.01,1)]\nprint(\"Results on test data\")\nprint(metrics.log_loss(test_labels, model.predict_proba(test_dataset)))\nprint(metrics.accuracy_score(test_labels, model.predict(test_dataset)))\nprint(\"Results on validation data without duplicated\")\nprint(metrics.log_loss(valid_labels_no_dup, model.predict_proba(valid_dataset_no_dup)))\nprint(metrics.accuracy_score(valid_labels_no_dup, model.predict(valid_dataset_no_dup)))\nprint(\"Results on test data without duplicated\")\nprint(metrics.log_loss(test_labels_no_dup, model.predict_proba(test_dataset_no_dup)))\nprint(metrics.accuracy_score(test_labels_no_dup, model.predict(test_dataset_no_dup)))\n", "intent": "Checking results on final test sample and samples without duplicates:\n"}
{"snippet": "np.round(accuracy_score(y_test, predict), 2)\n", "intent": "Now, call `accuracy_score` on your test data and your predictions. Print these scores using `np.round` and 2 decimal places.\n"}
{"snippet": "predictions = rfe_model.predict(X_test)\n", "intent": "Finally, let's evaluate our new reduced feature set model on our test data! You can call `.predict` directly on your `rfe_model`.\n"}
{"snippet": "print(accuracy_score(y_test, predictions))\n", "intent": "Finally, `print` the `accuracy_score` on your `predictions`.\n"}
{"snippet": "logReg.predict(X_test[20].reshape(1,-1))\n", "intent": "Let's look at the song at index 20 and see what the prediction probabilities were!\n"}
{"snippet": "logReg.predict_proba(X_test[20].reshape(1,-1))\n", "intent": "You can see the classifier predicted the song at index 20 as `dance`. How confident was it? We use the `predict_proba` method to find out!\n"}
{"snippet": "song_index = 20\nprint(logReg.predict(X_test[song_index].reshape(1,-1))) \nprint(knn.predict(X_test[song_index].reshape(1,-1))) \nprint(svc.predict(X_test[song_index].reshape(1,-1))) \n", "intent": "Now: Let us look at the prediction each classifier has for the song at index `20`.\n"}
{"snippet": "voting_classifier.predict(X_test[song_index].reshape(1,-1))\n", "intent": "Let's look at the prediction for song at index 19 !\n"}
{"snippet": "song_index = 20\nprint(knn.predict(X_test[song_index].reshape(1,-1))) \nprint(logReg.predict(X_test[song_index].reshape(1,-1))) \nprint(svc.predict(X_test[song_index].reshape(1,-1))) \n", "intent": "Now: Let us look at the prediction each classifier has for the song at index `20`.\n"}
{"snippet": "song_index = 20\nprint(voting_classifier.predict(X_test[song_index].reshape(1,-1))) \n", "intent": "Let's look at the prediction for song at index 20 !\n"}
{"snippet": "model_top.evaluate(validation_data, validation_labels) \n", "intent": "Evaluating our model on test set and visualising the results.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Pred_Test= stem_model.predict(X_test)\n", "intent": "We shall be predicting the model output \n"}
{"snippet": "Pred_Test_OLS = lm.predict(X_test)\n", "intent": "Calculate the predicted values from model - Ordinary Least Squares method\n"}
{"snippet": "from sklearn import metrics\nMSE_Train = metrics.mean_squared_error(y_train,lm.predict(X_train))\nMSE_Test = metrics.mean_squared_error(y_test,  Pred_Test_OLS)\nprint(\"Mean Squared Error (Train):\", MSE_Train)\nprint(\"Mean Squared Error (Test):\", MSE_Test)\n", "intent": "We need to the measure the effectiveness of the model. We shall use mean sqaured error for testing the same as for the Linear Regression model\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        loss += style_weights[i]*tf.reduce_sum(tf.square(gram_matrix(feats[style_layers[i]]) - style_targets[i]))\n    return  loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "log_accuracy_score = accuracy_score(y_test, predictions)\nlog_accuracy_score\n", "intent": "We can see that the predictions were roughly 63% precise.\nWe will now check the accuracy score below.\n"}
{"snippet": "print(classification_report(y_test, rfc_pred))\n", "intent": "The classification report will allow for us to see how precise the model was with respect to the test values and the predicted values.\n"}
{"snippet": "rf_accuracy_score = accuracy_score(y_test, rfc_pred)\nrf_accuracy_score\n", "intent": "The model was roughly 62% precise.\n"}
{"snippet": "y_mnb_predict = clf.predict(X_test_tfidf)\n", "intent": "It worked perfectly. Our Model guessed both sample text perfectly correct.\nLet's test our model on test data\n"}
{"snippet": "met.r2_score(y_pred=predict, y_true=y_test)\n", "intent": "We are using Regression Metrices for evaluating our model\n"}
{"snippet": "_ = model_pipe.predict(x_test)\n", "intent": "Let's check the new pipeline against the old one since with ensembles we're likely taking a perf hit on prediction. Life is a trade off!\n"}
{"snippet": "expected_def = y_train_dat\npredicted_def = dectree.predict(X_train_dat)\nprint(metrics.classification_report(predicted_def, expected_def))\nprint \"\\n\"\nprint(metrics.confusion_matrix(predicted_def, expected_def))\nprint \"\\n\"\nprint \"The accuracy score for the training data gives us:{0:.4f}\".format(accuracy_score(expected_def, predicted_def))\n", "intent": "Model 1 on the training data using default parameters:\n"}
{"snippet": "expected_def_test = y_test_dat\npredict_def_test = dectree.predict(X_test_dat)\nprint(metrics.classification_report(predict_def_test, expected_def_test))\nprint \"\\n\"\nprint (metrics.confusion_matrix(predict_def_test, expected_def_test))\nprint \"\\n\"\nprint \"The accuracy score for the test data gives us:{0:.4f}\".format(accuracy_score(expected_def_test, predict_def_test))\n", "intent": "Model 1 on the test data using default parameters:\n"}
{"snippet": "expected = y_train_dat\npredicted = dectree1.predict(X_train_dat)\nprint(metrics.classification_report(predicted, expected))\nprint \"\\n\"\nprint(metrics.confusion_matrix(predicted, expected))\nprint \"\\n\"\nprint \"The accuracy score for the training data gives us:{0:.4f}\".format(accuracy_score(expected, predicted))\n", "intent": "Model 2 Training Data:\n"}
{"snippet": "expected_test = y_test_dat\npredict_test = dectree1.predict(X_test_dat)\nprint(metrics.classification_report(predict_test, expected_test))\nprint \"\\n\"\nprint (metrics.confusion_matrix(predict_test, expected_test))\nprint \"\\n\"\nprint \"The accuracy score for the test data gives us:{0:.4f}\".format(accuracy_score(expected_test, predict_test))\n", "intent": "Model 2 Test Data using specified parameters:\n"}
{"snippet": "expected_gini_train = y_train_dat\npredict_gini_train = dectree_gini.predict(X_train_dat)\nprint(metrics.classification_report(expected_gini_train, predict_gini_train))\nprint \"\\n\"\nconf_matrix = metrics.confusion_matrix(expected_gini_train, predict_gini_train)\nprint(conf_matrix)\nprint \"\\n\"\nprint \"The accuracy score for the training data gives us:{0:.4f}\".format(accuracy_score(expected_gini_train, predict_gini_train))\n", "intent": "Model 3 Training Data:\n"}
{"snippet": "imdb_lstm_scores = imdb_lstm_model.evaluate(imdb_x_test_padded, imdb_y_test)\nprint('loss: {} accuracy: {}'.format(*imdb_lstm_scores))\n", "intent": "Assess the model. __This takes 2-10 minutes. You might not want to re-run it unless you are testing out your own changes.__\n"}
{"snippet": "one_unit_SRNN.predict(numpy.array([ [[3], [3], [7]] ]))\n", "intent": "We can then pass in different input values, to see what the model outputs.\nThe code below passes in a single sample that has three time steps.\n"}
{"snippet": "two_unit_SRNN.predict(numpy.array([ [[3], [3], [7], [5]] ]))\n", "intent": "This passes in a single sample with four time steps.\n"}
{"snippet": "embedding_model.predict(numpy.array([[wordvec.vocab['python'].index]]))[0][0] == wordvec['python']\n", "intent": "Looks good, right? But let's not waste our time when the computer could tell us definitively and quickly:\n"}
{"snippet": "not_pretrained_scores = not_pretrained_model.evaluate(x_test_padded, y_test)\nprint('loss: {} accuracy: {}'.format(*not_pretrained_scores))\n", "intent": "Assess the model. __This takes awhile. You might not want to re-run it.__\n"}
{"snippet": "predicted_data = rf.predict(y_test)\nprint (rf.score(y_test, X_test))\nconfusion_matrix(predicted_data, X_test)\n", "intent": "The results are predicted and accuracy percentage is displayed. Confusion Matrix is used to give the no. of perfomance issues.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "Classification Accuracy:\n- Proportion of correct predictions\n- Common evaluation metric for classification problems\n"}
{"snippet": "print((10+0+20+10)/4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "Mean Absolute Error\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "Root Mean Squared Error\n"}
{"snippet": "print('\\n')\nprint(classification_report(y_test,pred_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predict = lm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "svm_pred = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y_predict_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_test2 = y_test.copy(deep=True)\ny_test2.replace(to_replace=1, value=0,inplace = True)\ny_test2.replace(to_replace=5, value=1,inplace = True)\nmetrics.roc_auc_score(y_test2, y_predict_prob)\n", "intent": "**6:** Calculate the AUC.\n"}
{"snippet": "pred_lgbm = lgbm.predict(X_test)\npred_lgbm[pred_lgbm > 0.5] = 1\npred_lgbm[pred_lgbm < 0.5] = 0\nconf_mat = confusion_matrix(y_test, pred_lgbm)\nacc_lgbm = (conf_mat[0][0] + conf_mat[1][1])/len(y_test)\nprint('Accuracy:', acc_lgbm)\nsen_lgbm = conf_mat[0][0]/(conf_mat[0][0] + conf_mat[0][1])\nprint('Sensitivity:', sen_lgbm)\n", "intent": "**Accuracy and Sensitivity on Test Set **\n"}
{"snippet": "ddtest = xgb.DMatrix(X_test)\npred_xgb = bst.predict(ddtest, ntree_limit=bst.best_ntree_limit)\nconf_mat = confusion_matrix(y_test, pred_xgb.astype(int))\nacc_xgb = (conf_mat[0][0] + conf_mat[1][1])/len(y_test)\nprint('Accuracy:', acc_xgb)\nsen_xgb = conf_mat[0][0]/(conf_mat[0][0] + conf_mat[0][1])\nprint('Sensitivity:', sen_xgb)\n", "intent": "** Accuracy and Sensitivity on Test Set**\n"}
{"snippet": "pred_gbt = clf.predict(X_test)\nconf_mat = confusion_matrix(y_test, pred_gbt)\nacc_gbt = (conf_mat[0][0] + conf_mat[1][1])/len(y_test)\nprint('Accuracy:', acc_gbt)\nsen_gbt = conf_mat[0][0]/(conf_mat[0][0] + conf_mat[0][1])\nprint('Sensitivity:', sen_gbt)\n", "intent": "** Accuracy and Sensitivity on Test Set**\n"}
{"snippet": "pred_lr = lr.predict(X_test)\nconf_mat = confusion_matrix(y_test, pred_lr)\nacc_lr = (conf_mat[0][0] + conf_mat[1][1])/len(y_test)\nprint('Accuracy:', acc_lr)\nsen_lr = conf_mat[0][0]/(conf_mat[0][0] + conf_mat[0][1])\nprint('Sensitivity:', sen_lr)\n", "intent": "** Accuracy and Sensitivity on Test Set **\n"}
{"snippet": "class PusedoInvModel:\n    def __init__(self):\n        self.parameter = []\n    def fit(self, XX, YY):\n        temp0 = XX.T.dot(XX)\n        temp1 = np.linalg.pinv(temp0)\n        self.parameter = temp1.dot(XX.T).dot(YY)\n    def predict(self, XX):\n        YY = XX.dot(self.parameter)\n        return YY\n", "intent": "Now solving with Pusedo Inverse\n"}
{"snippet": "cv_scores = cross_val_score(new_model, X_sub_vec, y_sub, cv = 5, scoring = \"r2\")\nprint(cv_scores)\n", "intent": "The improved performance of the model comes with normally distributed residuals, so this is a good sign for the model.\n"}
{"snippet": "cv_scores = cross_val_score(svr, X_sub_vec, y_sub, cv = 5, scoring = \"r2\")\nprint(cv_scores)\n", "intent": "Here the residuals are reasonably normally distributed for the SVR model.\n"}
{"snippet": "cv_scores = cross_val_score(svr_new, X_sub_vec, y_sub, cv = 3, scoring = \"r2\")\nprint(cv_scores)\n", "intent": "Residuals are normally distributed.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lgmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "all_predictions = rating_model.predict(X_test)\nprint(all_predictions)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint (confusion_matrix(y_test, all_predictions))\nprint (classification_report(y_test, all_predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "embedding = vectorized_text                \nclassifier = lr_classifier     \npredicted_sentiment = classifier.predict(embedding['test']).tolist()\nprint ('DONE - [CLASSIFY] Apply Chosen Classifier to the Embedding')\n", "intent": "Classify the Vectorized Tweets with our Classifier of choice\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0, tf.float32)\n    for i in range(len(style_layers)):\n        gram_current = gram_matrix(feats[style_layers[i]])\n        loss = loss + style_weights[i]*tf.reduce_sum((gram_current-style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = lm.predict( X_test)\n", "intent": "Evaluate its performance by predicting off the test values.\n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint(all_predictions)\nfrom sklearn.metrics import classification_report\nprint (classification_report(messages['label'], all_predictions))\n", "intent": "There are several ways to evaluate a classification model; a summary of metrics is shown below:\n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nmse = rss / len(bos.PRICE)\nprint mse\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "accuracy = accuracy_score(clf.predict(Xtestlr), ytestlr)\nprint(\"Accuracy: %f\" % accuracy)\n", "intent": "**MY COMMENT:** It gives me a C of 1, different from the previous result of 0.001.\n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test Loss:', loss)\nprint('Test Accuracy:', accuracy)\n", "intent": "**Step 4.** Evaluate the accuracy on test data.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Lets predict values for the testing data.**\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake)\n    g_loss = ls_generator_loss(score_fake)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\n    print(\"Discriminator: \", d_loss, d_loss_true)\n    print(\"Generator: \", g_loss, g_loss_true)\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, s  in enumerate(style_layers):\n        style_loss += style_weights[i] * torch.sum((gram_matrix(feats[s])- style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, s in enumerate(style_layers):\n        style_loss += style_weights[i] * tf.reduce_sum( tf.square(gram_matrix(feats[s]) - style_targets[i]) )\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "scores = cross_val_score(bestcancertree, x_test, y_test, cv=10, scoring='accuracy') \nprint \"Mean ACCURACY from the cross-validation: %-4.1f%%\" % (np.mean(scores)*100.0)\nprint \"Full vector: [%s]\" % (', '.join(map(\"{:.3f}\".format, scores) ))\nscores2 = cross_val_score(bestcancertree, x_test, y_test, cv=10, scoring='precision') \nprint \"\\nMean PRECISION from the cross-validation: %-4.1f%%\" % (np.mean(scores2)*100.0)\nprint \"Full vector: [%s]\" % (', '.join(map(\"{:.3f}\".format, scores2)))\nscores3 = cross_val_score(bestcancertree, x_test, y_test, cv=10, scoring='recall') \nprint \"\\nMean RECALL from the cross-validation: %-4.1f%%\" % (np.mean(scores3)*100.0)\nprint \"Full vector: [%s]\" % (', '.join(map(\"{:.3f}\".format, scores3)))\n", "intent": "Next, ten iterations of cross validation using the the test set was carried out to find the accuracy, precision, and recall.\n"}
{"snippet": "X_encoded = encoder.predict(x_test)\n", "intent": "***GET ENCODINGS***\n"}
{"snippet": "from sklearn.metrics import log_loss\nlog_loss(y_test,y_pred)\n", "intent": "***\n - John wanted to check this too! \n - Let's check how he implemented this in Python, it's fairly easy thanks to sci-kit learn! \n"}
{"snippet": "y_pred = cv.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "Now its time to see how the optimized classifier performs:\n"}
{"snippet": "y_train = clf.predict(X_train)\ny_test = clf.predict(X_test)\n", "intent": "Perform predictions on both train and test data\n"}
{"snippet": "cf_train = mt.confusion_matrix(t_train, y_train)\nprint('Confusion matrix')\nprint(cf_train)\nprint ('Precision = {0:5.4f}'.format(mt.precision_score(t_train,y_train)))\nprint ('Recall = {0:5.4f}'.format(mt.recall_score(t_train,y_train)))\nprint ('Accuracy = {0:5.4f}'.format(mt.accuracy_score(t_train,y_train)))\n", "intent": "Compute measures in both cases\n"}
{"snippet": "Inc_predictions = [np.argmax(Inc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inc_predictions)==np.argmax(test_targets, axis=1))/len(Inc_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "logPredict = logModel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print \"Class predictions according to Scikit Learn:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_new, test_data)\n", "intent": "Now, let's use this function to evaluate the classification error of `my_decision_tree_new` on the **validation_set**.\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_old, test_data)\n", "intent": "Now, evaluate the validation error using `my_decision_tree_old`.\n"}
{"snippet": "print \"Training data, classification error (model 1):\", evaluate_classification_error(model_1, train_data)\nprint \"Training data, classification error (model 2):\", evaluate_classification_error(model_2, train_data)\nprint \"Training data, classification error (model 3):\", evaluate_classification_error(model_3, train_data)\n", "intent": "Let us evaluate the models on the **train** and **validation** data. Let us start by evaluating the classification error on the training data:\n"}
{"snippet": "print \"Validation data, classification error (model 1):\", evaluate_classification_error(model_1, test_data)\nprint \"Validation data, classification error (model 2):\", evaluate_classification_error(model_2, test_data)\nprint \"Validation data, classification error (model 3):\", evaluate_classification_error(model_3, test_data)\n", "intent": "Now evaluate the classification error on the validation data.\n"}
{"snippet": "print \"Validation data, classification error (model 4):\", evaluate_classification_error(model_4, test_data)\nprint \"Validation data, classification error (model 5):\", evaluate_classification_error(model_5, test_data)\nprint \"Validation data, classification error (model 6):\", evaluate_classification_error(model_6, test_data)\n", "intent": "Calculate the accuracy of each model (**model_4**, **model_5**, or **model_6**) on the validation set. \n"}
{"snippet": "print \"Validation data, classification error (model 7):\", evaluate_classification_error(model_7, test_data)\nprint \"Validation data, classification error (model 8):\", evaluate_classification_error(model_8, test_data)\nprint \"Validation data, classification error (model 9):\", evaluate_classification_error(model_9, test_data)\n", "intent": "Now, let us evaluate the models (**model_7**, **model_8**, or **model_9**) on the **validation_set**.\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision = precision_score(y_true=test_data['sentiment'].to_numpy(),\n                           y_pred=model.predict(test_matrix))\nprint \"Precision on test data: %s\"% precision  \n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(),\n                                                   y_pred=predictions_with_default_threshold.to_numpy())\nprecision_with_default_threshold\n", "intent": "For each of the values of threshold, we compute the precision and recall scores.\n"}
{"snippet": "probabilities = model.predict_proba(baby_matrix)[:,1]\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "deep_pred = deep_features_model.predict(test_deep_features)\ntrue_label = le.transform(image_test['label'])\naccuracy_score(true_label, deep_pred)\n", "intent": "As we can see, deep features provide us with significantly better accuracy (about 78%)\n"}
{"snippet": "print sqft_model.predict(house2['sqft_living'].reshape(-1,1))\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "X_test = np.array(X_test)\ny_test = np.array(y_test)\nscore = tflearn_model.evaluate(X_test, y_test, batch_size=128)\nprint(score)\n", "intent": "<a name=\"Model_Evaluation_TFlearn\"></a>\n"}
{"snippet": "X_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], 154587))\ny_test = np.array(y_test)\nscore = tf_model.evaluate(X_test, y_test, batch_size=128)\n", "intent": "<a name=\"Model_Evaluation_Tensorflow\"></a>\n"}
{"snippet": "print(\"\\n\\n ************ START Model Prediction  with Tensorflow **************  \\n\")\nprint(X_test.shape)\nLabel = tf_model.predict(np.reshape(X_test[0], (1, 154587)))\nprint(\"Label Prediction : \", Label)\nprint(\"True Label Value : \", y_test[0])\n", "intent": "<a name=\"Model_Prediction_Tensorflow\"></a>\n<a name=\"prediction_on_dataset_Tensorflow\"></a>\n"}
{"snippet": "print(\"Before reshape : \", jc_image.shape)\njc_image = jc_image[:, :, :, :3]\nprint(\"After reshape : \", jc_image.shape)\nscore = tf_model.predict(np.reshape(jc_image, (1, 154587)))\nprint(\"Prediction score : \", score)\nprint(\"True Label Value : [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]]\")\n", "intent": "<a name=\"prediction_on_otherFaces_Tensorflow\"></a>\n"}
{"snippet": "print(other_image.shape)\nscore = tf_model.predict(np.reshape(other_image, (1, 154587)))\nprint(\"Prediction score : \", score)\nprint(\"True Label Value : Here we have a picture of fishes\")\nprint(\"\\n\\n ************ END Model Prediction with Tensorflow **************  \\n\")\n", "intent": "<a name=\"prediction_on_NonFaces_Tensorflow\"></a>\n"}
{"snippet": "print(\"Before reshape : \", jc_image.shape)\njc_image = jc_image[:, :, :, :3]\nprint(\"After reshape : \", jc_image.shape)\nscore = tf_model.predict(np.reshape(jc_image, (1, 154587)))\nprint(\"Prediction score : \", score)\nprint(\"True Label Value : (4, 0)\")\n", "intent": "<a name=\"prediction_on_otherFaces_Tensorflow\"></a>\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error \nprint('MAE: {0}'.format(mean_absolute_error(y_test,y_pred)))\nprint ('RMSE: {0}'.format(mean_squared_error(y_test,y_pred)))\n", "intent": "Error rates of this model:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for index1,index2 in enumerate(style_layers):\n        feature = feats[index2]\n        gram1 = gram_matrix(feature)\n        loss += style_weights[index1] * torch.sum((gram1 - style_targets[index1])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\ndisplay_scores(tree_rmse_scores)\n", "intent": "**Better Evaluation Using Cross-Validation**\n"}
{"snippet": "ResNet50_predictions = [np.argmax(My_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resNet50]\ntest_accuracy_ResNet50 = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_ResNet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print( 'R^2:', r2_score(y_test, pred ))\nprint( 'MAE:', mean_absolute_error(y_test, pred))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "print(\"Predictions: \" + str(dt_clf.predict(X_test)))\nprint(\"True classes: \" + str(numpy.array(y_test)))\n", "intent": "What actually happens in the score method is it predicts on the X_test data, and then compares with the y_test true classes.\n"}
{"snippet": "predictions = dtree.predict(x_test)\n", "intent": "**Lets create predictions from the test set and check results using classification report and connfusion matrix**\n"}
{"snippet": "predictions = rfc.predict(x_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "test['PREDICTED_STATUS']=np.int_(model_gb.predict(test.drop(['DEFAULTER'],axis = 1)))\ntest.index.names = ['ID']\n", "intent": "Save the output to csv file in desired format\n"}
{"snippet": "model_3.reset_states()\nto_predict = chars_to_one_hot('L', chars, chars_to_indices, window_size)*1\npredicted = model_3.predict(to_predict)\nprint(predicted)\nindices_to_chars[np.argmax(predicted[0])]\n", "intent": "Algunas pruebas mas\n"}
{"snippet": "model_simple.load_weights('best_RNN_textdata_weights_20_epochs_step_1_epochs_valid.hdf5')\nmodel_simple.evaluate(X_validation, y_validation)\n", "intent": "**Overfitting a partir de aca**\n"}
{"snippet": "ResNet50_predictions = [np.argmax(Resmodel.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out our model on the test dataset of dog images.  Ensure that our test accuracy is greater than 1%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out our model on the test dataset of dog images. Ensure that our test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, KFold\ncv = KFold(n_splits=5)\ncv_score = cross_val_score(svc, fmri_masked, conditions)\nprint(cv_score)\n", "intent": "Scikit-learn has tools to perform cross-validation easier:\n"}
{"snippet": "incept_predictions = [np.argmax(incept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(incept_predictions)==np.argmax(test_targets, axis=1))/len(incept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cv_preds = gscv.predict(X_test)\nprint('F1 Score: ', f1_score(y_test, cv_preds))\nprint('Accuracy Score: ', accuracy_score(y_test, cv_preds))\n", "intent": "This model has an F1 score of approximately 81% and an accuracy score of 83%\n"}
{"snippet": "print(classification_report(y_test, cv_preds))\n", "intent": "More specifically, precision is much higher for death predictions than survival predictions. However recall is less variated between the two classes.\n"}
{"snippet": "from sklearn.metrics import f1_score\ny_pred = [1] * len(y_true)\nprint('F1: {0:.4f}%'.format(f1_score(y_true, y_pred, pos_label = None, average = 'weighted') * 100))\n", "intent": "The home teams' win percentage is 58.0%. If a model is considered useful, it must (at minimum) have a better score than the established baseline\n"}
{"snippet": "dout = np.random.randn(batch_size, pool_size, pool_size, input_depth)\ndx_num = eval_numerical_gradient_array(lambda x: max_pool_forward(x, pool_size, stride), X, dout)\nout = max_pool_forward(X, pool_size, stride)\ndx = max_pool_backward(dout, X, out, pool_size, stride)\nprint('Testing max_pool_backward function:')\ndiff = relative_error(dx, dx_num)\nif diff < 1e-12:\n  print 'PASSED'\nelse:\n  print 'The diff of %s is too large, try again!' % diff\n", "intent": "And we again use numerical gradient checking to ensure that the backward function is correct: \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\nimport numpy as np\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print (\"Mean score: {0:.3f} (+/-{1:.3f})\".format(\n        np.mean(scores), sem(scores)))\n", "intent": "Let's evaluate the accuracy of our model by doing a K-fold cross validation (training the model several times and check the error each time).\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\ncv_auc = cross_val_score(logreg,X,y,cv=5, scoring='roc_auc')\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n", "intent": "to predict the accuracy of our classification model.\n If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!\n"}
{"snippet": "pred = fitted_models['svc'].predict(X_test)\nnp.column_stack((pred,y_test))\n", "intent": "Predictions using support vector classifier\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ninception_test_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Inception Test accuracy: %.4f%%' % inception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "P = lgr_search.predict_proba(X_test)\nP[:5]\n", "intent": "This means we'll need to use \"`predict_proba`\" rather than \"`predict`\":\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nrf_preds = rf_search.predict(X_test)\nsvm_preds = svm_search.predict(X_test)\nprint(\"RF test score: %.5f\" % accuracy_score(y_test, rf_preds))\nprint(\"SVM test score: %.5f\" % accuracy_score(y_test, svm_preds))\n", "intent": "Now we can introduce the holdout set to each optimized model to select our ultimate, \"best\" model that will be used.\n"}
{"snippet": "joblib.load(otpt_file).predict(X_test)[:5]\n", "intent": "Once the model is on disk, we can easily load and generate predictions like so:\n"}
{"snippet": "def prediction_report(model, model_name):\n    preds = model.predict(X_test)\n    print(\"[%s] Num '0' predictions: %i\" % (model_name, (preds == 0).sum()))\n    print(\"[%s] Num '1' predictions: %i\" % (model_name, (preds == 1).sum()))\n    fn = (y_test == 1) & (preds == 0)\n    fp = (y_test == 0) & (preds == 1)\n    print(\"[%s] Num FN: %i\" % (model_name, fn.sum()))\n    print(\"[%s] Num FP: %i\\n\" % (model_name, fp.sum()))\nprediction_report(clf1, \"CLF 1\")\nprediction_report(clf2, \"CLF 2\")\n", "intent": "Model 1 appears to out-perform model 2. But does that mean it's actually better?\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(\"CLF 1 report:\")\nprint(classification_report(y_test, clf1.predict(X_test)))\n", "intent": "*__Since the prior probabilities of a positive class label were 0.02, we basically got 98% accuracy by almost never predicting 1 in estimator 1!!!__*\n"}
{"snippet": "test_results = model.evaluate(test_data,\n                              test_labels,\n                              batch_size=None,\n                              verbose=1)\nprint(\"Test accuracy = {}\".format(test_results[1]))\n", "intent": "We will use the test data we have loaded to test our model performance on unseen data.\n"}
{"snippet": "for i in range(10000):\n    inp_, labels_ = mnist.train.next_batch(128)\n    session.run(optimizer, \n                feed_dict={labels:labels_, inp:inp_})\n    if i % 1000 == 999:\n        print(\"{:.1f}%, \".format(test_error(pred, inp)*100), end=\"\")\n", "intent": "Training the network looks about the same as above except this is now much faster due to better initialization and because we use a better optimizer.\n"}
{"snippet": "fbp_recos = fbp_op_mod(test_data)\nprint('Average error:', F.mse_loss(fbp_recos, test_images).data[0] / len(test_images))\nresults = [test_images, fbp_recos]\ntitles = ['Truth', 'FBP']\nshow_image_matrix(results, titles, indices=slice(10, 20), clim=[0, 1], cmap='bone')\n", "intent": "First we make a simple FBP reconstruction and compare it to the ground truth.\n"}
{"snippet": "thresholds = np.linspace(0, .5, 200)\ny_scores = model.decision_function(val_x)\nscores = []\nfor threshold in thresholds:\n    y_hat = (y_scores < threshold).astype(int)\n    scores.append([recall_score(y_pred=y_hat, y_true=val_y),\n                 precision_score(y_pred=y_hat, y_true=val_y),\n                 fbeta_score(y_pred=y_hat, y_true=val_y, beta=1)])\nscores = np.array(scores)\nprint(scores[:, 2].max(), scores[:, 2].argmax())\n", "intent": "Checking for the best threshold\n"}
{"snippet": "def predict_outlier(data,model,threshold):\n    data_list=[]\n    data_list.append(data)\n    data_cleaned=clean_data(data_list)\n    data_vect=vectorizer.transform(data_cleaned)\n    data_vect=svd.transform(data_vect)\n    predicted_labels=(model.predict_proba(data_vect)<threshold).astype(int)[:,0]\n    predicted_labels=map_output(predicted_labels)\n    return predicted_labels[0]\n", "intent": "So this is our final model as a function.\n"}
{"snippet": "def conf_int(x):\n    x_mean=auto.horsepower.values.mean()\n    x_var=auto.horsepower.values.var()\n    res=auto.mpg-ols_model.fittedvalues\n    size=len(auto.mpg.values)\n    RSE=np.sqrt(np.sum(res**2)/(size-2))\n    unc=t.ppf(0.975,df=size-2)*RSE*np.sqrt(1.0/size+ (x-x_mean)**2/((size-1)*x_var))\n    y_pred=ols_model.predict(exog=np.array([1,x]), transform=False)[0]\n    return (y_pred-unc,y_pred+unc)\n", "intent": "where $s_{y}$ is the residual standard error.\n"}
{"snippet": "def pred_int(x):\n    x_mean=auto.horsepower.values.mean()\n    x_var=auto.horsepower.values.var()\n    res=auto.mpg-ols_model.fittedvalues\n    size=len(auto.mpg.values)\n    RSE=np.sqrt(np.sum(res**2)/(size-2))\n    unc=t.ppf(0.975,df=size-2)*RSE*np.sqrt(1.0 + 1.0/size + (x-x_mean)**2/((size-1)*x_var))\n    y_pred=ols_model.predict(exog=np.array([1,x]), transform=False)[0]\n    return (y_pred-unc,y_pred+unc)\n", "intent": "$$ \\hat{y} \\pm t_{n-2} \\ s_{y}\\ \\sqrt{1+ \\frac{1}{n}+ \\frac{(x_{0}-\\bar{x})^2}{(n-1)s_{x}^2}} $$\n"}
{"snippet": "OLS_test_R2 = r2_score(y_test, linear.predict(X_test))\nridge_test_R2 = r2_score(y_test, RCV.predict(X_test))\nlasso_test_R2 = r2_score(y_test, LCV.predict(X_test))\nPCR_test_R2 = r2_score(y_test, PCA_linear.predict(X_test_pca))\nscores=[OLS_test_R2, ridge_test_R2, lasso_test_R2, PCR_test_R2]\npd.Series(scores, index=['OLS', 'Ridge', 'Lasso', 'PCR'])\n", "intent": "I will now compare the test $R^2$ of the models, since this measure is easier to interpret.\n"}
{"snippet": "score = (VGG19_model.evaluate(test_VGG19, test_targets, verbose=0))[1] * 100\nprint(\"VGG19_model accuracy: %.4f%%\" % score)\nscore = Resnet50_model.evaluate(test_Resnet50, test_targets, verbose=0)[1] * 100\nprint(\"Resnet50_model accuracy: %.4f%%\" % score)\nscore = InceptionV3_model.evaluate(test_InceptionV3, test_targets, verbose=0)[1] * 100\nprint(\"InceptionV3_model accuracy: %.4f%%\" % score)\nscore = Xception_model.evaluate(test_Xception, test_targets, verbose=0)[1] * 100\nprint(\"Xception_model accuracy:  %.4f%%\" %score)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cv_prediction = cross_val_predict(model, data[feature_labels], data['oqmd.delta_e.value'], cv=KFold(10, shuffle=True))\n", "intent": "Quantify the performance of this model using 10-fold cross-validation\n"}
{"snippet": "from scipy.stats.stats import pearsonr\npearsonr(model_huber.predict(X_train), model_huber_resids)\n", "intent": "Huber model residuals seems to have some correlation with $\\hat y$\n"}
{"snippet": "from scipy.stats.stats import pearsonr\npearsonr(model_ridge.predict(X_train), model_ridge_resids)\n", "intent": "In a good model, residuals cannot be correlated to the fitted values. \nWe have already saw before that the Ridge is ok in his regard:\n"}
{"snippet": "print classification_report(y_test, rfc_pred)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint confusion_matrix(colleges['Cluster'], kmeans.labels_)\nprint '\\n'\nprint classification_report(colleges['Cluster'], kmeans.labels_)\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred = knn.predict(X_test)\npred\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "prediction = log_model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "def precision(tp, fp):\n    return tp / (tp + fn)\ndef f1_score(tp, fn, fp):\n", "intent": "F1 score to be used to evaluate algoritm results.\n"}
{"snippet": "y_1_train= dt_model_1.predict(X_train)\nprint('Accuracy score on the training set: ', accuracy_score(y_train, y_1_train))\n", "intent": "Test the tree on the train.\n"}
{"snippet": "y_1_test = dt_model_1.predict(X_test)\nprint('Accuracy score on the test set: ', accuracy_score(y_test, y_1_test))\n", "intent": "Test the tree on the test.\n"}
{"snippet": "y_2_train= dt_model_2.predict(X_train)\nprint('Accuracy score on the training set: ', accuracy_score(y_train, y_2_train))\ny_2_test = dt_model_2.predict(X_test)\nprint('Accuracy score on the test set: ', accuracy_score(y_test, y_2_test))\n", "intent": "The printed tree with 2 depth is in the 'DT with 2 depth.png'.\nTest the tree on the train. Then test the tree on the test.\n"}
{"snippet": "y_3_train= dt_model_3.predict(X_train)\nprint('Accuracy score on the training set: ', accuracy_score(y_train, y_3_train))\ny_3_test = dt_model_3.predict(X_test)\nprint('Accuracy score on the test set: ', accuracy_score(y_test, y_3_test))\n", "intent": "The printed tree with 3 depth is in the 'DT with 3 depth.png'. Test the tree on the train. Then test the tree on the test.\n"}
{"snippet": "log_prob_res = bnb.predict_log_proba(X_test)\npositive_diff = log_prob_res[:,1]-log_prob_res[:,0]\n", "intent": "If we explore the result of log probability:\n"}
{"snippet": "nl_hook.loss = []\neval_input_fn, eval_qinit_hook = reader.get_inputs(file_references=None, \n                                                   mode=tf.estimator.ModeKeys.EVAL,\n                                                   example_shapes=reader_example_shapes)\n_ = nn.evaluate(input_fn=eval_input_fn, hooks=hooks + [eval_qinit_hook], steps=1)\n", "intent": "Similarly, we can [`evaluate`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n"}
{"snippet": "ResNet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_unknown = mv.predict(X_unknown)\nwell_data['Facies'] = y_unknown\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "y_unknown0 = clf0.predict(X_unknown0)\ny_unknown1 = clf1.predict(X_unknown1)\nwell_data0['Facies'] = y_unknown0\nwell_data1['Facies'] = y_unknown1\nwell_data=pd.concat([well_data0,well_data1], axis=0)\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "y_test = model.predict( X_test , batch_size=32, verbose=0)\nfinal_predictions = np.zeros((len(y_test),1))\nfor i in range(len(y_test)):\n    final_predictions[i] = np.argmax(y_test[i]) + 1 \n", "intent": "---\nWe obtain the predictions for test data.\n"}
{"snippet": "predicted_labels = clf.predict(X_test)\n", "intent": "Now that the model has been trained on our data, we can use it to predict the facies of the feature vectors in the test set.  \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum(tf.pow((gram - style_targets[i]), 2))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n     target_names=twenty_test.target_names))\n", "intent": "scikit-learn further provides utilities for more detailed performance analysis of the results:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, ShuffleSplit\nbest_model = alpha_search_mae.best_estimator_\nscores = cross_val_score(best_model, x, y, scoring=\"neg_mean_absolute_error\", cv=kf)\nprint(\"KFold mae_mean: %.3f\\t\\tmae_std: %.3f\" % (-scores.mean(), scores.std()))\n", "intent": "We do it by CV, fist using KFold.\n"}
{"snippet": "y_pred = np.clip( cross_val_predict(best_model, x, y, cv=kf), 5., 50.)\n", "intent": "Next we get individual CV predictions and plot them against the true values. \n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "Now make predictions on the genre for the X_test data\n"}
{"snippet": "rfe_predictions = rfe_model.predict(X_test)\n", "intent": "We notice the selected features are the sames as used in SelectFromModel with median threshold\n"}
{"snippet": "rfe_predictions2 = rfe_model2.predict(X_test)\n", "intent": "Once again, same features as in SelectFromModel\n"}
{"snippet": "rfe_predictions = rfe_model.predict(X_test)\n", "intent": "The features selected are the same as in the above model\n"}
{"snippet": "t0 = datetime.now()\noptimized_mnist_priors = generatePriors(optimized_mnist_train_data_separated)\noptimized_mnist_gaussians = generateGaussians(optimized_mnist_train_data_separated)\nprint (\"Training time:\", (datetime.now() - t0))\noptimized_mnist_predictions = predict(np.array(mnist_test_data), optimized_mnist_gaussians, optimized_mnist_priors)\nprint(\"Accuracy of the optimized classifier for MNIST:\", score(optimized_mnist_predictions, mnist_test_label)*100, \"%\" )\n", "intent": "We have removed a lot of points, even though the threshold chosen was really \"soft\".\n"}
{"snippet": "t0 = datetime.now()\nbalanced_mnist_priors = generatePriors(balanced_mnist_train_data_separated)\nbalanced_mnist_gaussians = generateGaussians(balanced_mnist_train_data_separated)\nprint (\"Training time:\", (datetime.now() - t0))\nbalanced_mnist_predictions = predict(np.array(mnist_test_data), balanced_mnist_gaussians, balanced_mnist_priors)\nprint(\"Accuracy of optimized classifier for MNIST:\", score(balanced_mnist_predictions, mnist_test_label)*100, \"%\" )\n", "intent": "With this step, we really removed a lot of elements from the training set, let's see the effects...\n"}
{"snippet": "print(\"actual y[30]: %d\" % y[30])\nprint(\"prediction neigh.predict(X[30]): %d \" % neigh.predict(X[30]))\nprint(\"prediction neigh7.predict(X[30]): %d \" % neigh7.predict(X[30]))\n", "intent": "-mc: we will use the 30th element of training set to test the two classifier trained above:\n"}
{"snippet": "best_team_X = best_team_df.loc[:, ['MPG', 'TRG', 'APG']]\nbest_team_y = best_team_df.loc[:, 'PPS']\nbest_team_df['Predicted season points'] = regr.predict(best_team_X)\nbest_team_df.loc[:, ['MPG', 'TRG', 'APG', 'PPS', 'Predicted season points']]\n", "intent": "An R<sup>2</sup> of 0.82 is pretty good for a first stab a building our regression model. Lets use this model to see how our rock stars perform.\n"}
{"snippet": "predicted_angles = m.predict(x_valid, verbose=1)\nmodel_mae = mean_absolute_error(predicted_angles, y_valid)\nprint('Model Error: {0}'.format(model_mae))\n", "intent": "Now that it's trained, let's see if our model does any better than the median at predicting inc_angles:\n"}
{"snippet": "from sklearn.metrics import r2_score\nscore = r2_score(y_pred_lin_reg_2, memory_arr_mean)\nprint(\"r2 of 2nd degree curve is equal \" + str(round(score, 3)))\nscore = r2_score(y_pred_lin_reg_3, memory_arr_mean)\nprint(\"r2 of 3rd degree curve is equal \" + str(round(score, 3)))\nscore = r2_score(y_pred_moore_law_teoretic, memory_arr_mean)\nprint(\"r2 of Teoretic Moore's Law curve is equal \" + str(round(score, 3)))\nscore = r2_score(y_pred_moore_law_fitted, memory_arr_mean)\nprint(\"r2 of Fitted Moore's Law curve is equal \" + str(round(score, 3)))\n", "intent": "<a id=\"selecting_best_model\"></a>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\npredict_l = gs.predict(test_f)\nprint(classification_report(test_l, predict_l))\n", "intent": "Classification metrics.\n"}
{"snippet": "kf = KFold(gatrain.shape[0], n_folds=10, shuffle=True, random_state=0)\npred = np.zeros((gatrain.shape[0],n_classes))\nfor itrain, itest in kf:\n    train = gatrain.iloc[itrain,:]\n    test = gatrain.iloc[itest,:]\n    ytrain, ytest = y[itrain], y[itest]\n    prob = train.groupby('group').size()/train.shape[0]\n    pred[itest,:] = prob.values\nprint(log_loss(y, pred))\n", "intent": "Best we can do without any features.\n"}
{"snippet": "y_pred = rf.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n", "intent": "Legend of the prediction\n- 0 hybrid\n- 1 sativa\n- 2 Indica\n"}
{"snippet": "score_train = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy',)\nprint(score_train.mean())\n", "intent": "It's backing overfit\n"}
{"snippet": "prediction, bias, contributions = predict(single_tree, X_matrix)\n", "intent": "https://github.com/andosa/treeinterpreter\nThis great package decomposes the tree into a number of components\n"}
{"snippet": "sgd_score = cross_val_score(sgd_grid.best_estimator_, X_test, y_test, cv = 10, scoring = 'recall', n_jobs = -1).mean()\nsgd_score\n", "intent": "***We are clearly over-fitting here!***\n"}
{"snippet": "plot_confusion_matrix(confusion_matrix(y_test, lr_grid.best_estimator_.predict(X_test)), title = 'Logistic Regression', classes = np.array([0,1]))\nplot_confusion_matrix(confusion_matrix(y_test, sgd_grid.best_estimator_.predict(X_test)), title = 'SGD Classifier', classes = np.array([0,1]))\nplot_confusion_matrix(confusion_matrix(y_test, svm_grid.best_estimator_.predict(X_test)), title = 'SVC Classifier', classes = np.array([0,1]))\nplot_confusion_matrix(confusion_matrix(y_test, rf_grid.best_estimator_.predict(X_test)), title = 'Random Forest Classifier', classes = np.array([0,1]))\nplot_confusion_matrix(confusion_matrix(y_test, mlp_grid.best_estimator_.predict(X_test)), title = 'Multi-Layer Perceptron', classes = np.array([0,1]))\n", "intent": "[Table of Contents](\n"}
{"snippet": "print(mean_absolute_error(true_regression_line,model1))\nprint(mean_absolute_error(true_regression_line,model2))\n", "intent": "Let us see what the MAE looks like\n"}
{"snippet": "print(mean_absolute_error(true_regression_line,model1*.5+model2*.5))\n", "intent": "Now let us look at the straight average\n"}
{"snippet": "print('Model 1:',mean_absolute_error(true_regression_line,model1))\nprint('Model 2:', mean_absolute_error(true_regression_line,model2))\nprint('Average:',mean_absolute_error(true_regression_line,model1*.5+model2*.5))\nprint('MCMC:',mean_absolute_error(true_regression_line,intercept+x1param*model1+x2param*model2))\n", "intent": "Now is the time to see how well we have done!\n"}
{"snippet": "predictions_count = model_count_NB.predict(X_test_count)\npredictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)\n", "intent": "***\nOnce we have our model trained, we can predict the test values, and then compare them to the real values. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_count = accuracy_score(y_test_count, predictions_count)\naccuracy_tfidf = accuracy_score(y_test_tfidf, predictions_tfidf)\nprint('Count Vectorized Words Accuracy:', accuracy_count)\nprint('TfIdf Vectorized Words Accuracy:', accuracy_tfidf)\n", "intent": "***\nNext step is validate the model. We will import the `accuracy_score` function of sklearn, to see how is our accuracy.\n"}
{"snippet": "training_metrics = linear_classifier.evaluate(input_fn=predict_training_input_fn)\nvalidation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\nprint(\"AUC on the training set: %0.2f\" % training_metrics['auc'])\nprint(\"AUC on the validation set: %0.2f\" % validation_metrics['auc'])\n", "intent": "Next, let's calculate the [AUC (area under the curve)](https://developers.google.com/machine-learning/glossary\n"}
{"snippet": "rmse_val = []\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nrmse_val.append(rmse)\nprint(rmse)\n", "intent": "Root Mean square to check the performance of the model\n"}
{"snippet": "mse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nrmse_val.append(rmse)\nprint(rmse)\n", "intent": "Root Mean square to check the performance of the model\n"}
{"snippet": "non_idx = np.where(y_test[:,0] == 0.)[0]\nyes_idx = np.where(y_test[:,0] == 1.)[0]\ny_hat = model.predict(x_test)[:,0]\n", "intent": "We then use our trained net to classify the test set.\n"}
{"snippet": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "We add XGBoost and LightGBM to the StackedRegressor defined previously.\nWe first define a rmsle evaluation function\n"}
{"snippet": "randpred = randfor.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(\"predicted:\", multinomialNBmodel.predict(tfidf4)[0])\nprint(\"expected:\", messages['label'][3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "all_prediction = multinomialNBmodel.predict(messages_tfidf)\nprint(all_prediction)\n", "intent": "Now we want to determine how well our model will do on the entire dataset. Lets get all the predictions!\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"null.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "predictions = []\ntic = time.time()\nkNN(X_train, y_train, X_test, predictions, 10)\ntoc = time.time()\nprint(toc-tic)\npredictions = np.asarray(predictions)\naccuracy = accuracy_score(y_test, predictions)\nprint('\\nThe accuracy of our classifier is %d%%' % float(accuracy*100))\n", "intent": "* Run test manually modifing the test samples\n* Evaluate the results\n"}
{"snippet": "from sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\nmetrics.adjusted_rand_score(labels_true, labels_pred)  \n", "intent": "* measures similarity of labels_true & labels_pred\n"}
{"snippet": "from sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 2, 2]\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred) \n", "intent": "* measures agreement between labels_true & labels_pred\n* two normalized versions (NMI, AMI) available\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\nprint(accuracy_score(\n        y_true, y_pred))\nprint(accuracy_score(\n        y_true, y_pred, normalize=False))\n", "intent": "* either the fraction or count of correct predictions\n[demo](plot_permutation_test_for_classification.ipynb)\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(\n        y_true, y_pred, target_names=target_names))\n", "intent": "* returns text report with main metrics\n"}
{"snippet": "from sklearn.metrics import hamming_loss\ny_pred = [1, 2, 3, 4]\ny_true = [2, 2, 3, 4]\nprint(hamming_loss(\n        y_true, y_pred))\nprint(hamming_loss(\n        np.array([[0, 1], [1, 1]]), \n        np.zeros((2, 2))))\n", "intent": "* Finds hamming distance between two sets of samples\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import label_ranking_average_precision_score\ny_true = np.array([[1, 0, 0], [0, 0, 1]])\ny_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\nlabel_ranking_average_precision_score(y_true, y_score) \n", "intent": "* returns average over each ground truth label assigned to each sample\n"}
{"snippet": "from sklearn.metrics import median_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmedian_absolute_error(y_true, y_pred)\n", "intent": "* Robust to outliers\n* No multioutput support\n"}
{"snippet": "prices = classifier.predict(krenk_df)\ni = 1;\nfor x in np.nditer(prices):\n    print(\"Price for diamond number %2d is %6.0f.\"%  (i, x))\n    i += 1\n", "intent": "Now we train our model, I show diamond estimated prices for each one without decimals.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "We should get an accuraccy of over 90%.\n"}
{"snippet": "def discriminator_on_generator_loss(y_true,y_pred):\n    return K.mean(K.binary_crossentropy(y_pred,y_true), axis=(1,2,3))\n", "intent": "<h1> Loss for Discriminator\n<h2> (Binary cross entropy)\n"}
{"snippet": "def generator_l1_loss(y_true,y_pred):\n    return K.mean(K.abs(y_pred - y_true),axis=(1,2,3))\n", "intent": "<h1> Loss for generator \n<h2> Simply a l1 loss\n"}
{"snippet": "y_pred = model.predict(x_test)\ny_pred = y_pred.argmax(axis=1)\n", "intent": "<i>Lastly, i will show the misclassified images.</i>\n"}
{"snippet": "results = model.evaluate(x_test,y_test)\nresults\n", "intent": "Time to evaluate our model on Test data!\n"}
{"snippet": "y_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('accuracy: {:.3f}%'.format(accuracy*100))\n", "intent": "**Question 3.c**   \nUse the same dataset from 2.b using the linear kernel to find test set prediction accuracy\n"}
{"snippet": "y_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('accuracy: {:.3f}%'.format(accuracy*100))\n", "intent": "**Question 3.e**   \nUse the same dataset from 2.b using the RBF kernel  to find test set prediction accuracy\n"}
{"snippet": "scores = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Now we will test the accuracy of our trained model on the 10000 test images.\n"}
{"snippet": "print(metrics.classification_report(label, predictions))\nprint(metrics.confusion_matrix(label, predictions))\n", "intent": "A top 10 of the words that most influence each cluster was showed.\n"}
{"snippet": "model.evaluate(x=x_test, y=y_test, batch_size=None, verbose=1, sample_weight=None, steps=None)\n", "intent": "** 3-1. Evaluate model on test data using model.evaluate. Print the model accuracy on test set. **\n"}
{"snippet": "elm1.evaluate(x_test, y_test, batch_size=500) \n", "intent": "Now that the network has been trained, we can evaluate the performance on the test set via evaluate method. \n"}
{"snippet": "pred = elm1.predict(x_test, batch_size=500)\n", "intent": "To return a numpy array with actual predictions it exist a prediction method:\n"}
{"snippet": "iterator = data.make_one_shot_iterator()\nelm1.evaluate(tf_iterator = iterator,  batch_size=1024)\n", "intent": "As the iterator was made as one shot only it should be re-created: \n"}
{"snippet": "mlelm1.evaluate(x_test, y_test, batch_size=500) \n", "intent": "Now that the network has been trained, we can evaluate the performance on the test set via evaluate method. \n"}
{"snippet": "pred = mlelm1.predict(x_test, batch_size=500)\n", "intent": "To return a numpy array with actual predictions it exist a prediction method:\n"}
{"snippet": "iterator_init_op()\nmlelm1.evaluate(tf_iterator = iterator,  batch_size=1024)\n", "intent": "As the iterator was made initializable it should be re-initialized: \n"}
{"snippet": "y_pred=clf.predict(X_test)\ny_pred\n", "intent": "<h2>Use test sample to generate predictions</h2>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(res_logreg_clf, X_test, Y_test, cv=5, scoring='roc_auc').mean()\n", "intent": "<b>(c) Use 5-fold cross-validation to estimate test AUC. What test AUC do you obtain? Provide a 68% and 95% confidence interval for test AUC.</b>\n"}
{"snippet": "X_test = np.array(test_df[predictors])\nY_test = np.array(test_df['Loan_Status'])\nY_test[Y_test == 'Fully Paid'] = 1\nY_test[Y_test == 'Charged Off'] = 0\nY_test = Y_test.astype('int')\ncross_val_score(rf_res, X_test, Y_test, cv=5, scoring='roc_auc').mean()\n", "intent": "<b>(c) What test AUC do you obtain using 5-fold cross-validation?</b>\n"}
{"snippet": "cross_val_score(abc_res, X_test, Y_test, cv=5, scoring='roc_auc').mean()\n", "intent": "<b>(c) What test AUC do you obtain using 5-fold cross-validation?</b>\n"}
{"snippet": "activations = activation_model.predict(img_tensor)\n", "intent": "Get the output of input tensor\n"}
{"snippet": "predictedPrice = np.exp(estimator.predict(testData[predictors]))\npredictedPrice\n", "intent": "As earlier we had scaled our SalePrice so we need to revert. We will use np.exp.\n"}
{"snippet": "cost = tf.reduce_mean( tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels, NUM_SAMPLED, vocab_size))\n", "intent": "- Compute the average NCE loss for the batch.\n- tf.nce_loss automatically draws a new sample of the negative labels each time we evaluate the loss.\n"}
{"snippet": "X_new = np.array([[0.5, 0.4], [1.1, 4.0]])\nclf.predict(X_new)\n", "intent": "Now we can predict the class label of two new samples:\n"}
{"snippet": "predicton_logit=np.argmax(prd, axis=1)\nground_truth=np.argmax(np.array(y_val), axis=1)\naccuracy = accuracy_score(ground_truth, predicton_logit)\nprint(accuracy)\n", "intent": "Okay now lets calculate accuracy of our model.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "MSE = mean_squared_error(test.cnt,pred)\nprint(MSE)\n", "intent": "And the mean squared error is.....\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "When the training ends, we can evaluate the generalization performance of the model as follows:\n"}
{"snippet": "all_mses=[]\nstep=1;\nfor validation_image in validation_generator:   \n    if step>nb_validation_samples:\n        break;\n    print(step, sep=' ', end='>', flush=True)       \n    predicted_image = autoencoder.predict(validation_image)\n    mse_value= mse(predicted_image[0], validation_image[0])\n    all_mses.append(mse_value)\n    step=step+1\n", "intent": "Example of using mse\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50Model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "transfer_learning_predictions = [np.argmax(transfer_learning_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(transfer_learning_predictions)==np.argmax(test_targets, axis=1))/len(transfer_learning_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(accuracy_score(y_test, pred))\ncross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n", "intent": "Let's check the accuracies.\n"}
{"snippet": "cross_val_score(rfc, Xc, yc, cv=5)\n", "intent": "The model is more accurate when predicting whether or not a cat is adopted than distinguishing from the different outcome types.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(rfc, X, Y, cv=5)\n", "intent": "Looks like there is an issue with overfitting here.  The training score is much higher than the test set.  \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        loss += style_weights[i] * torch.sum((gram_matrix(feats[style_layers[i]]) - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "train_error = np.mean(y != clf.predict(X))\ntest_error = np.mean(y_test != clf.predict(X_test))\nprint(\"train/test error (for optimal k): %g/%g\" % (train_error, test_error))\n", "intent": "Let's see how this does on the test set.\n"}
{"snippet": "preds = clf.predict_proba(X_test)[:,1]\nclipped_preds = np.clip(preds, 0.05, 0.95)\ndf_sample_sub.Pred = clipped_preds\ndf_sample_sub.head()\ndf_sample_sub.shape\n", "intent": "Create predictions using the logistic regression model we trained.\n"}
{"snippet": "preds = cp_model.evaluate(x_test, y_test, verbose=1)\n", "intent": "Evaluate the model on test set\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only five iterations) performs on the test set.\n"}
{"snippet": "col = range(100)  \ny_predict_b = np.argmax(model_b.predict(x_data_b[col]), axis=1)\n", "intent": "Select 10 random columns denoted by `col`, and make inference using the loaded model:\n"}
{"snippet": "y_pred_iris_probs = gnarl_iris.predict(X_test_iris, truncate_labels=False)\nprint(y_pred_iris_probs[:5])\nprint(y_pred_iris_probs.shape)\n", "intent": "While, if we instead don't truncate the predictions, we get:\n"}
{"snippet": "from keras import backend as K\ndef mean_squared_percentage_error(y_true, y_pred):\n    return K.mean(K.square((y_true - y_pred) / K.clip(K.abs(y_true),\n                                            K.epsilon(),\n                                            None)), axis=-1)\nmspe = MSPE = mean_squared_percentage_error\n", "intent": "Replaced the Loss function that we are optimising on with the actual one being used in the competition to see how well our models rank\n"}
{"snippet": "bottleneck_predictions = [np.argmax(bottleneck_model.predict(np.expand_dims(feature, axis=0))) \\\n                          for feature in test_bottleneck]\ntest_accuracy = 100*np.sum(np.array(bottleneck_predictions) == \\\n                           np.argmax(test_targets, axis=1))/len(bottleneck_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def predict(sess, x_test, y_test, accuracy):\n    feed = {'tf_x:0': x_test,\n            \"tf_y:0\": y_test,\n            'fc_keep_prob:0': 1.0}\n    return sess.run(accuracy, feed_dict=feed)\n", "intent": "Let's test our trained model with our test data.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(m, X, y, scoring='accuracy', cv=5, n_jobs=-1)\n", "intent": "The model's accuracy is nearly perfect:\n"}
{"snippet": "predictions = model.predict(test_images)\npredictions[0]\n", "intent": "With the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "R2 = r2_score(df[\"selectivity\"].values[idx], \n              regr.predict(df[\"selectivity_simulated\"].values[idx].reshape(sum(idx), 1)))\nR2\n", "intent": "Compute $R^2$ value of linear regression; first argument is truth, second is prediction.\n"}
{"snippet": "prediction = logrm.predict(X_test)\n", "intent": "** Predict values for the testing data.**\n"}
{"snippet": "print(metrics.classification_report(y_test, prediction))\n", "intent": "** Classification report for the model.**\n"}
{"snippet": "pattern=pattern_model.predict(data_images)\nfabric=fabric_model.predict(data_images)\ncolor=color_model.predict(data_images)\nstyle=style_model.predict(data_images)\npart=part_model.predict(data_images)\nprint (\"Pattern : \" , pattern.shape)\nprint (\"fabric : \" , fabric.shape)\nprint (\"color : \" , color.shape)\nprint (\"Style : \" , style.shape) \nprint (\"Part : \" , part.shape)\n", "intent": "Predicting the category features for images in repository\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_train, pred2_tr['pred_class']))\n", "intent": "* As we can see our algorithm returned an accuracy of 84.6% on the training data\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_seed, shuffle=True) \n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf) \n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "test_preds =  best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1] \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "EXAMPLES = ['3rd may 1996', '3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "predictions = model.predict(x_test)\n", "intent": "Can verify that the `predict` method of created model instance returns probability distribution over all 46 topics:\n"}
{"snippet": "print('Accuracy score: %f' % accuracy_score(y_test, pred))\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\n", "intent": "TODO: Interpretation of accuracy score, class report, confusion matrix \n"}
{"snippet": "def skill_score(fcst, ref, perfect):\n    return (fcst - ref)/(perfect - ref)\n", "intent": "__**References:**__\n- [Forecast Skill](http://glossary.ametsoc.org/wiki/Skill)\n"}
{"snippet": "print(metrics.classification_report(y_test, y_pred))\n", "intent": "Based on the confusion matrix above, the model resulted in 218 correct predictions and 49 incorrect predictions on the test set.\n"}
{"snippet": "def sensitivity(m, x_in, variable):\n    inp = x_in.copy()\n    inp[:, variable] = x_in[variable].mean()\n    y_std = m.predict(x_in)\n    y_var = m.predict(inp)\n    return np.mean(np.square(np.asarray(y_std - y_var)))\n", "intent": "We fix a variable to its mean and check how much the results vary.\n"}
{"snippet": "print 'Area under the ROC curve',roc_auc_score(y_test,y_pred_prob)\n", "intent": "Finally get the area under the ROC curve to get the idea of predicting the unseen data which is also pretty good in this case.\n"}
{"snippet": "print classification_report(y_test,knn_pred)\n", "intent": "On the basis of classification report we can say that on average both the models have same performance for this dataset.\n"}
{"snippet": "y_pred = nn.predict(X_test)\nplot_samples(X_test, y_pred)\n", "intent": "Predict and visualize predictions\n"}
{"snippet": "start = time.time()\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nend = time.time()\nprint('runtime: %d min %d sec' % ((end-start)//60, (end-start)%60))\nprint('len(test_tensors) = %d' % len(test_tensors))\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "predictions_VGG16 = [np.argmax(model_VGG16.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(predictions_VGG16)==np.argmax(test_targets, axis=1))/len(predictions_VGG16)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "start = time.time()\nmodel_InceptionV3.load_weights('saved_models/weights.best.InceptionV3.hdf5')\npdf_per_example1 = model_InceptionV3.predict(test_InceptionV3)\npred1 = [np.argmax(pdf) for pdf in pdf_per_example1]\ntest_accuracy = 100*np.sum(np.array(pred1)==np.argmax(test_targets, axis=1))/len(pred1)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nend = time.time()\nprint('runtime: %d min %d sec' % ((end-start)//60, (end-start)%60))\nprint('len(test_targets) = %d' % len(test_targets))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf.predict([X[2]])\n", "intent": "Let's try to predict what it is X[2]\n"}
{"snippet": "np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)\n", "intent": "Let's calculate Mean Squared Error\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted, target_names = twenty_test.target_names))\n", "intent": "Question 3:  How well did your model perform?\n"}
{"snippet": "mean_squared_error(diabetes_y_test, diabetes_y_pred)\n", "intent": "> Mean squared error\n"}
{"snippet": "results_rf = model_selection.cross_val_predict(rf,predictors, targets, cv=kfold)\nprint(classification_report(targets,results_rf))\n", "intent": "Classification report : \n"}
{"snippet": "print(classification_report(test_target,test_prediction))\n", "intent": "__Classification model using given training dataset__\n"}
{"snippet": "def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors / test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    return accuracy\n", "intent": "Create a function to evaluate model improvement\n"}
{"snippet": "y_prob = rf1.predict_proba(scaled_val_X)\nfor i in range(len(scaled_val_X)):\n    print(\"Predicted=%s\" % (y_prob[i]))\n", "intent": "Print the probability of each match being classified as a loss [0] or a win [1].\n"}
{"snippet": "x_test = np.array(['feeling not good'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "len(np.where(y_test != Yelp_tfidf_rf_fit.predict(X_test))[0]) / len(y_test)\n", "intent": "Let's compute some basic scores.\n"}
{"snippet": "Y_pred = model.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n", "intent": "Make predictions using model\n"}
{"snippet": "print clf.predict(test_data)\n", "intent": "Using the ML model to predict the labels, based on the testing features:\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The testing accuracy is', test_accuracy) \n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "KNN.predict([3,5,4,2])\n", "intent": "- Now that we have trained our model, we are ready to pass it a new flower to classify.\n"}
{"snippet": "KNN.predict([[3,5,4,2],[5,4,3,2]])\n", "intent": "- Now, we can make predictions for multiple observations\n"}
{"snippet": "y_predict = logreg.predict(X)\n", "intent": "- This is obviously inconsistent with what KNN predicted, since logreg is a different model. How can we decide which model is more accurate?\n"}
{"snippet": "final_model = search.best_estimator_ \ntrain_rmse = np.mean(np.sqrt(mean_squared_error(df_y, model4.predict(df_x))))\ncv_rmse = np.mean(np.sqrt(-cross_val_score(model4, df_x, df_y, cv=10, scoring='neg_mean_squared_error')))\nprint('Train Error: {}'.format(train_rmse))\nprint('CV Error:    {}'.format(cv_rmse))\n", "intent": "Now with these parameters, let's fit our model and look at what variables our model determined were important.\n"}
{"snippet": "y_train_predict = knn.predict(x_train)\ny_test_predict = knn.predict(x_test)\nerr_train = numpy.mean(y_train != y_train_predict)\nerr_test  = numpy.mean(y_test  != y_test_predict)\nprint('errors: ',err_train, err_test)\n", "intent": "Get standard errors for train and test sets\n"}
{"snippet": "err_ada_train = []\nfor y_train_pred in ada_model.staged_predict(x_train):\n    err_ada_train.append(np.mean(y_train_pred != y_train))\nerr_ada_test = []\nfor y_test_pred in ada_model.staged_predict(x_test):\n    err_ada_test.append(np.mean(y_test_pred != y_test))\n", "intent": "Find a minimal error on test subset\n"}
{"snippet": "err_gbt_train = []\nfor y_train_pred in gbt_model.staged_predict(x_train):\n    err_gbt_train.append(np.mean(y_train_pred != y_train))\nerr_gbt_test = []\nfor y_test_pred in gbt_model.staged_predict(x_test):\n    err_gbt_test.append(np.mean(y_test_pred != y_test))\n", "intent": "Find a errors and optimal count of estimators.\n"}
{"snippet": "y_train_pred = mlp_model.predict(x_train)\ny_test_pred = mlp_model.predict(x_test)\nprint(numpy.mean(y_train != y_train_pred), numpy.mean(y_test != y_test_pred))\n", "intent": "Calculate an errors between predict and real values for both 'train' and 'test' subsets.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "Other metrics of quality:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i,l in enumerate(style_layers):\n        gram_cur = gram_matrix(feats[l])\n        gram_src = style_targets[i]\n        style_loss += style_weights[i] * tf.reduce_sum((gram_cur - gram_src)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn import metrics\nprediction_train = model.predict(X_train)\nprint metrics.accuracy_score(y_train, prediction_train)\n", "intent": "**Run on training data**\n"}
{"snippet": "prediction = model.predict(X_test)\nprint metrics.accuracy_score(y_test, prediction)\n", "intent": "**Run on test data**\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n    return f1_score(target.values, y_pred, average=\"weighted\")\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "We have to declare the utility functions to help us train/test the models.\n"}
{"snippet": "score = model.evaluate(X_test,Y_test,verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy (NORMALIZED):', score[1])\n", "intent": "Finally, let's evaluate on our test set:\n"}
{"snippet": "evaluate_classification_error(my_decision_tree, test_data, target)\n", "intent": "Now, let's use this function to evaluate the classification error on the test set.\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_new, validation_set, target)\n", "intent": "Now, let's use this function to evaluate the classification error of `my_decision_tree_new` on the **validation_set**.\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_old, validation_set, target)\n", "intent": "Now, evaluate the validation error using `my_decision_tree_old`.\n"}
{"snippet": "print(\"Training data, classification error (model 1):\", evaluate_classification_error(model_1, train_data, target))\nprint(\"Training data, classification error (model 2):\", evaluate_classification_error(model_2, train_data, target))\nprint(\"Training data, classification error (model 3):\", evaluate_classification_error(model_3, train_data, target))\n", "intent": "Let us evaluate the models on the **train** and **validation** data. Let us start by evaluating the classification error on the training data:\n"}
{"snippet": "print(\"Training data, classification error (model 1):\", evaluate_classification_error(model_1, validation_set, target))\nprint(\"Training data, classification error (model 2):\", evaluate_classification_error(model_2, validation_set, target))\nprint(\"Training data, classification error (model 3):\", evaluate_classification_error(model_3, validation_set, target))\n", "intent": "Now evaluate the classification error on the validation data.\n"}
{"snippet": "print(\"Validation data, classification error (model 4):\", evaluate_classification_error(model_4, validation_set, target))\nprint(\"Validation data, classification error (model 5):\", evaluate_classification_error(model_5, validation_set, target))\nprint(\"Validation data, classification error (model 6):\", evaluate_classification_error(model_6, validation_set, target))\n", "intent": "Calculate the accuracy of each model (**model_4**, **model_5**, or **model_6**) on the validation set. \n"}
{"snippet": "print(\"Validation data, classification error (model 4):\", evaluate_classification_error(model_7, validation_set, target))\nprint(\"Validation data, classification error (model 5):\", evaluate_classification_error(model_8, validation_set, target))\nprint(\"Validation data, classification error (model 6):\", evaluate_classification_error(model_9, validation_set, target))\n", "intent": "Now, let us evaluate the models (**model_7**, **model_8**, or **model_9**) on the **validation_set**.\n"}
{"snippet": "def evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "Following function is used for evaluation of the model.\n"}
{"snippet": "print(confusion_matrix(y_test, prediction,))    \nprint('')\ntarget_names = ['below_med', 'above_med']\nprint(classification_report(y_test, prediction, target_names = target_names))\n", "intent": "<span style=\"color:red\">\n--- comment ---\n<span style=\"color:\n"}
{"snippet": "loss_and_metrics = model.evaluate(X_test,y_test)\nprint( \"\\n\\nvalue of loss fucntion: {}\".format(loss_and_metrics[0]) )\nprint( \"value of metric (accuracy): {}\".format(loss_and_metrics[1]) )\n", "intent": "Here we use evaluate method to compute the loss function and metric.\n"}
{"snippet": "y_pred_test = model.predict(X_test)\nprint(precision_score(y_test, y_pred_test))\nprint(recall_score(y_test, y_pred_test))\nprint(f1_score(y_test, y_pred_test))\n", "intent": "Check the model peformance.\n"}
{"snippet": "pred_test = model.predict(X_test)\nprint(precision_score(y_test, pred_test))\nprint(recall_score(y_test, pred_test))\nprint(f1_score(y_test, pred_test))\n", "intent": "Check the model performance.\n"}
{"snippet": "def predict(trace, x1, x2, threshold):\n    alpha = trace.get_values('alpha').mean()\n    betas = np.mean(trace.get_values('beta'), axis=0)\n    linear = alpha + (x1 * betas[0]) + (x2 * betas[1])\n    probability = 1 / (1 + np.exp(-linear))\n    return [np.where(probability >= threshold, 1, 0), linear, probability]\ndef accuracy(predictions, actual):\n    return np.sum(predictions == actual) / len(predictions)\n", "intent": "Now let's see how well they perform:\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx in range(len(style_layers)):\n        gram_current = gram_matrix(feats[style_layers[idx]])\n        gram_target = style_targets[idx]\n        gram_diff = gram_current - gram_target\n        layer_style_loss = style_weights[idx] * torch.sum(gram_diff ** 2)\n        style_loss += layer_style_loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    content_image = 'styles/tubingen.jpg'\n    image_size =  192\n    tv_weight = 2e-2\n    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Error is {:.5f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.)\n    for idx in range(len(style_layers)):\n        gram_current = gram_matrix(feats[style_layers[idx]])\n        gram_target = style_targets[idx]\n        gram_diff = gram_current - gram_target\n        layer_style_loss = style_weights[idx] * tf.reduce_sum(gram_diff ** 2)\n        style_loss += layer_style_loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "knn.predict(iris_X_test)\n", "intent": "Then down to business. The prediction is\n"}
{"snippet": "knn.predict(iris_X_test) == iris_y_test\n", "intent": "Which ones did the Knn classifier get right?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nlinear_model_mse = mean_squared_error(y_predict,y_test) \nlinear_model_mse\n", "intent": "r-square value of testing data..not that good as training data.now calculate means square error\n"}
{"snippet": "mean_absolute_error(y_test, y_predicted)\n", "intent": "The average mean squared error is about 181 cars^2.\n"}
{"snippet": "prediction = improved_cnn.predict(X_test, verbose=1)\n", "intent": "The improved model has validation accuracy of 83.80%. This is becuase of more convolution layers.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i, s in enumerate(style_layers):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(feats[s]) - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))  \n", "intent": "Finally, the classification report shows the type 1 and type 2 error rates (well, 1 - the error) for each facies, along with the combined, F1, score:\n"}
{"snippet": "y_grid = gp.predict(X_grid).reshape(grid_x.shape)\n", "intent": "Now we can make a prediction:\n"}
{"snippet": "print(np.mean(bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "mean_squared_error(y_test, clf.predict(X_test))\n", "intent": "Measuring the performance using MSE\n"}
{"snippet": "r2_score(y_test, clf.predict(X_test))\n", "intent": "Measuring the performance sing r2\n"}
{"snippet": "def classification_metrics(Y_pred, Y_true):\n    accuracy = metrics.accuracy_score(Y_true, Y_pred)\n    auc = 0\n    precision = 0\n    recall = 0\n    f1 = 0\n    return accuracy, auc, precision, recall, f1\n", "intent": "Again, I start with some helper functions. I use cross validation to determine which model to eventually go with and for hyperparameter tuning.\n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "This function takes in a regression model, training data, the survival scores, and passes it into cross_val_score to guage out accuracy.\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(f, axis=0))) for f in test_ResNet]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Qmat = model.predict(obs)\nprint(Qmat)\n", "intent": "2\\. Predict the $Q$ values from `model.predict(obs)`.\n"}
{"snippet": "discount_rate = 0.05\ngamma = 1. / (1. + discount_rate)\ntarget = prediction\ntarget_obj = reward \nif not done:\n    next_q = np.amax(model.predict(next_state))\n    target_obj += gamma * next_q\ntarget[0][action] = target_obj\nprint(target)\n", "intent": "2\\. Compute `target` as R + $\\gamma$ max(`model.predict(S')`)\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc_train = accuracy_score(data_train['input']['y'], preds_train['y_pred'])\nprint('Training accuracy = {:.4f}'.format(acc_train))\n", "intent": "Let's see how well we do on our training data:\n"}
{"snippet": "acc_test = accuracy_score(data_test['input']['y'], preds_test['y_pred'])\nprint('Test accuracy = {:.4f}'.format(acc_test))\n", "intent": "How good is our test score?\n"}
{"snippet": "acc_test = accuracy_score(data_test['input']['y'], preds_test['y_pred'])\nprint('Test accuracy = {:.4f}'.format(acc_test))\n", "intent": "What is our test score?\n"}
{"snippet": "def eval_pred(title, y_true, y_pred):\n    print(title)\n    print(\"  Log-loss: \", log_loss(y_true=y_true, y_pred=y_pred))\n    choices = np.argmax(y_pred, axis=1)\n    print(\"  Accuracy: {:.2%}\".format(np.sum(choices == y_true) / len(y_true)))\neval_pred(\"Results on training\", y_true=y_train, y_pred=result['output'])\neval_pred(\"Results on validation\", y_true=y_valid, y_pred=result['output_valid'])\n", "intent": "A short function below summarizes the results.\n"}
{"snippet": "optimal_thresh = thresholds[precisions[:-1]==recalls[:-1]]\ny_train_pred = y_scores>optimal_thresh\nprint('Precision:',precision_score(y_train_0,y_train_pred)) \nprint('Recall:', recall_score(y_train_0,y_train_pred))\nprint('F1 Score:', f1_score(y_train_0,y_train_pred))\n", "intent": "From this analysis the optimal threshold for equal recall and precision is given by:`\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nms_error = mean_squared_error(test_labels,prediction)\nrmse = np.sqrt(ms_error)\nprint('RMSE from the OLS:',rmse)\n", "intent": "Check the RMSE between the predicted values and the actual labels using the linear model\n"}
{"snippet": "predictions=classifier.predict(pred_test)\nsklearn.metrics.confusion_matrix(tar_test,predictions)\n", "intent": "Run the prediction on the test data and check how well it performs.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ntrain_error = mean_squared_error(tar_train, model.predict(pred_train))\ntest_error = mean_squared_error(tar_test, model.predict(pred_test))\nprint ('training data MSE')\nprint(train_error)\nprint ('test data MSE')\nprint(test_error)\n", "intent": "MSE from training and test data\n"}
{"snippet": "sklearn.metrics.accuracy_score(tar_test, predictions)\n", "intent": "Check the accuracy score\n"}
{"snippet": "clf.predict(X_new_review)[0]\n", "intent": "The actual score of this review was 10 stars\n"}
{"snippet": "from confusion_matrix import evaluate\nfrom data import get_evaluate\nvencedor = clf\nevaluate(X_train, y_train, X_test, y_test, vencedor, ['tristeza', 'raiva', 'felicidade'])\n", "intent": "Building confusion matrix from classifier and evalueate the predict\n"}
{"snippet": "pred_train=clf.predict(X)\nacc=accuracy_score(y, pred_train)\nprint(\"The accuracy on training data is :  \", acc)\n", "intent": "<img src=\"nb8.png\">\n"}
{"snippet": "def predict(model,xtest):\n    predicted = model.predict(xtest)\n    return predicted\n", "intent": "Define prediction function\n"}
{"snippet": "print('Confusion Matrix:')\nprint(confusion_matrix(y_test,log_predictions))\nprint('\\n')\nprint('Classification Report:')\nprint(classification_report(y_test,log_predictions))\n", "intent": "**Logistic Model results:**\n"}
{"snippet": "print('Confusion Matrix:')\nprint(confusion_matrix(y_test,dtree_predictions))\nprint('\\n')\nprint('Classification Report:')\nprint(classification_report(y_test,dtree_predictions))\n", "intent": "**Decision Tree Model results:**\n"}
{"snippet": "print('Confusion Matrix:')\nprint(confusion_matrix(y_test,rfc_predictions))\nprint('\\n')\nprint('Classification Report:')\nprint(classification_report(y_test,rfc_predictions))\n", "intent": "**Random Forest Model results:**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg, features_array, target, cv=6)\nscores\n", "intent": "79% accuracy with the logistic regression.\nLet's use a cross validation method. We start with a 6 cross-validation model, and compare the accuracy.\n"}
{"snippet": "y_pred = clf.predict(X_train)\n", "intent": "Making prediction with Scikit-learn is easy.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_train, y_pred)\n", "intent": "As well as scoring our model.\n"}
{"snippet": "regressor = xgb.XGBRegressor(**params) \nscores = cross_val_score(regressor, X[:5], y[:5], cv=3, scoring = loss)\n", "intent": "The 'OR' section below is for max depth parameter selection using GridSearchCV\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    losses = 0\n    N = len(style_layers)\n    for n in range(N):\n        gram = gram_matrix(feats[style_layers[n]])\n        gram_target = style_targets[n]\n        loss = (gram - gram_target) ** 2\n        losses += tf.reduce_sum(loss) * style_weights[n]\n    return losses \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred_class = nb.predict(X_test_dtm)\n", "intent": "Make class predictions\n"}
{"snippet": "accuracy_score = metrics.accuracy_score(y_test, y_pred_class)\nprint('accuracy_score score: {0:0.2f}'.format(accuracy_score))\n", "intent": "Calculate accuracy of class predictions\n"}
{"snippet": "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob\n", "intent": "Calculate predicted probabilities \n"}
{"snippet": "y_pred_class = logreg.predict(X_test_dtm)\n", "intent": "Make class predictions \n"}
{"snippet": "accuracy_score = metrics.accuracy_score(y_test, y_pred_class)\nprint('accuracy_score score: {0:0.2f}'.format(\n      accuracy_score))\n", "intent": "Calculate accuracy of class predictions\n"}
{"snippet": "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob\n", "intent": "Calculate predicted probabilities\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    return sum(w * torch.sum((gram_matrix(feats[l], True) - t) ** 2) for l, t, w in zip(style_layers, style_targets, style_weights))\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_test_pred = ocSVM.predict(X_test)\n", "intent": "After the fitting is done proceed to classify the remaining test samples and \n"}
{"snippet": "input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'images': mnist.test.images}, y=mnist.test.labels,\n    batch_size=batch_size, shuffle=False)\ne = model.evaluate(input_fn)\nprint(\"Testing Accuracy:\", e['accuracy'])\n", "intent": "To evaluate the model, you need to specify the test input and run `.evaluate` instead of `.train`. Then, the mode will be set to `PREDICTION`.\n"}
{"snippet": "all_predict = picks.predict()\n", "intent": "To predict picks for all of the frames, we call predict without an argument.\n"}
{"snippet": "preds = model.predict(x)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n", "intent": "You can now run the pretrained network on the image and decode its prediction vec- tor back to a human-readable format:\n"}
{"snippet": "from sklearn.metrics import precision_score,recall_score,accuracy_score\ndt_accuracy= accuracy_score(Y, dt_predicted)\ndt_precision = precision_score( Y, dt_predicted)\ndt_recall = recall_score( Y, dt_predicted)\nprint( \"Accuracy = \" , dt_accuracy ,\" Precision = \", dt_precision, \", Recall =\" ,dt_recall)\n", "intent": "1.b.i) DECISION TREE - ACCURACY , PRECISION AND RECALL\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ndt_cv_accuracy = cross_val_score(dt_classifier, X, Y,cv=10,scoring=\"accuracy\")\ndt_cv_precision = cross_val_score(dt_classifier, X, Y,cv=10,scoring=\"precision\")\ndt_cv_recall = cross_val_score(dt_classifier, X, Y,cv=10,scoring=\"recall\")\n", "intent": "1.c. i)CROSS VALIDATION\n"}
{"snippet": "nb_cv_accuracy = cross_val_score(nb_classifier, X, Y,cv=10,scoring=\"accuracy\")\nnb_cv_precision = cross_val_score(nb_classifier, X, Y,cv=10,scoring=\"precision\")\nnb_cv_recall = cross_val_score(nb_classifier, X, Y,cv=10,scoring=\"recall\")\nprint(\"CrossValidated Accuracy_NB :\", nb_cv_accuracy.mean())\nprint(\"CrossValidated Precision_NB: \" ,nb_cv_precision.mean())\nprint(\"CrossValidated Recall_NB: \",  nb_cv_recall.mean())\n", "intent": "2.a.i.\tWhat is the 10-fold cross-validation accuracy, precision, and recall for this method?\n"}
{"snippet": "svc_cv_accuracy = cross_val_score(linearclassifier_SVC, X, Y,cv=10,scoring=\"accuracy\")\nsvc_cv_precision = cross_val_score(linearclassifier_SVC, X, Y,cv=10,scoring=\"precision\")\nsvc_cv_recall = cross_val_score(linearclassifier_SVC, X, Y,cv=10,scoring=\"recall\")\nprint(\"CrossValidated Accuracy_SVC :\", svc_cv_accuracy.mean())\nprint(\"CrossValidated Precision_SVC : \" ,svc_cv_precision.mean())\nprint(\"CrossValidated Recall_SVC : \",  svc_cv_recall.mean())\n", "intent": "2.b.i. What is the 10-fold cross-validation accuracy, precision, and recall for this method?\n"}
{"snippet": "CrossValidated_LR=cross_val_score(regression_LR, X, Y,cv=10,scoring=\"neg_mean_squared_error\")\nprint (\"MSE_Cross Validated: \", abs(CrossValidated_LR.mean()))\n", "intent": "3.A.i) \tUsing 10-fold cross-validation, what is the estimated mean-squared-error (MSE) of the model?\n"}
{"snippet": "y_pred = logreg_model.predict(X_test)\n", "intent": "Now it is time for prediction by our model for new data \n"}
{"snippet": "y_pred_knn = knn_model.predict(X_test)\n", "intent": " The test accuracy is again similar to the train set, so the model does appear to generalize well.\n"}
{"snippet": "y_pred_randomforest = rand_forest_model.predict(X_test)\n", "intent": "The test accuracy pretty close to the train set, so the model does appear to generalize well, too.\n"}
{"snippet": "devel_X = bigram_vectorizer.transform(devel_texts['text'])\npred_classes = bigram_classifier.predict(devel_X)\nsvm_accuracy = scores.accuracy(devel_classes, pred_classes)\nprint('SVM accuracy: {:.2%}'.format(svm_accuracy))\n", "intent": "And our latest, finest SVM:\n"}
{"snippet": "cm = confusion_matrix(y, clf.predict(x))\nprint (cm)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "cm = confusion_matrix(y, clf_gauss.predict(x))\ncm_norm = cm /cm.sum(axis=1) \nprint (cm_norm)\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nlabels_test = np.array([1])\npred = clf.predict([-0.8,-1])\naccuracy = accuracy_score(pred,labels_test,normalize=True)\naccuracy\n", "intent": "We can check the accuracy of our model by running the code below\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1)) / len(VGG16_predictions)\nprint('Accuracy of the test data: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "new_predictions = [np.argmax(new_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(new_predictions)==np.argmax(test_targets, axis=1))/len(new_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "X = data_test\ny_pred_test = cls.predict(X)\ny_pred_test_proba = cls.predict_proba(X)[:, 1]\n", "intent": "Model prediction on competition set and submitting: \n"}
{"snippet": "scores = model.evaluate(x_test, y_test)\nprint()\nprint('accuracy=', scores[1])\n", "intent": "<h2 style=\"background-image: linear-gradient( 135deg, \n"}
{"snippet": "all_x, all_y = preprocessTitanicData(all_df)\nall_probability=model.predict(all_x)\n", "intent": "<h2 style=\"background-image: linear-gradient( 135deg, \n"}
{"snippet": "bst = joblib.load('models/bst_f59.joblib.dat')\nprobs = bst.predict(dtest)\ntest['probs'] = probs\ntest.sort_values(['user_id','probs'], ascending=[True,False], inplace=True)\nuser_to_probs = test.groupby('user_id')['y'].agg(lambda x: list(x))\n", "intent": "Predict reorder probabilities, sort them, and aggregate for each user.\n"}
{"snippet": "print \"Probability predictions according to predict:\" \nprint sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_Resnet]\naccuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('\\nTest accuracy: %.4f' % accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_test_prediction=model.predict(X_test)\n", "intent": "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:\n"}
{"snippet": "noise = np.random.normal(0, 1, test_y.shape)\ntest2_y = test_y + noise\nprint(mse(mdl.predict(test_x), test_y))\nprint(mse(mdl.predict(train_x), train_y))\nprint(mse(mdl.predict(test_x), test2_y))\n", "intent": "c) Add some noise (with mean=0, std=1) to the test set's y, and predict it again. What happened to the MSE? Why?\n"}
{"snippet": "scores = cross_val_score(mdl,train_x,train_y,cv=5)\nprint(scores)\n", "intent": "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = bestAdaModFitted.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(y_test, preds)\nauc_ada = auc(fpr, tpr)\ngini_ada = 2 * auc_ada - 1\n(auc_ada, gini_ada)\n", "intent": "Finding the auc and gini metrics for AdaBoostClassifier\n"}
{"snippet": "preds = bestGbModFitted.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(y_test, preds)\nauc_gb = auc(fpr, tpr)\ngini_gb = 2 * auc_gb - 1\n(auc_gb, gini_gb)\n", "intent": "Finding the auc and gini metrics for the GradientBoostingClassifier\n"}
{"snippet": "rocZeroOne(y_test, bestGbModFitted.predict_proba(X_test))\n", "intent": "Finding the optimal point on roc_curve for GradientBoostingClassifier\n"}
{"snippet": "rocZeroOne(y_train, bestAdaModFitted.predict_proba(X_train))\n", "intent": "Finding the optimal point on roc_curve for AdaBoostClassifier \n"}
{"snippet": "preds_good_train = bestGbModFittedGood.predict_proba(X_full)\npreds_good_test = bestGbModFittedGood.predict_proba(test_full)\n", "intent": "Find the probability matrices for the test and training dataset for the variable y_delay_train\n"}
{"snippet": "predictions=dtree.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "newpredictions=rfor.predict(X_test)\nprint(confusion_matrix(y_test,newpredictions))\nprint(classification_report(y_test,newpredictions))\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predictions=model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions=lrm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogVGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_pred = model.predict(x[:, np.newaxis])\n", "intent": "Now, let's make predictions using this model:\n"}
{"snippet": "predictions = logisticRegr.predict(x_test)\n", "intent": "First, we are going to make predictions using our new model and the test data\n"}
{"snippet": "def compute_loss(a3,Y):\n    m = Y.shape[1]\n    logprobability = - np.multiply(Y, np.log(a3)) - np.multiply(1-Y, np.log(1-a3))\n    loss = 1./m * np.nansum(logprobability) \n    return loss\n", "intent": "The function below computes loss using sigmoid loss function (also called cross-entropy loss function). \n"}
{"snippet": "predictions=classifier.predict(test_array)\n", "intent": "6. Test the classifer by putting the data into the same format as the training array\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef mean_score(scores):\n", "intent": "Perform 5-fold cross-validation\n"}
{"snippet": "print(metrics.classification_report(y_test, y_predicted,\n                                    target_names=dataset.target_names))\n", "intent": "Print the classification report \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef mean_score(scores):\n", "intent": "This function will serve to perform and evaluate a cross validation:\n"}
{"snippet": "y_pred = clf.predict(X_test)\ndef print_cluster(images, y_pred, cluster_number):\n    images = images[y_pred==cluster_number]\n    y_pred = y_pred[y_pred==cluster_number]\n    print_digits(images, y_pred, max_n=10)\nfor i in range(10):\n     print_cluster(images_test, y_pred, i)\n", "intent": "To predict the clusters for training data, we use the usual predict method of the classifier. Predict and show predicted clusters.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores)))\n", "intent": "If we evaluate our algorithm with a three-fold cross-validation, we obtain a mean score of around 0.81.\n"}
{"snippet": "y_pred=clf.predict(X_test)\nprint (y_pred)\n", "intent": "After fitting (training), the model can predict labels for new samples. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "We can calculate prediction accuracy easily by using Scikit Learn's function accuracy_score.\n"}
{"snippet": "clf.predict_proba(X_test[:10])\n", "intent": "Predicting the probabilites of first ten instances in X_test\n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "Create a new object with ones and zeros as star ratings.\n"}
{"snippet": "x_test = np.array(['feeling kind of sad'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "def log_likelihood(model, x, y):\n    prob = model.predict_log_proba(x)\n    rotten = y == 0\n    fresh = ~rotten\n    return(prob[rotten, 0].sum() + prob[fresh, 1].sum())\nllk_train = log_likelihood(my_model, X_train, y_train)\nllk_test = log_likelihood(my_model, X_test, y_test)\nprint(\"Log-Likelihood for Training Set: %.4f\" % (llk_train))\nprint(\"Log-Likelihood for Testing Set: %.4f\" % (llk_test))\n", "intent": "Using `fitted_model.predict_logproba`, we can find the log-likelihood for the training and testing sets.\n"}
{"snippet": "accuracy_score(test_y, pred_y3)\n", "intent": "Let's check out the accuracy of our last run.\n"}
{"snippet": "Y_pred = model.predict(D_test['X'])\nprint(\"Flower\\tPredict\\tActual\\tCorrect prediction?\")\nfor i in xrange(len(test_idx)):\n    print(\"{}\\t{}\\t{}\\t{}\\t\".format(\n            test_idx[i], \n            Y_pred[i],\n            D_test['Y'][i],\n            Y_pred[i] == D_test['Y'][i]\n        ))\n", "intent": "Good news! But what about prediction for other flowers?\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\nmodel.load_weights('saved_models/weights.best.from_scratch.hdf5')\ndog_breed_predictions_DA = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions_DA)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy with Data Augmentation: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "X_test = preprocess_pipeline.transform(test_data)\ny_pred = svm_clf.predict(X_test)\n", "intent": "Try on the test data and make submission\n"}
{"snippet": "predictions[\"Support Vectors\"] = clf.predict(SVR_test) \n", "intent": "Now that our regressor is trained, let's compute the `SalePrice` predictions.\n"}
{"snippet": "from sklearn import cross_validation\nvariables = ['DayOfWeek', 'X', 'Y']\nscores = cross_validation.cross_val_score(knn, df[variables], df['Category'], cv=5)\nprint(scores.mean(), scores)\n", "intent": "Cross Validation with 5 folds. -> 22.6 accuracy\n"}
{"snippet": "predictions = naive_bayes.predict(testing_data) \n", "intent": "We use parameters such as _Accuracy Score_, _Precision Score_, _Recall Score_ and _F1 Score_ to determine the performance of our trained model.\n"}
{"snippet": "test_predict_labels = []\nfor i_example in range(len(test_full)):\n    wmd_avgs = [wmd_mod(test_full[i_example], i, no_top_optimal) for i in range(len(data_test))]\n    index_min = wmd_avgs.index(min(wmd_avgs))\n    test_predict_labels = test_predict_labels + [all_subreddit_names[index_min]]\n    output_string = \"\\rExample \" + str(i_example + 1) + \" / \" + str(len(test_full))\n    sys.stdout.write(output_string)\n    sys.stdout.flush()\ntest_acc = accuracy_score(test_true_labels, test_predict_labels)\ntest_acc\n", "intent": "Find the accuracy (same code as used before in the validation loop):\n"}
{"snippet": "nexutil.evaluate(x_test, y_test, hyper_params, model_dir)\n", "intent": "Let's evaluate the classification performance in the test dataset:\n"}
{"snippet": "nexutil.evaluate(x_val, y_val, hyper_params, model_dir)\n", "intent": ".. and now in the validation dataset:\n"}
{"snippet": "predict_struct_201709 = nexutil.predict(chip_struct_201709, hyper_params, model_dir)\n", "intent": "Let's predict the files mosaic_201709.tif\n"}
{"snippet": "chip_struct_201803 = nexutil.generate_chips(data_norm_struct, file_idx=3, chip_size=128)\npredict_struct_201803 = nexutil.predict(chip_struct_201803, hyper_params, model_dir)\n", "intent": "...and now it's the time of mosaic_201703.tif, which wasn't used in the training:\n"}
{"snippet": "predictions = model.predict(X_test)\nprint(predictions)\n", "intent": "If we do not predict classes, here is the prediction we should obtain. It is a matrix with the probabilities of each rating for reviews.\n"}
{"snippet": "target = train[\"Survived\"].values\nfeature_names =[\"Pclass\",\"Age\",\"Fare\",\"Embarked\",\"SibSp\",\"Sex\",\"Parch\"]\nfeature = train[features_names].values\nscores = model_selection.cross_val_score(decision_tree, features, target, scoring = 'accuracy', cv=50)\n", "intent": "^here there might be problem of overfitting\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrmse_naive=sqrt(mean_squared_error(valid.Count,y_hat.naive))  \nrmse_naive\n", "intent": "We will now calculate RMSE to check the accuracy of our model on validation data set.\n"}
{"snippet": "accuracy_score(ytest, ypred1)\n", "intent": "Accuracy of the model:\n"}
{"snippet": "cross_val_score(model2, X, y, cv=10)\n", "intent": "Using cross_val_score to perform k-fold cross validation (k=10) with this model.\n"}
{"snippet": "accuracy_score(ytest, ypred3)\n", "intent": "Accuracy of Gaussian NB Classifier:\n"}
{"snippet": "accuracy_score(ytest, ypred4)\n", "intent": "Accuracy of Random Forest Classifier:\n"}
{"snippet": "print(\"****** Test Data ********\")\npred = a_model.predict_classes(test)\nprint(metrics.classification_report(test_labels, pred))\nprint(\"Confusion Matrix\")\nprint(metrics.confusion_matrix(test_labels, pred))\n", "intent": "Use the test dataset to evaluate the model\n"}
{"snippet": "print(\"****** Test Data ********\")\npred = cnn_model.predict_classes(test)\nprint(metrics.classification_report(test_labels, pred))\nprint(\"Confusion Matrix\")\nprint(metrics.confusion_matrix(test_labels, pred))\n", "intent": "Use the test dataset to evaluate the model\n"}
{"snippet": "test_comments_padded = pad_text(test_df[TEXT_COLUMN], tokenizer_v1)\ntest_df[MODEL_NAME_V1] = model_v1.predict(test_comments_padded)[:, 1]\n", "intent": "Using our new model, we can score the set of test comments for toxicity.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', np.sqrt(mean_squared_error(y_test, predictions)))\n", "intent": "The `mean_squared_error` function takes two arrays. We'll apply `np.sqrt()` to obtain the rmse. \n"}
{"snippet": "print (\"R^2 is: \\n\", ridge_model.score(X_test, y_test))\nprint ('RMSE is: \\n', np.sqrt(mean_squared_error(y_test, predictions)))\n", "intent": "Now, let's compute and compare the scores. Next, we'll create another scatter plot of our predicted values versus actual values. \n"}
{"snippet": "predictions = model.predict(feats)\n", "intent": "Next, we generate our predictions. \n"}
{"snippet": "probas = [x[1] for x in model_lr.predict_proba(X)]\ndf['xg'] = probas\n", "intent": "The model is better than guessing!\nSo lets apply the model on all shots and explore the output.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    tot_loss = 0\n    for i,ind in enumerate(style_layers):\n        diff = gram_matrix(feats[ind])-style_targets[i]\n        loss = style_weights[i] * tf.reduce_sum((diff*diff))\n        tot_loss+=loss\n    return tot_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "num_correct = 0\nraw_predictions = []\nfor index,row in test.iterrows():   \n    prediction = predict(row['Lyrics'],genre_bags,1)\n    raw_predictions.append(prediction)\n    if(prediction == row['Genre']):\n        num_correct += 1\nprint('Accuracy for our custom Naive Bayes model: ' + str(num_correct / len(test)))\n", "intent": "Here, we train and test our Naive Bayes model and report its accuracy. \n"}
{"snippet": "Y_test, X_test = dmatrices(formula, data=test, return_type='dataframe')\npredict_res = best_model.predict(X_test)\n", "intent": "Create test vector and target vector for validation\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_prediction=lin_reg.predict(housing_prepared)\nlin_mse=mean_squared_error(housing_labels,housing_prediction)\nlin_rmse=np.sqrt(lin_mse)\nlin_rmse\n", "intent": "then we apply the RMSE on the whole training set\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores=cross_val_score(tree_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\nrmse_scores=np.sqrt(-scores)\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\ndisplay_scores(rmse_scores)\n", "intent": "divided training set into 10 parts\n"}
{"snippet": "lin_scores=cross_val_score(lin_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\nlin_rmse_scores=np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "it looks not good, let us compared with the linear model\n"}
{"snippet": "final_model=grid_search.best_estimator_\nY_prediction=final_model.predict(X_test_prepared)\nfinal_mse=mean_squared_error(Y_test,Y_prediction)\nfinal_rmse=np.sqrt(final_mse)\nfinal_rmse\n", "intent": "and then we apply the best estimator of SVM on the test dataset and see what is going on?\n"}
{"snippet": "y_train_pred=sgd_clf.predict(X_train)\n", "intent": "The classifier guesses right\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores=cross_val_score(sgd_clf,X_train,y_train_5,cv=3,scoring=\"accuracy\") \nscores\n", "intent": "Firstly we can use cross validation to evaluate the SGDClassifier\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score,f1_score\nprecision_score(y_train_5,y_train_pred)\n", "intent": "at first row, 54200 non-5 cases are correctly classified as non-5, 280 cases non-5 are wrongly classified as 5\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n", "intent": "then we can calculate the area under the curve (UNA) \n"}
{"snippet": "roc_auc_score(y_train_5,y_scores_forest)\n", "intent": "from the plot it is obvious that Random Forest is better than the SGD because it is close to the top-left corner which Ptp is 1.0,Pfp is 0.\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Next: we will evaluate these classifier using cross_val_score\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_grid_pred=grid_search.predict(X_test)\naccuracy_score(y_test, y_grid_pred)\n", "intent": "then we use the test set\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_5,y_train_pred)\n", "intent": "at first row, 54200 non-5 cases are correctly classified as non-5, 280 cases non-5 are wrongly classified as 5\n"}
{"snippet": "time_star = time()\npredictions = gs.predict(x_val_lsvc)\ntime_end = time()\ntest_time = time_end-time_star\nacc = metrics.accuracy_score(y_val, predictions)\nprint (u'avg fit time:%.3fs' % np.mean(gs.cv_results_['mean_fit_time']))\nprint (u'test time:%.3fs' % test_time)\nprint (u'accuracy:%.2f%%' % (100 * acc))\nprint(metrics.classification_report(y_val, predictions, target_names=['Bad', 'Good']))\n", "intent": "Then,predict the evaluation data.\n"}
{"snippet": "predictions = lm.predict(x_test)\n", "intent": "Evaluating the performance of the model by predicting off the test values!\n** Using lm.predict() to predict off the X_test set of the data.**\n"}
{"snippet": "feature_columns = [\"word_count\",\"long_word_count\",\"avg_word_length_per_essay\",\"wrong_words\",\"no_of_domain_words\",\"word_to_sent_ratio\",\"num_of_characters\",\"sentence_count\",\"noun_count\",\"verb_count\",\"comma_count\",\"punctuation_count\",\"adjective_count\",\"adverb_count\",\"quotation_mark_count\",\"spelling_mistakes\"]\nfeature_dict = {}\nfor f in feature_columns:\n    score = helper.Evaluate(X_all,y_all,[f]) \n    feature_dict[f] = round(float(score)*100,4)\nfeature_dict\n", "intent": "Please refer section 2.4.3 of the report for Forward feature algoritm.\n"}
{"snippet": "warnings.simplefilter(\"ignore\")\nfor idx in sorted(test_split[0][:20]):\n    label = classifier.predict(features[feature_set].iloc[idx,data])[0]\n    print \"{0:10s} => {1}\".format(features[feature_set][\"class\"][idx], \n                                  label)\n", "intent": "Predict genre label for all tracks of the test set based on the feature vector\n"}
{"snippet": "def show_result_digit(clf):\n    label_test_pred = clf.predict(dat_test)\n    print(f\"train accuracy score: {accuracy_score(label_train, clf.predict(dat_train))}\")\n    print(f\"test accuracy score: {accuracy_score(label_test, label_test_pred)}\")\n", "intent": "Thus, in training phase, we have $N=1347$, $M=65$, $C=10$, and hence $MC = 650$.\n"}
{"snippet": "def get_meshgrid(x, y, nx, ny, margin=0.1):\n    x_min, x_max = (1 + margin) * x.min() - margin * x.max(), (1 + margin) * x.max() - margin * x.min()\n    y_min, y_max = (1 + margin) * y.min() - margin * y.max(), (1 + margin) * y.max() - margin * y.min()\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n                         np.linspace(y_min, y_max, ny))\n    return xx, yy\ndef plot_result(ax, clf, xx, yy, X, t):\n    Z = (clf.predict(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.7)\n    ax.scatter(X[:,0], X[:,1], c=t, edgecolor='k')\n", "intent": "Having checked the validity of our implementation, here we apply our code to simple toy data.\n"}
{"snippet": "AUC = roc_auc_score(y_test, y_pred_prob)\nprint(\"AUC: {}\".format(AUC))\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n", "intent": "**AUC** - Area under ROC curve\n"}
{"snippet": "prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n        iris_dataset['target_names'][prediction]))\n", "intent": "Call the .predict() method of the knn object with X_new as argument to calculate the estimated Target value.\n"}
{"snippet": "print(\"Test set predictions: {}\".format(clf.predict(X_test)))\n", "intent": "**Third** we make a prediction based on the test set (i.e. X_test)\n"}
{"snippet": "get_rmse(models[7].predict(Xt[~msk, :]), yt[~msk])\n", "intent": "This is the RMSE for observations where users are in both the test and training set. We get an RMSE of 0.95.\n"}
{"snippet": "(Xt, yt) = make_X(testdf)\nget_rmse(models[7].predict(Xt), yt)\n", "intent": "We now consider only observations where we have new users, i.e. users that were not in the test set. As expected, the RMSE is bigger now.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, gnb.predict(X_test))\n", "intent": "That score looks good enough to me! I have to use the AUC scoring function, though, not just accuracy.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, gnb.predict(X_test))\n", "intent": "Oh, hmm. That's bad actually. Let's see that confusion matrix.\n"}
{"snippet": "y_predict = knn_model.predict(X_test)\n", "intent": "Now use your model to predict the labels from the scaled `X_test` data and store these predictions in the variable `y_predict`\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predict)\n", "intent": "Use `accuracy_score` to compare how your prediction performed against the unseen `y_test` labels.\n"}
{"snippet": "predictions = classifier.predict(Xtr)\naccuracy = 0.0\nfor i in range(len(predictions)):\n    if predictions[i] == Ytr[i]:\n        accuracy += 1\naccuracy /= len(predictions)\naccuracy *= 100\ntrain_accuracy = accuracy\nprint train_accuracy\n", "intent": "We can also compute the training accuracy, though it would be very high for both our models. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss_array = []\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        loss_array.append(style_weights[i] * tf.nn.l2_loss(gram - style_targets[i]) * 2)\n    return tf.reduce_sum(loss_array)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    losses = []\n    for i in range(len(style_layers)):\n        loss_i = style_weights[i] * tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2)\n        losses.append(loss_i)\n    loss = tf.reduce_sum(losses)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions_probs = lr.predict_proba(test_x)\n", "intent": "Let's now look at our model's log loss for the 2018 season compared to the odds.\n"}
{"snippet": "Incept_predictions = [np.argmax(incept.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(Incept_predictions)==np.argmax(test_targets, axis=1))/len(Incept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def eval_sentprob(subreddit, model, d):\n    docs = db[subreddit].find({\"date\": d}, {\"date\": 1, \"tokens\": 1, \"_id\": 0})\n    test_corpus = map(lambda d: d[\"tokens\"].split(), docs)\n    test_corpus = filter(lambda x: len(x) >= 2, test_corpus)\n    probs = np.array(map(lambda x: model.predict(x), test_corpus))\n    probs = probs[~np.isnan(probs)]\n    if len(probs) > 0:\n        return np.mean(probs)\n    else:\n        return np.nan\n", "intent": "Define helper function that computes the avearge of the log-transformed probabilities of posts in subreddit `subreddit` and in date `d`:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_R]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "tree_params = best_params_et\nlr_params = (results_df_lr\n             .sort_values('result', ascending=False)\n             .iloc[0]\n             .drop('result')\n             .to_dict())\ntblr = TreeBasedLR(tree_params, lr_params, forest_model='et')\npreds = cross_val_predict(tblr, X, y, cv=skf, method='predict_proba')\n", "intent": "No loss of AUC here as well, as the logistic model does a good job of learning from the forest's leaves. Let us check the calibration:\n"}
{"snippet": "predictions=mlp.predict(X_test)\n", "intent": "Now let's see how well the model works. We'll predict the test data from our model\n"}
{"snippet": "from sklearn import metrics\na = metrics.f1_score(y_test,log.predict(X_test),average = 'micro')\nb = metrics.f1_score(y_test,log.predict(X_test),average = 'macro')\nc = metrics.f1_score(y_test,log.predict(X_test),average = 'weighted')\nprint(a,b,c)\n", "intent": "F1 score is used to find the accuracy of a logistic regression.\n"}
{"snippet": "scores = cross_val_score(knn,X,y,cv = 10,scoring = 'accuracy')\nscores\n", "intent": "and that is what cross validation does.\n"}
{"snippet": "scores = cross_val_score(knn,X,y,cv = 10,scoring = 'neg_mean_squared_error')\nscores\n", "intent": "Instead of setting scoring parameter to accuracy we can also set it to mean squared error\n"}
{"snippet": "k_scores = []\nfor k in range(1,31):\n    knn = KNC(n_neighbors = k)\n    score = cross_val_score(knn,X,y,cv = 10,scoring = 'accuracy')\n    k_scores.append(score.mean())\nk_scores\n", "intent": "Searching for a optimal value of 'k' for KNN can be done using a for loop.\n"}
{"snippet": "from sklearn import cross_validation\nprint 'Predictive performance:'\nfor scoring in ['accuracy','precision', 'recall', 'f1', 'average_precision', 'roc_auc']:\n    scores = cross_validation.cross_val_score( estimator, X, y, cv = 3, scoring = scoring )\n    print( '%20s: %.3f +- %.3f' % ( scoring, np.mean( scores ), np.std( scores ) ) )\n", "intent": "evaluate the quality of the classifier using the cross-validation technique \n"}
{"snippet": "model_list = [xgbModel, ModelElanet, ModelLasso]\nstacked_averaged_models = StackingAveragedModels(model_list,\n                                                meta_model = ModelLasso)\nscore = np.sqrt(-cross_val_score(stacked_averaged_models, train_x, train_y, scoring = 'neg_mean_squared_error', cv = n_folds))\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n", "intent": "After several attempts, we reached that the ModelLasso is the best metamodel to be used. Here below the results.\n"}
{"snippet": "y_pred = reg.predict(x_test)\nclassifRate = np.mean(y_pred.ravel() == y_test.ravel()) * 100\nprint('Classification rate: ', classifRate)\n", "intent": "make predictions on test data:\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Creating predictions from the test set and a classification report and a confusion matrix.**\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "** Predicting the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "accuracy_score(y_pred, y_test), 1-accuracy_score(y_pred, y_test)\n", "intent": "Now we can calculate our accuracy.\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint precision_score(y_test, best_preds, average='macro')\n", "intent": "Determine the precision (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) of this prediction:\n"}
{"snippet": "roc_auc_score(y_train_6, y_scores_forest)\n", "intent": "RFClassifier ROC curve looks better than SGDClassifier. \n"}
{"snippet": "forest_scores = cross_val_score(forest_reg, X_validation, Y_validation, scoring = 'neg_mean_squared_error', cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)\n", "intent": "As can be seen the RandomForestRegressor is a little bit worse than the other two models. \n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train_8, cv=3, scoring=\"accuracy\")\n", "intent": "Okay so it correctly predicted that item is a bag. Lets check the accuracy\n"}
{"snippet": "y_train_pred_sgd = cross_val_predict(sgd_clf, X_train, y_train_8, cv=3)\nconfusion_matrix(y_train_8, y_train_pred_sgd)\n", "intent": "So it is a little less accurate that SVM. \n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "I can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ntrain_predictions = model.predict(train.row,\n                                  train.col)\n", "intent": "Let's try to get a handle on the model accuracy using the ROC AUC score.\n"}
{"snippet": "test_predictions = model.predict(test.row, test.col)\n", "intent": "We've got very high accuracy on the train dataset; let's check the test set.\n"}
{"snippet": "y_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1_liblinear', 'l1_saga', 'l2_newton-cg', 'l2_lbfgs', 'l2_sag']\ncoeff_models = [lr, lr_l1_lin, lr_l1_saga, lr_l2_newton, lr_l2_lbfgs, lr_l2_sag]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "y_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1_liblinear', 'l1_saga', 'l2_newton-cg', 'l2_lbfgs', 'l2_sag']\ncoeff_models = [lr, lr_l1_lin, lr_l1_saga, lr_l2_newton, lr_l2_lbfgs, lr_l2_sag]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test_new), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test_new).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "* let us predict the labels for Input values in the test set and check the Model accuracy\n"}
{"snippet": "def predict(estimator, price_dics, df, mute=True):\n    df = df_cleaner(df, isTrain=False, mute=mute)\n    price_ref, price_dics = get_reference_price(df, price_dics=price_dics, mute=mute)\n    X = np.stack((df['Age'].astype(float).values, price_ref), axis=-1)\n    return estimator.predict(X)\n", "intent": "<hr style='background-color: \n"}
{"snippet": "metrics.adjusted_rand_score(wines['Type'].values,labels)\n", "intent": "Sklearn luckily provides us with a function to compare the similarity of two arrays for similarity between them whilst ignoring the permutations\n"}
{"snippet": "cv_scores = cross_val_score(reg, X_test, y_test, cv=10)\nprint(cv_scores)\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n", "intent": "** Residuals for Test data is more alinged towards zero compared to training data **\n** but this is a overfitting probelem, R Square score is .99**\n"}
{"snippet": "recall_with_default_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(),\n                      y_pred=(predictions_with_default_threshold.to_numpy()))\nprecision_with_default_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n                            y_pred=(predictions_with_default_threshold.to_numpy()))\nprecision_with_high_threshold = precision_score(y_true = test_data['sentiment'].to_numpy(),\n                                         y_pred = (predictions_with_high_threshold.to_numpy()))\nrecall_with_high_threshold = recall_score(y_true = test_data['sentiment'].to_numpy(),\n                                         y_pred = (predictions_with_high_threshold.to_numpy()))\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "baby_matrix = vectorizer.transform(baby_reviews['review_clean'])\nprobabilities = model.predict_proba(baby_matrix)[:,1]\nprobabilities = sframe.SArray(probabilities)\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(new_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "batch_multiple_coefficients = regression_closedform_gradient_descent(feature_matrix, output)\nprint \"coefficients\"\nprint batch_multiple_coefficients\npredictions_test_data = predict_output(feature_matrix_test, batch_multiple_coefficients)\nprint \"\"\nprint \"RMSE\"\nprint(np.sqrt(metrics.mean_squared_error(output_test, predictions_test_data)))\n", "intent": "**Closed form Gradient Descent (least squares):**\n"}
{"snippet": "result = model.predict(df_test)\nresult\n", "intent": "Now we will predict the Loan_Status according to our model.\n"}
{"snippet": "X13 = IMDB2[['budget','director_nominee','actor_nominee','actress_nominee','comedy','biography',\n           'romance','sport','thriller','crime','adventure','budget&director_nominee',\n           'budget&actor_nominee','budget&actress_nominee','budget&comedy','budget&biography',\n           'budget&romance','budget&sport','budget&thriller','budget&crime','budget&adventure','imdb_score','budget&imdb']]\ny13 = IMDB2['worldwide_income']\nIMDB2['pred_gross'] = stats_model5.predict(X13)\n", "intent": "Run chosen model on unseen data\n"}
{"snippet": "response = predictor.predict(response.content)\nresponse\n", "intent": "Now we will call the prediction endpoint.\n"}
{"snippet": "alpha = 1\nbeta = 1\ngrid = np.linspace(0,1,1000)\nanalytical = AnalyticalBeta(alpha, beta, TUMORS, RATS)\nanalytic_posterior = analytical.predict(grid)\nprior = analytical.prior(grid)\nmcmc = SimulatedBeta(alpha, beta, TUMORS, RATS).predict()\n", "intent": "* Result for one experiment with 10 RATS and 4 TUMORS\n"}
{"snippet": "y_test_prediction = model.predict(X_test)\nprint(y_test_prediction)\n", "intent": "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:\n"}
{"snippet": "train_loss, train_accuracy = model.evaluate(X,y,batch_size=batch_size)\ntrain_loss, train_accuracy\n", "intent": "Let's evaluate the CNN on the training data.\n"}
{"snippet": "y_train_pred=rf.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\n", "intent": "Random Forest on Training dataset\n"}
{"snippet": "y_test_pred=rf.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\n", "intent": "Random Forest on Testing dataset\n"}
{"snippet": "y_train_pred=lm.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\n", "intent": "Linear Regression on Training dataset\n"}
{"snippet": "y_test_pred=lm.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\n", "intent": "Linear Regression on Testing dataset\n"}
{"snippet": "y_train_pred=lm.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\n", "intent": "Linear Regression on training dataset\n"}
{"snippet": "y_train_pred=mlp.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\n", "intent": "Neural Network on Training Dataset\n"}
{"snippet": "y_test_pred=mlp.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\n", "intent": "Neural Network on Testing Dataset\n"}
{"snippet": "y_test_pred=lm.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\n", "intent": "Linear Regression on training dataset\n"}
{"snippet": "y_train_pred=mlp.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\n", "intent": "Neural Networks on training dataset\n"}
{"snippet": "y_test_pred=mlp.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\n", "intent": "Neural Networks on testing dataset\n"}
{"snippet": "mse = ((bos.PRICE - lm.predict(X)) ** 2).mean()\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nyhat = blr.predict(X)\nprint('Accuracy of: ',accuracy_score(y,yhat))\n", "intent": "So how well did the training go?\n"}
{"snippet": "diabetes_y_pred = regr.predict(diabetes_X_test)\n", "intent": "__make predictions__\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nimport numpy as np \ny = np.array(y_test)\nx = np.array(x_test)\nprint(cross_val_score(model_rfg,x_test_vt,y_test,scoring='r2'))\n", "intent": "Reference method: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's look at this model's root-mean-square deviation\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "It looks like the decision tree model isn't any better than a linear reg model\n"}
{"snippet": "def predict(preprocessResult,inp):\n    predictResult={sty:[] for sty in preprocessResult[0].keys()}\n    ngrmResult=ngrmcal(preprocessResult[0],inp)\n    uswdResult=uswdcal(preprocessResult[1],inp)\n    wdfaResult=wdfacal(preprocessResult[2],inp)\n    for sty in preprocessResult[0].keys():\n        predictResult[sty].append(ngrmResult[sty])\n        predictResult[sty].append(uswdResult[sty])\n        predictResult[sty].append(wdfaResult[sty])\n    return predictResult\n", "intent": "This function takes the data _preprocess_ created and a string to be predicted as the inputs, and return the predict results.\n"}
{"snippet": "predicted_price_test= ls_grid_search.predict(F2Test_X)\nactual_price_test = F2Test_y.values\nresiduals_test = predicted_price_test - actual_price_test\npercentage = residuals_test/actual_price_test *100\n", "intent": "**Lets check out the price variance unexplained by fixed features **\n"}
{"snippet": "predicted_price_train= ls_grid_search.predict(F2Train_X)\nactual_price_train = F2Train_y.values\npredicted_price_test= ls_grid_search.predict(F2Test_X)\nactual_price_test = F2Test_y.values\nresiduals_train = predicted_price_train - actual_price_train\nresiduals_test = predicted_price_test - actual_price_test\n", "intent": "**Use my renovatable features(X) to predict the residuals(y)**<br>\nHence, we find our residuals train and test s\n"}
{"snippet": "r_k = kmeans.predict(true_data)\nr_g = EM_pred(true_data, locs, sig, weights)\n", "intent": "** Calculate the response for kmeans and GMM**\n"}
{"snippet": "features = ['average_weight','minage']\nr2 = np.mean(cross_val_score(reg,board_games[features],board_games['average_rating'],'r2',kf))\nprint('R^2='+str(r2))\n", "intent": "It seems that more features are beneficial.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))\nevaluate_cross_validation(clf, X_train, y_train, 3)\n", "intent": "If we evaluate our algorithm with a three-fold cross-validation, we obtain a mean\nscore of around 0.814.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))\n", "intent": "We will define a function that takes a classifier and performs the K-fold crossvalidation\nover the specified X and y values\n"}
{"snippet": "y_pred = gm.predict(X_test)\nprint (\"Adjusted rand score:{:.2}\".format(metrics.adjusted_rand_score(y_test, y_pred)))\nprint (\"Homogeneity score:{:.2}\".format(metrics.homogeneity_score(y_test, y_pred)))\nprint (\"Completeness score: {:.2}\".format(metrics.completeness_score(y_test, y_pred)))\n", "intent": "Let's see how it performs on our testing data:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"20171209_101209-1280x720.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def predictions(quest):\n    model.random.seed(7)\n    vec=np.array(model.infer_vector(quest.split(' '))).reshape(1,-1)\n    out_put=neu_model.predict(vec)\n    return content[np.argmax(out_put)]\n", "intent": "Prediction step for forming answer for users\n"}
{"snippet": "classification_report(y_test, y_pred)\n", "intent": "Classification report:\n"}
{"snippet": "tf_Xception_predictions = [np.argmax(tf_Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(tf_Xception_predictions)==np.argmax(test_targets, axis=1))/len(tf_Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.6f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i]*tf.reduce_sum(tf.pow(gram-style_targets[i], 2))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import precision_recall_curve\ny_us_score = rndf_us.predict_proba(X_us_test)\nprecision_us, recall_us, _ = precision_recall_curve(y_us_test, y_us_score[:,1])\nutils.drawPRCurves(((recall_us, precision_us, \"US\"),))\n", "intent": "Precision Recall curves\n"}
{"snippet": "from sklearn.metrics import precision_recall_curve\ny_us_score = rndf_us.predict_proba(X_us_test)\nprecision_us, recall_us, _ = precision_recall_curve(y_us_test, y_us_score[:,1])\ny_iran_score = rndf_us.predict_proba(X_iran_test)\nprecision_iran, recall_iran, _ = precision_recall_curve(y_iran_test, y_iran_score[:,1])\nutils.drawPRCurves(((recall_us, precision_us, \"US\"),(recall_iran, precision_iran, \"Iran\")))\n", "intent": "Precision Recall curves\n"}
{"snippet": "features = [[0, 0.2]]\nexpected_rating_A = lreg.predict(features)[0]\nprint(\"The expected rating is %f stars\" % expected_rating_A)\nexpected_rating_B = lreg.intercept_ + 0 * lreg.coef_[0] + 0.2 * lreg.coef_[1]\nprint(\"This is the same as %f stars\" % expected_rating_B)\n", "intent": "However, if the review contains no positive words (fpos==0) but 20% negative words (fneg==0.2), we expect the following rating:\n"}
{"snippet": "ft.predict([[1,1,1,1]])\n", "intent": "Given the fit, we can look at predictions. If all the stocks go up:\n"}
{"snippet": "ft.predict([[0,0,0,0]])\n", "intent": "And if all go down,\n"}
{"snippet": "ft.predict([[0,1,0,1]])\n", "intent": "If BA and IBM go up and the others go down,\n"}
{"snippet": "score = Xception_model.evaluate(test_xception, test_targets)\nprint('\\n Test Accuracy: {}'.format(score[1]))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions=[np.argmax(Xception_model.predict(np.expand_dims(feature,axis=0))) for feature in test_Xception]\ntest_accuracy= 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets,axis=1))/len(Xception_predictions)\nprint('test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = model.predict(x_test)\n", "intent": "we are going to do some predictions \n"}
{"snippet": "res_predictions = [np.argmax(res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Res]\nres_test_accuracy = 100*np.sum(np.array(res_predictions)==np.argmax(test_targets, axis=1))/len(res_predictions)\nprint('Test accuracy: %.4f%%' % res_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='accuracy', fit_params={'sample_weight':newY})\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %0.2f (%0.2f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n", "intent": "These last few steps take a while to compute due to the large size of the data set.\n"}
{"snippet": "num_steps = 3000\nfor i in range(num_steps):\n    if i % 1000 == 0:\n        print(\"Iter: \", i)\n        evaluate(batch_size)\n        print('\\n')\n    images_train, labels_train = next_batch(train_dataset, train_labels_encoded, batch_size)\n    loss = model.train_on_batch(images_train.reshape(batch_size, vsize, hsize, num_channels), labels_train) \n    if i % 200 == 0:\n        print(\"model loss is \", loss[0])\n", "intent": "Here we actually train the model. For additional control, we perform this step manually, batch by batch.\n"}
{"snippet": "evaluate(5000)\n", "intent": "First, we'll evaluate the accuracy of our model:\n"}
{"snippet": "predictions = model.predict(test_dataet)\n", "intent": "This may take a while.\n"}
{"snippet": "def evaluate(size, weights, biases):\n    data, labels = next_batch(valid_dataset, valid_labels_encoded, size)\n    prediction = predict(data, weights, biases)\n    accuracy = (np.argmax(prediction, axis=1) == np.argmax(labels, axis=1)).sum() * 100 / size\n    print(\"The accuracy of your model is %s%%!\" % accuracy)\n    return accuracy\n", "intent": "This will compute the argmax (index of maximum element of predicted encoded labels), and compare that to the actual label.\n"}
{"snippet": "for col in label_cols:\n    accuracy = cross_val_score(model, train_features, df_train[col].values, scoring='accuracy', cv=5)\n    print(col, \": \", accuracy.mean())\n", "intent": "Accuracy using Cross Validation Score\n"}
{"snippet": "predictions = model_5.predict(validation_X)\nprint type(predictions)\nprint type(validation_Y)\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].as_matrix(), y_pred=predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_true=test_data['sentiment'].as_matrix(), y_pred=predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(y_true=test_data['sentiment'].as_matrix(), y_pred=predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true=test_data['sentiment'].as_matrix(), y_pred=predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "middleAgedX=[[1, 0, 0, 0, 1, 0, 1, 0, 1, 0]]\npredictedY = clf.predict(middleAgedX)\nprint(\"predictedY: \" + str(predictedY))\n", "intent": "<p>Apparently, a middle-aged person with fair credit and low income:</p>\n"}
{"snippet": "seniorX=[[0, 1, 0, 1, 0, 0, 1, 0, 1, 0]]\npredictedY = clf.predict(seniorX)\nprint(\"predictedY: \" + str(predictedY))\n", "intent": "<p>has better chance than senior with excellent credit:</p>\n"}
{"snippet": "conv_image = model.predict(image_batch)\nconv_image.shape\n", "intent": "Answer -we'll have to expand it as it treats the first value of the size as the number of data points\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.0)\n    for ii in range(len(style_layers)):       \n        gram = gram_matrix(feats[style_layers[ii]])\n        w = style_weights[ii]\n        loss += w*tf.reduce_sum(tf.pow(gram-style_targets[ii],2))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "image_folder = \"../sample_data/foods/test\"\ndata_val = ObjectDetectionDataset.create_from_dir(dataset_name='val_dataset', data_dir=image_folder)\neval_result = my_detector.evaluate(dataset=data_val)\nfor label_obj in data_train.labels:\n    label = label_obj.name\n    key = 'PASCAL/PerformanceByCategory/AP@0.5IOU/' + label\n    print('{0: <15}: {1: <3}'.format(label, round(eval_result[key], 2)))\nprint('{0: <15}: {1: <3}'.format(\"overall:\", round(eval_result['PASCAL/Precision/mAP@0.5IOU'], 2))) \n", "intent": "Create an evaluation dataset, and use this to compute the accuracy of the model.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y,y_pred)\n", "intent": "- proportion of correct prediction\n- common evaluation metric for classification problem\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_mymodel.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "res_sumsq=np.sum((bos.PRICE - lm.predict(X)) ** 2)\nres_sumsq\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, clsf.predict(X_test)))\n", "intent": "Let's also look at the confusion matrix. \n"}
{"snippet": "train_cm = sklearn.metrics.confusion_matrix(clf.predict(train_x), train_y)\nROC_train = (train_cm[0][0] + train_cm[1][1]) / float(np.sum(train_cm))\nprint \"The train data set get ROC of:\", ROC_train\ntest_cm = sklearn.metrics.confusion_matrix(clf.predict(test_x), test_y)\nROC_test = (test_cm[0][0] + test_cm[1][1]) / float(np.sum(test_cm))\nprint \"The test data set get ROC of:\", ROC_test\n", "intent": "6\\. Using the classifier built in 2.2, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "We will try to measure the error i.e RMS, on the whole training set \n"}
{"snippet": "valid_range = range(n_burn_in, resampled_xray['time'].shape[0] - n_lookahead)\ny_array = resampled_xray['target'][valid_range].values\ny = y_array.reshape((y_array.shape[0], 1))\nfe = FeatureExtractor()\nX = fe.transform(resampled_xray, n_burn_in, n_lookahead, list(skf)[0])\nreg = Regressor()\nscores = np.sqrt(-cross_val_score(reg, X=X, y=y, scoring='mean_squared_error', cv=skf))\nprint scores.mean()\n", "intent": "This cell unit-tests your selected regressor and feature extractor and prints the RMSE.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse \n", "intent": "Measure the model's RMSE on the whole training set:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores) \n", "intent": "Perform a _K-fold cross-validation_:\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Compute the same scores for the ``LinearRegression`` model:\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\nnever_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "Compare with a very dumb classifier:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "The $F_{1}$ score (harmonic mean of precision and recall):\n\\begin{equation*}\nF_{1}= \\frac{TP}{TP+\\frac{FN+FP}{2}}\n\\end{equation*}\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n", "intent": "Get the scores for all instances of the training set:\n"}
{"snippet": "y_train_pred_90 = (y_scores > 200000)\nprecision_score(y_train_5, y_train_pred_90)\n", "intent": "Support you decide to aim for ~90% precision:\n"}
{"snippet": "def score_calculation(labels, prediction):\n    y_true = pd.Series(labels)\n    y_pred = pd.Series(prediction)\n    print(str(lr))\n    print('Confusion matrix: \\n{}'.format(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)))\n    print(classification_report(labels, prediction))\n    print(\"Accuracy score: {}\".format(metrics.accuracy_score(labels, prediction)))\n", "intent": "A function responsible for accuracy, precision, recall, f1 score calculation. Also it provides Confusion matrix.\n"}
{"snippet": "def report_clf(y_act, y_pred, y_proba, title=\"\", cmap=\"Blues\"):\n    plot_pr_roc(y_act, y_proba, \"\", \"darkorange\", True, title)  \n    plot_cm(y_act, y_pred, title + \" Confusion Matrix\", cmap) \n    print(\"\\n\\n Classification Report \", title, \"\\n\\n\")  \n    print(classification_report(y_act, y_pred))\n", "intent": "Following is the convenient utility for reporting, ROC, PR and F1 score .\n"}
{"snippet": "best_weight = 1000\nxgb_model_best_wt_sofar = xgb_models[best_weight]\nxgb_predict_best_weighted_obj = find_cv_model_predict(xgb_model_best_wt_sofar, X_train, y_train, weight=best_weight)\nxgb_best_wt_cls_1_proba = xgb_predict_best_weighted_obj['proba'][:,1]\nxgb_best_wt_y_pred = xgb_predict_best_weighted_obj['pred']\nprint(xgb_best_wt_cls_1_proba)\nprint(xgb_best_wt_y_pred[10:100])\nreport_clf(y_train, xgb_best_wt_y_pred, xgb_best_wt_cls_1_proba, title=\"for 2nd Attempt to Train Weighted Model \", cmap=\"Greens\")\nplot_pr_vs_th(y_train, xgb_best_wt_cls_1_proba, show=True, tag=\"for 2nd Attempt to Train Weighted Model \")\n", "intent": "Lets also see the confusion matrix and classification report for the best weighted models\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_results = cross_val_score(model, X, y, cv=5, scoring='roc_auc')     \n", "intent": "<a name=\"CrossVal\"></a>\n<h2>Cross-validation</h2>\n"}
{"snippet": "y_pred = linreg.predict(X_test)\n", "intent": "\"make predictions on the testing set\"\n"}
{"snippet": "true = [1, 0]\npred = [1, 0]\nprint(metrics.mean_absolute_error(true, pred))\nprint(metrics.mean_squared_error(true, pred))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "\"define true and predicted response values\" and print\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Compute the margin of error with the whole dataset. Root Mean Squared Error (RMSE) = 2.4\n"}
{"snippet": "odds = pd.Series()\nfor category in filters:\n    for pivot in params[category]:\n        non_pivots = [x for x in params[category] if x != pivot]\n        X_one = X[\"t1\"].copy() \n        X_one[non_pivots] = 0\n        X_one[pivot] = 1\n        proba_one = simple_tree_sk.predict_proba(X_one).T[1].mean()\n        odds[pivot] = proba_one/(1-proba_one)\n", "intent": "The odds ratios formula is:\n$$\\frac{p/(1-p)}{q/(1-q)} = \\frac{p(1-q)}{q(1-p)}$$\nwhere $p$ and $q$ are the probability of being **unemployed**.\n"}
{"snippet": "odds = pd.Series()\nfor category in filters:\n    for pivot in params[category]:\n        non_pivots = [x for x in params[category] if x != pivot]\n        X_one = X[\"t1\"].copy() \n        X_one[non_pivots] = 0\n        X_one[pivot] = 1\n        proba_one = random_forest_sk.predict_proba(X_one).T[1].mean()\n        odds[pivot] = proba_one/(1-proba_one)  \n", "intent": "The odds ratios formula is:\n$$\\frac{p/(1-p)}{q/(1-q)} = \\frac{p(1-q)}{q(1-p)}$$\nwhere $p$ and $q$ are the probability of being **unemployed**.\n"}
{"snippet": "odds = pd.Series()\nfor category in filters:\n    for pivot in params[category]:\n        non_pivots = [x for x in params[category] if x != pivot]\n        X_one = X[\"t1\"].copy() \n        X_one[non_pivots] = 0\n        X_one[pivot] = 1\n        proba_one = boost_sk.predict_proba(X_one).T[1].mean()\n        odds[pivot] = proba_one/(1-proba_one)  \n", "intent": "The odds ratios formula is:\n$$\\frac{p/(1-p)}{q/(1-q)} = \\frac{p(1-q)}{q(1-p)}$$\nwhere $p$ and $q$ are the probability of being **unemployed**.\n"}
{"snippet": "y_hat = model.predict(x_test)\n", "intent": "Now let's visualize the prediction using the model you just trained. \nFirst we get the predictions with the model from the test data.\n"}
{"snippet": "clf.predict([[1., .02],[0.1,1.0]])\n", "intent": "Using what was learned\n"}
{"snippet": "clf.predict([[2., 2.]])\n", "intent": "Using what was learned\n"}
{"snippet": "print(classification_report(y_test_mc, svm_predicted_mc))\n", "intent": "Get a classification report that summarizes multiple evaluation metrics for a multi-class classifier with an average metric computed for each class\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.1f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_pred = SVMClassifier.predict(X_test)\n", "intent": "For help understanding the confusion matrix- a tool analysising accuracy:\nhttps://en.wikipedia.org/wiki/Confusion_matrix\n"}
{"snippet": "print(\"Predict The sentiment\")\ndata = input(\"Enter your data to get the sentiment: \")\ndata = [data]\narray = cv.transform(data).toarray()\nr = SVMClassifier.predict(array)\nprint(r)\n", "intent": "Data entered here is not preprocessed, so may not be accurate.\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\nprint(\"preds shape: \",preds.shape)\npreds[:,0], np.mean(preds[:,0]), y_valid[0]\n", "intent": "We'll grab the predictions for each individual tree, and look at one example.\n"}
{"snippet": "scores = loaded_lstm_model.evaluate(lstm_seq_array_X_test, lstm_seq_array_y_test, verbose = 1, batch_size = 200)\nprint('Accurracy: {}'.format(scores[1]))\n", "intent": "In this section, we will look at the performance of the model on the test data. We have kept the last 25 machines (ID 76 to 100) for this purpose.\n"}
{"snippet": "predicted = classifier.predict(objects)\n", "intent": "Now we have to transform all segments into a segment models in order to classify them\n"}
{"snippet": "y_predict = body_reg.predict(x_chosen)[0][0] \ny_actual = dataframe.get_value(random_row_num, 'Second') \n", "intent": "Find the predicted and actual y-values as-per the randomly chosen x.\n"}
{"snippet": "Y_pred = pls.predict(X_test)\n", "intent": "Apply the dimension reduction learned on the train data.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_pred))\n", "intent": "Determine accuracy of model on test set\n"}
{"snippet": "print(accuracy_score(y_test, y_pred_keras))\n", "intent": "Determine accuracy of model on test set\n"}
{"snippet": "clf.predict(X_test)\n", "intent": "Make the prediction\n"}
{"snippet": "clf_sv.predict(X_test)\n", "intent": "Make the prediction\n"}
{"snippet": "from sklearn.metrics import confusion_matrix \nconfusion_matrix(y_train,svc.predict(X_train))\n", "intent": "<b>Key</b><br>\nTN=[0,0] <br>\nFN=[1,0] <br>\nFP=[0,1] <br>\nTP=[1,1]\n"}
{"snippet": "from sklearn.metrics import confusion_matrix \nconfusion_matrix(y,svc.predict(X_scaled))\n", "intent": "<b>Key</b><br>\nTN=[0,0] <br>\nFN=[1,0] <br>\nFP=[0,1] <br>\nTP=[1,1]\n"}
{"snippet": "mu = Y.mean()\nYd = Y - mu\nm = GPy.models.GPRegression(X, Yd, k)\nm.optimize_restarts(10)\nXplot = np.linspace(-0.2,1.2,100)[:,None]\nYplot, Ysd = m.predict(Xplot)\n", "intent": "The simplest way to fake a mean term is to calculate the mean of Y and subtract it from the data.\n"}
{"snippet": "mseFull = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mseFull)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "test_error = sorted([(abs_mean_error(i, rul1['rul']),i) for i in range(20,120)])[0] \ntest_error_rmse = sorted([(rmse(i, rul1['rul']),i) for i in range(20,120)])[0]\nprint('baseline errors with const prediction. ' + 'mae/const: ' + str(test_error), '| rmse/const: ' + str(test_error_rmse))\n", "intent": "For a starting baseline let's predict with constant and see whether we can find a model that outperforms that result, at least.\n"}
{"snippet": "y_prediction_tree = regressor_tree.predict(X_test)\ny_prediction_tree\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Decision Tree Regressor\n<br><br></p>\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "rf_tuned.predict(X_test)\nrf_tuned.score(X_test,y_test)\n", "intent": "Note that accuracy here can be very misleading. See the confusion matrix below for a better assessment of model performance\n"}
{"snippet": "y_pred = ppn.predict(X_test_std)\nprint('Misclassified samples: %d' % (y_test != y_pred).sum())\n", "intent": "Having trained a model in scikit-learn, we can make predictions via the predict\nmethod\n"}
{"snippet": "x = np.reshape(pattern, (1, len(pattern), 1))\nx = x / float(n_vocab)\nprediction = model.predict(x, verbose=0)\n", "intent": "Now, we'll use our model to generate a single prediction: we're not even going to learn a character for it, we simply want to check the output array\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "We evaluate the accuracy of the model using the testing set aiming for something over 85%\n"}
{"snippet": "pred = knnc.predict(X_ts)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions_train = logmodel.predict(X_train)\npredictions_test = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(\"Compare h(X_train) to Ground Truth:\\n\",classification_report(y_train,predictions_train),\"\\n\\n\")\nprint(\"Compare h(X_test) to Ground Truth:\\n\", classification_report(y_test,predictions_test))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "preds = model.predict(X_ts)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "VGG19_predictions = [np.argmax(vgg19model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\nvgg19test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('VGG19 Test accuracy: %.4f%%' % vgg19test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "exp_fn = create_experiment(\n        run_config=RUN_CONFIG,\n        hparams=hparams,\n        model_name=PARAMS['T2T_Model'],\n        problem_name=PARAMS['T2T_Problem'],\n        data_dir=PARAMS['DATA_DIR'],\n        train_steps=PARAMS['train_steps'],\n        eval_steps=PARAMS['eval_steps']\n    )\nexp_fn.train_and_evaluate()\n", "intent": "Check logs below to see progress of model loss and BLEU score\n"}
{"snippet": "predictions = logModel.predict(transformed_test)\n", "intent": "Making predictions on the test dataset.\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "We can test if a particular image is the number 5:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "We will use Cross-Validation to meassure the accuracy of our model.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "The % of accuracy looks high! What about the constant classifier that assigns everything to not 5?\n"}
{"snippet": "def predict(output):\n    y_pred = [1 if x >= 0 else -1 for x in output]\n    return y_pred\n", "intent": " Predict Limiar Output\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test , prediction)\n", "intent": "**a.) Accuracy Score**\n"}
{"snippet": "from sklearn.metrics import classification_report\nclassification_report(y_test , prediction,labels = None, digits = 3)\n", "intent": "**c.) Classification Report**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nscore = accuracy_score(y_test , pred)\nprint(score)\n", "intent": "q-5.)** Print the knn score**\n"}
{"snippet": "predicted = classifier.predict(test_images)\n", "intent": "Now we can predict the labels for the test image data:\n"}
{"snippet": "test_data = shuffled_data[-10:]\ntest_target = shuffled_target[-10:]\npredicted_target = knn.predict(test_data)\nprint('Predicted: {}'.format(predicted_target))\nprint('Actual:    {}'.format(test_target))\n", "intent": "We can now use our `knn` object to predict the class of the remaining target data:\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_Xception = 100 * np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Xception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "RSS = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nMSE = RSS / len(X)\nprint('MSE: ', MSE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n", "intent": "The Grid Search returned the same C value we foud that was a number >=0.1\n"}
{"snippet": "predictions = predict(parameters, X)\nrealResult = np.dot(Y,predictions.T)\npredictionResult = np.dot(1-Y,1-predictions.T)\nprint ('Accuracy: %d' % float((realResult + predictionResult)/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "predictions = predict(parameters, X)\nprint(X.shape)\nprint(predictions.shape)\nrealResult = np.dot(Y,predictions.T)\nprint(result)\npredictionResult = np.dot(1-Y,1-predictions.T)\nprint(pre)\nprint ('Accuracy: %d' % float((realResult + predictionResult)/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "logistic_score(train_dataset,train_labels,test_dataset,test_labels,50)\n", "intent": "Training on 50 training samples :\n"}
{"snippet": "logistic_score(train_dataset,train_labels,test_dataset,test_labels,100)\n", "intent": "Training on 100 training samples :\n"}
{"snippet": "logistic_score(train_dataset,train_labels,test_dataset,test_labels,1000)\n", "intent": "Training on 1000 training samples :\n"}
{"snippet": "logistic_score(train_dataset,train_labels,test_dataset,test_labels,5000)\n", "intent": "Training on 5000 training samples :\n"}
{"snippet": "logistic_score(train_dataset,train_labels,test_dataset,test_labels,200000)\n", "intent": "Training on all the training_dataset :\n"}
{"snippet": "def total_loss(style_layer_ls, style_targ_ls, style_wgt_ls, content_layer, content_targ, style2content_ratio,\n               hist_layer_ls, hist_targ_ls, hist_ratio, cc_ratio):\n    lr_loss = cc_loss(style_layer_ls, style_targ_ls, style_wgt_ls) / cc_ratio\n    s_loss = style_loss(style_layer_ls, style_targ_ls, style_wgt_ls)  \n    h_loss = hist_loss(hist_layer_ls, hist_targ_ls) / hist_ratio\n    c_loss = content_loss(content_layer, content_targ) / style2content_ratio\n    loss = s_loss + h_loss + c_loss \n    return loss\n", "intent": "Defining the total loss\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0)))\n                       for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==\n                          np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def Relative_Error(true,pred):\n", "intent": "Function that calculates the Relative Error:\n"}
{"snippet": "def Relative_Error(true,pred):\n    print str(abs(((true-pred)/true)*100)) + \"%\"\n", "intent": "A second round of filtering of Features does not improve the results. Lowest RMSE currently is 0.00339450031266.\n"}
{"snippet": "def error(result):\n    prediction = result.predict(test_X)\n    SSE = sum((test_Y - prediction) ** 2)\n    return SSE\n", "intent": "And I can use the following funciton to find the prediction error for different models:\n"}
{"snippet": "print('silhouette_score', silhouette_score(retail_data, cluster_assignment_dbscan_pca))\n", "intent": "Compute the silhouette score of the clusters made with DBSCAN and compare it with the silhouette score achieved with K-Means.\n"}
{"snippet": "model_top.evaluate(validation_data, validation_labels)\n", "intent": "Computing loss and accuracy:\n"}
{"snippet": "y_pred = clf.best_estimator_.predict(X_test, ntree_limit= n_trees)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))\n", "intent": "We can see the accuracy of best parameter combination on test set.\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))\n", "intent": "Now we are ready to evaluate accurracy of model trained on reduced set of features.\n"}
{"snippet": "test_predictions = loaded_model.predict(test_data[:10])\n", "intent": "You can make predictions to check that model has been loaded correctly.\n"}
{"snippet": "scores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Evaluation Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Store and archive the model in the notebook filesystem.\n"}
{"snippet": "pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,pred))\n", "intent": "** Creating a confusion matrix and classification report.**\n"}
{"snippet": "predictions=logmodel.predict(X_test)\n", "intent": "** Predicting values for the testing data.**\n"}
{"snippet": "predictions=dtree.predict(X_test)\n", "intent": "**Creating predictions from the test set and creating a classification report and a confusion matrix.**\n"}
{"snippet": "predictions=rfc.predict(X_test)\n", "intent": "Predicting y_test values \n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Creating a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = results.predict(X_test)\nprint(\"Mean squared error: {:.2f}\".format(np.mean((results.predict(X_test) - y_test) ** 2)))\nprint('\\nThe coefficients are: \\n{}'.format(results.params))\n", "intent": "Model validation with testing set.\n"}
{"snippet": "x_test = data_test[label_x].values\ny_test = model.predict(sm.add_constant(x_test))\nmse = mean_squared_error(y_test, y[-len(y_test):])\nprint('* MSE (Mean Square Error) : %.2f' %  mse)\nprint('  -      square root(mse) : %.2f' %  math.sqrt(mse))\n", "intent": "Now we will predict TRM values taking in count the regression coefficients, over test values got above.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_resnet = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_resnet)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "COBRA_diabetes.predict(diabetes_X_test)\n", "intent": "Predicting using the COBRA predictor is again similar to using a scikit-learn estimator.\n"}
{"snippet": "import sklearn\nr_squared = sklearn.metrics.r2_score(joined_df['funded_amnt'],joined_df['Predicted Annual Income'],multioutput='uniform_average')\nr_squared\n", "intent": "12- Remember to calculate your R Square.\n"}
{"snippet": "for epoch in range(20):\n    _, training_err = train_epoch()\n    _, testing_err = evaluate()\n    print epoch, training_err, testing_err\n", "intent": "Now train for 100 epochs and print the training and test set error.\n"}
{"snippet": "clf.predict([[2., 2.]])\n", "intent": "After being fitted, the model can then be used to predict the class of samples:\n"}
{"snippet": "clf.predict_proba([[2., 2.]])\n", "intent": "Alternatively, the **probability of each class** can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "clf.predict(iris.data[:1, :])\n", "intent": "After being fitted, the model can then be used to predict the class of samples:\n"}
{"snippet": "t = 100\nc = np.array([gmm[p].predict(V[p,t]) for p in pixels])\n", "intent": "For classification of a frame $V_t$ , the predict method should be called by the `GaussianMixture` object of each pixel:\n"}
{"snippet": "predicted_test = grid_cv.best_estimator_.predict(X_test)\ntrue_count = np.sum(predicted_test == y_test)\ntrue_count / float(len(predicted_test))\n", "intent": "Test the performance of our tuned KNN classifier on the test set.\n"}
{"snippet": "linkage_matrix_full = scipy.cluster.hierarchy.ward(newsgroupsCoocMat.toarray())\nhierarchicalClusters_full = scipy.cluster.hierarchy.fcluster(linkage_matrix_full, 4, 'maxclust')\nprint(\"For our complete clusters:\")\nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nprint(\"Adjusted Rand Score: {:0.3f}\".format(sklearn.metrics.adjusted_rand_score(newsgroupsDF['category'], hierarchicalClusters_full)))\nnewsgroupsDF\n", "intent": "We can use this *get clusters* like we did with k-means. What if we do the full data set?\n"}
{"snippet": "svmPredict = svmModel.predict(X_test)\n", "intent": "With our trained model, we can now generate predicted values for the test data to estimate the accuracy of our classifier.\n"}
{"snippet": "mlpPredict = mlpModel.predict(X_test)\n", "intent": "With our trained model, we can now generate predicted values for the test data to estimate the accuracy of our classifier.\n"}
{"snippet": "y_pred = comment_clf.predict(x_test)\n", "intent": "Now, we can predict the toxicity of our test dataset and view the relevant metrics.\n"}
{"snippet": "y_pred = model.predict(x_test)\n", "intent": "The model converges quite smoothly, so let's check the performance on the test set.\n"}
{"snippet": "y_pred = c.predict(x_test)\nscore = accuracy_score(y_test, y_pred) * 100\nprint(\"Accuracy using Decision Tree: \", round(score, 1), \"%\")\n", "intent": "This is how we will see how our different classifiers do, by predicting the results on the test data and then figuring out how well they did.\n"}
{"snippet": "pred = gbc.predict(playlistToLookAtFeatures[features])\nlikedSongs = 0\ni = 0\nfor prediction in pred:\n    if(prediction == 1):\n        print (\"Song: \" + playlistToLookAtFeatures[\"song_title\"][i] + \", By: \"+ playlistToLookAtFeatures[\"artist\"][i])\n        likedSongs= likedSongs + 1\n    i = i +1\n", "intent": "- The function looks like this user_playlist_add_tracks(\"username\", \"playlist_id\", \"track_id_to_add\")\n"}
{"snippet": "accuracy = accuracy_score(y_test, y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')\n", "intent": "Calculating model accuracy:\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepered, housing_labels, \n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "<b>NOTE:</b> Let's compute the same scores for Linear Regression model just to be sure:\n"}
{"snippet": "def standard_error(y_pred, y_test):\n    sum_value = 0\n    for p, t in zip(y_pred, y_test):\n        sum_value += (p - t)**2\n    return((sum_value/len(y_test))**(1/2))\n", "intent": "<b>NOTE:</b> Definition of the standard error function.\n"}
{"snippet": "prediction_values = classifier_RFC.predict(X_test)\n", "intent": "<a id=\"making_final_prediction\"></a>\n"}
{"snippet": "scores = model_DNN.evaluate(X_val, Y_val)\nprint(\"\\n%s: %.2f%%\" % (model_DNN.metrics_names[1], scores[1]*100))\n", "intent": "6.3.4. Evaluate the model\n"}
{"snippet": "import my_measures\nsgd_pm = my_measures.BinaryClassificationPerformance(sgd.predict(X), L, 'sgd')\nsgd_pm.compute_measures()\nprint(sgd_pm.performance_measures)\n", "intent": "We'll start with the first few measures in Flach, p. 57\n"}
{"snippet": "from sklearn import metrics\ny_hat = clf.predict(X_test)\nprint(np.average(y_hat == y_test))\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test,y_hat)))\n", "intent": "Next, make predictions using the test set and calculate the accuracy.\n"}
{"snippet": "import pickle\nfrom sklearn.externals import joblib\nclf = joblib.load('./sklearn_mnist_model.pkl')\ny_hat = clf.predict(X_test)\n", "intent": "Feed the test dataset to the model to get predictions.\n"}
{"snippet": "mean_loss = tf.losses.mean_squared_error(labels=targets,predictions=outputs) / 2.\noptimise = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(mean_loss)\n", "intent": "**Choose objective function and optimisation method**\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100 *np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG_19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG_19]\ntest_accuracy = 100*np.sum(np.array(VGG_19_predictions)==np.argmax(test_targets, axis=1))/len(VGG_19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def compute_square_loss(X, y, theta):\n    num_instances = X.shape[0]\n    temp = np.dot(X, theta) - y\n    loss = 0.5/num_instances * np.dot(temp.T, temp)    \n    return loss\ndef compute_square_loss_gradient(X, y, theta):\n    num_instances = X.shape[0]\n    grad = 1 / num_instances * np.dot(X.T, (np.dot(X, theta) - y))\n    return grad\n", "intent": "So we can compute the gradient of $J$ as:\n$$\\nabla_{\\theta}J(\\theta)=\\frac{1}{n}X^T(X\\theta-y)$$\n"}
{"snippet": "params = {\"objective\": \"multi:softprob\", \"num_class\": 9}\ngbm = xgb.train(params, xgb.DMatrix(X_train, Y_train), 20)\nY_pred = gbm.predict(xgb.DMatrix(X_test))\n", "intent": "I decided to add XGBoost to improve the model and it helped. The final result was using weighted predictions from both models.\n"}
{"snippet": "log_l1_median_scores = sklearn.model_selection.cross_val_score(log_l1, X_median, y_full, cv=10, scoring='accuracy')\ncv_log_l1_median_acc = log_l1_median_scores.mean()\nrf_median_scores = sklearn.model_selection.cross_val_score(rf, X_median, y_full, cv=10, scoring='accuracy')\ncv_rf_median_acc = rf_median_scores.mean()\nert_median_scores = sklearn.model_selection.cross_val_score(ert, X_median, y_full, cv=10, scoring='accuracy')\ncv_ert_median_acc = ert_median_scores.mean()\nab_median_scores = sklearn.model_selection.cross_val_score(ab, X_median, y_full, cv=10, scoring='accuracy')\ncv_ab_median_acc = ab_median_scores.mean()\ngbc_median_scores = sklearn.model_selection.cross_val_score(gbc, X_median, y_full, cv=10, scoring='accuracy')\ncv_gbc_median_acc = gbc_median_scores.mean()\n", "intent": "- Imputed data using median strategy will be used.\n- All models with predictive accuracy higher than 75% will be used\n"}
{"snippet": "predictions = logmodel.predict(X_test_std)\n", "intent": "Next I find the predictions of the logistic model using the test set\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "In the next two slides I see how my model did predicting based on the test set, this seems to be a very accurate model\n"}
{"snippet": "SVMpred = model.predict(X_test_std)\n", "intent": "Then we use this model to make a prediction based on the test set which turns out to be 100 percent accurate\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n", "intent": "Done!  We have an actual model.  Let's try it out on a few instances from the training set.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "It works...sort of.  Not great though.  Let's acutally measure the mean squared error (MSE).\n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Now that it's trained, let's evaluate it on the training set.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n", "intent": "You could create your own several validation sets using the train_test_split() function.  Of course, Scikit-Learn has something even easier.\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Let's see if it can predict the right answer for our some_digit.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "Looks pretty good.  But what about the \"not-5\" group?  How does the classifier do then?\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "F = TP / (TP + (FN + FP) / 2)\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3,\n                            method = \"decision_function\")\n", "intent": "So, how do you decide which threshold to use?  Use scikit-learns' cross_val_predict, but specify you want \"decision_function\" instead of \"accuracy\".\n"}
{"snippet": "forest_clf.predict_proba([some_digit])\n", "intent": "Let's see the list of class probabilities from the RandomForest.\n"}
{"snippet": "knn_clf.predict([some_digit])\n", "intent": "Let's make a prediction\n"}
{"snippet": "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\nf1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n", "intent": "Evaluate the model using the average F score (over all training examples)\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredict_train = svm_reg.predict(X_train)\ntrain_mse = mean_squared_error(y_train, predict_train)\ntrain_rmse = np.sqrt(train_mse)\ntrain_rmse\n", "intent": "Let's use mean squared error to evaluate fit on training data.\n"}
{"snippet": "predict_test = svm_reg.predict(X_test)\ntest_mse = mean_squared_error(y_test, predict_test)\ntest_rmse = np.sqrt(test_mse)\ntest_rmse\n", "intent": "Do the same thing with the test set.\n"}
{"snippet": "X_new = np.linspace(-3,3,100)\ny_pred = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))\n", "intent": "Now, we have an ensemble with 3 trees.  Make predictions by adding up the predictions of each tree.\n"}
{"snippet": "print img.shape\nres=D.predict(img)\nres\n", "intent": "We can verify that the discriminator works when applied to an image:\n"}
{"snippet": "prediction = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "print('Predictions:')\nprint(model.predict([[1,1], [1, 0], [0, 1], [0,0]]))\n", "intent": "Check the model output. It should give something close to [0, 1, 1, 0].\n"}
{"snippet": "predict = logmod.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "proba = final_model.predict_proba(X_test, batch_size=32)  \nclasses = np.argmax(proba, axis=1)\nprint(\"Test set performance: {}\".format(accuracy_score(y_test, classes)))\n", "intent": "Notice that we haven't touched X_test and y_test until now, and we ONLY use them once.\n"}
{"snippet": "pred = classifier.predict(X_test)\nprint(classification_report(y_test,pred))\nprint(confusion_matrix(y_test,pred))\nprint('Presicion:',accuracy_score(y_test,pred))\ncm=confusion_matrix(y_test,pred)\nsns.heatmap(cm,cmap='viridis')\n", "intent": "Now we check with the test set and evaluate the accuracy and the confusion matrix.\n"}
{"snippet": "from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nestimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, encoded_Y, cv = kfold)\nprint(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n", "intent": "Running this code produces the following output showing the mean and standard deviation of the estimated accuracy of the model on unseen data.\n"}
{"snippet": "r2 = cross_val_score(nbrs, features, target)\nr2.mean()\n", "intent": "Using cross_val_score, test our model against all of the features and labels. How did it do?\n"}
{"snippet": "target_pred = lr.predict(features_test)\nzip(target_pred, target_test)\n", "intent": "Use predict to get our predicted values\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(target_test, target_pred))\n", "intent": "Calculate the root mean squared error \n"}
{"snippet": "X_new = np.array([[0],[1],[2],[2.25],[2.6],[3]]) \ny_pred_new = lin_reg.predict(X_new)\ny_pred_new\n", "intent": "To predict the value for a new instance of feature X, all we need to do is this. (See below).\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        A = style_targets[i]\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(G - A))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(\"f1 score  most frequent: %.2f \" %f1_score(y_test,pred_most_frequent))\nprint(\"f1 score dummy: %.2f\" %f1_score(y_test,pred_dummy))\nprint(\"f1 score tree: %.2f\" %f1_score(y_test,pred_tree))\nprint(\"f1 score logistic regression: %.2f\" %f1_score(y_test,pred_logreg))\n", "intent": "There is a variant called _f1-score_.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred_most_frequent, target_names=[\"not nine\", \"nine\"]))\n", "intent": "It shows much more leverage in between the classifiers, but on the flipside is harder to interpret and explain. \n"}
{"snippet": "print(\"F1 score for SVC: %.2f\" %f1_score(y_test, svc.predict(X_test)))\nprint(\"F1 score for random forest: %.2f\" %f1_score(y_test, rf.predict(X_test)))\n", "intent": "We can see that random forest performs better at the extremes, for very high recall or very high precision requirements \n"}
{"snippet": "def predict(coef, history):\n    yhat = 0.0\n    for i in range(1, len(coef)+1):\n        yhat += coef[i-1] * history[-i]\n    return yhat\n", "intent": "The coefficients of a learned ARIMA model can be accessed from aARIMAResults object as follows:\n- ```model_fit.arparams```\n- ```model_fit.maparams```\n"}
{"snippet": "trainPredict = model.predict(trainX, batch_size=batch_size)\nmodel.reset_states()\ntestPredict = model.predict(testX, batch_size=batch_size)\n", "intent": "```Python\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n```\n"}
{"snippet": "new_data = []\nfor index, row in simulation_df.iterrows():\n    if pd.isnull(row[output_feature]):\n        X = row[input_features]\n        new_data.append(pipeline.predict(X.values.reshape(1,-1))[0])\n    else:\n        new_data.append(row[output_feature])\nsimulation_df[output_feature] = new_data\n", "intent": "Replace null values with model prediction:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(y_test, rfpmodel))\nprint(rms)\n", "intent": "Mean Square Error of 'Log Sales' variable\n"}
{"snippet": "def train_and_evaluate(output_dir, num_train_steps):\n  estimator = tf.estimator.LinearRegressor(model_dir = output_dir, feature_columns = create_feature_cols())\n  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n                                      max_steps = num_train_steps)\n  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n                                    steps = None, \n                                    start_delay_secs = 1, \n                                    throttle_secs = 5)  \n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n", "intent": "Note LinearRegressor default loss: loss is calculated by using mean squared error.\n"}
{"snippet": "client_data = [[5, 17, 15], \n               [4, 32, 22], \n               [8, 3, 12]]  \nfor i, price in enumerate(reg.predict(client_data)):\n    print \"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price)\n", "intent": "We will now use the optimized model to make predictions for each client's home.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(College_data['Cluster'],kmeans.labels_))\nprint(classification_report(College_data['Cluster'],kmeans.labels_))\ntn, fp, fn, tp = confusion_matrix(College_data['Cluster'],kmeans.labels_).ravel()\ntn, fp, fn, tp\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "def better_ensemble_error(num_classifier, base_error):\n    k_start = math.ceil(num_classifier/2)\n    probs = [comb(num_classifier, k)*(base_error**k)*((1-base_error)**(num_classifier-k)) for k in range(k_start, num_classifier+1)]\n    if num_classifier % 2 == 0:\n        probs.append(-0.5*comb(num_classifier, k_start)*(base_error**k_start)*((1-base_error)**(num_classifier-k_start)))\n    return sum(probs)\n", "intent": "Describe a better algorithm for computing the ensemble error.\n"}
{"snippet": "print \"Classification report:\"  \nprint classification_report(y_test, y_hat_CV)\n", "intent": "The CV and Accuracy scores are similar, and higher than the baseline model (0.5). \n"}
{"snippet": "predictions = best_gradientBostting_pca.predict(predict_df)\nscore_df['TARGET'] = pd.Series(map(lambda x: 'Normal' if x == 0 else 'Default', predictions))\nsaveCsv(score_df,'lm_exam01_score_result.csv');\n", "intent": "Predict based on score set and store results \n"}
{"snippet": "scores_dec = cross_val_score(pipeline, X=X_dec, y=y_dec, cv=5)\nprint(\"Scores for the decoding model: %.3f (%.3f)\" % (scores_dec.mean(), scores_dec.std()))\n", "intent": "And let's evaluate the cross-validated prediction accuracy:\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy of the CNN is: %.4f%%' % test_accuracy)  \n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_test, predictions))\n", "intent": "To analyze the performance of classifiers, we can check the precision and recall of each class:\n"}
{"snippet": "ResNet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet_50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "DIV3D_predictions = [np.argmax(DIV3D_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DIV3D]\ntest_accuracy = 100*np.sum(np.array(DIV3D_predictions)==np.argmax(test_targets, axis=1))/len(DIV3D_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    N = len(feats)\n    for i, l in enumerate(style_layers):\n        G = gram_matrix(feats[l], normalize=True)\n        style_loss += style_weights[i] * tf.reduce_sum((G - style_targets[i]) ** 2)\n    return style_loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def evaluator(clf, X, y):\n    y_pred = clf.predict(X)\n    accuracy = metrics.accuracy_score(y, y_pred) \n    print(\"Accuracy: \" +  str(accuracy))\n    print(metrics.classification_report(y, y_pred))\n    print(\"Confusion Matrix\")\n    pd.crosstab(np.array(y), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n    return\n", "intent": "Evaluate the trained classifier \n"}
{"snippet": "scores = cross_val_score(my_super_learner, X_train, y_train, cv=3)\nprint(scores)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(data['Class'],kmeans.labels_))\nprint(classification_report(data['Class'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "def loss(x, y, input_shape):\n    logit = forward(x, input_shape)\n    loss = cross_entropy_loss(labels=y, unscaled_logits=logit)\n    return loss\n", "intent": "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network.\n"}
{"snippet": "scores = []\nfor m in best_models:\n    model_class, _, params, _ = m\n    estimator = model_class(**params)\n    local_scores = cross_val_score(estimator, x, y, cv=20, scoring=\"accuracy\")\n    scores.append((estimator, local_scores.mean(), local_scores.std()))\n", "intent": "cross_val_score performs 20-fold cross-validation on the best models for each classifier type and selects a model with maximum mean score.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n", "intent": "Make predictions using hte trained model\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    loss = 0\n    for i in range(len(style_layers)):\n        ind = style_layers[i]\n        G = gram_matrix(feats[ind])\n        A = style_targets[i]\n        loss += style_weights[i] * tf.reduce_sum((G-A)*(G-A))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "for i in range(train_x.shape[1]):\n    print mean_absolute_error(train_x.ix[:,i],data.loss)\n", "intent": "Printing the loss for each model:\n"}
{"snippet": "ratio = 0.5\nprint mean_absolute_error(ratio*train_x.ix[:,0]+(1-ratio)*train_x.ix[:,7],data.loss)\n", "intent": "Simple Averaging:\n(But previously i was using leaderboard feedback)\n"}
{"snippet": "YPred = knn.predict(XTest)\n", "intent": "Below, a prediction will be run. The prediction will be made using the testing features data.\n"}
{"snippet": "print (metrics.accuracy_score(expected, predicted))\n", "intent": "The print function (with expected versus predicted passed) will be used in order to see the metrics for performance.\n"}
{"snippet": "res=forest.predict(testfeat)\n", "intent": "Prediction of test set\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n", "intent": "Test the pipeline on the training set\n"}
{"snippet": "res_predictions = [np.argmax(model_Arch.predict(np.expand_dims(feature, axis=0))) for feature in test_res50]\nmodel_accuracy = 100*np.sum(np.array(res_predictions)==np.argmax(test_targets, axis=1))/len(res_predictions)\nprint('Model Accuracy: %.4f%%' % model_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "msgs_pred = nb_clf.predict(msgs_tfidf)\n", "intent": "Now that we have a trained classifier, it can be used for prediction. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint 'Accuracy Score: {}'.format(accuracy_score(sms_spam_df['label'], msgs_pred))\n", "intent": "Lets check the accuracy of our classifier.\n"}
{"snippet": "print model.predict(['Hi! How are you?'])[0]\nprint model.predict(['Congratulations! You won free credits!'])[0]\n", "intent": "Now that our model is trained, lets try it out.\n"}
{"snippet": "msgs_pred = model.predict(msgs_test)\nprint 'Accuracy Score: {}'.format(accuracy_score(lbls_test, msgs_pred))\n", "intent": "For more fun, here is the classification report of our model.\n"}
{"snippet": "y1_pred = y1_true + 0.1 * np.random.random(y1_true.shape[0])\ny2_pred = y2_true + 0.5 * np.random.random(y2_true.shape[0])\ny_true = np.vstack((y1_true, y2_true)).T\ny_pred = np.vstack((y1_pred, y2_pred)).T\nprint 'raw values:        ', r2_score(y_true, y_pred, multioutput='raw_values')\nprint 'uniform average:   ', r2_score(y_true, y_pred, multioutput='uniform_average')\nprint 'variance_weighted: ', r2_score(y_true, y_pred, multioutput='variance_weighted')\n", "intent": "Same is done here, but with y2 have more noise than y1\n"}
{"snippet": "y_all = np.hstack((y_true,y_pred))\ny_all_norm = preprocessing.scale(y_all)\ny_true_norm = y_all_norm[:,0:2]\ny_pred_norm = y_all_norm[:,2:4]\nprint 'raw values:        ', r2_score(y_true_norm, y_pred_norm, multioutput='raw_values')\nprint 'uniform average:   ', r2_score(y_true_norm, y_pred_norm, multioutput='uniform_average')\nprint 'variance_weighted: ', r2_score(y_true_norm, y_pred_norm, multioutput='variance_weighted')\n", "intent": "In this part the true outputs and the predictions are normalized by removing the mean and scaling the variance.\n"}
{"snippet": "y_all = np.hstack((y_true,y_pred))\ny_all_norm = preprocessing.normalize(y_all,norm='l2',axis=0)\ny_true_norm = y_all_norm[:,0:2]\ny_pred_norm = y_all_norm[:,2:4]\nprint 'raw values:        ', r2_score(y_true_norm, y_pred_norm, multioutput='raw_values')\nprint 'uniform average:   ', r2_score(y_true_norm, y_pred_norm, multioutput='uniform_average')\nprint 'variance_weighted: ', r2_score(y_true_norm, y_pred_norm, multioutput='variance_weighted')\n", "intent": "Normalizing the outputs to have a unit norm\n"}
{"snippet": "t90 = np.rot90(test)\nt180 = np.rot90(t90)\nt270 = np.rot90(t180)\nprint(\"Prediction: \" + str(net.predict(t270)))\n", "intent": "We expect the perceptron to return [1,0,1] for blue, red, blue\n"}
{"snippet": "y_unknown_labels_svm = rsearch_svm.predict(X_unknown[:,3:])\n", "intent": "- Note that this is still \"32 keV\" data. We'll apply this classifier to the other lines in the next section\n"}
{"snippet": "y_unknown_labels_svm = svc.predict(X_unknown)\n", "intent": "- Unknown Set Prediction\n"}
{"snippet": "y_unknown_labels_nn = mlp.predict(X_unknown)\n", "intent": "- Unknown Set Prediction:\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(featuCre, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "nb_predictions = nb_model.predict(df_dev_features)\nnb_predictions[0]\n", "intent": "The predict function does the classification.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(df_dev.Product, nb_predictions)\n", "intent": "We'll use a `Scikit-Learn` function to calculate the accuracy.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(df_dev.Product, nb_predictions, target_names=class_labels))\n", "intent": "Run the full report\n"}
{"snippet": "nb_predictions = nb_model.predict(arr_dev_feature)\nnb_predictions[0]\n", "intent": "We can easily predict the labels using our new model and the feature vectors from the dev set.\n"}
{"snippet": "accuracy_score(df_dev.Product, nb_predictions)\n", "intent": "This function returns the accuracy of our Naive Bayes model.\n"}
{"snippet": "nb_predictions_probs = nb_model.predict_proba(arr_dev_feature)\nnb_predictions_probs.shape\n", "intent": "Instead of looking at the class labels, let's look at the probability of predicting each class.\n"}
{"snippet": "logreg_predictions_probs = logreg_model.predict_proba(arr_dev_feature)\n", "intent": "Let's compare the probabilities for the first observation in the dev set between logistic regression and naive bayes.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"cars.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "print clf.predict([7.9,3.05,6.9,2.5])\n", "intent": "Let us predict to which class does the flower with sepal legth of 7.9 cm , sepal width of 3.05 cm , petal length of 6.9 cm and petal width of 2.5 cm\n"}
{"snippet": "rescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(predictions)\nprint(mean_squared_error(Y_validation, predictions))\n", "intent": "Now scale the validation dataset inputs and generate predictions.\n"}
{"snippet": "predictions = model.predict(input_fn=predict_input_fn)\n", "intent": "Now we are happy with the accuracy of our model we can do a couple of predictions. \n"}
{"snippet": "scores = defaultdict(dict)\nfor tissue in multi_type_tissues:\n    for clf_name, clf in sorted(multicore_classifiers.iteritems()):\n        scores[tissue][clf_name] = np.mean(train_and_score(clf, norm_df, tissue, y_class='type', scaler=MinMaxScaler))\n", "intent": "Score multi-type tissues with a set of classifiers to see relative performance\n"}
{"snippet": "test_res = best_classifier.predict(test_vectors)\n", "intent": "At the end we make a prediction on test data using our best classifier. We see that the accuracy is 67.5% as expected.\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(\n            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\n    print(d_loss_true, d_loss)\n    print(g_loss_true, g_loss)\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output= sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    total_loss = tf.zeros([1])\n    for idx, i in zip(style_layers, range(len(style_layers))):\n        gram = gram_matrix(feats[idx])\n        loss = 2 * style_weights[i] * tf.nn.l2_loss(gram - style_targets[i])\n        total_loss += loss\n    return total_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores1 = cross_val_score(clf_1, student_data[['G1','G2']], student_data[['G3']], cv=5)\nscores2 = cross_val_score(clf_2, student_data[['age','Fedu','failures','activities','goout','higher','G1','G2']], \n                          student_data[['G3']], cv=5)\nprint('Cross validation score for the first model:\\n',np.mean(scores1),'\\n')\nprint('Cross validation score for the second model:\\n',np.mean(scores2),'\\n')\n", "intent": "We are using f test and cross validation here\n"}
{"snippet": "predictions = [np.argmax(resnet50_Model.predict(np.expand_dims(feature, axis = 0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(predictions) == np.argmax(test_targets, axis =1))/len(predictions)\nprint('Test accuracy : %.4f%%' %test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_test_0 = svm_model.predict(X_test[0].reshape(1, -1))\nprint(\"predicted: \", int(predicted_test_0), \"true value: \", y_test[0])\n", "intent": "SVM model is created. We can immediately check what will be the prediction of the model for the first digit in the test data.\n"}
{"snippet": "predictions = [int(a) for a in svm_model.predict(X_test)]\nnum_correct = sum(int(a == y) for a, y in zip(predictions, y_test))\nprint(\"SVM classifier (trained with subset of training data): \")\nprint(\"%s of %s values correct.\" % (num_correct, len(y_test)))\n", "intent": "Performance on the test data should be increased to around 94 %. \n"}
{"snippet": "import pickle\ns = pickle.dumps(clf)\nwith open(b\"digits.model.obj\", \"wb\") as f:\n    pickle.dump(clf, f)\nclf2 = pickle.loads(s)\nclf2.predict(digits.data[0:1])\n", "intent": "persist the model using pickle and load it again to ensure it works\n"}
{"snippet": "acc = mt.accuracy_score(Y_test,y_hat)\nconf = mt.confusion_matrix(Y_test,y_hat)\nprint('accuracy:', acc )\nprint(conf)\n", "intent": "*Assess how well the model performs*\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    layer_num = len(style_layers)\n    for i in range(layer_num):\n        feats_i = feats[style_layers[i]]\n        G, A = gram_matrix(feats_i), style_targets[i]\n        loss_i = style_weights[i] * tf.reduce_sum(tf.square(tf.subtract(G, A)))\n        style_loss += loss_i\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "rf_preds = rftree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, rf_preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "preds = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "preds = nb.predict(X_test)\nprint(preds)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "preds2 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "trainer.history.plot_loss()\n", "intent": "How have we been doing so far?\n"}
{"snippet": "trainer.history.plot_loss()\n", "intent": "Plot the loss and accuracy curves.\n"}
{"snippet": "y_pred = predict(net, predict_on_batch, cifar10.test_loader, verbose=True)\n", "intent": "Run prediction over the test set:\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\nprint metrics.accuracy_score(y_test, predicted) \n", "intent": "The accuracy of the model on test set is calculated below.\n"}
{"snippet": "from sklearn import metrics\nprint(\"accuracy: \"+str(metrics.accuracy_score(test_Y, prediction)))\nthe_most_outcome = pima_df['Outcome'].median()\nprediction2 = [the_most_outcome for i in range(len(test_Y))]\nprint(\"null accuracy: \"+str(metrics.accuracy_score(test_Y, prediction2)))\n", "intent": "**null accuracy**\nthe accuracy that could be achieved by always predicting the most frequent class.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm_lr = confusion_matrix(y_validation, clf_train_lr.predict(X_validation))\n", "intent": "Confusion matrix for logistic regression:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(\"Classification report for the logistic model\")\nprint(classification_report(y_validation, clf_train_lr.predict(X_validation), target_names=['unsuccessful', 'successful']))\n", "intent": "We can also do a classification report on the logistic model. \n"}
{"snippet": "cm_rf = confusion_matrix(y_validation, clf_train_rf.predict(X_validation))\nTP = cm_rf[0][0]\nFN = cm_rf[0][1]\nFP = cm_rf[1][0]\nTN = cm_rf[1][1]\nprint(\"Confusion matrix for Random Forest\")\ncm_rf\n", "intent": "Let's consider confusion matrix for the Random Forest now.\n"}
{"snippet": "print(classification_report(y_test, clf_final_training_lr.predict(X_test), target_names=['unsuccessful', 'successful']))\n", "intent": "Let's create a classification report to analyze full results:\n"}
{"snippet": "print(classification_report(y_test, clf_final_training_rf.predict(X_test), target_names=['unsuccessful', 'successful']))\n", "intent": "Let's create a classification report to analyze full results:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(random_forest, X, y, cv=10)\n", "intent": "Here, we validated accuracy using test / train split. However, we very often use cross-validation for that purpose.\n"}
{"snippet": "tweet_pred = clf.predict(tweet_tfidf)\n", "intent": "Now, we make predictions using our already-trained classifier.\n"}
{"snippet": "model.predict(new_observations)\n", "intent": "The `model.predict` function can take these values and ask the model to predict the class.\n"}
{"snippet": "plot_decision(model, new_observations, model.predict(new_observations))\n", "intent": "We can look at those values on the plot (zoomed in slightly):\n"}
{"snippet": "def predict_category(s):\n    pred = model.predict([s])\n    return data.target_names[pred[0]]\n", "intent": "We can also just test a few sample inputs and see if we get sensible results predicted:\n"}
{"snippet": "labels = model.predict(test.data) \n", "intent": "Now that we have trained the model, we can use it to predict the classes of the test data!\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"giraffe.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def evaluate(est, X, y):\n    f1_pos = make_scorer(f1_score, labels=[1])\n    prec_pos = make_scorer(precision_score, labels=[1])\n    rec_pos = make_scorer(recall_score, labels=[1])\n    return cross_validate(pipe, X, y, \n                          scoring={'f1_pos': f1_pos, 'prec_pos': prec_pos, 'rec_pos': rec_pos},\n                          return_train_score=True)\n", "intent": "This function do cross validation with 3 folds, and evaluates f1, prec, rec for positive class\n"}
{"snippet": "print('Accuracy:', accuracy_score(y_test_senti, predictions_senti))\nprint()\nprint('Precision weighted:', precision_score(y_test_senti, predictions_senti, average='weighted'))\nprint('Recall weighted:', recall_score(y_test_senti, predictions_senti, average='weighted'))\nprint('F1 score weighted:', f1_score(y_test_senti, predictions_senti, average='weighted'))\nprint()\nprint('Precision macro:', precision_score(y_test_senti, predictions_senti, average='macro'))\nprint('Recall macro:', recall_score(y_test_senti, predictions_senti, average='macro'))\nprint('F1 score macro:', f1_score(y_test_senti, predictions_senti, average='macro'))\n", "intent": "Let's make the same model's evaluation pipline as it was for simple model\n"}
{"snippet": "from sklearn.metrics import roc_auc_score, recall_score, auc, make_scorer\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.metrics import average_precision_score\nprint('ROC AUC:', roc_auc_score(df.y, df.yhat))\nprint('Avg Precision:', average_precision_score(df.y,df.yhat))\n", "intent": "Let's confirm the metrics are actually the same. Hmm.... They are not same, but similar.\n"}
{"snippet": "accuracy_score = cross_val_score(gnb_clf,X_data,y_target,cv=10)\nprecision_score = cross_val_score(gnb_clf,X_data,y_target,cv=10,scoring='precision')\nrecall_score = cross_val_score(gnb_clf,X_data,y_target,cv=10,scoring='recall')\nprint(\"GaussianNB  Accuracy: %0.3f (+/- %0.3f)\"%(accuracy_score.mean(),accuracy_score.std()*2))\nprint(\"GaussianNB  Precision: %0.3f (+/- %0.3f)\"%(precision_score.mean(),precision_score.std()*2))\nprint(\"GaussianNB  Recall: %0.3f (+/- %0.3f)\"%(recall_score.mean(),recall_score.std()*2))\n", "intent": "The 10-fold cross-validation accuracy, precision, and recall\n"}
{"snippet": "y_lr_predict = cross_val_predict(lr_clf,X_data,y_target_violentCrimesPerPop,cv=10)\npt_x1 = mean_squared_error(y_target_violentCrimesPerPop,y_lr_predict)\nprint(\"The estimated mean-squared-error (MSE): %f \"%(pt_x1))\n", "intent": "Using 10-fold cross-validation, the estimated mean-squared-error (MSE)\n"}
{"snippet": "test_data = [6]\ntest_data = np.asarray(test_data, dtype=np.float32)\neval_input_fn = tf.estimator.inputs.numpy_input_fn( x={\"x\":test_data}, num_epochs=1, shuffle=False )\neval_results = estimator.predict(input_fn=eval_input_fn)\nprint(list(eval_results))\n", "intent": "Now the model is trained, lets use it. Type in a number below to find which class it belongs to...\n"}
{"snippet": "test_data = [2]\ntest_data = np.asarray(test_data, dtype=np.float32)\neval_input_fn = tf.estimator.inputs.numpy_input_fn( x={\"x\":test_data}, num_epochs=1, shuffle=False )\neval_results = estimator.predict(input_fn=eval_input_fn)\nres = next(eval_results)\nprint(\"The number \"+str(test_data[0])+\" is \"+(\"greater than 4\" if res[\"classes\"]==1 else \"less than 5\"))\n", "intent": "You could also say:\n"}
{"snippet": "def gram_matrix_test(correct):\n    print(style_img_test.shape)\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for i in range(len(style_layers)):\n        ori_matrix = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * (tf.reduce_sum(tf.square(ori_matrix - style_targets[i])))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_test_proba = best_model.predict_proba(X_test)[:, 1]\ny_test_pred = best_model.predict(X_test)\n", "intent": "Now let's look at our new performance after using SMOTE:\n"}
{"snippet": "logistic_regression_report = classification_report(y_test, y_pred_test, target_names='class')\nprint(logistic_regression_report)\n", "intent": "Not bad... Let's take a look at precision, recall and F1-score:\n"}
{"snippet": "loss = tf.nn.l2_loss(tf.subtract(y, y_predicted), name='loss')\n", "intent": "Create the loss function, named as *loss*.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for layer, target, weight in zip(style_layers, style_targets, style_weights):\n        op = weight * tf.reduce_sum(tf.square(gram_matrix(feats[layer]) - target))\n        style_loss += op\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "RFC_preds = RFC.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,RFC_preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(data['Cluster'],model.labels_))\nprint('\\n')\nprint(classification_report(data['Cluster'],model.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pipe_preds = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "XCeption_predictions = [np.argmax(XCeption_model.predict(np.expand_dims(feature, axis=0))) for feature in test_XCeption]\ntest_accuracy = 100*np.sum(np.array(XCeption_predictions)==np.argmax(test_targets, axis=1))/len(XCeption_predictions)\nprint('Test accuracy (XCeption): %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "last_in_train = int(n*0.7)\ntraining_data = X[range(0,last_in_train)]\ntraining_labels = y[range(0,last_in_train)]\ntst_data  = X[range(last_in_train,n)]\ntst_labels = y[range(last_in_train,n)]\nmus,sigmas = GNB_fit(training_data,training_labels)\ny_hat = GNB_predict(mus,sigmas,tst_data,True)\na_score = GNB_score(y_hat,tst_labels)\nprint(\"the score of the model fitted in percentage is: \",a_score * 100)\n", "intent": "- Fit a Gaussian naive-Bayes model to 70% of the data, and test the model on the rest (30%) of it. \n(You should get an accuracy of more than 90%...)\n"}
{"snippet": "print 'Predicting...'\noutput = pla.predict(test_data).astype(int)\n", "intent": "3. Predict the TEST dataset\n- Make prediction on the \"test.csv\" dataset\n- Output prediction outcomes to a new file \"titanic_pla.csv\"\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\nscores = cross_val_score(estimator=model, X=X, y=y, cv=cv)\nprint(\"Test  r2:%.2f\" % scores.mean())\n", "intent": "Scikit-learn provides user-friendly function to perform CV:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nscores.mean()\ndef balanced_acc(estimator, X, y):\n    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\nscores = cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring=balanced_acc)\nprint(\"Test  ACC:%.2f\" % scores.mean())\n", "intent": "Scikit-learn provides user-friendly function to perform CV:\n"}
{"snippet": "print cross_val_score(model, standardised_merged_table[\"murder\"].reshape(-1, 1), y, scoring=\"r2\", cv=5)\nprint np.mean(cross_val_score(model, standardised_merged_table[\"murder\"].reshape(-1, 1), y, scoring=\"r2\", cv=5))\n", "intent": "_With the total of murders_\n"}
{"snippet": "print cross_val_score(model, standardised_merged_table[\"agg_assault\"].reshape(-1, 1), y, scoring=\"r2\", cv=5)\nprint np.mean(cross_val_score(model, standardised_merged_table[\"agg_assault\"].reshape(-1, 1), y, scoring=\"r2\", cv=5))\n", "intent": "_With the total of aggravated assaults_\n"}
{"snippet": "print cross_val_score(model, standardised_merged_table[\"robbery\"].reshape(-1, 1), y, scoring=\"r2\", cv=5)\nprint np.mean(cross_val_score(model, standardised_merged_table[\"robbery\"].reshape(-1, 1), y, scoring=\"r2\", cv=5))\n", "intent": "_With the total of murders_\n"}
{"snippet": "predictions = lr.predict(titanic_test[predictors])\n", "intent": "Make predictions using the test set\n"}
{"snippet": "from basic_model_tf import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\nclass supervised_training:\n    input_sequence = tf.placeholder('int32',[None,None])\n    reference_answers = tf.placeholder('int32',[None,None])\n    logprobs_seq = model.symbolic_score(input_sequence, reference_answers)\n    crossentropy = - select_values_over_last_axis(logprobs_seq,reference_answers)\n    mask = infer_mask(reference_answers, out_voc.eos_ix)\n    loss = tf.reduce_sum(crossentropy * mask)/tf.reduce_sum(mask)\n    train_step = tf.train.AdamOptimizer().minimize(loss, var_list = model.weights)\ninitialize_uninitialized(s)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess,  \"0086.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "preds = model.predict(img)\nprint('Predicted:', decode_predictions(preds, top=3))\n", "intent": "Now, let's call the model on our image to predict the top 3 labels.\n"}
{"snippet": "preds = model.predict(img)\n", "intent": "Now, let's call the model on our image to predict the top 3 labels.\n"}
{"snippet": "params = best_net[\"params\"]\ntest_acc = (predict(params, X_test) == y_test).mean()\nprint('Test accuracy: ', test_acc)\n", "intent": "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.\n"}
{"snippet": "np.random.seed(1)\nbase = np.random.randn(10,10,3)\noutput = np.random.randn(10,10,3)\na = K.constant(base)\nb = K.constant(output)\ntest = feature_reconstruction_loss(a, b)\nprint('Result:          ', K.eval(test))\nprint('Expected result: ', 605.62195)\n", "intent": "Test your implementation:\n"}
{"snippet": "np.random.seed(1)\nx = np.random.randn(10,10,3)\ny = np.random.randn(10,10,3)\na = K.constant(x)\nb = K.constant(y)\ntest = style_reconstruction_loss(a, b)\nprint('Result:  ', K.eval(test))\nprint('Expected:', 0.09799164)\n", "intent": "Test your implementation:\n"}
{"snippet": "np.random.seed(1)\nx_np = np.random.randn(1,10,10,3)\nx = K.constant(x_np)\ntest = total_variation_loss(x)\nprint('Result:  ', K.eval(test))\nprint('Expected:', 937.0538)\n", "intent": "Test your implementation:\n"}
{"snippet": "log_reg_predict = log_reg.predict(X)\n", "intent": "Now explicitly store the predictions using `X` in `log_reg_predict`, so we can calculate the accuracy.`m\n"}
{"snippet": "def get_residual_sum_of_squares(model, features, outcomes):\n    predictions = model.predict(features)\n    residuals = outcomes - predictions\n    return sum(residuals * residuals)\n", "intent": "Now that we can make predictions given the model, let's write a function to compute the RSS of the model.\n"}
{"snippet": "def get_classification_accuracy(model, data, true_sentiments):\n    predictions = model.predict(data)\n    correct_count = np.sum(predictions == true_sentiments)\n    return correct_count / len(predictions)\n", "intent": "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by:\n$$\\mbox{accuracy} = \\frac{\\mbox{\n"}
{"snippet": "def compute_accuracy(model, data):\n    predictions = model.predict(data[features])\n    actual = data[target].values\n    num_correct = np.sum(predictions == actual)\n    accuracy = num_correct / len(actual)\n    return accuracy\n", "intent": "Recall that accuracy is defined as follows:\n$$\n\\mbox{accuracy} = \\frac{\\mbox{\n$$\n"}
{"snippet": "model_5_predictions = model_5.predict(sample_validation_data[features])\nmodel_5_predictions\n", "intent": "For each row in the **sample_validation_data**, write code to make **model_5** predict whether or not the loan is classified as a **safe loans**.\n"}
{"snippet": "model_5_probabilities = model_5.predict_proba(sample_validation_data[features])\nmodel_5_probabilities\n", "intent": "For each row in the **sample_validation_data**, what is the probability (according to **model_5**) of a loan being classified as **safe**?\n"}
{"snippet": "model_5_predictions = model_5.predict(validation_data[features])\nmodel_5_actual = validation_data[target].values\nmodel_5_num_correct = np.sum(model_5_predictions == model_5_actual)\nmodel_5_num_total = len(validation_data)\nmodel_5_accuracy = model_5_num_correct / float(model_5_num_total)\nmodel_5_accuracy\n", "intent": "The accuracy is defined as follows:\n$$\n\\mbox{accuracy} = \\frac{\\mbox{\n$$\nEvaluate the accuracy of the **model_5** on the **validation_data**.\n"}
{"snippet": "X = df[features].values\ny = np.exp(df[target].values)\npred = np.exp(best_model.predict(X))\nsq_error = (y - pred) ** 2\nst_error = np.sqrt(np.sum(sq_error) / (len(y) - 1))\n", "intent": "Now that we have a pretty good model we can use it as our \"best fit line\" as before and then build Junky, OK, Good, and Great ranges off of it.\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Evaluate model pipeline on test data\n"}
{"snippet": "prediction=lm.predict(X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "mse2 = np.abs(cross_val_score(sk_m1,d[\"Inv_MAPE\"].reshape(1484,1),d[\"Return_10_fwd\"], scoring=\"neg_mean_squared_error\",cv=5))\nmse2.mean()\n", "intent": "b. What is the CV MSE of this model? How does it compare to the previous one?\n"}
{"snippet": "print(\"\\tCVRMSE \\t\\t R^2 \\t\\tNMBE\")\nprint(\"MTY:\\t\" + \"{:.3f}\".format(CVRMSE(jimnies.Price,res_MTY.predict(X_MTY)))+\"\\t\\t {:.3f}\".format(res_MTY.rsquared)+\"\\t\\t {:.3f}\".format(NMBE(jimnies.Price,res_MTY.predict(X_MTY))))\nprint(\"MT:\\t\" + \"{:.3f}\".format(CVRMSE(jimnies.Price,res_MT.predict(X_MT)))+\"\\t\\t {:.3f}\".format(res_MT.rsquared)+ \"\\t\\t {:.3f}\".format(NMBE(jimnies.Price,res_MT.predict(X_MT))))\nprint(\"TY:\\t\" + \"{:.3f}\".format(CVRMSE(jimnies.Price,res_TY.predict(X_TY)))+\"\\t\\t {:.3f}\".format(res_TY.rsquared)+ \"\\t\\t {:.3f}\".format(NMBE(jimnies.Price,res_TY.predict(X_TY))))\n", "intent": "Instead of using $R^{2}$ only, let's investigate CVRMSE and NMBE as well:\n"}
{"snippet": "forecast_sklearn=est.predict(X_building2015)\nTemp2015['forecast_sklearn']='' \nk=0 \nfor i in range (0,np.size(Temp2015.Temp)):\n    if pd.notnull(Temp2015.Temp[i]): \n        Temp2015.ix[i,'forecast_sklearn']=forecast_sklearn[k][0] \n        k+=1 \n    else:\n        Temp2015.ix[i,'forecast_sklearn']=np.NaN \nprint(\"CVRMSE:\\t\" + \"{:.3f}\".format(CVRMSE(energy2015.kW,Temp2015['forecast_sklearn']))+\"\\t NMBE:\" +\"{:.3f}\".format(NMBE(energy2015.kW,Temp2015['forecast_sklearn'])))\n", "intent": "This is done for illusration only, and doesn't add new information to the `statsmodels` forecast above.\n"}
{"snippet": "1 - metrics.accuracy_score(y_test, y_predict_class)\n", "intent": "**Metric 4: Mis - Classification Rate**\nThis is equal to the FP + FN / total observations\n"}
{"snippet": "metrics.precision_score(y_test, y_predict_class)\n", "intent": "**Metric 6: Precision**\nWhen a positive instance is predicted, how often is the prediction correct\n"}
{"snippet": "LRmodel.predict(X_test)[0:10]\n", "intent": "**Improving Performance: Changing the Classification Threshold to assist the performance model**\n"}
{"snippet": "metrics.recall_score(y_test, y_pred_class)\n", "intent": "**Sensitivity/ Recall (New)**\n"}
{"snippet": "def cv_rmse(model, X, y, cv=5, scoring='neg_mean_squared_error'):\n    cv_dict = {}\n    cvs = np.sqrt(-cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error'))\n    cv_dict['cv_mean'] = np.mean(cvs)\n    cv_dict['cvs'] = cvs\n    return cv_dict\n", "intent": "Function to cross-validate models\n"}
{"snippet": "confusion_mat = confusion_matrix(y_test, svc_rbf.predict(X_test))\nconfusion_mat\n", "intent": "Below we will create the confusion matrix generated by the rbf kernel svc model above.\n"}
{"snippet": "def make_prediction(clf, midi_path):\n    features = get_features(midi_path)\n    prediction_ind = list(clf.predict([features])[0]).index(1)\n    prediction = label_list[prediction_ind]\n    return prediction\ntest_midi_path =\"lmd_matched/B/F/E/TRBFELB128F426BFF2/289270d85c81802d912c9907c645dc2d.mid\"\nprint(make_prediction(classifier, test_midi_path))\n", "intent": "<a id=\"step-8\"></a>\nIn this step we will make a prediction on a midi file and print out the genre that\nthe classifier predicts!\n"}
{"snippet": "x_new = 1     \nlr_model.predict(x_new)\n", "intent": "<div style=\"text-align:right;font-size:80%\">[Back to contents](\n"}
{"snippet": "pred = clf.predict([[-0.8, -1]]) \nprint(pred)    \n", "intent": "X is the features, Y is the labels\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Score:', accuracy_score(labels_test, pred))\n", "intent": "Or the method below, which requires another package\n"}
{"snippet": "import numpy as np\nprint('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(x_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(x_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(regr, X_train, y_train, cv=5)\nprint(scores)\nprint(\"According to the cross valdiation score, the model is robust.\")\n", "intent": "**6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).**\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n", "intent": "Let's try it out on the training set:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's use [RMSE (Root Mean Squared Error)](http://www.statisticshowto.com/rmse/) to judge the quality of our predictions:\n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Let's try it out on the training set:\n"}
{"snippet": "logodds = logreg.intercept_ + logreg.coef_[0] * 2\nodds = np.exp(logodds)\nprob = odds / (1 + odds)\nprint(prob)\nlogreg.predict_proba(2)[:, 1]\n", "intent": "We can check logistic regression's coefficient does in fact generate the log-odds.\n"}
{"snippet": "y_pred = model.predict(X)\ncm = confusion_matrix(Y, y_pred)\nprint(cm)\nplot_confusion_matrix(cm, ['0', '1'], title = 'Confusion Matrix- Test Data')\npr, tpr, acc = show_data(cm, print_res = 1);\n", "intent": "Applying the model on test data and ploting the congusion matrix. \n"}
{"snippet": "y_pred = model.predict(X_fraud)\ncm = confusion_matrix(Y_fraud, y_pred)\nprint(cm)\nplot_confusion_matrix(cm, ['0', '1'], title = 'Confusion Matrix- Fraud Data')\npr, tpr, acc = show_data(cm, print_res = 1);\n", "intent": "Applying the model on fraud data only and ploting the congusion matrix. \n"}
{"snippet": "y_pred = model.predict(df[df.columns[0:30]])\ncm = confusion_matrix(df[df.columns[30]], y_pred)\nprint(cm)\nplot_confusion_matrix(cm, ['0', '1'], title = 'Confusion Matrix- Whole Data')\npr, tpr, acc = show_data(cm, print_res = 1);\n", "intent": "Applying the model on entire dataset and plotting the confusion matrix. \n"}
{"snippet": "pred = clf.predict(data_test_X)\ncm = confusion_matrix(data_test_y.values.ravel(), pred)\nprint(cm)\nplot_confusion_matrix(cm, ['0', '1'], title = 'Confusion Matrix- Test Data')\npr, tpr = show_data(cm, print_res = 1);\n", "intent": "Applying the model on test data.\n"}
{"snippet": "pred = clf.predict(X)\ncm = confusion_matrix(y, pred)\nprint(cm)\nplot_confusion_matrix(cm, ['0', '1'], title = 'Confusion Matrix- Entire Dataset')\npr, tpr = show_data(cm, print_res = 1);\n", "intent": "Applying the model on the entire dataset. \n"}
{"snippet": "pred = clf.predict(x_fraud_record)\ncm = confusion_matrix(y_fraud_record, pred)\nprint(cm)\nplot_confusion_matrix(cm, ['0', '1'], title = 'Confusion Matrix- Only fraud records')\npr, tpr = show_data(cm, print_res = 1);\n", "intent": "Applying the model on only the fraud data instances. \n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "Assess performance using Precision, Recall and F1 score metrics...\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "Predict labels with test data\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resNet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "answer = cross_val_score(Tree_clf, x_imp, y_2_new, scoring = \"accuracy\", cv = 10)\nprint(answer)\nprint(\"The average of accuracy:\", sum(answer)/len(answer))\n", "intent": "The CV results for Dirty data\n"}
{"snippet": "answer = cross_val_score(B_clf, x_imp, y_2_new, scoring = \"accuracy\", cv = 10)\nprint(answer)\nprint(\"The average of accuracy:\", sum(answer)/len(answer))\n", "intent": "The CV results for dirty data\n"}
{"snippet": "answer = cross_val_score(G_clf, x_imp, y_2_new, scoring = \"accuracy\", cv = 10)\nprint(answer)\nprint(\"The average of accuracy:\", sum(answer)/len(answer))\n", "intent": "The CV results for clean data\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint(\"Baseline Error: %.2f%%\" % (100-score[1]*100))\n", "intent": "We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same test data.\n"}
{"snippet": "nn_pred_test = nn.predict(X_test)\n", "intent": "Let's get a taste of our trained neural net.\n"}
{"snippet": "no_iterations = [50, 120, 200, 300, 400]\nfor n_iter in no_iterations:\n    ann_test = ANN(2,5,1)\n    ann_test.train(zip(X,y),n_iter,0.5,0.1,show_error= False)\n    predictions = ann_test.predict(X, 0.5)\n    accuracy = ann_test.accuracy(predictions, y)\n    print 'Training accuracy with %d iterations %f' % (n_iter,accuracy)\n", "intent": "Lets now also play with the number of iterations for a network with 5 hidden units and see its impact on accuracy.\n"}
{"snippet": "y_predict = forest.predict(X_bow_test)\n", "intent": "This is treated as a simple classification task, where label 1 represents English and 0 represents French.\n"}
{"snippet": "print (\"Predictions have an accuracy of {:.2f}%.\".format(accuracy_score(outcomes, predictions_final)*100))\nprint (\"Predictions have an recall score equal to {:.2f}%.\".format(recall_score(outcomes, \n                                                                                predictions_final)*100))\nprint (\"Predictions have an precision score equal to {:.2f}%.\".format(precision_score(outcomes,\n                                                                                      predictions_final)*100))\n", "intent": "Let's evaluate the quality of the prediction.\n"}
{"snippet": "trainScore = model.evaluate(trainX, trainY, verbose=0)\nprint('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = model.evaluate(testX, testY, verbose=0)\nprint('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "Estimate the performance of the model on the train and test datasets\n"}
{"snippet": "trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n", "intent": "On both train and test, predictions using the built model\n"}
{"snippet": "trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n", "intent": "Estimate the performance of the model on the train and test datasets. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is a more generalized model for the data. The resulting loss in accuracy has to do with the cross validation methods used.\n"}
{"snippet": "model.load_weights(models_filenames[0])\nyPreds = model.predict(testX, batch_size=128)\nyPred = np.argmax(yPreds, axis=1)\nyTrue = testY\naccuracy = metrics.accuracy_score(yTrue, yPred) * 100\nerror = 100 - accuracy\nprint(\"Accuracy : \", accuracy)\nprint(\"Error : \", error)\n", "intent": "We can load the weights of the single best model and make predictions\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "With this model, the test accuracy reaches over 79%!\n"}
{"snippet": "predict = model5.predict(df2)\nmean_squared_error(df1['ViolentCrimesPerPop'],predict)\n", "intent": "What is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "mse = cross_val_score(clf_poly,df2,df1['ViolentCrimesPerPop'], scoring = 'neg_mean_squared_error' ,cv=10)\nmse.mean()\n", "intent": "What is the estimated MSE of the model under 10-fold CV?\n"}
{"snippet": "mean_square_error = mean_squared_error(labelReg_y,y_pred)\nmean_square_error\n", "intent": "What is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    s_loss = 0\n    num_features = len(style_layers)\n    for i in range(num_features):\n        G = style_targets[i]\n        A = gram_matrix(feats[style_layers[i]])\n        s_loss += style_weights[i] * torch.sum((G - A) ** 2)\n    return s_loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    s_loss = 0\n    num_features = len(style_layers)\n    for i in range(num_features):\n        G = style_targets[i]\n        A = gram_matrix(feats[style_layers[i]])\n        s_loss += style_weights[i] * tf.reduce_sum(tf.squared_difference(G, A))\n    return s_loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(ytest, ypred)\n", "intent": "We can take a look at the accuracy on the test data:\n"}
{"snippet": "from  sklearn.metrics import mean_absolute_error\nmae_train = mean_absolute_error(pred_train, Y_train)\nprint(\"The mean absolute error on the training data is %f stars\" % mae_train)\n", "intent": "so looks like the above example simply didnt have enough \"Positive\" words. so \"no negative words\" didnt help improve the rating\n"}
{"snippet": "y_pred_NB = classifierNB.predict(X_test)\n", "intent": "Predicting the Test set results for Naive Bayes\n"}
{"snippet": "y_pred_KNN = classifierKNN.predict(X_test)\n", "intent": "Predicting the Test set results for KNN\n"}
{"snippet": "y_pred_SVM = classifierSVM.predict(X_test)\n", "intent": "Predicting the Test set results\n"}
{"snippet": "y_pred_RF = classifierRF.predict(X_test)\n", "intent": "Predicting the Test set results\n"}
{"snippet": "vgg19_predictions = [np.argmax(vgg19_model.predict(np.expand_dims(feature, axis=0)))\n                         for feature in test_vgg19]\ntest_accuracy = 100*np.sum(np.array(vgg19_predictions)==np.argmax(test_targets, axis=1))/ \\\n                    len(vgg19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredicted_home_prices = melbourne_model.predict(X)\naccuracy_score(y, predicted_home_prices)\n", "intent": "http://scikit-learn.org/stable/modules/classes.html\n"}
{"snippet": "model_predict = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "spotted_number = 5\nsingle_spotted = tfidf_transformer.transform(count_transformer.transform([spotteds.iloc[spotted_number]['message']]))\nprint(spotteds.iloc[spotted_number]['message'])\nprint('predicted:', approve_predictor.predict(single_spotted)[0])\nprint('expected:', spotteds.iloc[spotted_number]['reason'])\n", "intent": "Let's classify some random messages\n"}
{"snippet": "all_predictions = approve_predictor.predict(norm_bag_o_words)\nprint(all_predictions)\n", "intent": "Success!\nNow let's generate predictions for all spotteds\n"}
{"snippet": "x_test = np.array(['whos got two thumbs and loves coding this guy '])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "y_pred = model.predict(X_test.as_matrix())\ny_pred\n", "intent": "Let's predict on X_test\n"}
{"snippet": "accuracy_score(y_test,predicted_classes)\n", "intent": "Now we can compare the two arrays\n"}
{"snippet": "predictions = lm.predict(x_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "from sklearn.externals import joblib\nmodel = joblib.load('data/myLinReg.pkl')\ny = model.predict(X)\ny_min_max_scaler = joblib.load('data/y_min_max_scaler.pkl')\ny_scaled = (y / y_min_max_scaler.scale_) + y_min_max_scaler.min_\ny_scaled\n", "intent": "- 0 - means a lot of water\n- 1 - is medium\n- 2 - device should stay off\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, knn.predict(X)))\nprint(metrics.accuracy_score(y, logrg.predict(X)))\n", "intent": "We will use classication accuracy method\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(x_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(x_test, y_test))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "gold_raw = lz_article_loader(\"corpus18000.pickle\")\ngold = [pars for [(title, pars)] in gold_raw]\npred_raw = lz_article_loader(\"corpus18000_seg.pickle\")\npred = []\nfor [(article, pars)] in pred_raw:\n    titles = classifier.predict(vectorizer.transform(pars)).tolist()\n    text = [p for (t, p) in pars]\n    pred.append([(t, p) for (t, p) in zip(titles, text)])\nprint(\"Evaluation Score: %f\", sent_eval(gold, pred))\n", "intent": "Next, we'll test the classifier on the segmentation's output and evaluate the results - \n"}
{"snippet": "print '\"this is good food\" classified as:',\\\n    clf.predict(count_vectorizer.transform(['this is good food']))\nprint '\"that place was bad\" classified as:',\\\n    clf.predict(count_vectorizer.transform(['that place was bad']))\n", "intent": "Good, now let's try it by making up reviews and see if it can guess the sentiment:\n"}
{"snippet": "from sklearn.metrics import classification_report\nyfit = grid.best_estimator_.predict(xtest)\nprint(classification_report(ytest, yfit, target_names=ozone.target_names))\n", "intent": "Finally evaluate on the test set.\n"}
{"snippet": "from sklearn.metrics import classification_report\nyfit = grid.best_estimator_.predict(xtest)\nprint(classification_report(ytest, yfit, target_names=glass.target_names))\n", "intent": "The score isn't bad but a classification report may give a better view\nof how a multilabel classifier performs.\n"}
{"snippet": "yfit = grid.predict(xtest)\nprint(classification_report(ytest, yfit, target_names=faces.target_names))\n", "intent": "And the score is reasonable.\nBut the full report is what we should look at.\n"}
{"snippet": "log_model.predict(X_train)\n", "intent": "We will look at more details of the model we have.\n"}
{"snippet": "y_test = xgb_reg.predict(X_test)\n", "intent": "xgb_reg.fit(X_train, y_train)\n"}
{"snippet": "def get_ambience_prediction(ambience_int, combined_reviews):\n    if ambience_int != -1:\n        return ambience_map[ambience_int]\n    tfidf = vectorizer.transform([combined_reviews])\n    prediction_int = int(classifier.predict(tfidf))\n    return ambience_map[prediction_int]\nbusiness_df['ambience'] = np.vectorize(get_ambience_prediction)(\n    business_df['ambience_int'],\n    business_df['combined_reviews'],\n)\n", "intent": "Add the predicted ambience column\n"}
{"snippet": "predictions = model.predict(x_test)\n", "intent": "We are interested in confusion matrix as confusing March with April is more tolerable than confusing March with September.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, l_idx in enumerate(style_layers):        \n        dt = gram_matrix(feats[l_idx]) - style_targets[idx]\n        style_loss += style_weights[idx] * (dt**2).sum()\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"petur.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "from sklearn import metrics\ny_pred_class = knn.predict(X)\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "pipeline.predict([\"Iconic consoles of the IBM System/360 mainframes, 55 years old today\",\n                  \"Are Banned Drugs in Your Meat?\"])\n", "intent": "We can also classify new titles as follows:\n"}
{"snippet": "best_result_index = np.argmax([trial.last_result[\"mean_accuracy\"] for trial in all_trials])\nbest_result_path = os.path.join(all_trials[best_result_index].logdir, \"model.pkl\")\nwith open(best_result_path, \"rb\") as f:\n    pipeline = pickle.load(f)\nprint(\"Best result was {}\".format(np.mean(pipeline.predict(test.data) == test.target)))\nprint(\"Best result path is {}\".format(best_result_path))\n", "intent": "The best model can now be loaded and evaluated like so:\n"}
{"snippet": "evaluate(model)\n", "intent": "Lets evaluate the un-Tune-d model.\n"}
{"snippet": "prepared_data = prepare_data(data)\nprint(\"This model predicted your input as\", model.predict(prepared_data).argmax())\n", "intent": "(tip: don't expect it to work)\n"}
{"snippet": "y_pred = linreg.predict(X_test)\nprint y_pred, \"\\n\"\nfrom sklearn.metrics import mean_squared_error\nprint mean_squared_error(y_test, y_pred)\nfrom sklearn.metrics import r2_score\nprint r2_score(y_test,y_pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "ResNet_50_predictions = [np.argmax(ResNet_50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet_50]\ntest_accuracy = 100*np.sum(np.array(ResNet_50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Resnet50_predictions = Resnet50_model.predict(test_Resnet50)\npredict_labels = np.argmax(Resnet50_predictions, axis=1) \nreal_labels = np.argmax(test_targets,axis=1) \ntest_accuracy = 100*np.sum(predict_labels==real_labels)/len(real_labels)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "imdb_pred = bnb.predict(imdb_data)\nprint(\"Number of mislabeled points in the Imdb dataset out of a total {} points : {} ({}% accuracy)\".format(\n    imdb_data.shape[0],\n    (imdb_target != imdb_pred).sum(),\n    ((imdb_data.shape[0] - (imdb_target != imdb_pred).sum()) / imdb_data.shape[0]) * 100\n))\n", "intent": "The features did a decent job predicting the sentiment, missing only 232 out of 1000 reviews. Almost 77% accuracy.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfinal_model = grid_search_elastic_net.best_estimator_   \ny_te_estimation = final_model.predict(X_test_trans)\nfinal_mse = mean_squared_error(y_test_trans, y_te_estimation)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance\n"}
{"snippet": "print(confusion_matrix(df_stack_y, y_st_estimation_extra_trees))\nprint(\"Precision: \", precision_score(df_stack_y, y_st_estimation_extra_trees))\nprint(\"Recall: \", recall_score(df_stack_y, y_st_estimation_extra_trees))\n", "intent": "Extra Trees Classifier fits the data well since test set accuracy and stack set accuracy are exactly the same.\n"}
{"snippet": "print(confusion_matrix(df_stack_y, y_st_estimation_grad_boost_class))\nprint(\"Precision: \", precision_score(df_stack_y, y_st_estimation_grad_boost_class))\nprint(\"Recall: \", recall_score(df_stack_y, y_st_estimation_grad_boost_class))\n", "intent": "Gradient Boost Classifier seems to overfit the data since the accuracy on the train set is higher than on the test set.\n"}
{"snippet": "print(confusion_matrix(df_stack_y, y_st_estimation_log_reg))\nprint(\"Precision: \", precision_score(df_stack_y, y_st_estimation_log_reg))\nprint(\"Recall: \", recall_score(df_stack_y, y_st_estimation_log_reg))\n", "intent": "Logistic Regression seems to fit the data well since the accuracy on the test is nearly identical to the one on the train set.\n"}
{"snippet": "X_test_reduced = pca.transform(X_test)\ny_pred = rnd_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n", "intent": "Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n"}
{"snippet": "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\nmodel.reset_states()\nprint(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "As mentioned, we specify the batch size when evaluating the performance of the network on the entire training dataset.\n"}
{"snippet": "X = ...\nmodel = ...\nyhat = model.predict_proba(X)\n", "intent": "The prediction of probabilities can be made by calling the predict_proba() function on the model.\n"}
{"snippet": "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n\tfor i in range(n_seq):\n\t\tactual = [row[i] for row in test]\n\t\tpredicted = [forecast[i] for forecast in forecasts]\n\t\trmse = sqrt(mean_squared_error(actual, predicted))\n\t\tprint('t+%d RMSE: %f' % ((i+1), rmse))\n", "intent": "We can also simplify the calculation of RMSE scores to expect the test data to only contain the output values, as follows:\n"}
{"snippet": "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\nprint('Test Accuracy: %f' % (acc*100))\n", "intent": "Finally, once the model is trained, we can evaluate its performance by making predictions in the test dataset and printing the accuracy.\n"}
{"snippet": "train = [0.2, 0.1, 0.2, 0.6]\ntest = [0.3, 0.4, 0.5]\nmodel = fit(train)\nstatistic = evaluate(model, test)\n", "intent": "In the case of evaluating a machine learning model, the model is fit on the drawn sample and evaluated on the out-of-bag sample.\n"}
{"snippet": "i = 1\nprint 'Truth:', test_y.iloc[i]\nprint 'Pred: ', rf.predict(test_x.iloc[i].reshape(1, -1))[0]\n", "intent": "Let's pick a line to explain:\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy score: ', format(accuracy_score(y_test, predictions)))\nprint('Precision score: ', format(precision_score(y_test, predictions)))\nprint('Recall score: ', format(recall_score(y_test, predictions)))\nprint('F1 score: ', format(f1_score(y_test, predictions)))\n", "intent": "Compute the statics of prediction\n"}
{"snippet": "single_example = test_set[4:5,:] \nprediction = model.predict(single_example)\nprint '%.8f' % prediction[0]\n", "intent": "and for a single example\n"}
{"snippet": "first = test_first_mention[20:21,:]\nsecond = test_second_mention[20:21,:]\nprint model.predict([first,second])\n", "intent": "Let's calculate the prediction value for a single example from the test set, say, an example number 20\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "**The accuracy on the test data is similar to the one with the default $\\alpha$, but it does not show possible excessive overfitting.**\n"}
{"snippet": "txt = vectorizer.transform(['This movie is not bad and lame'])\nclf.predict(txt)\n", "intent": "**Since we are using bag-of-words, the negation remark 'not' is not taken into account misclassifying the negative review.**\n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X,y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "To evaluate our model we'll be using a 5-fold cross validation with the Accuracy metric.\nFor that lets define a scoring function.\n"}
{"snippet": "def pred_batch(imgs):\n    preds = model.predict(imgs)\n    idxs = np.argmax(preds, axis=1)\n    print('Shape: {}'.format(preds.shape))\n    print('First 5 classes: {}'.format(classes[:5]))\n    print('First 5 probabilities: {}\\n'.format(preds[0, :5]))\n    print('Predictions prob/class: ')\n    for i in range(len(idxs)):\n        idx = idxs[i]\n        print ('  {:.4f}/{}'.format(preds[i, idx], classes[idx]))\n", "intent": "**np.argmax(preds_matrix_like, axis=1)**: find max value by rows [link](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html)\n"}
{"snippet": "r_predictions = [np.argmax(r_model.predict(np.expand_dims(feature, axis=0))) for feature in test_r]\ntest_accuracy = 100*np.sum(np.array(r_predictions)==np.argmax(test_targets, axis=1))/len(r_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(\"Precision:\", sklearn.metrics.precision_score(true_labels, predictions))\n", "intent": "What's the precision of this classifier?\n"}
{"snippet": "print(\"Recall:\", sklearn.metrics.recall_score(true_labels, predictions))\n", "intent": "What's the recall of this classifier?\n"}
{"snippet": "threshold = 0.29\ntuned_pred =  y_score > threshold\nprint(\"Recall: %0.4f, Precision: %0.4f, False Positive Rate: %0.4f\" % (\n    recall_score(y_test, tuned_pred),\n    precision_score(y_test, tuned_pred),\n    confusion_matrix(y_test, tuned_pred)[0,1] / confusion_matrix(y_test, tuned_pred)[0,:].sum()))\n", "intent": "**Lowering the threshold to 0.29** allows as to identify 50% of all defaults with 39% precision and 23% false positive rate.\n"}
{"snippet": "probs = HLR1.predict_proba(X, cats)\n", "intent": "<h2> Predict and score the model </h2>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    layerloss = 0.0\n    for i in range(len(style_layers)):\n        g = gram_matrix(feats[style_layers[i]])\n        layerloss += style_weights[i] * 2 * tf.nn.l2_loss(g - style_targets[i])\n    return layerloss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram_feats = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((gram_feats - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predicted_prices_rf = m_rf.predict(df_test)\npredicted_prices_rf = np.expm1(predicted_prices_rf)\npredicted_prices_rf\n", "intent": "generate submission\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n", "intent": "Once the autoencoder has been trained, we can directly apply encoder and decoder on the test data.\n"}
{"snippet": "Y_pred = np.repeat(0, len(Y_test))\nprint(classification_report(Y_test, Y_pred))\n", "intent": "This is our null hypothesis: every person has a Benign tumor. If we create this model, we can measure its accuracy AND its AUC:\n"}
{"snippet": "uncategorized_labels = list(clf.predict(matrix))\ncategorized_labels = list(df[:number_to_classify_by_hand]['label'])\ndf_labels =  categorized_labels + uncategorized_labels\n", "intent": "You **DON'T NEED A NEW CLASSIFIER**, use the one you have! You'll use `clf.predict`, and feed it... what? What does it need to predict the labels?\n"}
{"snippet": "pred = fcnn.predict(fitted_params, X_test)\nprint ('Prediction accuracy on test set = {:5.2f}%'.format(100 * np.mean(pred == y_test)))\n", "intent": "Let's measure the accuracy of our NNs on the test-dataset\n"}
{"snippet": "X_test = beer_test.iloc[:, 1:-1]\ny_test = beer_test.iloc[:, -1]\nyhat_test = model.predict(X_test)\nevaluate(y_test, yhat_test, 'DecisionTreeRegressor')\n", "intent": "Training set and validation set saw an improvement in the cost. Let's see how the model evaluates on the test set.\n"}
{"snippet": "trainScore = mean_squared_error(X_train, y_train)\nprint('Train Score: %.4f MSE (%.4f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = mean_squared_error(predictions, y_test)\nprint('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "**Step 6:** measure accuracy of the prediction\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "**Step 5:** make prediction using test data\n"}
{"snippet": "trainScore = model.evaluate(X_train, y_train, verbose=0)\nprint('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "** Step 7:** Get the test score.\n"}
{"snippet": "predictions = model.predict(X_test, batch_size=batch_size)\n", "intent": "**Step 3:** Make prediction on improved LSTM model\n"}
{"snippet": "trainScore = model.evaluate(X_train, y_train, verbose=0)\nprint('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "**Step 5:** Get the test score\n"}
{"snippet": "not_categorized['is_yes'] = clf.predict(features_df)\ndf_new = categorized.append(not_categorized)\n", "intent": "You **DON'T NEED A NEW CLASSIFIER**, use the one you have! You'll use `clf.predict`, and feed it... what? What does it need to predict the labels?\n"}
{"snippet": "x_test = np.array(['I finished the assignment'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\n", "intent": "Now, let's check how the model performs on the test.\n"}
{"snippet": "lm1.predict(df)\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ListingPrice</p></h4>\n"}
{"snippet": "lm2.predict(df)\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ShippingPrice</p></h4>\n"}
{"snippet": "lm3.predict(df)\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: SellerFeedbackRating</p></h4>\n"}
{"snippet": "lm4.predict(df)\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ShippingTime_minHours</p></h4>\n"}
{"snippet": "lm5.predict(df)\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ShippingTime_maxHours</p></h4>\n"}
{"snippet": "predictionsLP = logreg1.predict(df)\npredictionsLP\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ListingPrice</p></h4>\n"}
{"snippet": "predictionsSP = logreg2.predict(df)\npredictionsSP\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ShippingPrice</p></h4>\n"}
{"snippet": "predictionsSFR = logreg3.predict(df)\npredictionsSFR\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: SellerFeedbackRating</p></h4>\n"}
{"snippet": "predictionsSTmin = logreg4.predict(df)\npredictionsSTmin\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ShippingTime_minHours</p></h4>\n"}
{"snippet": "predictionsSTmax = logreg5.predict(df)\npredictionsSTmax\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: ShippingTime_maxHours</p></h4>\n"}
{"snippet": "predictionsCFeats = logreg6.predict(df)\npredictionsCFeats\n", "intent": "<h4><p><i><u>Target Feature</u></i>: IsWinner</p>\n<p><i><u>Continuous Features</u></i>: Subset of all Continuous Features</p></h4>\n"}
{"snippet": "logreg_retrain.predict_proba(X_log[['Intercept', 'ShippingPrice', 'SellerFeedbackRating', 'ShippingTime_minHours', 'ShippingTime_maxHours']])\n", "intent": "<h3>(3.3) Evaluate the quality of the model on the training set.</h3>\n"}
{"snippet": "print(\"Accuracy: \", metrics.accuracy_score(y, rfc_predictions))\nprint(\"\\nConfusion matrix: \\n\", metrics.confusion_matrix(y, rfc_predictions))\nprint(\"\\nClassification report:\\n \", metrics.classification_report(y, rfc_predictions))\n", "intent": "<h3>4.1 Evaluate the quality of the model on the training set. </h3>\n"}
{"snippet": "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions))\nprint(\"\\nConfusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions))\nprint(\"\\nClassification report:\\n \", metrics.classification_report(y_train, rfc_predictions))\n", "intent": "<h3>4.3 Evaluate the quality of the model on the training set. </h3>\n"}
{"snippet": "lm10.predict(df_test30)\n", "intent": "<h3> (5.1) Evaluate their quality on the new test set.</h3>\n"}
{"snippet": "predictions_test = logreg7.predict(df_test30)\npredictions_test\n", "intent": "<h3> (5.1) Evaluate their quality on the new test set.</h3>\n"}
{"snippet": "logreg_retrain.predict_proba(X_testlog[['Intercept', 'ShippingPrice', 'SellerFeedbackRating', 'ShippingTime_minHours', 'ShippingTime_maxHours']])\n", "intent": "<h3> (5.1) Evaluate their quality on the new test set.</h3>\n"}
{"snippet": "print(\"Accuracy: \", metrics.accuracy_score(y_test1, rfc_predictions))\nprint(\"\\nConfusion matrix: \\n\", metrics.confusion_matrix(y_test1, rfc_predictions))\nprint(\"\\nClassification report:\\n \", metrics.classification_report(y_test1, rfc_predictions))\n", "intent": "<h3> (5.1) Evaluate their quality on the new test set.</h3>\n"}
{"snippet": "print(\"Accuracy: \", metrics.accuracy_score(y_testlog, predictions))\nprint(\"\\nConfusion matrix: \\n\", metrics.confusion_matrix(y_testlog, predictions))\nprint(\"\\nClassification report:\\n \", metrics.classification_report(y_testlog, predictions))\n", "intent": "<h4><i>Logistic Regression</i></h4>\n"}
{"snippet": "print(\"Accuracy: \", metrics.accuracy_score(y_test1, rfc_predictions))\nprint(\"\\nConfusion matrix: \\n\", metrics.confusion_matrix(y_test1, rfc_predictions))\nprint(\"\\nClassification report:\\n \", metrics.classification_report(y_test1, rfc_predictions))\n", "intent": "<h4><i>Random Forest</i></h4>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, index in enumerate(style_layers):\n        loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(feats[index]) - style_targets[i]))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_train) - y_train) ** 2))\nprint('Variance score: %.2f' % regr.score(X_train, y_train))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "parameters =\nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))\n", "intent": "Build the model with no dropout and regualrization,   \nassign **learning_rate = 0.01** and **num_iter = 10000**\n"}
{"snippet": "parameters = \nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))\n", "intent": "Now build the model with regularization.  \nassign **learning_rate = 0.01, num_iter = 10000 , regularization = True, lambd = 0.02**\n"}
{"snippet": "parameters = \nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))\n", "intent": "Train the model with dropout  \nassign **learning_rate = 0.01, num_iter = 10000 , regularization = False, drop_out = True**\n"}
{"snippet": "lr.predict(X_test_std[:3,:])\n", "intent": "If all we are interested in are class labels, then we can use the model's ``predict`` method directly:\n"}
{"snippet": "preds = RF_best_model.predict(test_X)\ncm = confusion_matrix(test_y,preds)\nplot_confusion_matrix(cm, ['0', '1'])\n", "intent": "Due to very imbalanced classes, the model does not work at all. The ration between 0 and 1 is \n"}
{"snippet": "train_preds = clf.predict(train_final_df)\ncm = confusion_matrix(train_y,train_preds)\nplot_confusion_matrix(cm, ['0', '1'])\n", "intent": "let's use the model to predict training data itself\n"}
{"snippet": "precision_score(y_test, log_reg.predict(x_test),average=None)\n", "intent": "**Precision:** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n"}
{"snippet": "recall_score(y_test, log_reg.predict(x_test),average=None)\n", "intent": "**Recall:** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n"}
{"snippet": "f1_score(y_test, log_reg.predict(x_test),average=None)\n", "intent": "**f1_score():** http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n"}
{"snippet": "log_reg.predict([[13, 4, 7]])\n", "intent": "Below, predict and predict_proba are random examples\n"}
{"snippet": "print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred_class))\n", "intent": "When the actual value is positive, how often is the prediction correct?\n"}
{"snippet": "x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1 \ny_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1 \nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                     np.linspace(y_min, y_max, 100))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n", "intent": "Then, let's build a input data matrix containing continuous values of sepal length and width (from min to max) and aply the predict function to it:\n"}
{"snippet": "arimaModel.plot_predict(h=20,past_values=50,figsize=(15,5))\n", "intent": "We can now predict future airline passanger numbers\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "Lets generate new data\n"}
{"snippet": "accuracy_score(y_test,predictions)\n", "intent": "The result is telling us that we have 1536 + 347 correct predictions and 281 +161 incorrect predictions.\nWhich shows our model is quite good\n"}
{"snippet": "test_pred = model.predict(test[['sqft_living']])\n", "intent": "**_RMSE in sklearn_**\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n__Ans__:\nFound test accuracy is 4.7847%.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\nFound Test accuracy: 80.0239%\n"}
{"snippet": "train_error = mean_squared_error(tar_train, model.predict(pred_train))\ntest_error = mean_squared_error(tar_test, model.predict(pred_test))\nprint('training data MSE', train_error)\nprint('test data MSE', test_error)\n", "intent": "MSE from training and test data\n"}
{"snippet": "predictions = model.predict(inputs[testing])\n", "intent": "And we can make predictions using our test partition:\n"}
{"snippet": "lm.evaluate(x,y,verbose=1)\n", "intent": "The weights are completely random at the beginning. So, compared to our data set, the loss function should be really high :\n"}
{"snippet": "clf.predict(test)==true_result\n", "intent": "We are ready to evaluate the performance of this classifier.\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size) \nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0] \nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size) \nprobs = model.predict_proba(val_data, batch_size=batch_size)[:,0] \nprobs[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = nn.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "** Generate Predictions **\n"}
{"snippet": "num_digits = 4\nidx = random.sample(range(0, len(X)+1), num_digits)\nsample_images = [X[i] for i in idx]; sampel_labels = [y[i] for i in idx]\ndegrees = [-45, -20, 20, 45]\nrotated_images, labels = rotate_images(sample_images, sampel_labels, degrees)\nrotated_pred = nn.predict(np.array(rotated_images).reshape((-1, 1, 28, 28)))\nlabels = [str(p) + \": \" + str(l) for p, l in zip(rotated_pred, labels)]\nplot_digits(rotated_images, labels, num_digits, 4)\n", "intent": "** Predicting Rotated Digits **\nJust for fun, lets see how well we are able to predict the values for rotated digits.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, target_names=[\"Perished\", \"Survived\"]))\n", "intent": "As you can see we are better at predicting who died than who actually survived. We are only able to correctly classify \n"}
{"snippet": "model.predict(X_test_normal)\n", "intent": "Use the trained model to predict whether X_test_normal data point are in the same distributions.  Calculate the fraction of \"false\" predictions.\n"}
{"snippet": "from collections import Counter\nL1=model.predict(X_test_uniform)\na = Counter(L1).values()[0] \nb = Counter(L1).values()[1]\nc = float(b)/a\nprint c\n", "intent": "Use the trained model to predict whether X_test_uniform is in the same distribution. Calculate the fraction of \"false\" predictions.\n"}
{"snippet": "pred_normal=model.predict(X_test_normal)\ncollections.Counter(pred_normal)\n", "intent": "Use the trained model to predict whether X_test_normal data point are in the same distributions.  Calculate the fraction of \"false\" predictions.\n"}
{"snippet": "pred_uniform=model.predict(X_test_uniform)\ncollections.Counter(pred_uniform)\n", "intent": "Use the trained model to predict whether X_test_uniform is in the same distribution. Calculate the fraction of \"false\" predictions.\n"}
{"snippet": "predicted=model.predict(X_test_normal)-1\nprint(np.count_nonzero(predicted))\n", "intent": "Use the trained model to predict whether X_test_normal data point are in the same distributions.  Calculate the fraction of \"false\" predictions.\n"}
{"snippet": "uniform=model.predict(X_test_uniform)-1\nprint(np.count_nonzero(uniform))\n", "intent": "Use the trained model to predict whether X_test_uniform is in the same distribution. Calculate the fraction of \"false\" predictions.\n"}
{"snippet": "new_predicted=new_model.predict(X_test_normal)-1\nnew_uniform=new_model.predict(X_test_uniform)-1\nnew_trained=new_model.predict(X_train_normal)-1\nprint(np.count_nonzero(new_trained))\nprint(np.count_nonzero(new_predicted))\nprint(np.count_nonzero(new_uniform))\n", "intent": "Redo the prediction on the training set, prediction on X_test_random, and prediction on X_test.\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nprecision = precision_score(y_test, clf.predict(X_test), average='binary')\nrecall = recall_score(y_test, clf.predict(X_test), average='binary')\nprint(precision)\nprint(recall)\n", "intent": "Since the test set data is quite skewed (3/4 of the samples are negative), let's look at our precision recall score.\n"}
{"snippet": "if __name__ == '__main__':\n    data = get_data(1_000_000_000)\n    features, labels = get_features(data)\n    f_train, f_test, l_train, l_test = get_train_test_data(features, labels)\n    clf = get_classifier(f_train, l_train)\n    score = get_score(clf, f_test, l_test)\n    print('SCORE: {}%'.format(int(score * 100)))\n", "intent": "Runs the classifier\n"}
{"snippet": "y_pred = linreg.predict(X)\nmetrics.r2_score(y, y_pred)\n", "intent": "Let's calculate the R-squared value for our simple linear model:\n"}
{"snippet": "print(mean_absolute_error(y_true, y_pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(mean_squared_error(y_true, y_pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(np.sqrt(mean_squared_error(y_true, y_pred)))\nprint(mean_squared_error(y_true, y_pred) ** 0.5)\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "detailed_report(np.zeros(y1_test01.shape), y1_test01)\n", "intent": "compare with a model that assignes zero to all\n"}
{"snippet": "detailed_report(y1_test01random, y1_test01)\n", "intent": "The following is a detailed report for a model returning random labels for each sample.\n"}
{"snippet": "detailed_report(np.zeros(y1_test01.shape), y1_test01)\n", "intent": "The following is a detailed report for a model always assigning the same labels for each sample.\n"}
{"snippet": "data_intrain=alldata.copy()\ndata_intrain['intrain']=data_intrain['Id'].map(lambda x:int(x in train['Id']))\ndata_intrain=data_intrain.drop(['Id','SalePrice'],axis=1).sample(frac=1)\nX_intrain=data_intrain.iloc[:,:-1]\ny_intrain=data_intrain['intrain']\ndef rmse_cv_acc(model, X, y):\n    scorer = make_scorer(accuracy_score, False)\n    return (cross_validation.cross_val_score(model, X, y, scoring=scorer,cv=20)).mean()\nxgb_est=xgb.XGBClassifier()\nprint(\"Cross val Accuracy: %.4f\" % abs(rmse_cv_acc(xgb_est, X_intrain, y_intrain)))\n", "intent": "<a id='video5'> </a>\n .\n"}
{"snippet": "clf.predict(X_test)\n", "intent": "We can use the trained classifier for a prediction\n"}
{"snippet": "pred_y_test_kNN = kNN.predict(X_test)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_DT = DT.predict(X_test)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_RF = RF.predict(X_test)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_ET = ET.predict(X_test)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_AB = AB.predict(X_test)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_SVM = SVM.predict(X_test_minmax)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_AB_eng = AB_eng.predict(X_test_new)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_GB = GB.predict(X_test)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_NB = NB.predict(X_test_std[:,:10])\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_LR = LR.predict(X_test_minmax)\n", "intent": "Predict the test set:\n"}
{"snippet": "pred_y_test_SGD = SGD.predict(X_test_minmax)\n", "intent": "Predict the test set:\n"}
{"snippet": "print preds[:10,:], '\\n'\ncounts = np.apply_along_axis(np.bincount, 1, preds, minlength=8, weights=acc)\npred_y_dev_ensemble = np.argmax(counts, axis=1)\nprint pred_y_dev_ensemble[:10], '\\n'\nacc_ensemble = metrics.accuracy_score(y_dev, pred_y_dev_ensemble)\nprint acc_ensemble\n", "intent": "If we give equal weight to each model, the accuracy is lower than that of the best model (AdaBoost).\n"}
{"snippet": "preds = kNN.predict(X_test)\nprint preds\n", "intent": "Here's what we need to send back to Kaggle:\n"}
{"snippet": "from sklearn.linear_model import ElasticNet\nresults = cross_val_score(ElasticNet(), X, Y, cv=kfold, scoring='neg_mean_squared_error')\nprint(results.mean()) ;\n", "intent": "Combines the properties of Lasso and Ridge regression.\n"}
{"snippet": "results = cross_val_score(model, X, Y, cv=kfold)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n", "intent": "Calculate the Accuracy\n"}
{"snippet": "model.predict(X[1,:])\n", "intent": "We report both the mean and the standard deviation of the performance measure.\n"}
{"snippet": "precision= precision_score(Y_test, predicted)\nprint('Precision score: {0:0.2f}'.format(precision))\n", "intent": "Calculate and print the Precision\n"}
{"snippet": "recall = recall_score(Y_test, predicted)\nprint('Recall score: {0:0.2f}'.format(recall))\n", "intent": "Calculate and print the Recall\n"}
{"snippet": "f1 = f1_score(Y_test, predicted)\nprint('F1 score: {0:0.2f}'.format(f1))\n", "intent": "Calculate and print the F1 score\n"}
{"snippet": "def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')\n", "intent": "The following function will be used to plot the decision function of a\nclassifier given some data.\n"}
{"snippet": "predicted = model_final.predict(X_test)\n", "intent": "Now let us see the confusion matrix for this model.\n"}
{"snippet": "accuracy_score(y_train,y_pred)\n", "intent": "It looks like our accuracy score was 80.9 % on the training set which isn't so bad. Lets see how it performs on the unseen test data. \n"}
{"snippet": "y_pred_test = knn.predict(X_test)\n", "intent": "It looks like we also returned an 80.1 % accuracy rate which is pretty good considering how simple k-nn is! Great Job\n"}
{"snippet": "y_pred_test_proba = knn.predict_proba(X_test)\n", "intent": "In addition, with a knn model we can use predict_proba to get our probability of churn. \n"}
{"snippet": "y_predlogos = logclassifieros.predict_proba(X_train)\n", "intent": "Like previously we can get our models probability's of fraud by using the predict_proba function.\n"}
{"snippet": "y_predtestlog = optimised_log_reg.predict_proba(X_test)\n", "intent": "Now that we have our final model, lets see how our model performs on unseen test data. \n"}
{"snippet": "ypred = classifier.predict(count)\n", "intent": "With the predict function we can then generate predictions based on the training set. Lets place these predictions into the ypred array.\n"}
{"snippet": "ypredtest = classifier.predict(testcount)\n", "intent": "Lets then generate predictions for our test data as we did for our training data. \n"}
{"snippet": "np.sqrt(mean_squared_error(y_train, Opt_RF_y_pred))\n", "intent": "The optimised model returned a mean squared error of 1000 less compared to the previous model. \n"}
{"snippet": "y_pred = regressor.predict(X_train)\n", "intent": "The predict function is then used to predict the results against the training data and assigned to array y_pred. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores2 = cross_val_score(regressor2, X_opt,y_train,scoring='neg_mean_squared_error',cv=10)\nrmse_scores2 = np.sqrt(-scores2)\nrmse_scores2\n", "intent": "The model averaged an error of .65 with a Standard Deviation of (+-) .05 which is no improvement from the previous model.\n"}
{"snippet": "y_hat = model.predict(X_sm)\n", "intent": "This means the model successfully predict 76.3% of the test data sets. We will then use confusion matrix to further validate the model.\n"}
{"snippet": "print(classification_report(y_test, y_hat_knn))\n", "intent": "We have slightly more correct negative classifications than with Logistic Regression on this dataset but `0` correct positives.\n"}
{"snippet": "print(classification_report(y_test, y_hat_lda))\n", "intent": "Though our true negatives are down slightly, we've classified 5 positives correctly, which is an improvement.\n"}
{"snippet": "print(\"[Test Classification Report:]\")\nprint(classification_report(Ytest, Ypredict))\n", "intent": "- Classification Report\n"}
{"snippet": "grid_result.predict_proba(np.array([[2, 1, 3, 0, 2, 0.0]]))\n", "intent": "Lets predict the chance of survival of a 3 year old boy travelling with parents in 2nd class.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", acc)\n", "intent": "For github this dataset is not available as its quite heavy and github has size restrictions.\n"}
{"snippet": "numOfEmployees = 0\nfor i in range(len(data)):\n    if y[i] == 0:\n        sample = X[i].reshape(1, -1)\n        prediction = svmClassifier.predict(sample)\n        if prediction == 1:\n            numOfEmployees = numOfEmployees + 1\nprint \"Number of Employees that are likely to leave the job = \", numOfEmployees\n", "intent": "- Prediction via SVM:\n"}
{"snippet": "numOfEmployees = 0\nfor i in range(len(data)):\n    if y[i] == 0:\n        sample = X[i].reshape(1, -1)\n        prediction = knn.predict(sample)\n        if prediction == 1:\n            numOfEmployees = numOfEmployees + 1\nprint \"Number of Employees that are likely to leave the job = \", numOfEmployees\n", "intent": "- Prediction via KNN\n"}
{"snippet": "numOfEmployees = 0\nfor i in range(len(data)):\n    if y[i] == 0:\n        sample = X[i].reshape(1, -1)\n        prediction = logReg.predict(sample)\n        if prediction == 1:\n            numOfEmployees = numOfEmployees + 1\nprint \"Number of Employees that are likely to leave the job = \", numOfEmployees\n", "intent": "- Prediction via Logistic Regression\n"}
{"snippet": "cross_validation.cross_val_score(forest.best_estimator_ , X, y, scoring='accuracy', cv=5)\n", "intent": "Computing Accuracy on the complete set using 5 fold cross validation\n"}
{"snippet": "print cross_validation.cross_val_score(forest.best_estimator_ , X, y, scoring='precision_micro', cv=5).mean()\nprint cross_validation.cross_val_score(forest.best_estimator_ , X, y, scoring='recall_micro', cv=5).mean()\n", "intent": "Also precision and recall\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision = precision_score(y_true=test_data['sentiment'].as_matrix(),y_pred=model.predict(test_matrix))\nprint(\"Precision on test data: %s\" % precision)\n", "intent": "Total cost of false positives is: $146,105\n"}
{"snippet": "print ('Mean squared error: ', np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "X = model.predict(X0)\nX[:, -2]\n", "intent": "Calculate stability of bath bomb product (ideally should be equal to ones):\n"}
{"snippet": "X0_bath_fizzies = X0.copy()\nX0_bath_fizzies[:, [4, 7]] = 0 \nX_bf = model.predict(X0_bath_fizzies)\n(X_bf/(X+1e-10))[:, -2] \n", "intent": "Remove SLES from components (modified input):\n"}
{"snippet": "water = np.array([0.64, 0.84, 0.88, 0.1 , 1.14, 0.76, 0.82, 0.36, 0.82, 0.5 ])\nX0_bath_fizzies[:, 6] = water;\nX0_bath_fizzies[:,-1] = water/10;\nX_bf = model.predict(X0_bath_fizzies)\n(X_bf/(X+1e-10))[:, -2]\n", "intent": "One have to change concentration of water to remain stability of the product at the same level:\n"}
{"snippet": "honey_price_futpred = forrest.predict(honey_pred_future[['stnum','marketsize','year']])\n", "intent": "Applying the model developed above to make price predictions from the forecasted market size values for each state\n"}
{"snippet": "n_clusters = 3\ny_lower = 10\ncluster_labels = kmeans3.fit_predict(X)\nsilhouette_avg = silhouette_score(X, cluster_labels)\nsilhouette_values = silhouette_samples(X, cluster_labels)\n", "intent": "$s_i = \\frac{b(i)-a(i)}{max(a_i, b_i)}$\n"}
{"snippet": "cross_val_score(dtc_model, X1_train,y_train).mean()\n", "intent": "Wow, totally overfitted model. \n"}
{"snippet": "gs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=2)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "Partition data into training, validation, and test sets in nested hierarchy\n<img src='./images/06_07.png' width=80%>\n"}
{"snippet": "kfold = KFold(n=len(X), n_folds=10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n", "intent": "The final step is to evaluate this baseline model. We will use 10-fold cross validation to\nevaluate the model.\n"}
{"snippet": "y_train_pred = crf_final.predict(X_train)\nmetrics.flat_f1_score(y_train, y_train_pred, \n                      average='weighted', labels=labels)\n", "intent": "Run on the Training set\n"}
{"snippet": "RMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction))\nprint(RMSE)\n", "intent": "Mean Squared Error calculation\n"}
{"snippet": "x1 = X.iloc[0:1]\nres = regressor.predict(x1)\nstddev = calc_dev(x1, 10)\nprint(res, stddev)\n", "intent": "Testing prediction and deviation calculation\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nfor idx, clf in enumerate(clfs.values()):\n    y_pred = cross_val_predict(clf, X, Y1, cv=10)\n    print('%15s : %f'%(list(clfs.keys())[idx], metrics.accuracy_score(y, y_pred)))\n", "intent": "Cross validation prodictions\n"}
{"snippet": "for idx, clf in enumerate(clfs.values()):\n    y_pred = cross_val_predict(clf, X, Y2, cv=10)\n    print('%15s : %f'%(list(clfs.keys())[idx], metrics.accuracy_score(Y2, y_pred)))\n", "intent": "Cross validation prodictions\n"}
{"snippet": "preds = grid_search.predict(X_val)\nprobs = grid_search.predict_proba(X_val)\nprint(\"Performance after hyperparameter tuning: %0.2f\" %accuracy_score(y_val,preds))\n", "intent": "** Step 5.8: Predict on Test Set **\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Naive Bayes Accuracy - BOW:\",accuracy_score(y_val,predictions_gaussian_nb_bow))\nprint(\"Logistic Regression Accuracy - BOW:\",accuracy_score(y_val,predictions_log_bow))\nprint(\"SGD Accuracy - BOW:\",accuracy_score(y_val,predictions_sgd_bow))\n", "intent": "** Step 6: Compare results from different algorithms **\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Naive Bayes Accuracy - TFIDF:\",accuracy_score(y_val,predictions_gaussian_nb_tfidf))\nprint(\"Logistic Regression Accuracy - TFIDF:\",accuracy_score(y_val,predictions_log_tfidf))\nprint(\"SGD Accuracy - TFIDF:\",accuracy_score(y_val,predictions_sgd_tfidf))\n", "intent": "** Step 9: Compare results from different algorithms for TFIDF **\n"}
{"snippet": "metrics.precision_score(data_y_test, y_pred, average='micro')\n", "intent": "**2. Precision and Recall**\n"}
{"snippet": "print(metrics.classification_report(data_y_test, y_pred, target_names = class_names))\n", "intent": "**4. Classification Report**\n"}
{"snippet": "y_krr = kr.predict(x_out_complex[:, np.newaxis])\n", "intent": "Now we make some predictions for the same range as we did for the GPR case.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "Classification report for the model:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier, X=X_train, y=y, cv=5)\nprint('accuracy mean: {0}'.format(accuracies.mean()))\nprint('accuracy std: {0}'.format(accuracies.std()))\n", "intent": "**Cross-validation test**\nEverything is ready to test the model, the way I decided to do it was through a cross-validation test with 5 folds:\n"}
{"snippet": "fitted_values = lm_PTRATIO.predict(X_PTRATIO)\nresiduals = bos.PRICE - fitted_values\ng = sns.jointplot(fitted_values, residuals, alpha=0.5);\ng.set_axis_labels('Fitted values', 'Residuals')\n", "intent": "><b>Exercise</b>: Construct a fitted values versus residuals plot. What does the plot tell you? Are there any violations of the model assumptions?\n"}
{"snippet": "import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import log_loss\nis_nn = lambda model: isinstance(model , NeuralNetClassifier)\ndef objective_func(weights, models, X, normX, y):\n    weighted_pred = np.array(\n        [w*model.predict_proba(normX if is_nn(model) else X) for w, model in zip(weights, models)]\n    ).sum(axis=0)\n    return log_loss(y, weighted_pred)\n", "intent": "- input of neural net should be normalized\n"}
{"snippet": "write_out_prediction(model.predict_proba(test_normalize.values), \"nn.csv\")\n", "intent": "<img src=\"otto_nn_history.png\" width=500/>\n"}
{"snippet": "write_out_prediction(clf.predict_proba(test_x))\n", "intent": "```py\nparameters = {\n        'n_estimators'      : 200,\n        'max_features'      : 30,\n        'max_depth'         : 30\n}\n```\n"}
{"snippet": "write_out_prediction(xgb1.predict(xg_test), filename=\"xgb1.csv\")\n", "intent": "```py\nparameters = {\n        'learning rate' : 0.3\n        'n_estimators' : 281\n}\n```\n"}
{"snippet": "predictedSVM = svm_model.predict(f_test)\nprint predictedSVM\n", "intent": "Predict class labels for the test set\n"}
{"snippet": "accuracySVM = accuracy_score(t_test, predictedSVM)\naccuracySVM\n", "intent": "Calculate accuracy on the test data\n"}
{"snippet": "def evaluate(inputs, labels, apply_thresholding=None):\n    predictions = model.predict(inputs)\n    if apply_thresholding:\n        predictions = [apply_thresholding(prediction) for prediction in predictions]\n    labels = np.array(labels) * 255.\n    predictions = np.array(predictions) * 255.\n    return ((labels - predictions) ** 2).mean(axis=None)\n", "intent": "We evaluate the network on testing dataset using MSE (mean squared error)\n"}
{"snippet": "X_test = test_df[test_df.columns.difference(['order_id', 'product_id'])]\ntest_df['reordered'] = lgb_model.predict(X_test, \n                                      num_iteration=lgb_model.best_iteration)\n", "intent": "<u>Predictions on test set</u>\n"}
{"snippet": "y_predict = random_forest.predict(flat_short)\naccuracy_score(strlabs_short, y_predict)\n", "intent": "The random forest performs well here\n"}
{"snippet": "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": shortintro_dat.astype(np.float32)},\n    num_epochs=1,\n    shuffle=False)\ncnn_predictions = list(mnist_classifier.predict(input_fn=predict_input_fn))\nint_cnn_preds= [i['classes'] for i in cnn_predictions]\ncnn_predict= np.array([label_encoder.classes_[i] for i in int_cnn_preds])\nfloat(sum(cnn_predict == strlabs_short))/500\n", "intent": "The cnn performs poorly here\n"}
{"snippet": "y_predict_sorted = np.array([''.join(sorted(strlab)) for strlab in y_predict])\naccuracy_score(sorted_strlabs_short, y_predict_sorted)\n", "intent": "Scoring without regard for direction with random forest:\n"}
{"snippet": "y_pred = nb.predict(X_test_dtm)\ndef eval_predictions(y_test, y_pred):\n    print 'accuracy:', metrics.accuracy_score(y_test, y_pred)\n    print 'precision:', metrics.precision_score(y_test, y_pred, average='weighted')\n    print 'recall:', metrics.recall_score(y_test, y_pred, average='weighted')\n    print 'F-measure:', metrics.f1_score(y_test, y_pred, average='weighted')\neval_predictions(y_test, y_pred)\n", "intent": "Evaluate the model.\n"}
{"snippet": "y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n", "intent": "Predicting the test set results and calculating the accuracy\n"}
{"snippet": "logisticRegr.predict(x_test[0].reshape(1,-1))\n", "intent": "Step 4. Predict the labels of new data (new images)\nUses the information the model learned during the model training process\n"}
{"snippet": "forecast_prediction = clf.predict(X_forecast)\nprint(forecast_prediction)\n", "intent": "Lastly, we will predict for the next 30 days. The following are our X_forecast values:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)\n", "intent": "Further Evaluation\nConfusion Matrix:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))\n", "intent": "Precision and Recall:\n"}
{"snippet": "X = np.linspace(0,1,10)\nY = reg.predict(X.reshape(10,1))\n", "intent": "**Predict.** Let's make an new array of `x` values and use the model's `predict` method to compute `y` values and then plot the result.\n"}
{"snippet": "pred = clf.predict(xtest)\nfbeta_score(ytest, pred, beta=1.5)\n", "intent": "Drumroll, please... time to see how the model performs!\n"}
{"snippet": "cv = cross_val_score(clf, x, y, cv=4, scoring=scorer)\nprint('Mean f-beta: {0:.4f} (Std dev: {1:.4f})'.format(cv.mean(),  cv.std()))\n", "intent": "Let's see how the classifier cross-validates with the entire dataset.\n"}
{"snippet": "pred = model.predict(xtest)\nroc_auc_score(ytest, pred)\n", "intent": "Drumroll, please... time to see how the model performs!\n"}
{"snippet": "cv = cross_val_score(model, x, y, cv=4, scoring='roc_auc')\nprint('Mean roc_auc: {0:.4f} (Std dev: {1:.4f})'.format(cv.mean(),  cv.std()))\n", "intent": "Let's see how the classifier cross-validates with the entire dataset.\n"}
{"snippet": "def loss(x, y, input_shape):\n    logits = forward(x, input_shape)\n    mean_loss = cross_entropy_loss(labels=y, unscaled_logits=logits)\n    return mean_loss\n", "intent": "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network.\n"}
{"snippet": "roc_auc_score(small_binary_target, small_data['SCHEDULED_ARRIVAL'])\n", "intent": "Let's check if late flights are often delayed:\n"}
{"snippet": "tree_predictions = classifiers[0].predict_proba(testX.to_pandas())[:, 1]\nknn_predictions  = classifiers[1].predict_proba(testX.to_pandas())[:, 1]\naveraged_predictions = tree_predictions * 0.5 + knn_predictions * 0.5\nprint roc_auc_score(testY, averaged_predictions)\n", "intent": "**Averaging** predictions. We could use different coefficient to mix results\n"}
{"snippet": "roc_auc_score(testY, (proba_mcmc + proba_gb) / 2.)\n", "intent": "Simple averaging of two models\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy for training data increases while for test data decreases\n"}
{"snippet": "print mean_squared_error(testY, sum(reg.predict(testX) for reg in regressors) / len(regressors))\n", "intent": "**Averaging** predictions. We could use different coefficient to mix results\n"}
{"snippet": "from scipy.special import expit\nproba = expit(testX.dot(w_optimal))\nprint roc_auc_score(testY, proba)\n", "intent": "Now predict output of logistic regression for the test sample and compute AUC\n"}
{"snippet": "scores = cross_val_score(\n    glm, newdata, target, cv=5, scoring='f1_macro')\n", "intent": "Let's find the 5-fold cross-validation score.\n"}
{"snippet": "def poisson_loss(W):\n    return 6*tf.log(W+100) + 50000/(W+100)\nminimize_cost(poisson_loss)\n", "intent": "Let's try to minimize this function.\n"}
{"snippet": "def strata_loss(W):\n    return 5*tf.log(W+100) + 4.5*10000/(W+100)\nminimize_cost(strata_loss, 1.5)\n", "intent": "Comparing (4) with (3), we see a slight difference. Let's use grandient descent to mimize it.\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['negative', 'positive']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n", "intent": "Another handy function from `sklearn` library is the `classification_report`.\n"}
{"snippet": "def test_score():\n    assert f1_score(y_test.tolist(), y_pred_new, average='macro') > 0.75\n", "intent": "Use a word embedding representation like word2vec for step 3 and or step 6. \n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nhits_train_is_signal = (hits_train.label == 1)\nroc_auc_score(hits_train_is_signal, is_signal(hits_train))\n", "intent": "Check how good the model describes the data.\n"}
{"snippet": "print(\"The available metrics are: {}\".format([s for s in dir(sklearn.metrics) if s[0] != '_']))\nprint(\"for our clusters:\")\nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(newsgroupsDF['category'], km.labels_)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(newsgroupsDF['category'], km.labels_)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(newsgroupsDF['category'], km.labels_)))\n", "intent": "Once we have the clusters, we can evaluate them with a variety of metrics that sklearn provides. We will look at a few.\n"}
{"snippet": "print(classification_report(predict, y_test))\n", "intent": "__Classification Report__\n"}
{"snippet": "accuracy_score(zero,y)*100\n", "intent": "__Print the accuracy score__\n"}
{"snippet": "y_pred_new = list()\ny_prob_new = list()\ncoeff_labels_new = ['lr new', 'l1 new', 'l2 new']\ncoeff_models_new = [lr_new, lr_l1_new, lr_l2_new]\nfor lab,mod in zip(coeff_labels_new, coeff_models_new):\n    y_pred_new.append(pd.Series(mod.predict(X_test_new), name=lab))\n    y_prob_new.append(pd.Series(mod.predict_proba(X_test_new).max(axis=1), name=lab))\ny_pred_new = pd.concat(y_pred_new, axis=1)\ny_prob_new = pd.concat(y_prob_new, axis=1)\ny_pred_new.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\ndef measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred, average = 'micro'),\n                      'recall': recall_score(y_true, y_pred, average = 'micro'),\n                      'f1': f1_score(y_true, y_pred, average = 'micro')},\n                      name=label)\n", "intent": "A function to return error metrics.\n"}
{"snippet": "y_train_pred_gr = GRI.predict(X_train)\ny_test_pred_gr = GRI.predict(X_test)\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n                                 measure_error(y_test, y_test_pred_gr, 'test')],\n                                axis=1)\ntrain_test_gr_error\n", "intent": "These test errors are a little better than the previous ones. So it would seem the previous example overfit the iris_data, but only slightly so.\n"}
{"snippet": "print(metrics.classification_report(y_test, y_pred))\n", "intent": "Even filtering on Pinot Noir naive bayes is determining the sommelier reviewer with great accuracy\n"}
{"snippet": "def proximal_policy_optimization_loss(train_model, old_probs, rewards, advantage, clip_val):\n    r = 1  \n    old_onehots_expand = train_model.inputs\n    old_onehots = [K.squeeze(x, 1) for x in old_onehots_expand[1:]]\n    curr_probs = train_model.outputs\n    for t, (p, old_p, old_onehot) in enumerate(zip(curr_probs, old_probs, old_onehots)):\n        r = r * K.exp(K.log(K.sum(old_onehot * p, axis=1)) - K.log(K.sum(old_onehot * old_p, axis=1))) \n    surr_obj = K.mean( K.abs(1/(rewards+1e-8)) * K.minimum(r * advantage, \n                             K.clip(r, min_value=1 - clip_val, max_value=1 + clip_val) * advantage))\n    return - surr_obj \n", "intent": "The end product is the training function `atrain_fn` that minimizes the proximal-policy loss `aloss`.\n"}
{"snippet": "sync_weights(train_model, sample_model)\nstate_inputs_arr = np.array([1,0,0,0,1,0]).reshape((2,1,3))\ns = sample_model.predict(state_inputs_arr)\nprob_sample, action_sample = s[:4], s[4:]\naction_input = [np.expand_dims(x,1) for x in action_sample] \nprob_train = train_model.predict([state_inputs_arr]+action_input)\nprint(prob_sample) \nprint(prob_train)\n", "intent": "We will test the predict and train functions for `sample_model` and `train_model`. First synchronize the weights.\n"}
{"snippet": "print(accuracy_score(y_test, rf_pred))\n", "intent": "Return the accuracy score of the predicted outcomes on the actual outcomes using the `accuracy_score()` function.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.4f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.4f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = []\n    for ly in range(len(style_layers)):\n        loss.append(style_weights[ly] * 2 * tf.nn.l2_loss(\n                    gram_matrix(feats[style_layers[ly]]) - style_targets[ly]\n                                                          )\n                   )\n    style_loss =  tf.add_n(loss)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = []\n    for ly in range(len(style_layers)):\n        loss[ly] =  style_weights[i] *\\\n            tf.norm(gram_matrix(feats[style_layers[i]], normalize=True) - style_targets[ly])**2\n    style_loss = tf.add_n(loss)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def compute_l2_cost(parameters, lambd):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    W3 = parameters['W3']\n    l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)\n    l2_cost = lambd * l2_loss\n    return l2_cost\n", "intent": "Now, you will bring it all together! \n**Exercise:** Implement the model. You will be calling the functions you had previously implemented.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "**Values for the testing data.**\n"}
{"snippet": "rate = 0.001\nconv1_w, conv2_w, fc1_w, fc2_w, fc3_w, logits = LeNet(x)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\nbeta = 0.0007\nregularizers = tf.nn.l2_loss(conv1_w)+tf.nn.l2_loss(conv2_w)+\\\n            tf.nn.l2_loss(fc1_w)+tf.nn.l2_loss(fc2_w)+tf.nn.l2_loss(fc3_w)\nloss_operation = tf.reduce_mean(loss_operation + beta * regularizers)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)\n", "intent": "Create a training pipeline that uses the model to classify Trafic Sign data.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cross_validation import train_test_split\ndef train_classifier(clf, X_train, y_train):\n    start = time()\n    probas = clf.predict_proba(features)\n    end = time()\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return roc_auc_score(target.values, probas[:,1].T)\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "The code below initializes some helper functions used to test the models later.\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    probas = clf.predict_proba(features)\n    end = time()\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return roc_auc_score(target.values, probas[:,1].T)\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "The code below initializes some helper functions used to test the models later.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\naccuracy = (scores[1]*100)\nprint(\"Accuracy: {:.2f}%\".format(scores[1]*100))\n", "intent": "We can now evaluate our model's performance.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\naccuracy = (scores[1]*100)\nprint(\"Accuracy: {:.2f}%\".format(scores[1]*100))\n", "intent": "We can now evaluate our new model's performance.\n"}
{"snippet": "train_pred = rfc.predict(train_X)\ntest_pred = rfc.predict(test_X)\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(train_y, train_pred)\naccuracy_test = accuracy_score(test_y, test_pred)\nprint(\"Training Accuracy: %f \\t Testing Accuracy: %f\" % (accuracy_train, accuracy_test))\n", "intent": "Using this default model, we obtain the training accuracy and test accuracy:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(test_y, pred_test, average='macro')\n", "intent": "Simple prediction accuracy is not a good metric to evaluate our model. Thus, we will use F1 scoring instead:\n"}
{"snippet": "accuracy_score(test_y, best_pred)\n", "intent": "We can also check the simple prediction accuracy of the optimized model:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nMyDataset_predictions_LR = lin_reg.predict(MyDataset_prepared)\nlin_mse = mean_squared_error(MyDataset_labels, MyDataset_predictions_LR)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Root Mean Squared Error (RMSE)\n"}
{"snippet": "from sklearn import metrics\npredicted = model.predict(X_test)\nexpected = Y_test\nprint metrics.accuracy_score(expected,predicted)\n", "intent": "Now lets get parameter how good our model to fit the data by comparing the prediction using the model and \nfrom the actual data we randomly peaked. \n"}
{"snippet": "y_pred = reg.predict(6.5)\nprint('Predicted salary: $', np.round(float(y_pred), 2))\n", "intent": "Okay, with the model created and fit, let predict whether or not that \\$160k should be taken on good advice.\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "We will create the vector of predictions. This is the vector populated with predictions of our test set, made by the model we just created.\n"}
{"snippet": "mse_train_nocv =  np.mean((bos.PRICE - lm.predict(X)) ** 2)\nrmse_train_nocv = np.sqrt(mse_train) \nprint mse_train_nocv, rmse_train_nocv\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "VGG19_pred = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\nVGG19_acc = 100*np.sum(np.array(VGG19_pred)==np.argmax(test_targets,axis=1))/len(VGG19_pred)\nprint('Test accuracy is: %.4f%%' % VGG19_acc)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "maj_class_pred_accuracy = accuracy_score(y_true=test_data['sentiment'], y_pred=np.array([1.0]*len(test_data)))\nprint maj_class_pred_accuracy\n", "intent": "Baseline: Majority class prediction\n"}
{"snippet": "print 'Predict time'\nprint''\nfrom sklearn.metrics import accuracy_score\nresult = ('ClassifyRF accuracy:---------->%.2f %%' % (accuracy_score(y_pred, y_validate)*100))\nprint result\n", "intent": "Predict time\nCPU times: user 1.73 s, sys: 220 ms, total: 1.95 s\nWall time: 707 ms\nClassifyRF accuracy:---------->27.10 %\n"}
{"snippet": "g1_prediction = g1_model3_new.predict(out_images)\ng2_prediction = g2_model4.predict(out_images)\n", "intent": "Predictions with both models: g1 predicts all 30 keypoints, g2 predicts 8 keypoints.\n"}
{"snippet": "g1_prediction = g1_model3_new.predict(out_images)\ng2_prediction = g2_model4.predict(out_images)\n", "intent": "Predictions with both models - g1 predicts all 30 keypoints, g2 predicts 8 keypoints.\n"}
{"snippet": "predictions = sentiment_model.predict(test_matrix)\nnum_correct = 0\nj = 0\nfor i in test_data['sentiment']:\n      if predictions[j] == i:\n         num_correct = num_correct + 1\naccuracy = num_correct / len(test_data)\nprint(\"Classifier accuracy based on results from test_data: \", accuracy)\n", "intent": "We will now evaluate the accuracy of the trained classifier.\nRecall that the accuracy is given by accuracy=\n"}
{"snippet": "RESULT = DataManager()\nframe_num=11\nRESULT.X = M.predict(DM.X, num_predicted_frames=frame_num)\n", "intent": "Predict the next frame_num frames:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, y_pred)*100   \nacc1 = accuracy_score(y_test1, y_pred1)*100  \nprint \"The Kernel-SVM Accuracy (sentiment)= %f\" % acc, \"%\"\nprint \"The Kernel-SVM Accuracy (Total_Comment_Sentiment)= %f\" % acc1, \"%\"\n", "intent": "Calculate the __Kernel-SVM__  Model Accuracy\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(arabic_text_sentiment_list[-1855:], result)*100   \nprint \"The Accuracy= %f\" % acc, \"%\"\n", "intent": "It is a __36*36 Confusion Matrix__.\n"}
{"snippet": "mse = np.mean((test_target.IndividualRate - lm.predict(test_features))**2)\nprint(mse)\n", "intent": "**Mean Squared Error:** This is for Linear Model __lm__\n"}
{"snippet": "mse_tobacco = np.mean((test_target.IndividualTobaccoRate - lm_tobacco.predict(test_features))**2)\nprint(mse_tobacco)\n", "intent": "Individual Rates for tobacoo users.\n"}
{"snippet": "r2_score(test_target.IndividualTobaccoRate, y_predict_tobacco)\n", "intent": "Calculation of Mean Squared error produced same result wether calculated manully or usking sklearn package.\n"}
{"snippet": "cv_pred = cv_dt.predict(X_test)\n", "intent": "The model improves significantly on the test data when we use 5 fold cross validation on the training data. \n"}
{"snippet": "cv_tobacco_pred = cv_tobacco.predict(X_test)\n", "intent": "Again we see a significant improvement from **32.5%** to **73.73%** when we use cross validation score.\n"}
{"snippet": "def performance_metric(y_true, y_predict):\n    score = r2_score(y_true, y_predict)\n    return score\n", "intent": "**GridSearchCV** implementation\n"}
{"snippet": "preds = gs_reg.predict(X_test)\n", "intent": "**Make Prediction:** for individual rate using the above model.\n"}
{"snippet": "preds_tobacco = gs_reg_tobacco.predict(X_test)\n", "intent": "**Make Prediction:** for individual tobacco rate using the above model.\n"}
{"snippet": "y_real_pred_lr_clf = lr_clf.predict(X_test_real)\n", "intent": "** LogisticRegression **\n"}
{"snippet": "from fastFM import mcmc\nfrom fastFM.mcmc import FMRegression\ndef mcmc_fastfm(trainX, trainY, testX, rank=8, n_iter=100):\n    clf = FMRegression(rank=rank, n_iter=n_iter)\n    return clf.fit_predict(trainX, trainY, testX) \n", "intent": "(It allows to intergrate the regularizarion parameters into the model)\n"}
{"snippet": "start = time.time()\nprediction = ridge_regression(trainX, trainY, testX, classification=False)\nspent_time = time.time() - start\nname = 'Ridge regression using \"Summary\"'\nresults.ix[name, 'time'] = spent_time\nresults.ix[name, 'RMSE'] = numpy.sqrt(mean_squared_error(testY, prediction))\n", "intent": "(regression model where the loss function is the linear least squares function and regularization is given by the l2-norm)\n"}
{"snippet": "y_predict = clf.predict( X_testing )\n", "intent": "Short look at the prediction\n"}
{"snippet": "model.load_weights(checkpoint_path)\nloss, acc = model.evaluate(test_images, test_labels,)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Now, load the weights from checkpoint and train the model.\n"}
{"snippet": "test_predict = dt_train.predict(X)\n", "intent": "Let's test the model with a prediction on the remaining 20% of the dataset \n"}
{"snippet": "estimators = [('svm',svm_pipe), ('rf',rf), ('et',et)]\npred_validation = []\npred_test = []\nfor e in estimators:\n    pred_validation.append(e[1].predict(X_validation))\n    pred_test.append(e[1].predict(X_test))\npred_validation = np.array(pred_validation).T\npred_test = np.array(pred_test).T\n", "intent": " - First train models using train_data:\n"}
{"snippet": "pred = clf.predict(X_test)\n", "intent": "To make our prediction:\n"}
{"snippet": "predicted = model_knn.predict(test_X)\npredicted[:5]\n", "intent": "Making predictions for our test documents using the KNN model and the test document-term matrix\n"}
{"snippet": "predicted = model_NB.predict(test_X)\npredicted[:5]\n", "intent": "Making predictions for our test documents using the Naive Bayes model and the test document-term matrix\n"}
{"snippet": "for name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, articles_X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n", "intent": "Running cross validation procedure using 10 -fold cross validation\n"}
{"snippet": "prediction = model.predict(X_test)\n", "intent": "Now we can apply it to the test data\n"}
{"snippet": "y_pred = rx_predict(trees, test)\nprint(y_pred.head())\nprint(y_pred.tail())\nfrom sklearn.metrics import confusion_matrix\nconf = confusion_matrix(test[\"Label\"], y_pred[\"PredictedLabel\"])\nprint(conf)\n", "intent": "Let's see the :epkg:`confusion matrix`.\n"}
{"snippet": "y_pred2 = rx_predict(trees2, test)\nconf = confusion_matrix(test[\"Label\"], y_pred2[\"PredictedLabel\"])\nprint(conf)\n", "intent": "We look into the :epkg:`confusion matrix`.\n"}
{"snippet": "mse, acc = model.evaluate(X_test, y_test)\nprint(\"mean square error = \", mse)\n", "intent": "Evaluate model on the test set\n"}
{"snippet": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_train, y_pred_loo))\nrmse_dollar = np.sqrt(mean_squared_error(np.exp(y_train), np.exp(y_pred_loo)))\nr2 = r2_score(y_train, y_pred_loo)\nprint('RMSE LOOCV: {:.3f}, (${:.2f})'.format(rmse, rmse_dollar))\nprint('R^2 LOOCV: {:.3f}'.format(r2))\n", "intent": "**Evaluation metrics**\n"}
{"snippet": "y_pred = gs.predict(X_test)\n", "intent": "**Make prediction**\n"}
{"snippet": "y_pred = ransac.predict(X_test_sfs)\n", "intent": "**Make prediction**\n"}
{"snippet": "print(\"Training set:\")\npred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\nprint('Test set:')\npred_test = predict(X_test, Y_test, W, b, word_to_vec_map)\n", "intent": "Our model has pretty high accuracy on the training set. Lets now see how it does on the test set. \n"}
{"snippet": "X_my_sentences = np.array([\"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\", \"Do not try to piss me off\"])\nY_my_labels = np.array([[1], [0], [3],[2],[4]])\npred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\nfor i in range(Y_my_labels.shape[0]):\n    print(X_my_sentences[i] +' '+ label_to_emoji(pred[0][i]))\n", "intent": "Our test set also yielded accuracy close to 90% which is pretty good. Check some random sentences and see how the model performs.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(train_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "The model isn't performing brilliantly. We can measure the RMSE using SKLs `mean_squared_error` function:\n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(train_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Let's evaluate the model:\n"}
{"snippet": "from sklearn import model_selection, preprocessing, ensemble\ncv_scores = []\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        cv_scores.append(log_loss(val_y, preds))\n        print(cv_scores)\n        break\n", "intent": "We will perform cross validation on the training dataset in order to see when the scores will stop decreasing.\n"}
{"snippet": "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'x': eval_data},\n    y=eval_labels,\n    num_epochs=1,\n    shuffle=False)\neval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\nprint(eval_results)\n", "intent": "Evaluate the model:\n"}
{"snippet": "with open('submission_file.csv','w') as f:\n    f.write('id,label\\n')\nwith open('submission_file.csv','a') as f:\n    for data in test_data:\n        img_num = data[1]\n        img_data = data[0]\n        orig = img_data\n        data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n        model_out = model.predict([data])[0]\n        f.write('{},{}\\n'.format(img_num,model_out[1]))\n", "intent": "Alright, so we made a couple mistakes, but not too bad actually! \nIf you're happy with it, let's compete!\n"}
{"snippet": "clf = GBC()\nscores = cross_val_score(clf, X_transformed, y, cv=10, scoring='precision')\nprint(\"Precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "That is just a horrible precision. Hideous. What if I try with the principal components?\n"}
{"snippet": "accuracy_score(predicted, y_test)\n", "intent": "Adding week of year when quit gives about 5% increase in accuracy when not including it.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test2.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "knn.predict([[3., -2.5]])\n", "intent": "What if we received a new observation at `[3., -2.5]`. What would you predict its colour to be?\n"}
{"snippet": "y_val_pre = model_ridge.predict(X_val)\nprint('RMSE of validation data:', np.sqrt(metrics.mean_squared_error(y_val_pre, y_val)))\n", "intent": "> RidgeCV run cross-validation and searched for the best hyperparameter alpha. The best alpha is **7.9**.\n"}
{"snippet": "y_val_pre = model_lasso.predict(X_val)\nprint('RMSE of validation data:', np.sqrt(metrics.mean_squared_error(y_val_pre, y_val)))\n", "intent": "> LassoLarsCV run cross-validation and searched for the best hyperparameter alpha. The best alpha is **0.0001678814665479999**.\n"}
{"snippet": "y_val_pre = model_rf.predict(X_val)\nprint('RMSE of validation data:', np.sqrt(metrics.mean_squared_error(y_val_pre, y_val)))\n", "intent": "> The best hyperparameters for Random Forest are:\n> - max_features: **auto**\n> - n_estimators: **450**\n> - min_samples_leaf: **50**\n"}
{"snippet": "y_val_pre = grid.predict(X_val)\nprint('RMSE of validation data:', np.sqrt(metrics.mean_squared_error(y_val_pre, y_val)))\n", "intent": "Now we can predict the validation data and get the RMSE.\n"}
{"snippet": "test_pred = grid.predict(df_test_x)\n", "intent": "Finally, we use the stacking model to predict test data.\n"}
{"snippet": "predictions = dt.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preds = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,preds))\nprint('\\n')\nprint(confusion_matrix(y_test,preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "import xgboost as xgb\nparam_grid = {'objective': 'multi:softprob', 'num_class': len(np.unique(y_train))}\nX_train_xgb = xgb.DMatrix(X_train, y_train)\nX_test_xgb = xgb.DMatrix(X_test)\nxgb = xgb.train(param_grid, X_train_xgb, 20)\npred_train = xgb.predict(X_train_xgb)\npred_test = xgb.predict(X_test_xgb)\nxgb_train_logloss = log_loss(y_train, pred_train)\nxgb_test_logloss = log_loss(y_test, pred_test)\nprint 'XGBoost train/test log loss: {:.3f}/{:.3f}'.format(xgb_train_logloss, xgb_test_logloss)\n", "intent": "Lastly we can try the king of all classification techniques, XGBoost\n"}
{"snippet": "pred = lm.predict(X_test)\nRSS = np.mean((pred - y_test)**2)\nprint 'The mean squared error is %.2f' %(RSS)\n", "intent": "The lowest mean squared error with any combination of the feature variables was the model with all the variabels included\n"}
{"snippet": "player_avg = player_lifetime_stats[['1B_rate', '2B_rate', '3B_rate', 'HR_rate', 'BB_rate']].values\nplayer_lifetime_stats['PW'] = lm.predict(player_avg)\n", "intent": "Now use the prediction model that we created previously to add a new column to the `player_lifetime_stats` dataframe for the players Predicted Wins\n"}
{"snippet": "scores = cross_val_score(linreg, X, y, cv = 10, scoring = 'mean_squared_error')\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5) \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n", "intent": "We now make predictions using the predict method and print the confusion matrix\n"}
{"snippet": "scores = model.evaluate(np.array(x_test), np.array(y_test), verbose=0)\nprint(\"====================[TEST SCORE]====================\")\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Lets now observe the test and training set accuracies.\n"}
{"snippet": "in_sample_accuracy = (clf.predict(images)==labels).mean()\nprint('In sample Accuracy {:.2%}, In Sample Error {:.4f} '.format(in_sample_accuracy,1-in_sample_accuracy))\nout_of_sample_accuracy = (clf.predict(images_test)==labels_test).mean()\nprint('Out of sample Accuracy {:.2%}, In Sample Error {:.4f}'.format(out_of_sample_accuracy,1-out_of_sample_accuracy))\n", "intent": "Let's see how well it does.\n"}
{"snippet": "from sklearn import svm\ndef predict(input):\n    tmp = np.load('myfile.npz')\n    my_model = tmp['my_model'].item()  \n    res = my_model.predict(input)\n    assert len(res) == input.shape[0]\n    return res\n", "intent": "Then, your `predict.py` may define the function `predict` as follows:\n"}
{"snippet": "classifier = joblib.load('classifier.pkl') \ny_pred = classifier.predict(nt_test)\n", "intent": "Load the classifier and predict the test dataset.     \nWarning : you need to dezip the file classifier.pkl.zip for this part to work.\n"}
{"snippet": "def print_model_accuracy(model, test_dataset):\n    predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_dataset]\n    test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n    print('Test accuracy: %.4f%%' % test_accuracy)\nprint_model_accuracy(XCEP_model, test_XCEP)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nY_test_pred = ... \naccuracy_score(Y_test, Y_test_pred)\n", "intent": "This time we should get a better result for the accuracy on the test set.\n"}
{"snippet": "Y_pred = clf.predict(X_test_trans)\nY_pred_prob = clf.predict_proba(X_test_trans)\n", "intent": "After all this work, we can finally test our classifier. We can predict binary labels but also get probability estimates.\n"}
{"snippet": "np.array_equal( grid_forest.predict(X_test), forest.predict(X_test))\n", "intent": "Here is the proof that the two estimators are the same on the test set. (They have exactly the same predictions.)\n"}
{"snippet": "sum(Neural_network.predict(X_test))\n", "intent": "The number of predicted viral tweets.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nc_mat=confusion_matrix(y_test,Neural_network.predict(X_test))\nprint(c_mat)\n", "intent": "The confusion matrix (rows are true classes and columns are predicted classes, that is, the diagonal values are successifuly predicted classes).\n"}
{"snippet": "sum(Neural_network.predict(X_test_std))\n", "intent": "And the number of predicted viral tweets.\n"}
{"snippet": "c_mat=confusion_matrix(y_test,Neural_network.predict(X_test_std))\nprint(c_mat)\n", "intent": "The confusion matrix.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(rfc,\n                            vectors_train,\n                            target_train,\n                            cv = 5,\n                            scoring=\"accuracy\")\ncv_scores\n", "intent": "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(lg,\n                            vectors_train,\n                            target_train,\n                            cv = 5,\n                            scoring=\"accuracy\")\ncv_scores\n", "intent": "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)\n"}
{"snippet": "Resnet50Data_predictions = [np.argmax(Resnet50Data_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50Data]\ntest_accuracy = 100*np.sum(np.array(Resnet50Data_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50Data_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print_report(knn_mnist_fit,\"k-Nearest Neighbour\",\"report\",\"mnist\")\nprint_report(lr_mnist_fit,\"Logistic Regression\",\"report\",\"mnist\")\n", "intent": "In this next bit of code, the Report is printed. This report tells us to how accurately each number is correctly guessed by the algorithm.\n"}
{"snippet": "X_sample_test = X_test[3]\ny_predict = naiveBayes.predict(X_sample_test)\nprint('Predicted class %s, real class %s' % (\n        y_predict, y_test[3]))\n", "intent": "**Step3**: Testing the model\n"}
{"snippet": "print('Probabilities of membership in each class: %s' % \n      naiveBayes.predict_proba(X_sample_test))\n", "intent": "**Step4**: View probablity of each class\n"}
{"snippet": "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\nout, _ = relu_forward(x)\ncorrect_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n                        [ 0.,          0.,          0.04545455,  0.13636364,],\n                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\nprint('Testing relu_forward function:')\nprint('difference: ', rel_error(out, correct_out))\n", "intent": "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:\n"}
{"snippet": "optimalSVM_predictions = optimalSVM.predict(X_test)\n", "intent": "**Making predictions on the test set**\n"}
{"snippet": "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nf1_optimalSVM = f1_score(y_test,optimalSVM_predictions)\nprecision_optimalSVM = precision_score(y_test,optimalSVM_predictions)\nrecall_optimalSVM = recall_score(y_test,optimalSVM_predictions)\naccuracy_optimalSVM = accuracy_score(y_test,optimalSVM_predictions)\nprint('F1 score: ' + str(f1_optimalSVM))\nprint('Precision: ' + str(precision_optimalSVM))\nprint('Recall: ' + str(recall_optimalSVM))\nprint('Accuracy: ' + str(accuracy_optimalSVM))\n", "intent": "**Calculating F1, precision, recall and accuracy on the test set**\n"}
{"snippet": "three_cluster_pred = km_3_clusters.predict(documents_vec)\n", "intent": "It seems that three clusters can get the positive, negative and in-between types.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for idx,val in enumerate(style_layers):\n        A=gram_matrix(feats[val])\n        B=style_targets[idx]\n        style_loss += style_weights[idx]* ((A-B)**2).sum()\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from scipy.stats import mode\nlabels = np.zeros_like(clusters)\nfor i in range(10):\n    mask = (clusters == i)\n    labels[mask] =mode(digits.target[mask])[0]    \nfrom sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "a very useful trick : help you compute accuracy when you do clustering\n"}
{"snippet": "knn1.predict([3,4,5,2])\n", "intent": "Predict the response for new observation\n"}
{"snippet": "y_new = logreg.predict(X)\n", "intent": "For Logistic regression\n"}
{"snippet": "print (f'Model name: {model_name}')\npredictions = [np.argmax(models[model_name].predict(np.expand_dims(feature, axis=0))) for feature in bottleneck_features[model_name]['test']]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Yhat=lm.predict(X)\nYhat[:10]\n", "intent": " We can output a prediction \n"}
{"snippet": "Y_predict_multifit = lm.predict(Z)\nY_predict_multifit\n", "intent": " we produce a prediction \n"}
{"snippet": "-1*cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n", "intent": " We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error': \n"}
{"snippet": "yhat=poly.predict(x_test_pr )\nyhat[0:5]\n", "intent": " We can see the output of our model using the method  \"predict\", then assign the values to \"yhat\":\n"}
{"snippet": "yhat = neigh.predict(X_test)\nyhat[:5]\n", "intent": "we can use the model to predict the test set:\n"}
{"snippet": "p_test =clf.predict(V_test)\n", "intent": "Once we are satisfied with the training, we predict and measure accuray on the test set\n"}
{"snippet": "info_string, models = run_train()\nensemble_predict(info_string)\n", "intent": "Run the training and prediction code\n"}
{"snippet": "logistic_model_probs = logistic_model.predict_proba(X)\nlog_loss= log_loss(y, logistic_model_probs)\nlog_loss\n", "intent": "Log loss is something that can be used to compare different classification models\n"}
{"snippet": "boosted_model_probs = boosted_model.predict_proba(X)\nlog_loss_boosted= log_loss(y, boosted_model_probs)\nlog_loss_boosted\n", "intent": "Log loss is something that can be used to compare different classification models\n"}
{"snippet": "rf_model_probs = rf_model.predict_proba(X)\nlog_loss_rf= log_loss(y, rf_model_probs)\nlog_loss_rf\n", "intent": "Log loss is something that can be used to compare different classification models\n"}
{"snippet": "print(metrics.classification_report(df_test[\"Label_test\"], y_predicted,\n                                    target_names=df[\"Label\"]))\n", "intent": "Well, honeslty the model worked very well, a 97% accuracy, not bad at all. I have the classification report as shown below. \n"}
{"snippet": "y_prediction = regressor.predict(X_test)\ny_prediction[:10]\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:\nPerform Prediction using Linear Regression Model\n<br><br></p>\n"}
{"snippet": "RMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction))\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:\nEvaluate Linear Regression Accuracy using Root Mean Square Error\n<br><br></p>\n"}
{"snippet": "y_prediction = regressor.predict(X_test)\ny_prediction[:10]\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:\nPerform Prediction using Decision Tree Regressor\n<br><br></p>\n"}
{"snippet": "RMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction))\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:\nEvaluate Decision Tree Regression Accuracy using Root Mean Square Error\n<br><br></p>\n"}
{"snippet": "precision_score(true_labels,predictions)\n", "intent": "What's the precision of this classifier?\n"}
{"snippet": "recall_score(true_labels,predictions)\n", "intent": "What's the recall of this classifier?\n"}
{"snippet": "print(y[20:50])\nprint(model.predict(X[20:50]))\n", "intent": "Optionally you can print out a sample and see for yourself how the classification performs.\n"}
{"snippet": "print(y[20:55])\nprint(np.round(model.predict(X)[20:55]).astype('i4'))\n", "intent": "Optionally you can print out a sample and see for yourself how the linear regression model performs.\n"}
{"snippet": "iso_outliers = iso_forest.predict(X)==-1\nX_iso = X[~iso_outliers]\ny_iso = y[~iso_outliers]\n", "intent": "Carefully read through the API documentation for Isolation Forest!\nPull **inliers** into variables X_iso and y_iso.\n"}
{"snippet": "svm_outliers = svm.predict(X)==-1\nX_svm = X[~svm_outliers]\ny_svm = y[~svm_outliers]\n", "intent": "Pull **inliers** into variables X_svm and y_svm.\n"}
{"snippet": "from sklearn.metrics import accuracy_score, confusion_matrix\nmetrics = classifier.evaluate(input_fn=input_fn,steps=1)\nprint(\"Loss\",metrics['loss'],\"\\nAccuracy\",metrics['accuracy'])\nconfusion_matrix(y_test,y_pred)\n", "intent": "Measure accuracy and create confusion matrix.\n"}
{"snippet": "metrics = svm_classifier.evaluate(input_fn=my_input_fn, steps=1)\nprint(\"Loss\", metrics['loss'], \"\\nAccuracy\", metrics['accuracy'])\n", "intent": "Now we make a quick evaluation.\n"}
{"snippet": "y_pred= classifier.predict(input_fn = predict_fn)\n", "intent": "Then call classifier.predict() to create **y_pred** as predictions.\n**Hint**: See LinearSVM lab.\n"}
{"snippet": "import sklearn.model_selection\nsklearn.model_selection.cross_val_score(model, X, y, cv=5)\n", "intent": "Let's look at the cross-validation for comparison.\n"}
{"snippet": "from sklearn.metrics import accuracy_score, confusion_matrix\ny_pred=loaded_model.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n", "intent": "Test your new model using the testing data set.\n * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n"}
{"snippet": "X_new = np.array([[2.5]])\nmodel.predict(X_new)\n", "intent": "Using ``predict`` method, we can estimate the new response value\n"}
{"snippet": "X_new = np.array([[100,25,25]])\nLR.predict(X_new)\n", "intent": "Predict the Sales for ddvertisement spending TV=100, Radio=25 and Newspaper=25 respectively.\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "print(metrics.accuracy_score(y_test, y_pred))\n", "intent": "- Proportion of correct predictions\n- Common evaluation metric for classification problems\n"}
{"snippet": "predictions = dtree.predict(X_test)\nprint(classification_report(y_test,predictions))\n", "intent": "We then use the trained decision tree to predict the outcome of the testing data. We evaluate the model using a confusion matrix\n"}
{"snippet": "ypred = result.predict(add_constant(X_valid))\nprint(mse(ypred,y_valid))\nypred_alternate = result_alternate.predict(add_constant(X_valid[:, 2]))\nprint(mse(ypred_alternate,y_valid))\n", "intent": "Lets predict on the test data and see how much is the error \n"}
{"snippet": "cross_validation.cross_val_score(lm, X_train, y_train, scoring='r2')\n", "intent": "Lets see how is the R square\n"}
{"snippet": "ypred = lm.predict(X_valid)\nmean_squared_error(ypred,y_valid)\n", "intent": "Lets predict the on the test data\n"}
{"snippet": "y_pred = model.predict_proba(x_test)\ny_pred_flag = y_pred[:,1] > 0.7\nprint pd.crosstab(y_test.Survived\n                  ,y_pred_flag\n                  ,rownames = ['Actual']\n                  ,colnames = ['Predicted'])\nprint '\\n \\n'\nprint classification_report(y_test,y_pred_flag)\n", "intent": "Let's check out the precision and recall for it\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = clf.predict(x_test)\nprint(pd.crosstab(y_test.greater_than_50k,y_pred,rownames = ['Actual'],colnames = ['Predicted']))\nprint('\\n \\n')\nprint(classification_report(y_test.greater_than_50k,y_pred))\n", "intent": "Let's see how the model is performing\n"}
{"snippet": "y_pred = clf.predict(x_test)\nprint(pd.crosstab(y_test.greater_than_50k\n                  ,y_pred\n                  ,rownames = ['Actual']\n                  ,colnames = ['Predicted']))\nprint('\\n \\n')\nprint(classification_report(y_test.greater_than_50k,y_pred))\n", "intent": "Ater building the model, let's cross validate the model on the test data\n"}
{"snippet": "x_valid_pred = np.argmax(model_lstm.predict(xvalid_ext),axis=1)\n", "intent": " - Verify the LSTM model:\n"}
{"snippet": "x_valid_pred = np.argmax(model.predict(xvalid_ext),axis=1)\n", "intent": " - Verify the Convolution model:\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==\n                           np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "X_new=np.array([[3,5,4,2],[5,4,3,2]])\nknn.predict(X_new)\n", "intent": "- Returns an array\n- Can pridict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y,y_pred))\n", "intent": "- **Proportion ** of Correct Prediction\n- Common ** Evaluation metric ** for classification problem\n"}
{"snippet": "def gan_loss(outputs, targets, real, fake):\n    EPS = 1e-12\n    l1_weight = 100.0\n    gan_weight = 1.0\n    discrim_loss = tf.reduce_mean(-(tf.log(real + EPS) + tf.log(1 - fake + EPS)))\n    gen_loss_GAN = tf.reduce_mean(-tf.log(fake + EPS))\n    gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n    gen_loss = gen_loss_GAN * gan_weight + gen_loss_L1 * l1_weight\n    return discrim_loss, gen_loss\n", "intent": "Make sure noise is the correct shape and type:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train,y_train_5,cv=3)\n", "intent": "check how much the model is confused\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5,y_scores)\n", "intent": "for perfect, its 1, for pure random, its .5\n"}
{"snippet": "from sklearn import metrics\ndef print_accuracies(y_val, y_predict):\n    print(\"Kappa: %.3f\" % metrics.cohen_kappa_score(y_val, y_predict))\n    print(\"Overall Accuracy: %.3f\" % metrics.accuracy_score(y_val, y_predict))\nprint_accuracies(y_val, y_predict)\n", "intent": "In addition, it can be interesting to give some numeric accuracy measures using the accuracy and the kappa score:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(expected_values, predicted_values))\n", "intent": "Final model precision- 0.8354430379746834\n"}
{"snippet": "predictions = cross_validation.cross_val_predict(clf, features, is_nuclear)\n", "intent": "We now use cross-validation to obtain predictions:\n"}
{"snippet": "mean_price = np.mean(np.log10(df_explore[\"price\"]))\ny_pred = np.zeros(df_explore.shape[0]) + np.power(10, mean_price)\ny_true = df_explore[\"price\"].values\nprint \"Baseline: SVR loss=%.3f, Utility=%.3f\" % (svr_loss(y_pred, y_true, EPSILON), utility(y_pred, y_true, EPSILON))\n", "intent": "For the reference, I compute the baseline prediction: predict average price in any situation.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.0\n    for i in range(len(style_layers)):\n        G = style_targets[i]\n        A = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum((G-A)**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint(r2)\n", "intent": "Measure r-squared error to find out whether it's overfitting. From 0 to 1, the more the better fit\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('VGG19 Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(\"-> SVM optimization with backtracking line search\")\nsvm = SVM(optimization_method=\"barrier\", \n          barrier_centering_method=\"newtonLS\", verbose=True)\nsvm.train(X, y)\ny_pred, acc = svm.predict(X, y)\nprint(\"Accuracy on the training set:\", round(acc, 2))\n", "intent": "Let's test our implementation by training the SVM on the whole data:\n"}
{"snippet": "preds = model.predict(test_x)\nprint('Predictions over 46 classes:', preds[0])\nprint('Index of max prediction:', np.argmax(preds[0]))\n", "intent": "Let's use the test set to make predictions on, and see the output of the softmax activation.\n"}
{"snippet": "activations = activation_model.predict(img_tensor)\n", "intent": "Now we can pass the image through the model and obtain our list of activations..\n"}
{"snippet": "predicted = svc_model.predict(X_test)\nprint(\"%s    %18s   %15s\" % (\"X sample\", \"predicted\", \"actual\"))\nfor x_sample, pred, actual in zip(X_test, predicted, y_test):\n    correct = \"True\" if pred == actual else \"False\"\n    print(\"{0:20s} {1:20s} {2:20s} {3:20s}\".format(str(x_sample), pred, actual, correct))\n", "intent": "Why is the corss-validation accuracy much larger than the test score?\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.evaluate(x_test, y_test)\n", "intent": "And, last but not least, verify how it works on the never seen before test data:\n"}
{"snippet": "predictions=rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df[\"Cluster\"],kmeans.labels_))\nprint(classification_report(df[\"Cluster\"],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions=knn.predict(x_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions=model.predict(x_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "def check_log_score(clf,delta=0):   \n    pred = (clf.predict_proba(train)).clip(min=delta,max=100.0-delta)\n    print (\"Final log loss on the training data: {:.4f}\".format(log_loss(y_train,pred,eps=1e-15)))\n    test_pred = (clf.predict_proba(test)).clip(min=delta,max=100.0-delta)\n    print (\"Final log loss on the test data: {:.4f}\".format(log_loss(y_test,test_pred,eps=1e-15)))\n", "intent": "We will prepare some functions to check model performance.\n"}
{"snippet": "clf, type_pred = compute_SVM(data_train, data_test, type_train, kernel='rbf')\nncorr = len(np.where(type_pred == type_test)[0])\nprint \"Completeness:\",ncorr/float(len(type_test))\nprint \"Class distribution test input:\", np.unique(type_test, return_counts=True)\nprint \"Class distribution test prediction:\", np.unique(type_pred, return_counts=True)\nfinal_class = clf.predict(data)\nprint \"Class distribution full prediction:\", np.unique(final_class, return_counts=True)\n", "intent": "select the classifier of interest\noutput the predicted classifications and frequency of these classifications\n"}
{"snippet": "pred = knn.predict(X_test)\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "vfs2_y, vfs2_y_score = get_mx(c_new)\nypred = cross_val_predict(svm_clf, vfs2_y_score.reshape(-1,1), vfs2_y, cv=kf)\nfrom sklearn.metrics import f1_score\nt2_f1 = f1_score(vfs2_y, ypred)  \nprint(f\"F1 Score: {t2_f1}\")\n", "intent": "**Calculate F1 Score**\n"}
{"snippet": "vfs2_y, vfs2_y_score = get_mx(d_new)\nypred = cross_val_predict(svm_clf, vfs2_y_score.reshape(-1,1), vfs2_y, cv=kf)\nfrom sklearn.metrics import f1_score\nt2_f1 = f1_score(vfs2_y, ypred)  \nprint(f\"F1 Score: {t2_f1}\")\n", "intent": "**Calculate F1 Score**\nF1 Score was not so high, since the results for this task has a high false positive rate (see results and discussions).\n"}
{"snippet": "new_x = [ [0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,396.9,4.98] ]\nmodel.predict(new_x)\n", "intent": "Let's make 2 predictions with our new model:\n"}
{"snippet": "prediction = ml.predict(x_test)\nprediction\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "no_vector_scores = no_vector_model.evaluate(x_test_padded, y_test)\nprint('loss: {} accuracy: {}'.format(*no_vector_scores))\n", "intent": "Assess the model. __This takes awhile. You might not want to re-run it.__\n"}
{"snippet": "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))\n", "intent": "After plotting the ROC curve, it would appear that the Logistic regression method provides us with the best results.\n"}
{"snippet": "y_pred_tst=model_rnd_frst.predict(X_std_tst)\n", "intent": "Predicting the Duration for Test Data\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier ( n_neighbors=10, p=2, metric='minkowski' ).fit ( X_sh, y_sh )\ny_pred_knn = clf_knn.predict(X_pred_sh)\ny_prob_knn =  [ row[1] for row in clf_knn.predict_proba(X_pred_sh) ]\nC_knn= confusion_matrix( y_true_sh, y_pred_knn)\nprint_conf_m(C_knn)\nprint( '' )\nmetrics_knn = find_metrics ( y_true_sh, y_pred_knn )\n", "intent": "KNeighborsClassifier has shown worse results then DecisionTreeClassifier on this very sets of train and test\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nrandom.seed(2015)\ndef get_silhouette_score(data, model):\n    predicted_labels = model.fit_predict(data)\n    score = silhouette_score(X = data, labels = predicted_labels, sample_size = 500)\n    return score\nprint (\"Silhouette Score for KMeans with 5 clusters = {}\".format(get_silhouette_score(user_np_matrix, model_5)))\nprint (\"Silhouette Score for KMeans with 25 clusters = {}\".format(get_silhouette_score(user_np_matrix, model_25)))\nprint (\"Silhouette Score for KMeans with 50 clusters = {}\".format(get_silhouette_score(user_np_matrix, model_50)))\n", "intent": "**c.** Calculate the Silhouette Score using 500 sample points for all the kmeans models.\n"}
{"snippet": "random.seed(2015)\nprint (fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response))\n", "intent": "**b.** Now that you've added the categorical data, let's see how it works with a linear model!\n"}
{"snippet": "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\ntest_accuracy = np.mean(predictions == testY[:,0], axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "After you're satisified with your hyperparameters, you can run the network on the test set to measure its performance.\n"}
{"snippet": "from sklearn.metrics import recall_score\nfinal_idea_recall = recall_score(expected_values, predicted_values)\nprint('Final model recall- {}'.format(final_idea_recall))\n", "intent": "Final model accuracy- 0.87\n"}
{"snippet": "from sklearn.metrics import precision_score\nfinal_idea_precision = precision_score(expected_values, predicted_values)\nprint('Final model precision- {}'.format(final_idea_precision))\n", "intent": "Final model recall- 0.9428571428571428\n"}
{"snippet": "from sklearn.metrics import f1_score\nfinal_idea_f1 = f1_score(expected_values, predicted_values)\nprint('Final model precision- {}'.format(final_idea_f1))\n", "intent": "Final model precision- 0.75\n"}
{"snippet": "test_ds = next(fit_gen())\ny_anchors, y_scores = rcnn_model.predict(test_ds[0])\nprint(y_anchors.shape, y_scores.shape)\n", "intent": "Here we apply the model to the test data\n"}
{"snippet": "r2 = r2_score(weights_test, weights_predicted)\nprint(r2)\n", "intent": "Like the MSE function, ``r2_score`` takes the true values and the predicted values as its argument.\n"}
{"snippet": "y_predicted = clf.predict(X_test)\ny_predicted[:5]\n", "intent": "Using the classifier's ``predict`` method, let's see what labels the classifier would assign to the *unseen* test data. \n"}
{"snippet": "estimated_weights = lr.predict([[102], [151], [92], [43]])\nestimated_weights\n", "intent": "The ``predict`` method can handle a list of heights. If 5 aliens come in and we need to estimate their weights:\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors\n"}
{"snippet": "roc_auc_score(y_test, rf_clf_cv.predict(X_test))\n", "intent": "Using the GridSearchCV object: rf_clf_cv we can calculate the AUC score for test data.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from imblearn.metrics import (geometric_mean_score,\n                              make_index_balanced_accuracy)\nprint('The geometric mean is {}'.format(geometric_mean_score(\n    y_test_up, y_pred_up)))\n", "intent": "Since the data is imbalanced, it is better to change the metric to g-mean\n"}
{"snippet": "extra_predicted_labels = extra_tree.predict(train_data_2_p)\nprint(\"Extra Tree Classification Report: \\n\", classification_report(train_labels_2, extra_predicted_labels))\nprint(\"Confusion Matrix: \\n\",confusion_matrix(train_labels_2, extra_predicted_labels))\n", "intent": "We can see that our new features have made it to the top of the list on the table of features by importance.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "[`Efectividad`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n"}
{"snippet": "from sklearn.metrics import classification_report\nclases = ['Setosa', 'Versicolour', 'Virginica']\nprint(classification_report(y_test, y_pred, target_names=clases))\n", "intent": "[`Reporte`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n"}
{"snippet": "test_bow = bow_vectorizer.transform(traint_text_generator(df_test))\ny_true = df_test.is_robbery.values\ny_pred = regression.predict(test_bow)\n", "intent": "Time to see the metrics - **evaluate model**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    loss = 0\n    j = 0\n    for i in style_layers:\n        G = gram_matrix(feats[i])\n        loss += style_weights[j]*((G-style_targets[j])**2).sum()\n        j+=1\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "loss , overall_acc = model.evaluate(x_test , y_test , batch_size = 16 , verbose = 1)\n", "intent": "<br>\n** 2 - Perform Evaluation: **\n<br>\n"}
{"snippet": "y_predict = model.predict(x_test , batch_size = 16 , verbose = 1)\n", "intent": "<br>\n** 3 - Perform prediction:**\n<br>\n"}
{"snippet": "loss , overall_acc = model.evaluate({'imag_in' : xi_test , 'num_in' : xn_test } , {'comb_out' : y_test} , \n                                    batch_size = 16 , verbose = 1)\n", "intent": "<br>\n** 2 -Evaluate model Model:**\n<br>\n"}
{"snippet": "y_predict = model.predict({'imag_in' : xi_test , 'num_in' : xn_test } , batch_size = 16 , verbose = 1)\n", "intent": "<br>\n** 3 - generate pridictions:**\n<br>\n"}
{"snippet": "loss , overall_acc = model.evaluate(x_test , y_test , batch_size = 64 , verbose = 1)\n", "intent": "<br>\n** 2 - Perform Evaluation:**\n<br>\n"}
{"snippet": "y_predict = model.predict(x_test , batch_size = 16 , verbose = 1)\n", "intent": "<br>\n** 3 - Perform Prediction: **\n<br>\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100.*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100.*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100.*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def get_metrics(X,y, model):\n    num_validations = 5\n    accuracy = cross_validation.cross_val_score(model, X, y, scoring='accuracy', cv=num_validations)\n    print 'Accuracy =' , accuracy.mean(), '%'\n    f1 = cross_validation.cross_val_score(model, X, y, scoring='f1_weighted', cv=num_validations)\n    print 'F1 Score =' , f1.mean() , '%'\n    precision = cross_validation.cross_val_score(model, X, y, scoring='precision_weighted', cv=num_validations)\n    print 'Precision =' , precision.mean() , '%'\n    recall = cross_validation.cross_val_score(model, X, y, scoring='recall_weighted', cv=num_validations)\n    print 'Recall =' , recall.mean() , '%'   \n", "intent": "$ \\text{F1 Score } = \\frac{2 * \\text{ precission } * \\text{ recall } } {\\text{ precission } + \\text{ recall }} $\n"}
{"snippet": "target_names = ['Class-0', 'Class-1', 'Class-2', 'Class-3'] \nprint(classification_report(y, y_pred, target_names=target_names))\n", "intent": "We directly have a function to calculate all required metrics for us and that is metrics.classification report.\n"}
{"snippet": "benchmark_results = [0.5] * len(test_labels)\nprint \"The number to beat is: \" + str(log_loss(test_labels, benchmark_results))\n", "intent": "First, start by splitting the restaurants that the user has reviewed into training and test sets\n"}
{"snippet": "rt = rt.reshape(rt.shape[0],1,1)\nout = model.predict(rt)\nbs = prepareBootstrap(out)\nout2 = out[:,np.newaxis,:]*bs[:,:,np.newaxis]\nouproj = np.sum(out,axis=0)      \noustd  = np.std(np.sum(out2,axis=0),axis=0)\ngeproj = np.sum(gtcat,axis=0)\n", "intent": "Unfold the reference sample, prepare bootstrap replica of the result to estimate stat. uncertainties.\n"}
{"snippet": "print(\"SVM accuracy on CV dataset using tf-idf: {}\".format(accuracy_score(svm.predict(cv_data),cv_labels)))\n", "intent": "**Testing our model on the test data**\n"}
{"snippet": "print(\"SVM accuracy on CV dataset using word2vec: {}\".format(accuracy_score(svm_w2v.predict(cv_data),cv_labels)))\n", "intent": "**Testing our SVM model on the test data**\n"}
{"snippet": "err_cv, acc_cv = dnn.evaluate(cv_data,cv_labels)\nprint(\"DNN accuracy on CV dataset using word2vec: {}\".format(acc_cv))\n", "intent": "**Testing our DNN model on the test data**\n"}
{"snippet": "predictions = lin_model.predict(X_test)\n", "intent": "using the predict method I can create an array of predictions\n"}
{"snippet": "CV_result = cv_score(RF, X, y_log,mean_squared_error, nfold=5)\nprint('Review texts random forest fit with log of sales rank')\nprint('Training set Cross Validated Mean RMSE: ',\n      CV_result[0])\nprint('Testing set Cross Validated Mean RMSE: ',\n      CV_result[1])\n", "intent": "As seen above, the sales ranks are highly skewed. Taking the log transformation may helps improve the model performances. \n"}
{"snippet": "treereg = DTR(min_samples_split=22, min_samples_leaf=10)\nfrom sklearn.cross_validation import cross_val_score\nscores = cross_val_score(treereg, X, y, cv=10, scoring='neg_mean_squared_error')\nnp.mean(np.sqrt(-scores))\n", "intent": "> *Iterating over 1 feature (**min_samples_leaf**) , the tree built with min. **10** samples in each leaf exhibits the lowest RMSE.*\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(\"R^2 score: %.2f\"%regr.score(X_test,y_test))\nprint(\"Residual sum of squre: %.2f\"% np.mean((regr.predict(X_test)-y_test)**2))\nprint('Coefficients: \\n', regr.coef_)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(regr, X_train, y_train, cv=5)\nprint(scores)\nprint(\"Judgeing from cross valdiation score, my model is robust.\")\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "cross = cross_val_score(svm, X.reshape(-1, 1), y, cv=10, verbose=100)\n", "intent": "Simple cross validation\n"}
{"snippet": "cv = StratifiedKFold(2)\nscore, permutation_scores, pvalue = permutation_test_score(\n    svm, X.reshape(-1, 1), y, scoring=\"accuracy\", cv=cv, n_permutations=100, n_jobs=1)\nprint(\"Classification score %s (pvalue : %s)\" % (score, pvalue))\n", "intent": "Test with permutations the significance of a classification score\n"}
{"snippet": "ridge_test_preds = ridgecv.predict(X_test_scaled_ridge)\nlasso_test_preds = lassocv.predict(X_test_scaled_lasso)\nelastic_test_preds = elasticcv.predict(X_test_scaled_elastic)\n", "intent": "Interpret Regression Metrics for each of your models. Choose one of the following:\n* R2\n* MSE / RMSE\n* MAE\nWhat are your top 3 performing models? \n"}
{"snippet": "from ***** import *****\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Let's check the **RMSE**...\n"}
{"snippet": "from sklearn import metrics \nprint( metrics.accuracy_score(y_test, y_pred))\n", "intent": "How accurate is our classifier?\n+ recalll that we have the true labels for the testing data\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])    \n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    tL_s = tf.zeros(1)\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        tL_s += (style_weights[i] * tf.reduce_sum(tf.squared_difference(G, style_targets[i])))\n    return tL_s\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = logisticRegr.predict(x_test)\n", "intent": "predict the labels of new data\n"}
{"snippet": "def predict(img, label):\n    NN_result = sess.run(Y, feed_dict={img_x: [img], pkeep: 1})\n    img_class_confident = NN_result[0][label] * 100\n    return img_class_confident\n", "intent": "We are going to create method that will get the confident level of certain label.\n"}
{"snippet": "print('Adjusted Rand Index')\nprint('K-means')\nprint(metrics.adjusted_rand_score(targets.Class,y_pred))\nprint('Agglomerative')\nprint(metrics.adjusted_rand_score(targets.Class,y_pred2))\n", "intent": "So since we actually do know the truth we can defined better metrics...\n"}
{"snippet": "blend1 = 0.25*(nnpred+xgbpred+rfpred+etpred)\nmulticlass_log_loss(yte.values, blend1.values)\n", "intent": "Let's try giving each model equal weights:\n"}
{"snippet": "y_pred = sm_pois.predict(X_test)\ndef r2(a,b):\n    SSE = 0\n    SST = 0\n    for i, j in zip(a,b):\n        SSE += (j - i)**2\n        SST += (i - a.mean())**2 \n    return 1 - SSE/SST\nr2(y_test,y_pred)\n", "intent": "Find R^2 manually, since the the GLM doesn't automatocally do this.\n"}
{"snippet": "y_pred = sm_pois.predict(X_test)\ndef r2(a,b):\n    SSE = 0\n    SST = 0\n    for i, j in zip(a,b):\n        SSE += (j - i)**2\n        SST += (i - a.mean())**2 \n    return 1 - SSE/SST\nr2(y_test,y_pred)\n", "intent": "Find pseudo-R^2 for GLM model\n"}
{"snippet": "y_prob = pipe_baseline.predict_proba(X_test)\n", "intent": "Ok that's a beginning! Lets work with predict_proba to find the best threshold that optimizes our f1_score for insincere questions (target = 1)\n"}
{"snippet": "df_predict = logreg.predict(X_test)\n", "intent": "<h2>\nNow that we trained on the subset, we can now run prediction on the test subset.\n</h2>\n"}
{"snippet": "df_predict2 = logreg2.predict(X_test2)\n", "intent": "<h2>\nThe new predictions and score is better now: \n</h2>\n"}
{"snippet": "df_predict1 = logreg1.predict(XX_test)\n", "intent": "<h2>\nThen we run the prediction and score on the test data. \n</h2>\n"}
{"snippet": "labels = hc.fit_predict(X)\nlabels\n", "intent": "Determine the labes of the clustering model.\n"}
{"snippet": "def map_score(dataframe,customers,prob):\n    dataframe['Propensity'] = 0\n    for i in range(len(dataframe)):\n        idx = dataframe.index[i]\n        for j in range(len(customers)):\n            if dataframe.loc[idx,'Client'] == customers[j]:\n                dataframe.loc[idx, \"Propensity\"] = prob[j]\n", "intent": "This function maps the probability values (Propensity) calculated by the models with the Corresponding Dataframe\n"}
{"snippet": "pscore = logistic.predict_proba(X_test)[:,1] \n", "intent": "The accuracy obtained is 62.93 %, now the propensity for the Test Cases will be calculated to determine which customers to contact\n"}
{"snippet": "map_score(data_loan,Clients,pscore)\n", "intent": "The map_score function assigns the propensity of each client to buy the consumer loan\n"}
{"snippet": "pscore = clf.predict_proba(X_test)[:,1]\nClients = X_test[:,-1]\n", "intent": "The probability score of the test values is then computed to determine the propensity of customers interesed in credit cards\n"}
{"snippet": "map_score(data_credit,Clients,pscore)\n", "intent": "The function to map the propensity score with the Credit card Dataset\n"}
{"snippet": "pscore = clf.predict_proba(X_test)[:,1]\n", "intent": "An accuracy of 71.32 % was obtained by modeling using this method<br>\nProbability score of the test dataset is then computed based on the model\n"}
{"snippet": "Clients = X_test[:,-1]\nmap_score(data_mutual_fund,Clients,pscore)\n", "intent": "The propensity thus calculated is mapped with the client details of the Mutual fund data using the map function\n"}
{"snippet": "y_preds2 = xgb.predict(X_test2)\n", "intent": "But first, let's see what the performance of the new XGBoost model is on its test2 data:\n"}
{"snippet": "import sklearn\nimport pickle\nmodel = pickle.load(open('Data/classifiers/classifier.pkl', 'r'))\nmodel\nbla = fashion.map(lambda x: eval(x)['reviewText']).first()\nmodel_b = sc.broadcast(model)\nfashion.map(lambda x: eval(x)['reviewText']).map(lambda x: (x, model_b.value.predict([x])[0])).first()\n", "intent": "<h1>Optional: Integrating Spark with popular Python libraries</h1>\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(precision_score(y_train_5, y_train_pred))\nprint(recall_score(y_train_5, y_train_pred))\n", "intent": "Also called _sensitivity_\nCommonly used along with _recall_\nIllustrated Example\n"}
{"snippet": "print(\"Accuracy {0:.1f}%\".format(100*accuracy_score(y, np.repeat(\"Standard\", y.shape))))\n", "intent": "Accuracy might not always be the best metric. Since only 10% of cases are declined, by predicting all cases to be true we can get an accuracy of 90%.\n"}
{"snippet": "print(\"Accuracy {0:.1f}%\".format(100*accuracy_score(y, rule_predictions)))\n", "intent": "The accuracy from the rules based method is impressive.\n"}
{"snippet": "print(\"Accuracy of custom rules: {0:.1f}%\".format(100*accuracy_score(y, rule_predictions)))\nprint(\"Accuracy of decision tree: {0:.1f}%\".format(100*accuracy_score(y, predictions)))\n", "intent": "Let's look at the accuracy of the custom rules versus the decision tree\n"}
{"snippet": "prediction = model.predict(imgs_val)\nvisualize_points(imgs_val[55], prediction[55])\nvisualize_points(imgs_val[56], prediction[56])\nvisualize_points(imgs_val[47], prediction[47])\nvisualize_points(imgs_val[89], prediction[89])\n", "intent": "Model is used to predict key facial points on sample images.\n"}
{"snippet": "y_train_true,y_train_pred = y_train,mpl.predict(x_train)\ny_test_true, y_test_pred  = y_test, mpl.predict(x_test)\nprint('The R2 of test dataset is%0.3f,and\\n the R2 of train dataset is %0.3f'%(r2_score(y_test_true, y_test_pred),\n      r2_score(y_train_true,y_train_pred)))\n", "intent": "EVELUATION OT THE BEST MODEL\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "We will run prediction on the X_test and store the result as y_pred\n"}
{"snippet": "my_model.predict([[4.9,3.1,1.5,0.2]])\n", "intent": "then we test our model's predictions with any value to check results.\n"}
{"snippet": "clf.predict(mqns[:1]),vals[0]\n", "intent": "What is our prediction?\n"}
{"snippet": "print(\"R^2: %0.2f\" % good_model.score([mqn for m,mqn,val in testset], [val for m,mqn,val in testset]))\nprint(\"R: %0.2f\" % np.sqrt(good_model.score([mqn for m,mqn,val in testset], [val for m,mqn,val in testset])))\nprint(\"MSE: %0.2f\" %  mean_squared_error(good_model.predict([mqn for m,mqn,val in testset]), [val for m,mqn,val in testset]))\n", "intent": "Here are the numbers:\n"}
{"snippet": "y_pred = m.predict(x_val, verbose=1)\n", "intent": "Generate predictions on the validation set.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(x_test, y_test)\nprint('     Test loss:', test_loss)\nprint(' Test accuracy:', test_acc)\n", "intent": "The validation accuracy tops at around 85%. But the network quickly overfits. Let's evaluate the network on the\ntest set.\n"}
{"snippet": "yhat_train = linreg.predict(Xtrain)\nresiduo = np.power(yhat_train - ytrain, 2)\nimport pylab \nimport scipy.stats as stats\nstats.probplot(residuo,dist=\"norm\", plot=pylab)\npylab.title(\"Residuos\")\npylab.show()\n", "intent": "error de entrenamiento y QQ-plot\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipe, cancer.data, cancer.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n", "intent": "* Now with cross-validation:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"commuter-electric-locomotive-engine-159148.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfolds = 5\nscores = cross_val_score(estimator=lr, X=X_train, y=y_train, scoring='precision', cv=folds)\nprint 'mean precision score over {} folds: '.format(folds), np.mean(scores)\n", "intent": "That's better than baseline accuracy. but let's validate with cross val score.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfolds = 5\nscores = cross_val_score(estimator=lr, X=X_train, y=y_train, scoring='f1', cv=folds)\nprint 'mean f1 score over {} folds: '.format(folds), np.mean(scores)\n", "intent": "That's better than baseline!\n"}
{"snippet": "y_pred = gs.best_estimator_.predict(X_test)\n", "intent": "our Baseline fast food precision for 1s: 0.263, so this is a pretty good precision. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is worse because it has a score of 0.74, while before, without optimization, the score was 0.77\n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(X) \n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nimport pickle\nmodel = pickle.loads(b'\\x80\\x02csklearn.svm.classes\\nSVC\\nq\\x00)\\x81q\\x01}q\\x02(X\\x17\\x00\\x00\\x00decision_function_shapeq\\x03NX\\x05\\x00\\x00\\x00_implq\\x04X\\x05\\x00\\x00\\x00c_svcq\\x05X\\x06\\x00\\x00\\x00kernelq\\x06X\\x06\\x00\\x00\\x00linearq\\x07X\\x06\\x00\\x00\\x00degreeq\\x08K\\x03X\\x05\\x00\\x00\\x00gammaq\\tX\\x04\\x00\\x00\\x00autoq\\nX\\x05\\x00\\x00\\x00coef0q\\x0bG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00X\\x03\\x00\\x00\\x00tolq\\x0cG?PbM\\xd2\\xf1\\xa9\\xfcX\\x01\\x00\\x00\\x00Cq\\rG?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00X\\x02\\x00\\x00\\x00nuq\\x0eG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00X\\x07\\x00\\x00\\x00epsilonq\\x0fG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00X\\t\\x00\\x00\\x00shrinkingq\\x10\\x88X\\x0b\\x00\\x00\\x00probabilityq\\x11\\x89X\\n\\x00\\x00\\x00cache_sizeq\\x12K\\xc8X\\x0c\\x00\\x00\\x00class_weightq\\x13NX\\x07\\x00\\x00\\x00verboseq\\x14\\x89X\\x08\\x00\\x00\\x00max_iterq\\x15J\\xff\\xff\\xff\\xffX\\x0c\\x00\\x00\\x00random_stateq\\x16NX\\x10\\x00\\x00\\x00_sklearn_versionq\\x17X\\x06\\x00\\x00\\x000.18.1q\\x18ub.')\nscores = cross_val_score(model, features.drop(\"label\", axis=1).values, features[\"label\"].as_matrix(), cv=5, n_jobs=-1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Turns out that you can pickle a model in python 2 and unpickle it in python 3 and it works just fine!  Which is pretty neat-o.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    loss = 0\n    for i in range(len(style_layers)):\n        style_source = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum((style_source - style_targets[i]) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "detector_scores = cbc.predict_proba(train_data)[xrange(len(train_data)), 1 - train_targets_polluted]\n", "intent": "Calculate **Detector** scores:\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "<h4>Predicting test set results</h4>\n"}
{"snippet": "accuracy_score(y_test, np.zeros_like(y_test))\n", "intent": "Baseline Accuracy (If we only predict majority class)\n"}
{"snippet": "accuracy_score(y_test, y_pred_class)\n", "intent": "Accuracy - Test Set\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(clf1, X_train, y_train, scoring = 'roc_auc', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "AUC - Cross-validated\n"}
{"snippet": "print classification_report(y_test, y_pred_class)\n", "intent": "Classification Report - Test Set\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(clf2, X_train, y_train, scoring = 'roc_auc', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "AUC - Cross-validated\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(clf, X_train, y_train, scoring = 'neg_log_loss', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "Cross-validated log loss\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(clf, X_train, y_train, scoring = 'roc_auc', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "Cross-validated AUC\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(clf_tuned, X_train, y_train, scoring = 'neg_log_loss', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "Cross-validated log loss\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(clf_tuned, X_train, y_train, scoring = 'roc_auc', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "Cross-validated AUC\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(gbm_tuned, X_train, y_train, scoring = 'neg_log_loss', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "Cross-validated log loss\n"}
{"snippet": "stratification = StratifiedKFold(y_train,n_folds = 5)\nscores = cross_val_score(gbm_tuned, X_train, y_train, scoring = 'roc_auc', cv = stratification, n_jobs = -1)\nprint np.mean(scores), np.std(scores)\n", "intent": "Cross-validated AUC\n"}
{"snippet": "y = clf_tuned.predict_proba(X_test)[:,1]\n", "intent": "Using the tuned random forest model to make predictions.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ntree_predicted = clf.predict(X_test)\nconfusion = confusion_matrix(y_test, tree_predicted)\nprint('Decision tree classifier (max_depth = 5)\\n', confusion)\n", "intent": "Based on the best parameter derived from the GredSearchCV we are calculating and comparing the various evaluation metrics.\n"}
{"snippet": "new_x = np.array([30, 40])\nlr_ing.predict(new_x.reshape(-1, 1))\n", "intent": "Aunque no salta a la vista se tiene que a mayor edad mayor ingreso \nPor un aumento de uno en la edad en promedio aumenta 542.2 el ingreso\n"}
{"snippet": "from sklearn import metrics\ningreso_est = lr_ing.predict(income['Age'].values.reshape(-1, 1))\nMSE = metrics.mean_squared_error(income['Income'].values.reshape(-1, 1), ingreso_est)\nprint('MSE:  ',round(MSE,2))\nprint('RMSE: ',round(np.sqrt(MSE),2))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "loss += settings['continuity'] * continuity_loss(dream_in,img_height, img_width) / np.prod(img_size)\nloss += settings['dream_l2'] * K.sum(K.square(dream_in)) / np.prod(img_size)\n", "intent": "Some additional loss terms are added to make the image look nicer:\n"}
{"snippet": "feats_occ = extractor.predict(ims_acc, batch_size=32, verbose=0)\nfeats_occ.shape\n", "intent": "```ims_occ``` contains all images with the occluder set at different positions. Let's run these through our extractor:\n"}
{"snippet": "y_pred = model_LR.predict(X_test)\nconfusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix\n", "intent": "Predicting the outputs using Logistic Regression\n"}
{"snippet": "auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc\n", "intent": "Calculating Area under the curve for the classifier\n"}
{"snippet": "print 'predicted:', spam_detector.predict(tfidf_BOW.transform(BOW[2]))[0]\nprint 'expected:', df_message['label'][2]\n", "intent": "Lets try to see how does it perform:\n"}
{"snippet": "preds = model.predict(img)\nprint('Predicted:', decode_predictions(preds, top=3))\n", "intent": "Now, let's call the model on our image to predict the top 3 labels. This will take a few seconds.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    L = len(style_layers)\n    for i in range(L):\n        style_current = gram_matrix(feats[style_layers[i]])\n        style_loss+=style_weights[i]*torch.sum((style_current-style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions =[]\nfor feature in test_Resnet50:\n    feature = np.expand_dims(feature, axis=0)\n    prediction = np.argmax(Resnet50_model.predict(feature))\n    predictions.append(prediction)\ntest_accuracy = 100*np.sum(predictions==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv =kfold, scoring=sco_MSE)\n    results.append(cv_results)\n    names.append(name)\n    print \"{:s}: {:f} {:f}\".format(name, cv_results.mean(), cv_results.std())\n", "intent": "The algorithms all have the default tuning parameters. Let's compare them and display the mean and standard deviation of MSE for each algorithm\n"}
{"snippet": "n_test_unit = 500\ny_pred = nn.predict(X_pred=X_test[:n_test_unit])\nprint_fore(\"Testing on {} images\".format(n_test_unit))\nprint_fore('Accuracy: %f' % ( np.mean(y_pred == y_test[:n_test_unit]) ))\n", "intent": "If running on smaller dataset, skip to k-Nearest Neighbor Classifier\n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(X.T)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_hat = model.predict(x_test, batch_size=64)\n", "intent": "y_hat consists of class probabilities (corresponding to the one-hot encoding of the training labels).\n"}
{"snippet": "w = naive_bayes_fit( x, y, ds )\nprint(naive_bayes_predict( 1, ds, w ))\nprint(clf.predict_proba( 1 ))\n", "intent": "<p>Looks like pomegranate is faster at Naive Bayes too, but lets make sure we've come to the same distribution in the end.</p>\n"}
{"snippet": "def evaluate(x,y,accuracy_operation,sess,X_data, y_data, BATCH_SIZE=64):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy=sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "defining some functions to evaluate the CNN\n"}
{"snippet": "def evaluate(xTensor,yTensor,accuracy_operation,X_data, y_data, BATCH_SIZE=64):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy=sess.run(accuracy_operation, feed_dict={xTensor: batch_x, yTensor: batch_y})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "It gives the accuracy of the model with respect to all the class labels given\n"}
{"snippet": "def get_kNN_test_score(U,d):\n    print('(info) Evaluationg with d=%i.'%d)\n    new_training_set = U[:,:d].T@view1[0]\n    new_training_set = np.concatenate((view1[0][117:156,:].T,new_training_set.T),axis=1)\n    new_testing_set = U[:,:d].T@view1[2]\n    new_testing_set = np.concatenate((view1[2][117:156,:].T,new_testing_set.T),axis=1)\n    score = kNN_test(new_training_set,new_testing_set,test_label=testing_code)\n    print('(result) Testing accuracy for k=4 kNN is %.4f with d=%i.'%(score,d)) \n    return score\nget_kNN_test_score(U,90)\n", "intent": "Therefore, for PLS, Best accuracy for k=4 kNN is 0.8286 with d=90.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    layerloss = 0\n    for i in range(len(style_layers)):\n        g = gram_matrix(feats[style_layers[i]])\n        layerloss += style_weights[i] * 2 * tf.nn.l2_loss(g - style_targets[i])\n    return layerloss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy score:', accuracy_score(y, labels))\n", "intent": "Predicted label are NOT same a the input labels. Why?\n"}
{"snippet": "model1_predict = model1.predict(X_test.values.reshape(-1,1))\n", "intent": "Predict labels for the test dataset using the trained model\n"}
{"snippet": "from sklearn import metrics\nprint (model1.coef_, model1.intercept_)\nprint(model1.predict(8))\nRMSE_linear_rooms = np.sqrt(metrics.mean_squared_error(model1_predict, y_test))\nprint (RMSE_linear_rooms)\n", "intent": "Print the coefficient and the intercept of the line of regression. Calculate RMSE for values in test dataset and corresponding predicted values\n"}
{"snippet": "model_multi_predict = model_multi.predict(X_test)\n", "intent": "Predict labels for the test dataset\n"}
{"snippet": "model_poly.predict(8)\n", "intent": "Predict price given the number of rooms in a house\n"}
{"snippet": "print (model1.coef_, model1.intercept_)\nprint(model1.predict(8))\nRMSE_linear_rooms = np.sqrt(metrics.mean_squared_error(model1_predict, y_test))\nprint (RMSE_linear_rooms)\n", "intent": "Print the coefficient and the intercept of the line of regression. Calculate RMSE for values in test dataset and corresponding predicted values\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, accuracy_score\nprint('Accuracy score:', accuracy_score(y, labels))\nprint(confusion_matrix(y, labels))\n", "intent": "Predicted label are NOT same a the input labels. Why?\n"}
{"snippet": "model1_predict = model1.predict(X_test.values.reshape(-1,1))\n", "intent": "Predict the labels for the test dataset using the trained model\n"}
{"snippet": "model_multi_predict = model_multi.predict(X_test)\n", "intent": "Predict the labels for the test dataset\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from distiller.quantization import PostTrainLinearQuantizer, LinearQuantMode\nfrom copy import deepcopy\nman_model = torch.load('./manual.checkpoint.pth.tar')\nval_loss = evaluate(man_model, val_data)\nprint('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))\n", "intent": "We quantize the model after the training has completed.  \nHere we check the baseline model perplexity, to have an idea how good the quantization is.\n"}
{"snippet": "model_fp16 = deepcopy(man_model).half()\nval_loss = evaluate(model_fp16, val_data)\nprint('val_loss: %8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))\n", "intent": "A tiny bit better, but still no good. Let us try the half precision version of the model:\n"}
{"snippet": "[loss,mae] = model.evaluate(test_data,test_labels,verbose=1)\nprint(\"Testing set Mean Absolute Error: ${:7.2f}\".format(mae*1000))\n", "intent": "Let's see how did the model performs on the test set:\n"}
{"snippet": "model.load_weights(checkpoint_path)\nloss,acc = model.evaluate(test_images,test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Then load the weights from the checkpoint, and re-evaluate:\n"}
{"snippet": "        XtXinv = np.linalg.inv(np.matmul(X.T,X))      \n        self.w = np.matmul(np.matmul(XtXinv,X.T),y)\n        return self.w\n    def predict(self,X):\n        self.W = self.W.append(pd.DataFrame.from_dict({i:np.array(self.w)},orient='index'))\n        pass\n", "intent": "The w that minimizes the least square problem is:\n\\\\( w = (X^TX)^{-1}X^Ty \\\\) .Where the vector w is the parameter vectors.\n"}
{"snippet": "train_predictions = lin_reg.predict(train_prepared)\nlin_mse = mean_squared_error(train_labels, train_predictions)\nlin_rmse = np.sqrt(lin_mse)\nRMSE = int(lin_rmse)\nRMSE\n", "intent": "Calculate the rmse error\n"}
{"snippet": "test_predictions = lin_reg.predict(test_prepared)\n", "intent": "I'm not sure why get_dummies is returning so many columns for the test data..\n"}
{"snippet": "EXAMPLES = ['July 23, 1975', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "kmeans.predict([my_taste])\n", "intent": "We can map my taste into one of the clusters like this.\n"}
{"snippet": "my_taste = [5.0, 0.0, 1.0, 0.0, 4.0, 1.0, 2.0, 4.0, 0.0, 5.0, 5.0]\npred = kmeans.predict([my_taste])\npred\n", "intent": "Show the list of people who have the same taste as mine.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Then we can evaluate the model.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Let's evaluate the model again.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Evaluate test accuracy.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint('Training loss:', score[0])\nprint('Training accuracy:', score[1])\n", "intent": "We check our test loss and training loss again.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint('Training loss:', score[0])\nprint('Training accuracy:', score[1])\n", "intent": "Now our training accuracy is lower (72%) but our test accuracy is higher (69%).  This is more like what we expect.\n"}
{"snippet": "X, y = data, labels\ny_pred = net.predict(X)\nerror = cost(y_pred, y)\nprint('predicted %0.2f for example 0, actual %0.2f, total cost %0.2f'%(pred_y[0], y[0], error))\n", "intent": "Now to do a forward pass, we can simply run the networks `predict` function:\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Evaluate the performance of the network.\n"}
{"snippet": "y_pred = model.predict(x_test)\nfor yp, ya in list(zip(y_pred, y_test))[0:10]:\n    print(\"predicted %0.2f, actual %0.2f\" % (yp, ya))\n", "intent": "To get the raw predictions:\n"}
{"snippet": "x_sample = x_test[0].reshape(1, 3)   \ny_prob = model.predict(x_sample)\nprint(\"predicted %0.3f, actual %0.3f\" % (y_prob[0][0], y_test[0]))\n", "intent": "We can also predict the value of a single unknown example or a set of them in th following way:\n"}
{"snippet": "salary_15years = regressor.predict(15)\nsalary_15years\n", "intent": "Lets predict the salary when years of experience is, say 15 years.\n"}
{"snippet": "salary_2years = regressor.predict(2)\nsalary_2years\n", "intent": "Lets predict the salary when years of experience is, say 2 years this time.\n"}
{"snippet": "score = model.evaluate(X_test, keras.utils.to_categorical(y_test, 2), verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "1. loss and accuracy\n2. confusion matrix\n3. classification report\n4. plots of loss,accuracy versus epochs\n"}
{"snippet": "print 'Mean squared error:', np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "yfit = model.predict(x_test)\n", "intent": "get predicted 'y' values for held back data X_test\n"}
{"snippet": "yfit = model.predict(X_test)\n", "intent": "*4. Get predictions by applying your model to the test data set (call .predict() with your model).\n"}
{"snippet": "from sklearn.metrics import r2_score, mean_squared_error\nresults ={\n    \"R2\": r2_score(yfit, y_test),\n    \"MSE\": mean_squared_error(yfit, y_test),\n}\nprint(results)\n", "intent": "*5. Measure the accuracy of your model by comparing the predicted values to the actual labels in your test data.\n"}
{"snippet": "yfit = knn.predict(Xtest)\n", "intent": "__v. Get predictions by applying your model to the test data set__\n"}
{"snippet": "val_acc = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Accuracy on the validation set:\", val_acc[1])\n", "intent": "**Check validation and training accuracy**\n"}
{"snippet": "Y_pred = regressor.predict(X_test)\n", "intent": "Now, predict the test set results. Y_pred is the vector of predictions of the dependent variable\n"}
{"snippet": "Y_prediction = regressor.predict(X_test)\n", "intent": "Predict the test set results\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred = pl.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n", "intent": "- For simple linear regression model r2 is 0.34\n- RMSE is 0.09\n"}
{"snippet": "input_fn_dnn_pred = tf.estimator.inputs.pandas_input_fn(\n    x=x_test,\n    batch_size=batch_size,\n    num_epochs=1,\n    shuffle=False\n)\ndnn_preds = list(dnn_model.predict(input_fn=input_fn_dnn_pred))\ndnn_preds\n", "intent": "Before even visualizing the data, the DNN is classifying with an accuracy of ~96%, which is fantastic.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df.Cluster, kmeans.labels_))\nprint('\\n')\nprint(classification_report(df.Cluster, kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "def precision_no(y_true, y_pred):\n    return precision_score(y_true, y_pred, pos_label=0)\ndef recall_no(y_true, y_pred):\n    return recall_score(y_true, y_pred, pos_label=0)\n", "intent": "Making the model starting with a simple 3 layer model with 20\n"}
{"snippet": "train_pred = model.predict(X_train)\ntest_pred = model.predict(X_test)\n", "intent": "As we can see, there is no overfitting and the gradient is steadily descending. This also shows the mathematical nature of our signal\n"}
{"snippet": "net = tn.Classifier(layers=[train[0].shape[1], 1000, 500, 100, (7, 'softmax')], weighted=True)\nfor tr, val in net.itertrain((train[0], train[1], weights), (test[0], test[1], weights_t),  algo='nag', \n                             learning_rate = 1e-3, momentum = 0.9, weight_l1 = 10):\n    print('training loss:', tr['loss'], tr['acc'])\n    print('most recent validation loss:', val['loss'], val['acc'])\npredictions = net.predict(test[0])\nprint('classification_report:\\n', classification_report(test[1], predictions))\nprint('accuracy: ', accuracy_score(test[1], predictions))\n", "intent": "Poor performance, was stoped mannualy.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"traffic.jpeg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ncontent_loss_test(answers['cl_out'])  \n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        G_style = style_targets[i]\n        G_content = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * 2 * tf.nn.l2_loss(G_style-G_content)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L = 0\n    for i, sl in enumerate(style_layers):\n        g = gram_matrix(feats[sl])\n        L += style_weights[i] * ((g - style_targets[i]) ** 2).sum()\n    return L\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = cross_val_predict(regr.best_estimator_, X_train, y_train)\nrmsle(y_train, y_pred)\n", "intent": "Calculated RMSLE using 10 fold cross validation.\n"}
{"snippet": "def gridsearchcv_report(predictor):\n    print(\"Best parameters: {}\".format(predictor.best_params_))\n    print(\"Best score: {}\".format(predictor.best_score_))\n", "intent": "Used GridSearchCV to find out which regularization parameters fit best. Used 5 fold cross validation to speed up training.\n"}
{"snippet": "preds = trained_classifier.predict_proba(X_test)\nprint(preds)\n", "intent": "That's it, the classifier is trained and can be used to predict the topic of a new question from the test set.\n"}
{"snippet": "print(\"AUROC: {}\".format(round(roc_auc_score(y_test, (1-lr_preds)), 2)))\nprint(\"f1 score: {}\".format(round(f1_score(y_test, lr_preds), 3)))\n", "intent": "* https://elitedatascience.com/imbalanced-classes \n* http://scikit-learn.org/\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "metrics.roc_auc_score(Y_roc_true, Y_pred)\n", "intent": "As seen with the ROC graph the AUC is close to 0.5\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores_train = cross_val_score(clf, X_train, y_train)\nscores_test = cross_val_score(clf, X_test, y_test)\n", "intent": "We can do a little bit of testing to see how well the classifier predicts using cross validation of the testing data.\n"}
{"snippet": "scores = cross_val_score(regr, X_train, y_train, cv=5)\nprint(scores)\nprint(\"According to the cross valdiation score, the model is robust.\")\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "from sklearn import metrics\na =metrics.accuracy_score(y_test, y_pred_class)\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "from sklearn import metrics\na =metrics.accuracy_score(y_test, y_pred_class)\nprint((metrics.accuracy_score(y_test, y_pred_class))*100)\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "print (((TP + TN) / float(TP + TN + FP + FN))*100,' ','Percent')\nprint ((metrics.accuracy_score(y_test, y_pred_class))*100,' ','Percent')\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "print (((FP + FN) / float(TP + TN + FP + FN))*100,' ','Percent')\nprint ((1 - metrics.accuracy_score(y_test, y_pred_class))*100,' ','Percent')\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "X_new = [[22, 31, 35, 7, 9]] \nclf.predict(X_new) \n", "intent": "Once the model is trained, it can be used to predict the most likely outcome on unseen data.\n"}
{"snippet": "y_model = clf2.predict(X)\n", "intent": "Let's get a rough evaluation our model by using\nit to predict the values of the training data:\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_true = train.Survived\ny_pred = train.PredEveryoneDies\nAccEveryoneDies = accuracy_score(y_true, y_pred)\nAccEveryoneDies\n", "intent": "(7) Create variables `AccEveryoneDies` and `AccGender` using a calculation of accuracy of predictions for the `training` dataset.  \n"}
{"snippet": "predict = train_forest.predict(X_test)\n", "intent": "And get the predictions!\n"}
{"snippet": "print ('Accuracy of the model: ', metrics.accuracy_score(y_test, predict))\n", "intent": "At first we get the *accuracy* of the model..\n"}
{"snippet": "print ('F beta : ', metrics.fbeta_score(y_test, predict, beta=1.9))\n", "intent": "The values of the AUC shows that our classifier is not so far from a random classifier!!\n"}
{"snippet": "print ('AUC : ', metrics.roc_auc_score(y_test, average_model_predictions))\n", "intent": "Finally, we compare also the AUC and the Fbeta with the ones obtained [above](\n"}
{"snippet": "metrics.silhouette_score(ref_std, pred, metric='euclidean')\n", "intent": "We then compute the silhouette score of the clustering.\n"}
{"snippet": "predictions = model_all.predict(testing[all_features])\nRSS= ((predictions- testing['price'])**2).sum()\nprint(\"RSS of test: \", RSS/1e14)\n", "intent": "Now that you have selected an L1 penalty, compute the RSS on TEST data for the model with the best L1 penalty.\n"}
{"snippet": "output_test = model.predict(X_test)\ntest_casses = np.argmax(output_test, axis=-1)\nprint(\"Test accuracy:\", np.mean(test_casses == target_test))\n", "intent": "Compute model accuracy on test set\n"}
{"snippet": "from sklearn.metrics import adjusted_rand_score\nadjusted_rand_score(y, labels)\n", "intent": "You can use a different scoring metric, such as ``adjusted_rand_score``, which is invariant to permutations of the labels:\n"}
{"snippet": "print (metrics.accuracy_score(y_test, predicted))\nprint (metrics.roc_auc_score(y_test, probability[:, 1]))\n", "intent": "This is predicting a made shot (1) any time column two is greater than 0.5\n"}
{"snippet": "multi_target_forest.predict(X[199])\n", "intent": "Caution: Only if you are subject to any crime, prediction of crime is likely to happen.\n"}
{"snippet": "scores = model.evaluate(np.array(x_test), np.array(y_test), verbose=0)\nprint(\"====================[TEST SCORE]====================\")\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Let's use our test set for evaluating this model.\n"}
{"snippet": "preds = model.predict(np.array(x_test))\npreds\n", "intent": "This model shows great performance on test set. Let's explore the prediction value.\n"}
{"snippet": "score = ANN_regression_Model.evaluate(Val_X, Val_Y, verbose=0)\nprint('Model Loss: {:2f}, Model Accuracy: {:2f}%'.format(score[0], score[1]*100))\n", "intent": "Calling the data for validation\n"}
{"snippet": "doggie_converter_predictions = [np.argmax(doggie_converter.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(doggie_converter_predictions)==np.argmax(test_targets, axis=1))/len(doggie_converter_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = rfr.predict(xtest)\nrmse = metrics.mean_squared_error(preds, ytest)**0.5\nprint 'RMSE for model is: {0}'.format(rmse)\n", "intent": "Let's look at the accuracy and compute the RMSE and then the R^2\n"}
{"snippet": "sklearn.metrics.accuracy_score(dfTest['category'],clf_tree.predict(np.stack(dfTest['vect'], axis = 0)))\n", "intent": "Lets look at accuracy:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n", "intent": "Now, we import the model and the metric.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "_this means the decision tree error is roughly 71k with a std of 3k_\n_Note: we're able to see the std since we're using k-fold cross validation_\n"}
{"snippet": "preds = model.predict(X_valid)\n", "intent": "Our first task in evaluating the model performance is to predict mortality using the hold out dataset (i.e. test data).\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc\npreds = model.predict(X_valid)\nrnn_roc = roc_curve(y_valid[:, 0, :].squeeze(), preds[:, -1, :].squeeze())\nrnn_auc = auc(rnn_roc[0], rnn_roc[1])\n", "intent": "* Evaluate performance on the hold out set\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "labels = clf.predict(breast.data)\nlabels\n", "intent": "We can get the label predictions with the `.predict` method\n"}
{"snippet": "probs = clf.predict_proba(breast.data)\nprobs\n", "intent": "And similarly the predicted probabilities with `.predict_proba`\n"}
{"snippet": "probs = clf.predict_proba(breast.data)[:,1]\nprobs\n", "intent": "We can extract the second column of the probs by slicing, just like how we did it in Julia\n"}
{"snippet": "preds = cart.predict(boston.data)\npreds\n", "intent": "Like for classification, we get the predicted labels out with the `.predict` method\n"}
{"snippet": "print accuracy_score(test_y, model.predict(test_X))\nprint roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])\n", "intent": "And now we can get performance on the test set:\n"}
{"snippet": "print(metrics.recall_score(y_test, y_pred_class))\n", "intent": "**Sensitivity**: When the actual value is positive, how often is the prediction correct? (also known as recall)\n"}
{"snippet": "print(metrics.precision_score(y_test, y_pred_class))\n", "intent": "**Precision**: When a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "ed.evaluate('mean_squared_error', data={X: X_test, y_posterior: y_test})\n", "intent": "Calculate mean squared error.\n"}
{"snippet": "ed.evaluate('mean_squared_error', data={X: X_test, y_posterior: y_test})\n", "intent": "Next, compute the mean squared and absolute error of the prediction on the test data set.\n"}
{"snippet": "y_cv_pred = classifier.predict(X_cv, as_iterable=True)\ny_cv_pred = np.array([i[0] for i in y_cv_pred])\n", "intent": "Get the binary predictions and the probability prediction for each class.\n"}
{"snippet": "print('Accuracy score: ', accuracy_score(y_cv, y_cv_pred))\n", "intent": "Accuracy score is how many examples we got correct. 1.0 is all correct.\n"}
{"snippet": "print('Log loss: ', log_loss(y_true=y_cv, y_pred=y_cv_pred_proba, labels=[0,1,2]))\n", "intent": "Logloss for handling probability classifications.\nhttps://www.kaggle.com/wiki/LogarithmicLoss\n"}
{"snippet": "y_pred = gbm.predict(X_Test_data)\n", "intent": "Obtaining the predicted values\n"}
{"snippet": "accuracy_score(Y_Test_data,y_pred)*100\n", "intent": "Determining the Accuracy as well as the FDR, MDR, and TPR\n"}
{"snippet": "accuracy_score(Y_Test_data,y_pred)*100\n", "intent": "The ML model has an accuracy of 96.89%:\n"}
{"snippet": "y_pred = rf.predict(test[features])\n", "intent": "obtaining the predicted values:\n"}
{"snippet": "pred_lasso = lasso_reg.predict(X_test)\nprint(np.sqrt(mean_squared_error(pred_lasso, y_test)))\n", "intent": "wow, 3 out of 10 features are non-zero! The model thinks only 3 features are important with an `alpha` set to 1\nWhat do the predictions look like:\n"}
{"snippet": "cross_val_score(clf,X_digits,y_digits)\n", "intent": "** Nested cross-validation**\n"}
{"snippet": "y_pred=linreg.predict(X_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred+200000)))\n", "intent": "the graph seem move toward the line of x=y\n"}
{"snippet": "y_pred=linreg.predict(X_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n", "intent": "The plot is different with PCA, all the value seem placed in horizon line\n"}
{"snippet": "y_pred=linreg.predict(X_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n", "intent": "the graph are similar with PCA and holdout , I don't know if i'm do something wrong . the rms still stay the same\n"}
{"snippet": "valid_predictions = model.predict([x_valid_band, x_valid_meta])[:,0]\npd.Series(valid_predictions).sort_values().reset_index(drop=True).plot()\n", "intent": "Our predictions are soft - we can skip knowledge distillation.\n"}
{"snippet": "print(confusion_matrix(cd['Cluster'],kmeans.labels_))\nprint(classification_report(cd['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "Y_doubleTest=clf.predict(X_doubleTest)\n", "intent": "And, we run prediction. \n"}
{"snippet": "preds = model.predict(calcium)\np_s, p_n = preds[did_spike == 1], preds[did_spike == 0]\nn_total = calcium.shape[0]\nss, ns = np.sum(p_s > 0.5) / n_total, np.sum(p_s <= 0.5) / n_total\nsn, nn = np.sum(p_n > 0.5) / n_total, np.sum(p_n <= 0.5) / n_total\nprint('                     spike    no spike')\nprint('predicted spike    | %.3f  | %.3f' % (ss, ns))\nprint('predicted no spike | %.3f  | %.3f' % (sn, nn))\n", "intent": "Let's also print a confusion matrix so we can get a bit more information about how the model performed.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint('Accuracy: '+ str(cross_val_score(grid_search, X_train, y_train.ravel(), scoring='accuracy', cv=3)))\nprint('Precision: '+ str(cross_val_score(grid_search, X_train, y_train.ravel(), scoring='precision', cv=3)))\nprint('Recall: '+ str(cross_val_score(grid_search, X_train, y_train.ravel(), scoring='recall', cv=3)))\nprint('F1: '+ str(cross_val_score(grid_search, X_train, y_train.ravel(), scoring='f1', cv=3)))\n", "intent": "<b>Cross validation: precision, accuracy, recall and f1<b>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint('Accuracy: '+ str(cross_val_score(classifier, X_train, y_train.ravel(), scoring='accuracy', cv=3)))\nprint('Precision: '+ str(cross_val_score(classifier, X_train, y_train.ravel(), scoring='precision', cv=3)))\nprint('Recall: '+ str(cross_val_score(classifier, X_train, y_train.ravel(), scoring='recall', cv=3)))\nprint('F1: '+ str(cross_val_score(classifier, X_train, y_train.ravel(), scoring='f1', cv=3)))\n", "intent": "<b>Cross validation: precision, accuracy, recall and f1<b>\n"}
{"snippet": "model_name.append(\"Backward/Bayes\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Backward/SVC\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Backward/Decision Tree\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Backward/Random Forest\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nmodel_name.append(\"Forward/KNN\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/Bayes\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/SVC\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/Decision Tree\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/Random Forest\")\naccuracy_col.append(accuracy_score(y_test, y_pred))\nprecision_col.append(precision_score(y_test, y_pred))\nrecall_col.append(recall_score(y_test, y_pred))\nf1_col.append(f1_score(y_test, y_pred))\nauc_col.append(roc_auc_score(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "sse = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint(\"{0:.2f}\".format(sse))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(\"{0:.2f}\".format(mse))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictions = lm.predict(test_data[train_cols])\n", "intent": "Now that we've fit the model, we can make some predictions!\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n", "intent": "<b>Better evalution of the model using cross validation</b>\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n", "intent": "<b>Confusion matrices:</b>\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_5, y_train_pred)\n", "intent": "<b>Precision and recall</b>\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method = 'decision_function')\n", "intent": "<b>Precision/recall tradeoff</b>\n"}
{"snippet": "Xception_prediction = [np.argmax(modelXception.predict(np.expand_dims(feature,axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_prediction)==np.argmax(test_targets, axis=1))/len(Xception_prediction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss  = 0\n    for i in range(len(style_layers)):\n        gram_G = gram_matrix(feats[style_layers[i]], normalize=True)\n        gram_S = style_targets[i]\n        loss +=tf.reduce_sum((gram_G - gram_S) ** 2) * style_weights[i]\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "score = cv_score(clf2, Xtestlr, ytestlr)\nprint(score)\n", "intent": "The best C is listed above. It is much different than the previously found best C.\n"}
{"snippet": "classifier.predict(\n    [\n        [2, 2]\n    ]\n)\n", "intent": "Now, predict the class of some example collection of features.\n"}
{"snippet": "classifier.predict_proba(\n    [\n        [2, 2]\n    ]\n)\n", "intent": "The *probability* of each class can be predicted too, which is the fraction of training samples of the same class in a leaf.\n"}
{"snippet": "classifier.predict(iris.data)\n", "intent": "Right, so now let's make some predictions.\n"}
{"snippet": "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_prediction = regressor.predict(X_test)\n", "intent": "Ok, let's make some predictions and see how it does.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(dependent_test, dependent_pred))\n", "intent": "You can also run a classification report to see how the model is behaving within\neach class.\n"}
{"snippet": "R2_scores_1 = cross_val_score(lm_scikit_1, X.astype(int), y.astype(int), cv=5, scoring=\"r2\")\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (R2_scores_1.mean(), R2_scores_1.std()))\n", "intent": "**5-Fold Cross Validation**\n"}
{"snippet": "R2_scores_2 = cross_val_score(lm_scikit_2, X.astype(int), y.astype(int), cv=5)\nprint(R2_scores_2.mean())\n", "intent": "**Cross Validation**\n"}
{"snippet": "y_pred2 = model2.predict(X_holdout_transformed) \nnp.sqrt(mean_squared_error(y_holdout,y_pred2))\n", "intent": "To calculate the RMSE in the holdout set:\n"}
{"snippet": "y_pred3 = pl3.predict(X_holdout)\nnp.sqrt(mean_squared_error(y_holdout,y_pred3))\n", "intent": "As we expected, we were able to reproduce the rmse using the pipeline object's predictions.\n"}
{"snippet": "y_pred4_holdout = pl4.predict(X_holdout)\nnp.sqrt(mean_squared_error(y_holdout,y_pred4_holdout))\n", "intent": "It looks like RandomForest fits into the training set better than the untuned Ridge. Let's look at the performance in the holdout set:\n"}
{"snippet": "pl5testpred = pl5.predict(X_train)\nnp.sqrt(mean_squared_error(y_true=y_train,y_pred= pl5testpred))\n", "intent": "XGBoost regressor took only 7 minutes to train, which was remarkably fast. Let's look at the performance:\n"}
{"snippet": "pl5testholdout = pl5.predict(X_holdout)\nnp.sqrt(mean_squared_error(y_true=y_holdout,y_pred= pl5testholdout))\n", "intent": "This performance is not as good as the Ridge pipeline 3. Let's also look at the performance using the holdout set:\n"}
{"snippet": "network1.evaluate(X_net_train,y_net_train)\n", "intent": "Using the .evaluate method with training set will give loss and metric for the final network using the training set:\n"}
{"snippet": "network1.evaluate(X_net_validation,y_net_validation)\n", "intent": "Similarly, for the validation set:\n"}
{"snippet": "np.sqrt(network2.evaluate(X_net_holdout,y_holdout))\n", "intent": "It looks like we have some improvement of the network based on the validation score. Let's evaluate the new model using the transformed holdout set:\n"}
{"snippet": "time_steps = 10\nfor ii in range(time_steps):\n    ch_sim.run(phi_sim)\n    phi_sim = ch_sim.response\n    phi_pred = model.predict(phi_pred)\n", "intent": "In order to move forward in time, we need to feed the concentration back into the Cahn-Hilliard simulation and the MKS model.\n"}
{"snippet": "for ii in range(1000):\n    ch_sim.run(phi_sim)\n    phi_sim = ch_sim.response\n    phi_pred = model.predict(phi_pred)\n", "intent": "Once again we are going to march forward in time by feeding the concentration fields back into the Cahn-Hilliard simulation and the MKS model. \n"}
{"snippet": "  = model.predict(, periodic_axes=[0,1])\n", "intent": "Now let's predict the stress values for the new microstructures. \n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Rs]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('accuracy: ',metrics.accuracy_score(y_train, y_predict))\n", "intent": "I know you should predict on y_test but since we have do not have that yet I will just use y_train for now\n"}
{"snippet": "print(\"\\nTest Set\")\ntest_corpus_file = 'mte-corpus-test.pickle'\nprinttable(*evaluate(tagger, test_corpus_file))\n", "intent": "Row sum is total true labels\nColumn sum is predictions total labels\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_new, X_val, target)\n", "intent": "Now, let's use this function to evaluate the classification error of `my_decision_tree_new` on the **validation_set**.\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_old, X_val, target)\n", "intent": "Now, evaluate the validation error using `my_decision_tree_old`.\n"}
{"snippet": "print(\"Training data, classification error (model 1):\", evaluate_classification_error(model_1, X_train, target))\nprint(\"Training data, classification error (model 2):\", evaluate_classification_error(model_2, X_train, target))\nprint(\"Training data, classification error (model 3):\", evaluate_classification_error(model_3, X_train, target))\n", "intent": "Let us evaluate the models on the **train** and **validation** data. Let us start by evaluating the classification error on the training data:\n"}
{"snippet": "print(\"Validation data, classification error (model 1):\", evaluate_classification_error(model_1, X_val, target))\nprint(\"Validation data, classification error (model 2):\", evaluate_classification_error(model_2, X_val, target))\nprint(\"Validation data, classification error (model 3):\", evaluate_classification_error(model_3, X_val, target))\n", "intent": "Now evaluate the classification error on the validation data.\n"}
{"snippet": "print(\"Validation data, classification error (model 4):\", evaluate_classification_error(model_4, X_val, target))\nprint(\"Validation data, classification error (model 5):\", evaluate_classification_error(model_5, X_val, target))\nprint(\"Validation data, classification error (model 6):\", evaluate_classification_error(model_6, X_val, target))\n", "intent": "Calculate the accuracy of each model (**model_4**, **model_5**, or **model_6**) on the validation set. \n"}
{"snippet": "print(\"Validation data, classification error (model 7):\", evaluate_classification_error(model_7, X_val, target))\nprint(\"Validation data, classification error (model 8):\", evaluate_classification_error(model_8, X_val, target))\nprint(\"Validation data, classification error (model 9):\", evaluate_classification_error(model_9, X_val, target))\n", "intent": "Now, let us evaluate the models (**model_7**, **model_8**, or **model_9**) on the **validation_set**.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(y_val, gbc.predict(X_val)).ravel()\nfp\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "evaluate_classification_error(small_data_decision_tree_subset_20, train_data,target)\n", "intent": "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:\n"}
{"snippet": "precision_with_default_threshold = precision_score(test_data['sentiment'],\n                                        predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(test_data['sentiment'],\n                                        predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(test_data['sentiment'],\n                                        predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(test_data['sentiment'],\n                                        predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "probabilities = model.predict_proba(baby_matrix)\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "predictions3 = finalmodel3.predict(hiredPredictionData)\npredicted_prob_true3 = predictions3[:,1]\npredicted_prob_false3 = predictions3[:,0]\nprint('True positives: '+str(predicted_prob_true2.mean())+'\\t False negatives: '+str(predicted_prob_false2.mean()))\n", "intent": "Let's consider the false positives ratio, as we've done before:\n"}
{"snippet": "preds = gs.best_estimator_.predict_proba(X_test)\n", "intent": "Now we'll take the best performing model and make probability predictions on our local holdout set.\n"}
{"snippet": "submission_format['yield'] = estimator.predict(test)\n", "intent": "We'll make predictions and fill in the yield column with actual outputs from our model:\n"}
{"snippet": "1 - model.predict()[1:10]\n", "intent": "So all we need to get the probability of an Up movement is to take one minus the predicted probabilities:\n"}
{"snippet": "predictions = [ \"Up\" if x < 0.5 else \"Down\" for x in model.predict()]\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Smarket[['Direction']], predictions)\nprint(cm)\n", "intent": "Now let's create a vector of predicted movements from those probabilities, and compare that with the actual movements by means of a confusion matrix:\n"}
{"snippet": "y_predicted = model.predict(X_test_feats)\n", "intent": "Now that we have our model, we can use it to predict labels on a fresh test set.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = iowa_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "**Checking MAE using built-in method**\n"}
{"snippet": "Xnew = np.random.randn(10, D)\nlabels_new = kmeans_model.predict(Xnew)\nprint(labels_new)\n", "intent": "**Predictions on unseen data.** We can obtain the labels of new datapoints as follows.\n"}
{"snippet": "all_labels = gmm_model.predict(X)\n", "intent": "**Obtaining the labels.**\n"}
{"snippet": "Xnew = np.random.randn(10, D)\nlabels_new = gmm_model.predict(Xnew)\nprint(labels_new)\n", "intent": "**Predictions on unseen data.**\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(data_test['target'], X_test_predicted,\n                                    target_names=data_test['target_names']))\n", "intent": "**Evaluation: Precision and recall.** Scikit-learn can also compute more advanced metric, such as precision, recall, and F1-score.\n"}
{"snippet": "my_model_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = np.sum(np.array(my_model_predictions)==np.argmax(test_targets, axis=1))/len(my_model_predictions)\nprint('Test accuracy: {:.4f}%'.format(test_accuracy * 100))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(Y_test, y_hat_knn))\n", "intent": "**The f1-score for our KNN run is: 0.44828.**\n"}
{"snippet": "print(classification_report(Y_test, y_hat))\n", "intent": "**The f1-score for our KNN run is: 0.33922.**\n"}
{"snippet": "y_hat_MNIST_full = mlp_classifier_relu_adam.predict(X_test)\n", "intent": "Create predictions based on the trained model and store them as the set y_hat:\n"}
{"snippet": "predictions = stops_model.predict(X_test)\n", "intent": "The above model is used to make predictions and create a confusion matrix and classification report.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Then re-run predictions can be re-run on this new grid object just like with the original model.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(model_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "accuracy = cross_val_score(logistic_reg, X_scaled, y, cv=5)\n", "intent": "Now I will conduct 5-fold cross validation to see how well this model performs. The default scoring system for the random forrest is accuracy\n"}
{"snippet": "predicts=logistic_reg_test.predict(X_test)\n", "intent": "now lets use the model to make the predictions\n"}
{"snippet": "accuracy = cross_val_score(forrest_classifier, X, y, cv=5)\n", "intent": "Now I will conduct 5-fold cross validation to see how well this model performs. The default scoring system for the random forrest is accuracy\n"}
{"snippet": "predicts=forrest_classifier.predict(X_test)\n", "intent": "now lets use the model to make the predictions\n"}
{"snippet": "predicts=forrest_classifier_test_data.predict(X_test)\n", "intent": "now lets use the model to make the predictions\n"}
{"snippet": "accuracy = cross_val_score(Support_Vector_Machine, X_scaled, y, cv=5)\n", "intent": "Now I will conduct 5-fold cross validation to see how well this model performs. The default scoring system for the random forrest is accuracy\n"}
{"snippet": "predicts=Support_Vector_Machine_test.predict(X_test)\n", "intent": "now lets use the model to make the predictions\n"}
{"snippet": "preds = model.predict(x)\nprint('Predicted:')\nfor p in decode_predictions(preds, top=5)[0]:\n    print(\"Label: {}, Score: {}\".format(p[1], p[2]))\n", "intent": "This is the image:\n<img src=\"dog-food-test.jpg\">\nThis is the predicted classes from the pretrained model:\n"}
{"snippet": "x = X_dataset[0]\nx = np.expand_dims(x, axis=0)\npreds = model.predict(x)\nprint('Predicted:')\nfor p in decode_predictions(preds, top=5)[0]:\n    print(\"Label: {}, Score: {}\".format(p[1], p[2]))\n", "intent": "Just to clafify that the pretrained InceptionV3 model cannot predict correcty Dog Food/Cat Food dataset.\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Now we have trained the model with training set. This step is to see how well the model perform on the test set (the data it hasn't seen before)\n"}
{"snippet": "print(r2_score(y_test, y_pred))\n", "intent": "We will use $R^2$ score and meam squared error (MSE) to evaluate the model performance.\n"}
{"snippet": "wine_1_quality = wine_snob.predict(X_test.iloc[[1]]) \n", "intent": "Now we can use ``wine_snob`` to predict wine quality\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint(-cross_val_score(model, data[['Exper', 'Educ']], data['Bsal'], cv=10, \n               scoring=\"neg_mean_squared_error\").mean())\nprint(-cross_val_score(model2, X_standardized, data['Bsal'], cv=10, \n               scoring=\"neg_mean_squared_error\").mean())\n", "intent": "**Exercise 3.** Use cross-validation to determine whether or not standardizing the input variables improves the predictive performance of your model.\n"}
{"snippet": "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "To evaluate our model's performance, we'll calculate the residual sum of squares and the explained variance score (R^2).\n"}
{"snippet": "y_pred = decision_tree_model.predict(X_test)\ny_pred\n", "intent": "<h2>To predict target variable for the complete test data set</h2>\n"}
{"snippet": "accuracy_score(y_test,y_pred)*100\n", "intent": "<h2>Calculating accuracy for Decision Tree Model</h2>\n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n    return (\"Mean score: {0:.3f} (+/- {1:.3f})\").format(np.mean(scores), sem(scores))\nprint(mean_score(scores))\n", "intent": "We get an array of k scores. We can calculate the mean and the standard error to obtain a final figure\n"}
{"snippet": "print(\"Predicted probabilities\", model.predict_proba(x_train[:10]))\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "import pickle\ns = pickle.dumps(model)\nmodel2 = pickle.loads(s)\nmodel2.predict(x_iris[0:1])\n", "intent": "Now we are going to save the model to a data structure called *pickle*. A pickle is a dictionary and can be used as a file or a string.\n"}
{"snippet": "cv = KFold(X.shape[0], n_folds=5, shuffle=False, random_state=33)\nscores = cross_val_score(model, X, y, cv=cv)\nprint(\"Scores in every iteration\", scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "This is alternative to splitting the dataset into train and test. It will run k times slower than the other method, but it will be more accurate.\n"}
{"snippet": "r2_scores = cross_val_score(lin_reg,X,Y,cv=10)\nmse_scores = cross_val_score(lin_reg,X,Y,cv=10,scoring='neg_mean_squared_error')\n", "intent": "The plot violates the homoscedasticity assumption, which is about residuals being random and not following any pattern\n"}
{"snippet": "time=[]\nprint(\"The last \"+str(numtimes)+\" times for \"+first_name)\nfor i in range(0,numtimes):\n    time.append(int(input(\"Enter the time in seconds with no decimals and press enter  \")))\nnew_X=np.array(time)\nnew_prediction = model.predict(new_X.reshape(1, -1))\nprint (\"Your time for your next race is estimated to be \")\nprint (new_prediction)\nprint (\" seconds\")\n", "intent": "<h3>Let's try a new prediction</h3>\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==\n                           np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "one_image = imgs[5]\none_output = model.predict(one_image[numpy.newaxis, :, :, :])\n", "intent": "We extract the neighbors for a new image.\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\naccuracy_score(y_true, y_pred)\n", "intent": "$$ accuracy(y, \\hat{y}) = \\frac{1}{n_{samples}}\\sigma_{i=0}^{n_{samples}-1}1(\\hat{y_i}=y_i) $$\n"}
{"snippet": "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n", "intent": "In the multilabel case with binary label indicators:\n"}
{"snippet": "from sklearn.metrics import cohen_kappa_score\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\ncohen_kappa_score(y_true, y_pred)\n", "intent": "[Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n", "intent": "The ```classification_report``` function builds a text report showing the main classification metrics.\n"}
{"snippet": "from sklearn.metrics import hamming_loss\ny_pred = [1, 2, 3, 4]\ny_true = [2, 2, 3, 4]\nhamming_loss(y_true, y_pred)\n", "intent": "$$ L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_{labels}}\\sum_{j=0}^{n_{labels}-1}1(\\hat{y_j}\\neq y_j) $$\n"}
{"snippet": "metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n", "intent": "For multiclass classification with a \"negative class\", it is possible to exclude some labels:\n"}
{"snippet": "metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n", "intent": "Similarly, labels not present in the data sample may be accounted for in macro-averaging.\n"}
{"snippet": "from sklearn.metrics import zero_one_loss\ny_pred = [1, 2, 3, 4]\ny_true = [2, 2, 3, 4]\nzero_one_loss(y_true, y_pred)\n", "intent": "$$ L_{0-1}(y_i, \\hat{y_i}) = 1(\\hat{y_i} \\neq y_i) $$\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import brier_score_loss\ny_true = np.array([0, 1, 1, 0])\ny_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\ny_prob = np.array([0.1, 0.9, 0.8, 0.4])\ny_pred = np.array([0, 1, 1, 0])\nbrier_score_loss(y_true, y_prob)\n", "intent": "$$ BS = \\frac{1}{N}\\sum_{t=1}^{N}(f_t - o_t)^2 $$\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(explained_variance_score(y_true, y_pred))\nexplained_variance_score(y_true, y_pred, multioutput='raw_values')\n", "intent": "$$ explained\\_variance(y, \\hat{y}) = 1 - \\frac{Var\\{y-\\hat{y}\\}}{Var\\{y\\}} $$\nThe best possible score is 1.0, lower values are worse.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_absolute_error(y_true, y_pred)\n", "intent": "$$ MAE(y, \\hat{y}) = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}\\lvert y_i - \\hat{y_i} \\rvert $$\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_squared_error(y_true, y_pred)\n", "intent": "$$ MSE(y, \\hat{y}) = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}(y_i - \\hat{y_i})^2 $$\n"}
{"snippet": "from sklearn.metrics import median_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmedian_absolute_error(y_true, y_pred)\n", "intent": "$$ MedAE(y, \\hat{y})=median(\\lvert y_1-\\hat{y_1} \\rvert, ..., \\lvert y_n - \\hat{y_n} \\rvert) $$\n"}
{"snippet": "from sklearn.metrics import r2_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nr2_score(y_true, y_pred)\n", "intent": "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{samples}-1}(y_i-\\hat{y_i})^2}{\\sum_{i=0}^{n_{samples}-1}(y_i-\\bar{y_i})^2} $$ \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print scores\n    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores))\n", "intent": "Perform 5-fold cross-validation\n"}
{"snippet": "print clf.predict(scaler.transform([[4.7, 3.1]]))\nprint clf.decision_function(scaler.transform([[4.7, 3.1]]))\n", "intent": "Evaluate a particular instance\n"}
{"snippet": "from sklearn import metrics\ny_train_pred = clf.predict(X_train)\nprint metrics.accuracy_score(y_train, y_train_pred)\n", "intent": "Measure accuracy on the training set\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint metrics.accuracy_score(y_test, y_pred)\n", "intent": "Measure  accuracy on the testing set\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred, target_names=iris.target_names)\nprint metrics.confusion_matrix(y_test, y_pred)\n", "intent": "Evaluate results using Precision, Recall and F-score, and show the confusion matrix\n"}
{"snippet": "vgg19_predictions = [np.argmax(vgg19_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_vgg19]\ntest_accuracy = 100*np.sum(np.array(vgg19_predictions)==\n                           np.argmax(test_targets, axis=1))/len(vgg19_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "valid_auc = pd.Series()\nfor model in valid_y_prob:\n    valid_auc[model] = roc_auc_score(np.array(valid_y).reshape(-1), valid_y_prob[model])\nvalid_auc.sort_values()\n", "intent": "Model performance is improved after parameter tuning.\n"}
{"snippet": "result = model.evaluate(x=np.expand_dims(x_test,axis=0), y=np.expand_dims(y_test, axis=0))\n", "intent": "Performance on test set, we run the test set in as one long batch\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "Now we are able to make predictions with our model.\nFirst we run the entire test set through the model and gather the MAPE score.\n"}
{"snippet": "some_data          = housing.iloc[:5]\nsome_labels        = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predicions:\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))\n", "intent": "Done! Now we've fit the model! Time to try out its effectiveness!\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse  = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's look at the effectiveness of the linear fit using the $\\textbf{Mean Squared Error}$:\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n", "intent": "Evaluate Your System on the Test Set\n----\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "Now, let's check out the $\\textbf{cross_val_score()}$ to evaluate our model using $\\textbf{K-Fold Cross Validation}$.\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint(\"F1 score classifying 5's: \" + str(f1_score(y_train_5, y_train_pred)))\n", "intent": "This leads us to the $\\textbf{F1 score}$ = $\\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$\nWhich is a balance between precision and recall.\n"}
{"snippet": "print(\"Random Forest Cross-val: \" + str(cross_val_score(forest_clf, X_train, y_train, cv=3, scoring=\"accuracy\")))\nprint(\"SGD Cross-val: \" + str(cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")))\n", "intent": "Let's get $\\textbf{Random Forest's cross-validation}$ score.\n"}
{"snippet": "predictions = [np.argmax(Resnetmodel.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50Data]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def predict(inp,w):\n    return np.dot(inp,w[:-1])+w[-1] > 0\n", "intent": "We can find the result after 5 iterations. Now let's plot the decision boundary we found.\n"}
{"snippet": "def fmean_squared_error(ground_truth, predictions):\n    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n    return fmean_squared_error_\nRMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n", "intent": "Kagge uses root MSE as scoring function. So let's implement this based on the standard MSE scoring function. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error([np.mean(y_train)]*len(y_train), y_train)\nmse**0.5\n", "intent": "Let's find out the RMSE of the constant algorithm. Let the constatnt be equal to the mean value of the y_train \n"}
{"snippet": "predicted_array = clf.predict(new_wind_array);\nsum_of_predicted_values = np.sum(predicted_array);\nprint(sum_of_predicted_values)\n", "intent": "Now we want to load in different data and try to do a prediction! \n"}
{"snippet": "my_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1))/len(my_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "inputText = predictionFormat(\"This shoe is horrible and awful\")\nprediction = model.predict(inputText)\nprint(prediction)\n", "intent": "Now we will get a prediction with our custom input text.\n"}
{"snippet": "print('predicted: ', spam_detect_model.predict(tfidf4)[0])\nprint('expected: ', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(messages['label'], all_predictions))\n", "intent": "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did.\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def predict(thetas, xs):\n    return xs * thetas[1] + thetas[0]\n", "intent": "Ok so with one variable its easy, we just multilpy the datapoint by theta 1\n"}
{"snippet": "reg.predict(X[:3])\n", "intent": "We can now use the `predict` method to predict the value of `petal_width` from some values of `petal_length`:\n"}
{"snippet": "opening_price = float(input('Open: '))\nhigh = float(input('high: '))\nlow = float(input('low: '))\nclose = float(input('close: '))\nprint('My Prediction the opening price will be:', lr.predict([[opening_price, high, low, close]])[0])\n", "intent": "A 0.95 score is great for such a simple implementation. Let's build it into a simple application now!\n"}
{"snippet": "predictions = sequence_model.predict(test_images)\npredictions[0]\nnp.argmax(predictions[0])\n", "intent": "**Making Predictions and checking for how the prediction is happening**\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\npredictions = model_final.predict(X_test)\nps = precision_score(y_test, predictions, average='weighted')\nprint \"Precision score is \", ps\nrs = recall_score(y_test, predictions, average='weighted')\nprint \"Recall score is \", rs\n", "intent": "The final test accuracy is 41%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint test_accuracy\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(bn_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Test set predictions: {}'.format(classifier.predict(X_test)))\n", "intent": "Make predictions on the test data\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels\n                             ,scoring = \"neg_mean_squared_error\", cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores)\n", "intent": "This actually looks worse than the Linear Regression (\\$70545 vs \\$68,628). Let's check by doing the same cross validation for Lienar Regression.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y = None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1), dtype=bool)\n", "intent": "Above 95% looks good. But what if we were to simply classify every image as not the number 5? \n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "90% probability that the image is a 5. \nTo evaluate, as usual you can use cross validation\n"}
{"snippet": "y_pred = bag_clf.predict(X_test)\n", "intent": "We are likely to get this accuracy on the test set.\n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n", "intent": "Now we have an ensemble containing three trees. It can make predictions on a new\ninstance simply by adding up the predictions of all the trees:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracies on both sets are lower, but as the score are closer to one another which shows no over or underfitting, this classifier is better.\n"}
{"snippet": "lr = LinearRegressionWithSGD()\ntrain, test = data.randomSplit([0.7,0.3])\ntrain.cache()\ntest.cache()\nmodel = lr.train(train,iterations=1000, step=0.01,intercept=-6.42,initialWeights=[-0.34,7.6,-1.0])\nprint(\"Intercept, model weights: \", model.intercept, model.weights)\nprint(\"Some predictions: \")\ntest.map(lambda p: (float(model.predict(p.features)), p.label)).take(10)\n", "intent": "As the next step, fit a Linear Regression model on the training set.\n"}
{"snippet": "preds = test.map(lambda p: (float(model.predict(p.features)), p.label))\nmetrics = RegressionMetrics(preds)\nprint(\"MSE = %s\" % metrics.meanSquaredError)\nprint(\"RMSE = %s\" % metrics.rootMeanSquaredError)\nprint(\"R-squared = %s\" % metrics.r2)\nprint(\"MAE = %s\" % metrics.meanAbsoluteError)\nprint(\"Explained variance = %s\" % metrics.explainedVariance)\n", "intent": "Now validate the model on the test set, and check the Root Mean Squared Error.\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n     return r2_score(y_true, y_predict)\n", "intent": "As I mentioned in Metrics section, I used coefficient of determination to measure the performance of the models.  \n"}
{"snippet": "predicted_test = best_model.predict(test.loc[:,\"B\":\"F\"])\nprint \"Out of sample:\",(test.loc[:,'A']==predicted_test['A']).mean()\n", "intent": "c) Use the model to predict \"A\" for the testing dataset. Report the out-of-sample prediction accuracy. (10 points)   \n"}
{"snippet": "range_n_clusters = range(2,10)\nZ = linkage(X, 'single')\nfor n_clusters in range_n_clusters:\n    cluster_labels=fcluster(Z, n_clusters, criterion='maxclust')    \n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters = {},\".format(n_clusters)+\" the average silhouette_score is : {}\".format(silhouette_avg))\n", "intent": "Using the silhouette method, the ideal number of clusters to use appears to be 2. \n"}
{"snippet": "range_n_clusters = range(2,10)\nZ = linkage(X, 'complete')\nfor n_clusters in range_n_clusters:\n    cluster_labels=fcluster(Z, n_clusters, criterion='maxclust')    \n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters = {},\".format(n_clusters)+\" the average silhouette_score is : {}\".format(silhouette_avg))\n", "intent": "Using the silhouette method, the ideal number of clusters to use appears to be 2, the same result as the guassian mixture clustering.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint classification_report(ytest, yfit, target_names=faces.target_names)\n", "intent": "Or we can assess the performance of the model more quantitatively:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(bnb, data, target, cv=10)\n", "intent": "Creating several holdout groups\n"}
{"snippet": "Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\nmean_squared_error(Y_pred, Y_valid)\n", "intent": "Dump predictions for test and validation sets to the files.\n"}
{"snippet": "def accuracy_score(y_preds, y_true):\n    score = 0\n    for i in range(len(y_preds)):\n        print(y_preds[i], \"  \",  y_true[i])\n        if(y_preds[i] == y_true[i]):\n            score += 1\n    return score/len(y_preds)\n", "intent": "Function to predict accuracy score. If predicted value exactly matches actual value score increases by 1. Default score = 0\n"}
{"snippet": "new_samples = [[ 5.7, 4.4, 1.5, 0.4],\n               [ 6.5, 3., 5.5, 1.8],\n               [ 5.8, 2.7, 5.1, 1.9]]\nnew_labels = model.predict(new_samples)\nprint(new_labels)\n", "intent": "To test out how well this works, we will use new samples to try to predict the cluster\n"}
{"snippet": "from sklearn.metrics import precision_recall_curve\ny_scores = xgbc.predict_proba(X_training)\ny_scores = y_scores[:,1]\nprecision, recall, threshold = precision_recall_curve(y_training, y_scores)\n", "intent": "https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/\n"}
{"snippet": "def predict_cluster(test_data, test_np_matrix, kmeans_model):\n    predicted = pd.Series(kmeans_model.predict(test_np_matrix))\n    artists = test_data['ArtistID']\n    retval = pd.concat([artists, predicted], axis=1)\n    retval.columns = ['ArtistID', 'ClusterID']\n    return retval\nkmeans_5_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_5)\nkmeans_25_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_25)\nkmeans_50_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_50)\n", "intent": "**b.** For each artist in the test set, call **[predict](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "print (fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response))\nresiduals(train_basic_features, train_log_response, model).hist(bins=50)\n", "intent": "**b.** Now that you've added the categorical data, let's see how it works with a linear model!\n"}
{"snippet": "print np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "pip_predictions = pipeline.predict(msg_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "logits = LeNet(x)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nregularization = tf.nn.l2_loss(weights['conv0']) + tf.nn.l2_loss(biases['bconv0']) + \\\n                 tf.nn.l2_loss(weights['conv1']) + tf.nn.l2_loss(biases['bconv1']) + \\\n                 tf.nn.l2_loss(weights['conv2']) + tf.nn.l2_loss(biases['bconv2']) + \\\n                 tf.nn.l2_loss(weights['fc1']) + tf.nn.l2_loss(biases['bfc1']) + \\\n                 tf.nn.l2_loss(weights['fc2']) + tf.nn.l2_loss(biases['bfc2']) + \\\n                 tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['bout'])\nloss_operation = tf.reduce_mean(cross_entropy + reg_rate * regularization)\n", "intent": "definition of objective function \n"}
{"snippet": "def cross_entropy_loss(X_data, y_data):\n    num_examples = len(X_data)\n    total_loss = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        loss = sess.run(loss_operation, feed_dict={x: batch_x, y: batch_y})\n        total_loss += (loss * len(batch_x))\n    return total_loss / num_examples\n", "intent": "computation of cross-entropy loss for a given dataset\n"}
{"snippet": "result_KNN = knn.testing_error(y_test, pred1)\nresult_KNN\n", "intent": "Now lets see the testing error on each digits and see whether there is something interesting out there for us.\n"}
{"snippet": "result_KNNCV = knn.testing_error(y_test, pred2)\nresult_KNNCV\n", "intent": "Now lets see the testing error on each digits for this model and see whether there is something interesting out there for us.\n"}
{"snippet": "result_KNNCV = knn.testing_error(y_test, pred)\nresult_KNNCV\n", "intent": "Now lets see the testing error on each digits for this model and see whether there is something interesting out there for us.\n"}
{"snippet": "result_KNNCV = knn.testing_error(y_test, pred)\nresult_KNNCV\n", "intent": "Now lets see the testing error on each digits and see whether there is something interesting out there for us.\n"}
{"snippet": "def predict(parameters, X):\n    A2, cache = forward_prop(X, parameters)\n    predictions = (A2 >0.5) \n    return predictions\n", "intent": "Making the predictions:\n"}
{"snippet": "EXAMPLES = ['May 03 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "all_embeddings = encoder.predict(emojis).astype(np.float64)\ninf_mask = np.isinf(all_embeddings).any(axis=1)\nprint(f'{100 * inf_mask.mean():.3}% of values are `np.inf`.')\nall_embeddings = all_embeddings[~inf_mask]\n", "intent": "Some emoji embeddings contain `np.inf` values, unfortunately. We could likely mitigate this by further tweaking hyperparameters of our autoencoder.\n"}
{"snippet": "mix_valid_pred = best_logit_weight * logit_sgd_valid_pred + (1-best_logit_weight) * rfc.predict_proba(Xval)\n", "intent": "Here is the quality increase, accuracy climbed up by 2%, even though random forest has only 8% of weight in the ensemble.\n"}
{"snippet": "pred = d_tree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "prediction = model.predict(test)\nprediction = np.argmax(prediction,axis = 1)\n", "intent": "**Use the train model to predict the original test set:**\n"}
{"snippet": "predictions = clf.predict(train[columns])\nroc_auc = roc_auc_score(predictions, train[\"high_income\"])\nprint(\"roc_auc:\", roc_auc)\n", "intent": "Print out the AUC score between predictions and the high_income column of train.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprobs = [ 0.98200848,  0.92088976,  0.13125231,  0.0130085,   0.35719083,  \n         0.34381803, 0.46938187,  0.53918899,  0.63485958,  0.56959959]\nobs = [1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\ntesting_auc = roc_auc_score(obs, probs)\nprint(\"testing_auc:\", testing_auc)\nauc = roc_auc_score(credit[\"paid\"], credit[\"model_score\"])\nprint(\"auc:\", auc)\n", "intent": "Compute the AUC using sklearn.metrics.roc_auc_score, model_score, and the observed data, paid. Assign the computed AUC to auc.\n"}
{"snippet": "spam_detect_model.predict(tfidf4)[0]\n", "intent": "Let's use our trained model do a single prediction on the message we transformed to tfidf earlier... \n"}
{"snippet": "sent_encoded = encoder.predict(np.array(train), batch_size = 500)\n", "intent": "The encoder takes the training set of sentence vectors (concatenanted word vectors) and embeds them into a lower dimensional vector space.\n"}
{"snippet": "sent_decoded = generator.predict(sent_encoded)\n", "intent": "The decoder takes the list of latent dimensional encodings from above and turns them back into vectors of their original dimension.\n"}
{"snippet": "model = VGG16(include_top=False, weights='imagenet')\npred = model.predict(x)\nprint(pred.shape)\nprint(pred.ravel().shape)\n", "intent": "We can now load our model in and try feeding the image through.\n"}
{"snippet": "inc.predict(X_test)  \n", "intent": "Finally we can also call `Incremental.predict` and `Incremental.score` on our testing data \n"}
{"snippet": "y_pred = clf.predict(X_large)\ny_pred\n", "intent": "Now that training is done, we'll turn to predicting for the full (larger than memory) dataset.\n"}
{"snippet": "i = 1\ny_true = y_test[i]\ny_pred = model.predict(sme.transform(X_test.iloc[i:i+1]))\nprint y_true - y_pred\n", "intent": "Not the best model until now. However, prolonged training with lowering of the learning rate towards the end will improve the model.\n"}
{"snippet": "yts_hat = regr.predict(xts)\nRSS = np.mean((yts_hat - yts)**2) / (np.std(yts)**2)\nprint(RSS)\n", "intent": "Now we make the prediction with xts, and measure the accurancy by compute the normalized RSS.\n"}
{"snippet": "pred_slr = regress.predict(X)\nrss1 = sum((pred_slr-Y)**2)\nprint ('Residual sum of squares for simple linear regression model is:', rss1)\n", "intent": "$\\textbf{Residual Sum of squares}$\n"}
{"snippet": "pred2 = regress.predict(X2)\nrss2 = sum((pred2-Y)**2)\nprint ('Residual sum of squares for second order poly. regression model is:', rss2)\n", "intent": "$\\textbf{Residual Sum of squares}$\n"}
{"snippet": "pred3 = regress.predict(X3)\nrss3 = sum((pred3-Y)**2)\nprint ('Residual sum of squares for third order poly. regression model is:', rss3)\n", "intent": "$\\textbf{Residual Sum of squares}$\n"}
{"snippet": "pred4= regress.predict(X4)\nrss4 = sum((pred4-Y)**2)\nprint ('Residual sum of squares for fourth order poly. regression model is:', rss4)\n", "intent": "$\\textbf{Residual Sum of squares}$\n"}
{"snippet": "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\nvalidation_eval_result = estimator.evaluate(input_fn=predict_validation_input_fn)\nprint(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\nprint(\"Validation set accuracy: {accuracy}\".format(**validation_eval_result))\n", "intent": "Run predictions for the validation set and training set.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_Xception = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy - Xception: %.4f%%' % test_accuracy_Xception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than **60%.**\n"}
{"snippet": "xgbFinal_train_preds = xgbFinal.predict(x_train)\nxgbFinal_test_preds = xgbFinal.predict(x_test)\n", "intent": "Calculate predictions for both train and test sets, and then calculate MSE and RMSE for both datasets\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in model.estimators_])\nprint('First row preds', preds[:,0])\nprint(\"Actual value {:2f} and mean of the 10 predicited {:2f}\".format(np.mean(preds[:, 0]), y_valid[0]))\n", "intent": "Grab the predicitions for each individual tree\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],km.labels_))\nprint(classification_report(df['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = logitreg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "x_test_mod = x_test.reshape(x_test.shape[0],784)\ny_test_mod = np_utils.to_categorical(y_test, 10)\nloss, accuracy = modelB.evaluate(x_test_mod, y_test_mod)\nprint(\"loss:\",loss,\" | accuracy:\",accuracy)\npredictions = modelB.predict(x_test_mod)\nprint(\"prediction for 0th test data example:\",predictions[0])\n", "intent": "Now's the time to check how your model performs!\n"}
{"snippet": "prediction = classifier.predict(X_test)\n", "intent": "** Model is done. Let's check accuracy**\n"}
{"snippet": "prediction1 = [1,1,1,1,0,1,0,0,0,0]\nprint(log_loss(ground_truth,prediction1))\n", "intent": "Accuracy of .9 with total confidence\n"}
{"snippet": "prediction2 = [0.51,0.51,0.51,0.51,0.01,0.51,0.01,0.01,0.01,0.01]\nprint(log_loss(ground_truth,prediction2))\n", "intent": "Accuracy of .9 with low confidence\n"}
{"snippet": "prediction3 = [0.99,0.99,0.99,0.99,0.49,0.51,0.01,0.01,0.01,0.01]\nprint(log_loss(ground_truth,prediction3))\n", "intent": "A really good model\n"}
{"snippet": "print np.mean((bos.PRICE - lm.predict(X))**2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfrom validation import report_results\ncv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\nprint(\"\\n\")\nreport_results(cv_scores)\n", "intent": "<a id='cross_validation'></a>\n[back to index](\n"}
{"snippet": "bins = np.arange(max_len + 1)\nbstr = SlimBootstrap(func = sample_to_distribution, func_kwargs = {'bins' : bins})\nwl_dist = bstr.fit_predict(word_lengths)\n", "intent": "The bootstrap estimate and confidence bands are stored in the `wl_dist` array.\n"}
{"snippet": "ref_word_lengths = name_df['name_len'].values\nref_bstr = SlimBootstrap(func = sample_to_distribution, func_kwargs = {'bins' : bins})\nref_wl_dist = bstr.fit_predict(ref_word_lengths)\n", "intent": "We then bootstrap the reference cryptocurrency names and compare the two distributions.\n"}
{"snippet": "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in test_samples])\n", "intent": "evaluating the classifier on test data set\n"}
{"snippet": "from sklearn.metrics import recall_score,f1_score,log_loss\nxgb_first_train_score1 = log_loss(y_train[1:100000], xgb_best.predict_proba(X_train[1:100000]))\nxgb_first_train_score2 = recall_score(y_train[1:100000], xgb_best.predict(X_train[1:100000]))\nxgb_first_train_score3 = f1_score(y_train[1:100000], xgb_best.predict(X_train[1:100000]))\nprint(\"log_loss = {}, recall = {}, f1_score = {}\".format(xgb_first_train_score1, xgb_first_train_score2, xgb_first_train_score3))\n", "intent": "The first tuning scores on the train data are\n"}
{"snippet": "xgb_first_test_score1 = log_loss(y_test, xgb_best.predict_proba(X_test))\nxgb_first_test_score2 = recall_score(y_test, xgb_best.predict(X_test))\nxgb_first_test_score3 = f1_score(y_test, xgb_best.predict(X_test))\nprint(\"log_loss = {}, recall = {}, f1_score = {}\".format(xgb_first_test_score1, xgb_first_test_score2, xgb_first_test_score3))\n", "intent": "The first tuning scores on the test data are\n"}
{"snippet": "rf_first_train_score1 = log_loss(y_train[1:100000], rf_best.predict_proba(X_train[1:100000]))\nrf_first_train_score2 = recall_score(y_train[1:100000], rf_best.predict(X_train[1:100000]))\nrf_first_train_score3 = f1_score(y_train[1:100000], rf_best.predict(X_train[1:100000]))\nprint(\"log_loss = {}, recall = {}, f1_score = {}\".format(rf_first_train_score1, rf_first_train_score2, rf_first_train_score3))\n", "intent": "The first scores on the train data are\n"}
{"snippet": "rf_first_test_score1 = log_loss(y_test, rf_best.predict_proba(X_test))\nrf_first_test_score2 = recall_score(y_test, rf_best.predict(X_test))\nrf_first_test_score3 = f1_score(y_test, rf_best.predict(X_test))\nprint(\"log_loss = {}, recall = {}, f1_score = {}\".format(rf_first_test_score1, rf_first_test_score2, rf_first_test_score3))\n", "intent": "The first scores on the test data are\n"}
{"snippet": "score = classifier.evaluate(x=test_set[0], y=test_set[1])\nprint('Accuracy: {0:f}'.format(score['accuracy']))\n", "intent": "The last thing we want to do with this model is test the accuracy against some held-out test dataset.\n"}
{"snippet": "predictions_prob=model.predict(X_test_all) \n", "intent": "Let's observe the images by our model.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\ndef error_det(pred, correct):\n    err = mean_absolute_error(correct, pred)\n    return err\n", "intent": "Determine Mean Absolute Error\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer_index in enumerate(style_layers):\n        GM_current = gram_matrix(feats[layer_index], normalize=True)\n        GM_target = style_targets[i] \n        loss_weight = style_weights[i]\n        style_loss += loss_weight*tf.norm(GM_current - GM_target)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "sns.regplot(y=lm.predict(X), x=\"PRICE\", data=bos, fit_reg = True)\n", "intent": "Statsmodels provides statistical outputs which help in evaluating how good a the fit model is.\n"}
{"snippet": "y_pred = dtree.predict(X_test)\n", "intent": "Predict some values:\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test, dtree.predict(X_test))\n", "intent": "Calculate the score on the training and test sets:\n"}
{"snippet": "y_1 = regr1.predict(X_test)\ny_2 = regr2.predict(X_test)\n", "intent": "The predicted y values can then be obtained with the predict method:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import pickle\ns = pickle.dumps(clf)\nclf2 = pickle.loads(s)\nclf2.predict(X[0:1])\ny[0]\n", "intent": "pickle is a wrapper for python objects. It's very useful but be aware of differences in protocols between python 2 and python 3. \n"}
{"snippet": "coverage_error(y, y_predicted_prob)\n", "intent": "**Coverage Error**: \n"}
{"snippet": "label_ranking_average_precision_score(y, y_predicted_prob)\n", "intent": "** Label Ranking Average Procesion Scores **:\n"}
{"snippet": "label_ranking_loss(y, y_predicted_prob)\n", "intent": "** Label ranking loss **:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n        target_names=twenty_test.target_names))\n", "intent": "A more through metric output could be achieve by using metrics from sklearn\n"}
{"snippet": "test_feature_data = test[feature_list]\npredicted_data = final_forest.predict(test_feature_data)\n", "intent": "Predict on the train_test data\n"}
{"snippet": "print(\"[INFO] evaluating network...\")\npredictions = model.predict(testX, batch_size=128)\nprint(classification_report(testY.argmax(axis=1),\n\tpredictions.argmax(axis=1),\n\ttarget_names=[str(x) for x in le.classes_]))\n", "intent": "And then evaluate it on our testing set:\n"}
{"snippet": "def predict(seed,i):\n    x=np.zeros((1,1))\n    x[0][0]= seed\n    indices=[]\n    for t in range(i):\n        p=sess.run(prediction,{X:x})\n        index = np.random.choice(range(unique_chars), p=p.ravel())\n        x[0][0]=index\n        indices.append(index)\n    return indices\n", "intent": " Now we define the function called predict which results the indices of next char according to our model\n"}
{"snippet": "MSE = np.sum((bos.PRICE - lm.predict(X)) ** 2) / len(bos.PRICE)\nMSE\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n", "intent": "> The GridSearchCV tool picked C=1 as the best parameter. In my own trial, this was one of the 'C' values that gave the best score as well.\n"}
{"snippet": "import numpy as np\nprint(predict(np.asarray([0,0,0,0,0]).reshape(1,5)))\n", "intent": "Here we test using the prediction function. \n"}
{"snippet": "print clf.predict(X.loc[1])\nX.loc[1]\nclf.predict([\"38.0\",\"1.0\",\"0.0\",\"0.0\",\"0.0\",\"1.0\",\"0.0\",\"0.0\"])\nclf.predict([38.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0])\n", "intent": "[my] troubleshooting:  \nhad difficulty consuming API (see later)  - wanted to figure out how send parameters for prediction\n"}
{"snippet": "predicted_test_fashion_class, predicted_test_fashion_proba \\\n                = predict(fashion_test_x, fashion_mean, fashion_var)\npredicted_test_cifar_class, predicted_test_cifar_proba = predict(cifar_test_x, cifar_mean, cifar_var)\n", "intent": "<div class=\"alert alert-info\">\ne) <b>(code)</b> Display the confusion matrix on the test data  <b>[5]</b> </div>\n"}
{"snippet": "plot_all_loss(error_tr_sgd, loss_ts_sgd, error_tr_bfgs, loss_ts_bfgs, error_tr_lbfgs, loss_ts_lbfgs,\\\n             xscale='log', yscale='log')\n", "intent": "- training and validation loss vs iterations\n"}
{"snippet": "X2 = X[['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Baby%']]\ncluster_labels = dbscan_alg.fit_predict(X2)\ncluster_labels\n", "intent": "Ouch! This algorithm only found 3 clusters (\nLet's try to remove Health% and Pets% as we have seen those features are irrelevant.\n"}
{"snippet": "import math\nfrom sklearn.metrics import mean_squared_error\nrmse = math.sqrt(mean_squared_error(real_stock_price[:,0], predicted_stock_price[:,0]))\nprint(\"rmse is:\",rmse)\nperformance = rmse/np.mean(real_stock_price[:,0])\nprint (\"% Performance is:\", performance)\n", "intent": "<b> Evaluation </b>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Not really, since the accuracy using this method (0.73) is slightly lower than the accuracy from our previous method (0.77). \n"}
{"snippet": "print \"F1 score for all \\\"yes\\\" on test set: {:.4f}\".format(\n    f1_score(y_test, ['yes']*95, pos_label='yes', average='binary'))\n", "intent": "If we predict all \"yes\" values for each instance...\n"}
{"snippet": "results = loaded_model.evaluate(X, test_labels)\nprint(\"\\n%s: %.2f%%\" % (loaded_model.metrics_names[1], results[1]*100))\n", "intent": "Finally, evaluate the model on the test data.\n"}
{"snippet": "y_trn_1d = y_trn.reshape((-1,))\nrand_pred_trn = np.random.choice(y_trn_1d, X_trn.shape[0])\nrmse_trn = np.sqrt(mean_squared_error(y_trn, rand_pred_trn))\nrmse_trn\n", "intent": "Bootstrap labels and compute RMSE of random predictions\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(\n    lr, \n    X_trn, \n    y_trn,\n    scoring='neg_mean_squared_error', \n    cv=10\n)\n-scores.mean()\nscores.std()\n", "intent": "Although on the test set it performs much worse... Why? Now I repeat the analysis on `LinearRegression`\n"}
{"snippet": "thresholds = [0, 0.5, 1, 2, 3, 4, 5]\nscores = []\nfor t in thresholds:\n    pred_y = [swn_polarity(text, t) for text in X_test]\n    scores.append(accuracy_score(y_test, pred_y))\n", "intent": "We need to find the best sentiment threshold that leads to the best score\n"}
{"snippet": "inception_resnet_v2_predictions = [np.argmax(my_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_inception_resnet_v2]\ntest_accuracy = 100*np.sum(np.array(inception_resnet_v2_predictions)==np.argmax(labels_test_inception_resnet_v2, axis=1))/len(labels_test_inception_resnet_v2)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Accuracy over test data set.\n"}
{"snippet": "Xception_predictions = [np.argmax(my_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(labels_test_exception, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Accuracy over test data set.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg_scores = cross_val_score(lin_reg, housing_train_prepared, housing_train_labels, \n                         scoring='neg_mean_squared_error', cv=10, n_jobs=-1)\nlin_reg_rmse_scores = np.sqrt(-lin_reg_scores)\n", "intent": "For the sake of completeness let's take the cross validation scores with Linear Regression as well.\n"}
{"snippet": "los, acc = model.evaluate([X_test[:,0:5], X_test[:,5:9], X_test[:,9:13]], y_test, batch_size=32)\nlos, acc\n", "intent": "**Evaluate on the Test Set:**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nimport math\ny_pred = reg.predict(X_test)  \nmse = mean_squared_error(y_test, y_pred)  \nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint ('mse = {}, rmse = {} \\nmae = {} r2 = {}'.format(mse,math.sqrt(mse), mae, r2))\n", "intent": "Let's print some other metrics\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint (y_pred)\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities\n"}
{"snippet": "def L2Norm(weights):\n    L2NormTot = 0\n    for w in weights:\n        L2NormTot += tf.nn.l2_loss(w)\n    return L2NormTot\n", "intent": "The function *l2_loss(t)* caculates the L2 norm of a tensor and it is used to calculate the regularizer.\n - *output = sum(t ** 2) / 2*\n"}
{"snippet": "score_SVC = accuracy_score(yTest, yPredictSVC)\nprint(score_SVC)\n", "intent": "Using accuracy_score to test our models accuracy\n"}
{"snippet": "print model.predict(x_test[:5])\nprint model.predict_classes(x_test[:5])  \n", "intent": "we can see that the trained model has a 3% loss, and 99% accuracy :)\n"}
{"snippet": "print(classification_report(df_preds.label, df_preds.pred,\n                            target_names=class_names, digits=3))\n", "intent": "Overall these are terrific results, with **97.8%** accuracy (recall). The best model was generated on the 13th.\n"}
{"snippet": "sequence=\"AAVPP\"\nions=\"y1;b1\"\nintensities=\"1;1\"\ntest_data = input_data(sequence, ions, intensities,8)\nnw=test_data.iloc[:, :-2]\npredictions = model.predict(nw)\nrounded = [x for x in predictions]\nrounded\n", "intent": "Lets test our model now \n"}
{"snippet": "test=np.array([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\ntest=test.reshape((1,10,1))\npredict=model.predict(test)\nprint(predict)\n", "intent": "now we will predict next number from our model, this is just a toy dataset it wont give a good accuracy but is good for understanding concepts\n"}
{"snippet": "print(\"\\tActual price\\t\\tPredicted price\")\nfor e in enumerate(zip(y_test[100:120], estimator.predict(X_test)[100:120])):\n    print(e[0],\"{:20,}\".format(e[1][0]), \"\\t{:20,.0f}\".format(e[1][1]))\n", "intent": "So what does a RMSLE of 0.55 mean? Let's compare the actual and predicted prices for a couple of apartements.\n"}
{"snippet": "all_pred = spam_detect_model.predict(messages_tfidf)\nall_pred\n", "intent": "Let's find out how the model will perform on the entire dataset.\n"}
{"snippet": "start = time()\ny_pred_fashionMNIST = fashionMNIST_clf.predict(X_test_fashionMNIST)\nend = time()\nprint('Prediction completed in {:.02f} seconds'.format(end-start))\n", "intent": "Let's now predict the test set and show both the accuracy and the confusion matrix for those predictions.\n"}
{"snippet": "def predict(x, t, x_test, K=1):       \n    X_test = get_X(x_test, K)\n    w_hat = get_w_hat(get_X(x, K), t, K)\n    predictions = w_hat.dot(X_test.T)\n    return predictions\n", "intent": "<div class=\"alert alert-info\">\nWrite a function that, when given `x`, `t` and `x_test`, computes `w_hat` and makes predictions at `x_test`.</div>\n"}
{"snippet": "test_input_fn = generate_input_fn(file_names = [eval_data_file],\n                                 mode = tf.estimator.ModeKeys.EVAL,\n                                 batch_size = 1000)\nestimator = get_estimator(run_config, hparams)\nprint(estimator.evaluate(input_fn = test_input_fn, steps = 1))\n", "intent": "* Use the new data, not used for training.\n"}
{"snippet": "test_input_fn = generate_input_fn(file_names=[eval_data_file],\n                                  mode=tf.estimator.ModeKeys.EVAL,\n                                  batch_size=1000)\nestimator = get_estimator(run_config, hparams)\nprint(estimator.evaluate(input_fn=test_input_fn, steps=1))\n", "intent": "* Use the new data, not used for training.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions:\\n', lin_reg.predict(some_data_prepared))\nprint('Labels:\\n', list(some_labels))\n", "intent": "Let's try it on some instances of the training set\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions))\nlin_rmse\n", "intent": "Not very accurate. Let's see the RMSE for the regression model\n"}
{"snippet": "scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\nlin_rmse_scores = np.sqrt(-scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Let's try cross-validation with the linear regression model\n"}
{"snippet": "dec_clf.predict_proba([[5,1.5]])\n", "intent": "<h3> Estimating the Probabilities </h3>\n"}
{"snippet": "print((TP+TN)/(TP+TN+FP+FN))\nprint(accuracy_score(y_test, y_pred_class))\n", "intent": "<b> Classification Accuracy:</b> Overall, how often your classifier is correct\n"}
{"snippet": "print(((FN+FP)/(FN+FP+TN+TP)))\nprint(1-accuracy_score(y_test, y_pred_class))\n", "intent": "<b> Classification Error:</b> Overall, how often your classifier is incorrect\n"}
{"snippet": "R2 = 1 - ( np.sum( (y - reg.predict(X))**2 ) / np.sum( (y-np.mean(y))**2 ) );\nprint(R2)\n", "intent": "In the formula above, $\\overline{y}$ is the sample mean and $\\hat{y}$ is the value predicted by the model.  \n"}
{"snippet": "y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm\n", "intent": "The smaller the entropy is, the node is more homogeneous which is preferable. \n"}
{"snippet": "y_pred = classifier.predict(X_test) \ny_pred\n", "intent": "y_pred is a vector of predictions (purchases=1) for observations in the test set\n"}
{"snippet": "y_pred = regressor.predict(6.5)\ny_pred\n", "intent": "Checking prediction value\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "Predict the test set's results\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "TP: 1299 (accurately predicted no-claim) FN: 232 (falsely predicted claim)\nFP: 416 (falsely predicted no-claim) TN: 485 (accurately predicted claim)\n"}
{"snippet": "cv_scores = cross_val_score(logreg1, X_train, y_train, cv=5)   \nprint (\"Cross-validated scores:\", cv_scores)\nprint ('Mean:', cv_scores.mean())\n", "intent": "---\nThe score is approximately baseline.\n---\n"}
{"snippet": "print (confusion_matrix(y_test, logreg1.predict(X_test)))  \nprint ('\\n')\nprint(classification_report(y_test, logreg1.predict(X_test), target_names=['Fully Paid', 'Default']))\n", "intent": "---\nThe model also performs consistently across folds of the test set.\n---\n"}
{"snippet": "cv_scores = cross_val_score(logreg_gr, X_train, y_train, cv=5)   \nprint (\"Cross-validated scores:\", cv_scores)\nprint ('Mean:', cv_scores.mean())\n", "intent": "---\nThe training score has been slightly improved.\n---\n"}
{"snippet": "cv_scores = cross_val_score(logreg_gr, X_test, y_test, cv=5)  \nprint (\"Cross-validated scores:\", cv_scores)\nprint ('Mean:', cv_scores.mean())\n", "intent": "---\nMiniscule improvement in testing score.\n---\n"}
{"snippet": "print (confusion_matrix(y_test, logreg_gr.predict(X_test)))  \nprint ('\\n')\nprint(classification_report(y_test, logreg_gr.predict(X_test), target_names=['Fully Paid', 'Default']))\n", "intent": "---\nConsistent performance across folds.\n---\n"}
{"snippet": "dtr3_scores = cross_val_score(dtr3, X_train, y_train, cv=5)\ndtrN_scores = cross_val_score(dtrN, X_train, y_train, cv=5)\nprint((dtr3_scores, np.mean(dtr3_scores)))\nprint((dtrN_scores, np.mean(dtrN_scores)))\n", "intent": "---\nThe tree with no maximum depth has a score of 1.0. It is most likely overfitted.\n---\n"}
{"snippet": "print (confusion_matrix(y_test, dtr3.predict(X_test)))    \nprint('\\n')    \nprint(classification_report(y_test, dtr3.predict(X_test), target_names=['Fully Paid', 'Default']))\n", "intent": "---\nThe no maximum depth tree has a significantly worse testing score than the other models\n---\n"}
{"snippet": "cv_scores = cross_val_score(logreg, X, y, cv=5)   \nprint (\"Cross-validated scores:\", cv_scores)\nprint ('Mean:', cv_scores.mean())\n", "intent": "---\nThe accuracy score of the model is significantly higher than the baseline.\n---\n"}
{"snippet": "predictions = model.predict(X)\n", "intent": "I can now check the confusion matrix and classification report to check for false positives/negatives.\n---\n"}
{"snippet": "cv_scores = cross_val_score(logreg, X_test, y_test, cv=5)  \nprint (\"Cross-validated scores:\", cv_scores)\nprint ('Mean:', cv_scores.mean())\n", "intent": "---\nThe model performs well on the test data and the score is only slightly worse than the training score.\n---\n"}
{"snippet": "dtr1_scores = cross_val_score(dtr1, X_train, y_train, cv=5)\ndtr2_scores = cross_val_score(dtr2, X_train, y_train, cv=5)\ndtr3_scores = cross_val_score(dtr3, X_train, y_train, cv=5)\ndtrN_scores = cross_val_score(dtrN, X_train, y_train, cv=5)\nprint((dtr1_scores, np.mean(dtr1_scores)))\nprint((dtr2_scores, np.mean(dtr2_scores)))\nprint((dtr3_scores, np.mean(dtr3_scores)))\nprint((dtrN_scores, np.mean(dtrN_scores)))\n", "intent": "---\nAll test scores are roughly the same, only just above the baseline. The tree that has no maximum depth has the best training score of 0.877.\n---\n"}
{"snippet": "print (confusion_matrix(y_test, dtrN.predict(X_test)))    \nprint('\\n')    \nprint(classification_report(y_test, dtrN.predict(X_test), target_names=['business analyst', 'data scientist'])) \n", "intent": "---\nThe test scores are very close to the training scores. Again, the tree without a maximum depth performs the best.\n---\n"}
{"snippet": "print (confusion_matrix(y_test, model.predict(X_test)))    \nprint('\\n')    \nprint(classification_report(y_test, model.predict(X_test), target_names=['business analyst', 'data scientist'])) \n", "intent": "---\nAs with the decision trees, the logistic regression is getting accuracy scores of roughly the baseline.\n---\n"}
{"snippet": "model.fit_generator(datagen.flow(train_dataset, train_labels, batch_size=32),\n                    samples_per_epoch=len(train_dataset), nb_epoch=nb_epoch)\nscore = model.evaluate(train_dataset,train_labels, verbose=0)\nmodel.save_weights('leafcnn.h5')\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "<h3>Model Training with Results</h3>\n"}
{"snippet": "t0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(classification_report(y_test, y_pred, target_names=target_names))\n", "intent": "<h3>Predictions for Test Set</h3>\n"}
{"snippet": "output, output_certainty = classify_data(C45Tree,test_feats)\naccuracy = MulticlassAccuracy()\nprint 'Accuracy : ' + str(accuracy.evaluate(output, test_labels))\n", "intent": "Check prediction results and get accuracy\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(testy, p4(testx))\nprint(r2)\n", "intent": "so performance if training data will inc as complexity inc and test data will reduce - > overfitting\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = model.extract_features()[content_layer]\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats), {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.9f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for i in range(len(style_layers)):\n        layer_style_loss = style_weights[i] * \\\n            tf.reduce_sum(tf.square(tf.subtract(gram_matrix(feats[style_layers[i]]), style_targets[i])))\n        style_loss = tf.add(style_loss, layer_style_loss)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_test_pred = model.predict(X_test)\n", "intent": "We can make predictions for new data by doing:\n"}
{"snippet": "def predict_category(s, train=train, model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n", "intent": "The very cool thing here is that we now have the tools to determine the category for any string, using the predict() method of this pipeline.\n"}
{"snippet": "y_pred = clf.predict(X_test)\ny_pred.shape\n", "intent": "**Predictions on the test set**\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n", "intent": "Have now successfully fit a linear regressor to the data\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "The regression works! But the predictions are far from accurate. With the first value being 40% off. \nInvestigating the RMSE. \n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "The SGD classifier uses random samples during training, therefore specifying a random_state allows for reproducbility of results. \n"}
{"snippet": "forest_clf.predict_proba([some_digit])\n", "intent": "Calling predict_proba() to return a list of the probabilities that the classifier assigned to each of the instances for the class.\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Using cross-validation we can evaluate the accuracy of the models. Using the cross_val_score() function:\n"}
{"snippet": "knn_clf.predict([some_digit])\n", "intent": "We use the K-Neighbors classifier as it supports multi-label classification. It is trained on the multiple targets array as the target array. \n"}
{"snippet": "print np.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simply the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "preds_proba = lr.predict_proba(data[expl_cols])[:, 1]\npreds_link = np.log(preds_proba / (1 - preds_proba))\nprint('predicted probabilities:', preds_proba[:5])\nprint('predictions link scale:', preds_link[:5])\n", "intent": "Get predicted probabilites and predictions on the link scale before the logit function is applied.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    styleLoss = tf.constant(0.0)\n    for i in range(0, len(style_layers)):        \n        gram = gram_matrix(feats[style_layers[i]])\n        styleLoss += style_weights[i] * tf.reduce_sum((style_targets[i] - gram)**2)\n    return styleLoss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print classification_report(y_test, y_test_pred)\n", "intent": "This is actually pretty good\n"}
{"snippet": "clf = joblib.load('bc_nn_model.pkl') \ns_predictions = clf.predict(X_test)\nmd(\"**Accuracy:**\")\nprint(\"{:.2%}\\n\".format(np.mean(s_predictions==y_test)))\nmd(\"**Confusion Matrix:**\")\nprint(\"{}\\n\".format(confusion_matrix(y_test,s_predictions)))\nmd(\"**Classification Report:**\")\nprint(\"{}\\n\".format(classification_report(y_test,s_predictions)))\n", "intent": "Load, predict and report again.\n"}
{"snippet": "predicted = text_clf.predict(docs_test)\nnp.mean(predicted == twenty_test.target)\n", "intent": "... and now run the trained classifi.er\n"}
{"snippet": "predictions = lr.predict(X_test)\n", "intent": "Using unseen dataset X_test (heights) as input to the trained model, make predictions for the corresponding weights\n"}
{"snippet": "lr.predict([[67]])\n", "intent": "Given height in inches, try to predict weight in pounds.\nActual height = 67 inches, and\nActual weight = 171 pounds\n"}
{"snippet": "print(lm.predict(X[10]))\nprint(lm.predict(X[60]))\nprint(lm.predict(X[100]))\n", "intent": "Predict some values.\n"}
{"snippet": "from azureml import services\n@services.publish(workdspace_id, authorization_token)\n@services.types(sl=float, sw=float, pl=float, pw=float)\n@services.returns(float)\ndef predict_species(sl, sw, pl, pw):\n    return lm.predict([sl, sw, pl, pw])\n", "intent": "Publish a web service.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n", "intent": "scikit-learn comes with metric functions to check accuracy\n"}
{"snippet": "pred_train = vgg.predict(X_train_channels)\npred_test = vgg.predict(X_test_channels)\n", "intent": "The convolutional layers more or less function as a preprocessing step for the data that we will feed into our fully-connected layers.\n"}
{"snippet": "trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n", "intent": "Finally, we can generate predictions using the model for both, the training and the testing datasets. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, ind in enumerate(style_layers):\n        style_loss += style_weights[i] * ((gram_matrix(feats[ind]) - style_targets[i])**2).sum()\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, ind in enumerate(style_layers):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.pow(gram_matrix(feats[ind]) - style_targets[i], 2))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "lambdas = np.logspace(-6, 1, 100)\nds = np.arange(1,np.shape(X)[1])\nscores = {}\nfor d in ds:\n    for lamb in lambdas:\n        ede_model = ede(lamb,d)\n        this_scores = np.absolute(cross_validation.cross_val_score(ede_model, X, y,scoring='mean_squared_error',cv=10))\n        scores[(lamb,d)]=np.mean(this_scores)\nprint('finished')\n", "intent": "<h3> Tuning the parameters </h3>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Answer: The accuracy score is lower than before, both on the training set and test set.\n"}
{"snippet": "X_vals = regression.getXVals(selected_feature_names)\nfitted_production_model.predict(X_vals.iloc[0 , :].values.reshape(1,-1))[0]\n", "intent": "The predicted number of trips produced from zone 1, given the original number of residential areas.\n"}
{"snippet": "X_vals = regression2.getXVals(selected_feature_names)\nfitted_production_model.predict(X_vals.iloc[0 , :].values.reshape(1,-1))[0]\n", "intent": "The predicted number of trips produced from zone 1, given that the number of residential areas were increased.\n"}
{"snippet": "log_fpr, log_tpr, _ = roc_curve(y_test, \n                    log_clf_est_b.predict_proba(X_test)[:,1])\nlog_roc_auc = auc(log_fpr, log_tpr)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, \n                    rf_clf_est_b.predict_proba(X_test)[:,1])\nrf_roc_auc = auc(rf_fpr, rf_tpr)\n", "intent": "Basic Estimators: no bag of words or PCA\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\ndef get_metrics(y_test, y_predicted):  \n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n", "intent": "Let's define a function to help us assess accuracy of our trained models\n"}
{"snippet": "train_accuracy = 1 - np.sum(np.abs(clf.predict(X_train) - y_train))/len(y_train)\nprint('Accuracy on the train dataset is '+ str(train_accuracy))\n", "intent": "Ok, we fitted a classifier, but how should we evaluate performance? First let's look at the accuracy on the train dataset: it better be good!!!\n"}
{"snippet": "test_accuracy = 1 - np.sum(np.abs(clf.predict(X_test)-y_test))/len(y_test)\nprint('Accuracy on the test dataset is '+ str(test_accuracy))\n", "intent": "But that does not matter, what we want to know is how the method performs on the test dataset, whose labels we have not seen in training.\n"}
{"snippet": "y_pred = clf.predict(X_test)\ny_score = clf.predict_proba(X_test)[:,1]\nprint(y_score)\n", "intent": "Note: the Random Forest Classifier is a probalistic algorithm and in fact can output a score for the chance of belonging to a class.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = bag_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "`BaggingClassifier` is likely to achieve about 84.4% accuracy on the test set.\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint(\"f1 score most frequent: {:.2f}\".format(f1_score(y_test, pred_most_frequent)))\nprint(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\nprint(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\nprint(\"f1 score logistic regression: {:.2f}\".format(f1_score(y_test, pred_logreg)))\n", "intent": "This particular variant is also known as the `f1-score`\n"}
{"snippet": "print(classification_report(y_test, pred_dummy,target_names=[\"not nine\", \"nine\"]))\n", "intent": "the `support` of each class, which simply means the number of samples in this class according to the ground truth.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\n", "intent": "We can compute the area under the ROC curve using the roc_auc_score function:\n"}
{"snippet": "cross_val_score(logit, X, y, cv=3,scoring=\"precision\")\n", "intent": "proportion of observations predicted correctly.\n"}
{"snippet": "cross_val_score(logit, X, y, cv=3, scoring=\"recall\")\n", "intent": "Recall is the proportion of every positive observation that is truly positive.\n"}
{"snippet": "cross_val_score(logit, X, y, cv=3, scoring=\"f1\")\n", "intent": "of observations labeled as positive, how many are actually positive:\n"}
{"snippet": "roc_auc_score(target_test, target_probabilities)\n", "intent": "The better a model is, the higher the curve and thus the greater the area under the curve\n"}
{"snippet": "new_observation = [[ 4,  4,  4,  0.4]]\nmodel.predict(new_observation)\n", "intent": "Gaussian naive Bayes is best used in cases when all our features are continuous\n"}
{"snippet": "new_observation = [[0, 0, 0, 1, 0, 1, 0]]\nmodel.predict(new_observation)\n", "intent": "In practice, this means that this classifier is commonly used when we have discrete data (e.g., movie ratings ranging from 1 to 5)\n"}
{"snippet": "from sklearn.model_selection import LeaveOneOut\nscores = cross_val_score(model, X, y, cv=LeaveOneOut())\nscores\n", "intent": "the extreme case in which our number of folds is equal to the number of data points; that is, we train on all points but one in each trial.\n"}
{"snippet": "r2 = r2_score(weights_test, weights_predicted)\nprint(r2)\n", "intent": "* ``r2_score`` takes the true values and predicted values as its argument.\n"}
{"snippet": "y_predicted = clf.predict(X_test)\ny_predicted[:50]\n", "intent": "Using the classifier's ``predict`` method, let's see what labels the classifier would assign to the *unseen* test data. \n"}
{"snippet": "precision_score(y_test, y_predicted, average=\"weighted\")\n", "intent": "We can take the average of prediction scores across label categories, or we can indicate the weighted average using the precision_score method. \n"}
{"snippet": "tick=datetime.datetime.now()\nresult = classifier.predict(x_test_vectorised)\ntock=datetime.datetime.now()\nlr_pred_train_time = tock - tick\nprint('Time taken to predict the data points in the Test set is : ' + str(lr_pred_train_time))\n", "intent": "now we predict the classes of tweets on the test set\n"}
{"snippet": "y_train_pred = forest.predict(train)\nnp.sqrt(mean_squared_error(y, y_train_pred))\n", "intent": "Looking at the root mean squared error of our training set, we can see that there overfitting is likely happening. \n"}
{"snippet": "y_train_pred = gs.predict(train)\nnp.sqrt(mean_squared_error(y, y_train_pred))\n", "intent": "Looking at the root mean squared error of our training set, we can see that there overfitting is likely happening. \n"}
{"snippet": "features = []\nnew_images=[]\nfor image_path in tqdm(images):\n    try:\n        img, x = get_image(image_path);\n        feat = feat_extractor.predict(x)[0]\n        features.append(feat)\n        new_images.append(image_path)\n    except:\n        print(\"problem with {}\".format(image_path))\n", "intent": "Loading files from target folder, preprocess and extract feature vector. Exception handling is necessary due to corrupted images.\n"}
{"snippet": "train_test_inner['Pred1'] = log_clf.predict_proba(X_train)[:,1]\ntrain_test_missing['Pred1'] = log_clf.predict_proba(X_test)[:,1]\n", "intent": "Here we store our probabilities\n"}
{"snippet": "print(classification_report(y_true, y_pred))\n", "intent": "Precision, recall and F1-measure for each class\n"}
{"snippet": "MoreVar_Predict = MoreVar_Fit.predict([(2,1,25,60)])\nMoreVar_Predict\n", "intent": "**Predict**\nWhat is **TOTAL/COUNT** when <font color='blue'><b>season = 2, weather = 1, temp = 25 degree celcius, humidity = 60</b></font>\n"}
{"snippet": "MoreVar_Fit.predict([(0,0,0,0)])\n", "intent": "When all I/P Var is 0, the predict should give same as Intercept\n"}
{"snippet": "pred_tree_two = my_tree_two.predict(X)\nnp.count_nonzero(pred_tree_two)\n", "intent": "**Predicting Survivals for X with this new tree-two**\n"}
{"snippet": "Xception_predictions = [np.argmax(m1.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "eval = evaluate(\"snapshots/lodbrok-2018-08-08T17:10:00.403928.h5py\", \"../SENSITIVE/spambrainz_dataset_eval.pickle\")\nprint_stats(eval)\n", "intent": "500 spam non-spam entries each which the model has never seen before.\n"}
{"snippet": "spam = evaluate(\"snapshots/lodbrok-2018-08-08T17:10:00.403928.h5py\", \"../SENSITIVE/spambrainz_dataset_spam.pickle\")\nprint_stats(spam)\n", "intent": "About 150.000 spam editors, 5.000 of which the model has been trained on.\n"}
{"snippet": "nonspam = evaluate(\"snapshots/lodbrok-2018-08-08T17:10:00.403928.h5py\", \"../SENSITIVE/spambrainz_dataset_nonspam.pickle\")\nprint_stats(nonspam)\n", "intent": "Around 8.000 editors, 5.000 of which the model has been trained on.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nstart = clock()\nscores = cross_val_score(clf, train, label, n_jobs = 6)\nprint(\"Performed {:d}-fold cross validation in {:.0f} seconds with accuracy {:0.4f} +/- {:0.4f}.\".format(\n    len(scores), clock() - start, scores.mean(), scores.std()))\n", "intent": "We can use cross-validation to get an idea of how well the classifier generalizes.\n"}
{"snippet": "predicted_ratings = predictions[test_matrix.nonzero()]\ntest_truth = test_matrix[test_matrix.nonzero()]\nmath.sqrt(mean_squared_error(predicted_ratings,test_truth))\n", "intent": "Considering only those ratings that are not zero in the test matrix and using them to find error in our model.\n"}
{"snippet": "print metrics.accuracy_score(y_test, predicted)\nprint metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "The classifier is predicting a 0 (no Concession) any time the probability in the second column is 0.\n"}
{"snippet": "model.predict_proba(np.array([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 25, 3, 1, 4,\n                              16]))\nQuantity + I(Quantity ** 2.0) + I(Quantity ** 3) + Level +I(Level ** 2.0)+ AreaPerUnit + age+I(age ** 2.0)+I(age ** 3.0) + RenovationAge+I(RenovationAge ** 2.0) +I(RenovationAge ** 3.0) + NewSupply + YoungAgeGroup + I(YoungAgeGroup ** 2.0) + LaborForceParticipation + UR + I(UR ** 2)+ AskingRentChange +I(AskingRentChange ** 2.0) + +I(AskingRentChange ** 3.0)+YoungAgeGroup_year+LaborForceParticipation_year+HomePriceIndex_year+ AskingRentChange_year\n", "intent": "Predicting the probability of concession for a random property not present in the dataset. \n"}
{"snippet": "y_pred = model.predict(x=images)\nimages = data.test.images[0:9]\ncls_true = data.test.cls[0:9]\ncls_pred = np.argmax(y_pred, axis=1)\nplot_utils.plot_images(images=images, img_shape=img_shape, cls_true=cls_true, cls_pred=cls_pred)\n", "intent": "**Step 7**: Test model.\n"}
{"snippet": " predictions = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_train,predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions=KNN.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "evaluate(y_train, y_pred_train)\n", "intent": "**Training evaluation**\n"}
{"snippet": "evaluate(y_test, y_pred_test)\n", "intent": "**Test evaluation**\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_pmodel.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "metrics_ = ['precision', 'recall', 'f1']\nfor metric_ in metrics_:\n    scores = cross_val_score(higgsModel_1, features, labels, scoring= metric_,\n                             cv = StratifiedKFold(shuffle=True, n_splits=5), \n                             verbose= 1, n_jobs= 4)\n    print('The Metric [{}] : {}'.format(metric_, scores))\n    print('Mean of The Metric [{}] : {}'.format(metric_, scores.mean()))\n    print('Standard Deviation of The Metric [{}] : {}'.format(metric_, scores.std()))\n    print('\\n')\n", "intent": "***I chose precision as a metric however I was curious to see the others.***\n"}
{"snippet": "log_preds = learn.predict()\nlog_preds.shape\n", "intent": "Predictions are in log scale\n"}
{"snippet": "predictions = dnn.predict(X_val)\n", "intent": "Cool 93% accuracy, this is an improvement over our previous tagger!\n"}
{"snippet": "predictions = cross_val_predict(X=X_train, y=Y_train, cv=3, estimator=log_reg)\n", "intent": "Much better results than KNN right of the bat. Let's see what our confusin matrix looks like.\n"}
{"snippet": "n = Network(cifar.input_size, cifar.output_size)\nN = 100\ninput_data = training['images'][0:N].transpose()\nground_truth = training['one_hot_labels'][0:N].transpose()\nprobabilities, _ = n.evaluate(input_data)\ngrad_w_num, grad_b_num = compute_grads_num(input_data, ground_truth, n)\n", "intent": "Computing the gradients with the finite difference method\n"}
{"snippet": "n = Network(cifar.input_size, cifar.output_size, l=2.0)\nN = 100\ninput_data = training['images'][0:N].transpose()\nground_truth = training['one_hot_labels'][0:N].transpose()\nprobabilities, _ = n.evaluate(input_data)\ngrad_w_num, grad_b_num = compute_grads_num(input_data, ground_truth, n)\n", "intent": "Computing the gradients with the finite difference method\n"}
{"snippet": "best_net = df.iloc[0]['model']\n_, predictions = best_net.evaluate(test['images'].transpose())\ntest_accuracy = n.accuracy(predictions, test['one_hot_labels'].transpose())\nprint(df.iloc[0]['summary'])\nprint(df.iloc[0]['cost_accuracy_string'])\nprint('Test:\\n- accuracy {:.2%}'.format(test_accuracy))\n", "intent": "Now let's select the the model that performs better on the validation set and use the test set to give an unbiased score of its performances.\n"}
{"snippet": "to_check_train, to_check_y = train, y_train\nto_check_pred = model_naive.predict(to_check_train).round()\nprint_metrics(to_check_y, to_check_pred)\n", "intent": "The predicted is_claim_bool is then done by the model, and simply rounded as 0/1 for accuracy measurement.\n"}
{"snippet": "to_check_train, to_check_y = train, y_train\nto_check_pred = model.predict(to_check_train).round()\nprint_metrics(to_check_y, to_check_pred)\n", "intent": "Accuracy measurement from feature engineering approach:\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy=100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test Accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    knn = answer_five()\n    prediction = answer_seven()\n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score(y_test, prediction)\n    return acc\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "eval_results = linear_classifier.evaluate(input_fn=dataset_input_fn(\"test\"))\nprint(eval_results)\n", "intent": "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:\n"}
{"snippet": "eval_results = mlp_classifier.evaluate(input_fn=dataset_input_fn('test'))\nprint(eval_results)\n", "intent": "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:\n"}
{"snippet": "print(vgg.predict(imgs, all_probs=True))\n", "intent": "Let's try predicting now\n"}
{"snippet": "def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content))\nlayer_features = layers['block2_conv2']\ncontent_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(content_image_features, combination_features)\n", "intent": "According to the original paper the content loss is given by:\n$$ J_{content}(C,G) = \\sum (a^{(c)} - a^{(g)})^{2} $$\n"}
{"snippet": "def total_variation_loss(x):\n    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return backend.sum(backend.pow(a + b, 1.25))\nloss += total_variation_weight * total_variation_loss(combination_image)\n", "intent": "Add total variation loss for regularization\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nr50_test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % r50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tens]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"t6.jpeg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "print np.sum((bos.PRICE - lm.predict(X)) ** 2) / 506\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n", "intent": "Now we have an ensemble containing three trees. It can make predictions on a new\ninstance simply by adding up the predictions of all the trees:\n"}
{"snippet": "row_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  \ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nmy_model.predict_proba(data_for_prediction_array)\n", "intent": "<a id = 'SHAP-values'></a>\n"}
{"snippet": "print('Accuracy: {}'.format(round(metrics.accuracy_score(yPred, yTest), 3)))\n", "intent": ">Remarks - 3 out of 45 samples are incorrectly predicted, yieled an misclassification percent of ~6.7%\n"}
{"snippet": "yTestPred = nn.predict(XTest)\nacc = (np.sum(yTest == yTestPred).astype(np.float) / XTest.shape[0])\nprint('Test accuracy: {:.2f}%'.format(acc * 100))\n", "intent": "> Remarks - The plot shows an increasing gap between the training and validation increases. \n"}
{"snippet": "proba = rnn.predict(XTest, return_proba = True)\n", "intent": "> Remarks - This result is comparable to what was achieved in chapter 8, mainly due to the small size of the dataset. \n"}
{"snippet": "mean_squared_error(predicted_y, ytest)\n", "intent": "We can also access the mean squared error:\n"}
{"snippet": "print (clf.predict([[150,0]]))\n", "intent": "<h3>Train your Model<h/3>\n"}
{"snippet": "num_class = len(group_label_encoder.classes_)\nequal_pred = np.ones((len(df_gender_age_train), num_class)) / num_class\nprint('log_loss of non-preference prediction:', log_loss(group_labels, equal_pred))\n", "intent": "A naive strategy to predict the probability that a sample could be assigned to a group, is to assume that each group has an equal probability.\n"}
{"snippet": "rate = 0.0001\nlogits,weights = LeNet(x,CHANNELS,KEEP_PROBAB)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\nloss_operation = tf.reduce_mean(cross_entropy) + BETA*( tf.reduce_mean( [ tf.nn.l2_loss(w) for w in weights]) )\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)\n", "intent": "Create a training pipeline that uses the model to classify MNIST data.\nYou do not need to modify this section.\n"}
{"snippet": "y_pred = regressor.predict(6.5)\n", "intent": "Now that looks way better! What we ended up doing was getting a much closer view of all of the intervals!\n"}
{"snippet": "predict(image_path='data/images/parrot_cropped1.jpg')\n", "intent": "We can then use the VGG16 model on a picture of a parrot which is classified as a macaw (a parrot species) with a fairly high score of 79%.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_hat = model.predict(X_test)\nroc_auc_score(y_test, y_hat)\n", "intent": "The Keras model can bne used with scikit-learn functions to evaluate its performance.\n"}
{"snippet": "prediction=knn.predict(X_test)\n", "intent": "Now that we fitted our model let's see the predictions that it makes.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for (idx, l) in enumerate(style_layers):\n        G_l = gram_matrix(feats[l])\n        A_l = style_targets[idx]\n        w_l = style_weights[idx]\n        loss += w_l * (G_l - A_l).pow(2).sum()\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "reg.predict([[3000, 3, 40]])\n", "intent": "**Find price of home with 3000 sqr ft area, 3 bedrooms, 40 year old**\n"}
{"snippet": "reg.predict([[2500, 4, 5]])\n", "intent": "**Find price of home with 2500 sqr ft area, 4 bedrooms,  5 year old**\n"}
{"snippet": "y_predicted = model.predict(X_test)\n", "intent": "<h4 style='color:purple'>Confusion Matrix</h4>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(my_Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data) \nprint(\"Predictions:\\t\",lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "We can try it out on a few instances from the training set.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(\"The model's RMSE:\\t\",lin_rmse)\n", "intent": "We can measure the model's RMSE on teh whole training set using scikit-learn's `mean_squared_error` function.\n"}
{"snippet": "lm.evaluate(x, y, verbose=0)\n", "intent": "The weights have now been trained. The next step is to evaluate our loss. Obviously, we're looking for a very small number here... \n"}
{"snippet": "y_pred = model.predict(X_test_flat, batch_size=500)\ny_pred = np.argmax(y_pred, axis=1)\naccuracy_score(y_test, y_pred)\n", "intent": "And now we get predictions and compare them to the true values:\n"}
{"snippet": "y_pred = model.predict(X_test_flat, batch_size=500)\ny_pred = np.argmax(y_pred, axis=1)\naccuracy_score(y_test, y_pred)\n", "intent": "Now we get about 87% test error:\n"}
{"snippet": "y_pred = model.predict(X_test_flat, batch_size=500)\ny_pred = np.argmax(y_pred, axis=1)\naccuracy_score(y_test, y_pred)\n", "intent": "The basic trend we see so far is more layers corresponds to more expressive functions, which in turn leads to better test error.\n"}
{"snippet": "predictions = [np.argmax(chosen_model.predict(np.expand_dims(feature, axis=0))) for feature in chosen_set['test']]\ntest_accuracy = 100 * np.sum(np.array(predictions) == np.argmax(test_targets, axis=1)) / len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(mean_squared_error(crime_arr, indus_arr, w_hat[0], w_hat[1]))\n", "intent": "Let's calculate the Mean Squared Error\n"}
{"snippet": "print(mean_squared_error(crime_arr, indus_arr, m, b))\n", "intent": "Let's again calculate Mean Squared Error and $r^2$\n"}
{"snippet": "predictions = knn.predict(X_test)\npredictions\n", "intent": "Use your KNN model to predict and view these predictions\n"}
{"snippet": "newPatient = knn.predict([14, 14,88,566,1,0.08,0.06,0.04,0.18,0.05,0])\n", "intent": "Pass this new observation to the KNN prediction model.  The result will be either 0 = Benign and 1 = Malignant\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target,\n                              predicted,\n                              target_names=twenty_test.target_names))\n", "intent": "The model performed quite well given the high scores for both classifiers.\n"}
{"snippet": "def t_repredict_accuracy(clf,t, xtest, ytest):\n    probs=clf.predict_proba(xtest)\n    p0 = probs[:,0]\n    p1 = probs[:,1]\n    ypred = (p1 > t)*1\n    return metrics.accuracy_score(ytest, ypred)\n", "intent": "Next, we are repurposing a bit of code from Homework \n"}
{"snippet": "print (adjusted_mutual_info_score(iris['target'], clabels))\n", "intent": "We can compare the true labels with the ones obtained using this clustering algorithm using for example the mutual information score\n"}
{"snippet": "scores = cross_val_score(lr, numerical_features_all, target, cv = 5)\nprint scores.mean()\n", "intent": "Use cross validation to score your model\n"}
{"snippet": "for row in dataset:\n    prediction = predict(tree, row)\n    print('Predicted = %d,   Actual = %d' % (prediction,row[-1]))\n", "intent": "Now we can test out the tree and see how it does.\n"}
{"snippet": "n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "I choose 5 folds, since that seems to be standard and work well. I also define how our evaluation metric will be calculated.\n"}
{"snippet": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "I first define another evaluation function:\n"}
{"snippet": "probs_log = model_log_eval.predict_proba(X_test)\nprint(probs_log)\n", "intent": "To get more meaningful accuracy matrix, you should change the **cutoff**.\n"}
{"snippet": "cutoff_try=(probs_log > ___)[:,1]\nprint(metrics.classification_report(y_test, cutoff_try))\nmetrics.confusion_matrix(y_test, cutoff_try)\n", "intent": "Your turn: Try changing the cut-off and see how accuracy, sensitivity and specificity change!\n"}
{"snippet": "predicted = classifier.predict(X_test)\nplot_confusion_matrix(confusion_matrix(y_test,predicted), \n                      labels=labels, \n                      norm=True,\n                      title='Normalized confusion matrix',\n                      xlabel='Predicted Category',\n                      ylabel='True Category',\n                      precision=1\n                     )\n", "intent": "Predict categories on the test set and print confusion matrix.\n"}
{"snippet": "model.evaluate(x_test, y_test)\n", "intent": "Evaluating the trained model in the unseen test set shows a loss of 0.29 and an accuracy of 88%:\n"}
{"snippet": "output_loss = 0\nfor _y, _Y in zip(preds, y):\n    output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\nl_r = 1e-3\nupdate_step = tf.train.AdamOptimizer(l_r).minimize(output_loss)\n", "intent": "- Loss and optimizer\n"}
{"snippet": "x_test = np.array(['i hate you'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "Ytest_proba = classifier.predict_proba(Xtest)[:,1]\n", "intent": "* Using only the parameters learned on training and test set\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    grams = [gram_matrix(feats[i]) for i in style_layers]\n    diffs = np.array([((g - t) ** 2).sum() for (g, t) in zip(grams, style_targets)]).reshape(4) \n    return (diffs * style_weights).sum()\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "score = network.evaluate(test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Question 5: Evaluate your model** \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint ('Accuracy = {:0.2f}%'.format(100.0 * accuracy_score(train_result, model.predict(train_features))))\n", "intent": "So how good is this model? Well, compute how well our predictions matched with the known values. This is called accuracy (or conversely error).\n"}
{"snippet": "avgRates = playerLS[[\"1B\",\"2B\",\"3B\",\"HR\",\"BB\"]].values\nplayerLS[\"OPW\"] = clf.predict(avgRates)\nplayerLS.head()\n", "intent": "We compute the OPW for each player based on the average rates in the playerLS DataFrame. The OPW will represent a team of 9 such similar players.\n"}
{"snippet": "y_train_pred = clf.predict(xtrain)\ny_test_pred = clf.predict(xtest)\nscores_train = metrics.precision_recall_fscore_support(ytrain,y_train_pred,average='samples')\nscores_test = metrics.precision_recall_fscore_support(ytest,y_test_pred,average='samples')\n", "intent": "Get the evaluation metrics for the new classifier\n"}
{"snippet": "x_test = np.array(['give me present'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "sample = 0\nfor i in range(1, 11):\n    model = ho.plsr2.nipalsPLS2(X_train[:i*100], y_train[:i*100], numComp=50)\n    for c in range(1, 51):\n        y_pred = model.Y_predict(X_test, numComp=c)\n        y_pred = (y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n        acc_df.loc[sample] = [i*100, c, 1 - (y_pred != y_test).sum()/(2*len(y_test))]\n        sample += 1\n", "intent": "Train the models and collect the results.\n"}
{"snippet": "X_sub = test[['Age', 'SibSp', 'Parch', 'Sex', 'Priority']]\ny_sub = gs.predict(X_sub)\n", "intent": "Make predictions on selected features.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n", "intent": "http://scikit-learn.org/stable/modules/classes.html\n"}
{"snippet": "scoring = 'neg_mean_squared_error'\nresults=[]\nnames=[]\nfor modelname, model in regressors:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, x_train,y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(modelname)\n    print(modelname,\"\\n\\t CV-Mean:\", cv_results.mean(),\n                    \"\\n\\t CV-Std. Dev:\",  cv_results.std(),\"\\n\")\n", "intent": "___Validating our models using K-Fold Cross Validation for Robustness___\n"}
{"snippet": "from learners.svm_restrained import SVMRestrained\nlearner2= SVMRestrained(training_instances=training_data)\nlearner2.train()\nlearner2.predict(testing_data)\n", "intent": "The following is a demonstration of a robust learner of SVM_restrained:\n"}
{"snippet": "r2_scores = cross_val_score(rf, X, y, scoring='r2', cv=crossvalidation, n_jobs=-1)\nscores = cross_val_score(rf, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=-1)\nrmse_scores = [np.sqrt(abs(s)) for s in scores]\nprint('Cross-validation results:')\nprint('Folds: %i, mean R2: %.3f' % (len(scores), np.mean(np.abs(r2_scores))))\nprint('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))\n", "intent": "At least on the training data, we have very low RMSE and very high R2 - this is good! But let's see if these numbers hold up on cross-validation.\n"}
{"snippet": "from matminer.figrecipes.plot import PlotlyFig\npf_rf = PlotlyFig(x_title='DFT (MP) bulk modulus (GPa)',\n                  y_title='Random forest bulk modulus (GPa)',\n                  title='Random forest regression',\n                  mode='notebook',\n                  filename=\"rf_regression.html\")\npf_rf.xy([(y, cross_val_predict(rf, X, y, cv=crossvalidation)), ([0, 400], [0, 400])], \n      labels=df['formula'], modes=['markers', 'lines'],\n      lines=[{}, {'color': 'black', 'dash': 'dash'}], showlegends=False)\n", "intent": "Looks like upon cross-validation, we do slightly better than a linear regression but not *too* much better. Let's plot this one as well.\n"}
{"snippet": "cv_prediction = cross_val_predict(model, data[feature_labels], data['delta_e'], cv=KFold(10, shuffle=True))\n", "intent": "Quantify the performance of this model using 10-fold cross-validation\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n", "intent": "** Classification report **\n"}
{"snippet": "print classification_report(df_test['topic'], df_test['predict'])\n", "intent": "The precision, recall and f1-score can be outputted to see how well the model is performing on the test data.\n"}
{"snippet": "preds = np.array([.5]*1000)\ntarget = np.random.randint(0,2,1000)\nprint('acc:', (target == np.random.randint(0,2,1000)).mean()) \nprint('logloss:', log_loss(target, preds))\nprint('ed.logloss:', sess.run(binary_crossentropy(target, preds)))\nx = preds\nz = target\nprint('tf.doc.logloss', np.mean(x - x * z + np.log(1 + np.exp(-abs(x)))))\n", "intent": "The example below showcases this, under maximum uncertainty $logloss$ should be $ -ln(.5) \\approx 0.69314$\n"}
{"snippet": "model.predict([[2014]])\n", "intent": "Print the actual 2014 'WhiteMale' life expectancy from your loaded dataset\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nimport sys\nprint \"make a confusion matrix on the best initial model\"\ncm = confusion_matrix(dev_test_label, model_logistic.predict(dev_test_vec))\nfor row in cm:\n    for col in row: sys.stdout.write(\"%4d \"%col)\n    sys.stdout.write(\"\\n\")\nprint\nprint \"classes:\"\nprint str(model_logistic.best_estimator_.classes_)\n", "intent": "Now we take a look at the confusion matrix to see which cousines the basic model is having trouble to classify:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(svm, X_train_features, dev_train_label) \nprint \"Mean Score: \", np.mean(scores)\nprint \"All Scores for 3 folds: \",scores\n", "intent": "Step 3) Calculate Accuracy on random samples of the all the training data using Cross Validation (k_folds=3)\n"}
{"snippet": "pred_train = m.predict(x_train)\npred_val = m.predict(x_val)\n", "intent": "Examinemos los resultados\n"}
{"snippet": "X_test_reduced = pca.transform(X_test)\naccuracy_score(y_test, rnd_clf2.predict(X_test_reduced))\n", "intent": "*Exercise: Next evaluate the classifier on the test set: how does it compare to the previous classifier?*\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred = lin_svr.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\n", "intent": "Let's see how it performs on the training set:\n"}
{"snippet": "loss = tf.losses.log_loss(labels=y, predictions=y_proba) \n", "intent": "But we might as well use TensorFlow's `tf.losses.log_loss()` function:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nprint(scores)\n", "intent": "Finally get the cross-validation scores:\n"}
{"snippet": "result = model.evaluate(x=data.x_test,\n                        y=data.y_test)\n", "intent": "Now that the model has been trained we can test its performance on the test-set. This also uses numpy-arrays as input.\n"}
{"snippet": "y_pred = model.predict(x=data.x_test)\n", "intent": "We can plot some examples of mis-classified images from the test-set.\nFirst we get the predicted classes for all the images in the test-set:\n"}
{"snippet": "result = model2.evaluate(x=data.x_test,\n                         y=data.y_test)\n", "intent": "Once the model has been trained we can evaluate its performance on the test-set. This is the same syntax as for the Sequential Model.\n"}
{"snippet": "y_pred = model2.predict(x=data.x_test)\n", "intent": "We can plot some examples of mis-classified images from the test-set.\nFirst we get the predicted classes for all the images in the test-set:\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.predict([[2014]])[0][0]\n", "intent": "Print the actual 2014 'WhiteMale' life expectancy from your loaded dataset\n"}
{"snippet": "forest_scores = cross_val_score(forest_reg_make,train_X_make,train_Y.values.ravel(),scoring=\"neg_mean_squared_error\",cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"scores:\",forest_rmse_scores)\nprint(\"mean:\",forest_rmse_scores.mean())\nprint(\"std dev:\",forest_rmse_scores.std())\n", "intent": "This is clearly a better fit compared to DecisionTree, and we validate this with k-fold CV\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0)))\n                           for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "valAcc_svm1 = accuracy_score(y_val, pred_svm1) * 100\nvalAcc_svm2 = accuracy_score(y_val, pred_svm2) * 100\nvalAcc_knn1 = accuracy_score(y_val, pred_knn1) * 100\nvalAcc_knn2 = accuracy_score(y_val, pred_knn2) * 100\n", "intent": "Choose the Best Classifier (Continued)\n===\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        sl = style_layers[i]\n        feat = feats[sl]\n        mat = tf.square(gram_matrix(feat)-style_targets[i])\n        curr_loss = tf.multiply(tf.cast(style_weights[i], tf.float32), tf.reduce_sum(mat))\n        loss += curr_loss\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The classifier appears to be worse as it has a lower accuracy on the test data ~73% vs. the non-alpha classfier of ~77%\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "View the classification report and confusion matrix for this classifier.\n"}
{"snippet": "labels = model.predict(patches_hog)\nlabels.sum()\n", "intent": "Take the HOG-featured patches and use our model to evaluate whether each patch contains a face:\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "Generate some new data and predict the label:\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "We need to reshape these *x* values into a ``[n_samples, n_features]`` features matrix.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Use the ``accuracy_score`` utility to see how well we did. 97% isn't too bad.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \n", "intent": "Run the full_pipeline to transform the data (call transform(), :) not fit_transform()!), and evaluate the final model on the test set:\n"}
{"snippet": "L1_y_test_prob = best_logreg_model.predict_proba(X_test)\n", "intent": "Compare to feature list, the largest weight is `bool` and second is `session_overlaps`. Interestingly, `is_loc_diff`  contributes to zero importance.\n"}
{"snippet": "tree_y_test_prob = best_tree_model.predict_proba(X_test)\n", "intent": "The optimal number of features is 16, and optimal depth is 10. Decision tree doesn't have obviously outperform linear models.\n"}
{"snippet": "print (gnb.predict(X_test))\n", "intent": "and according to the model, the preidctions $\\hat{y}$ are\n"}
{"snippet": "gnb.predict_proba(X_test)\n", "intent": "We can examine the NB classifier gives the probability estimate for each observation as\n"}
{"snippet": "logreg.predict(X_test)\n", "intent": "we can still see the logistic regression model the given preidctions $\\hat{y}$\n"}
{"snippet": "train_pred = best_ridge_model.predict(X_train_scaled)\ntest_pred = best_ridge_model.predict(X_test_scaled)\nprint ('MSE for training set:', round(np.mean([(train_pred[i]-y_train[i])**2 for i in range(len(train_pred))]),2),\n      ', MSE for test set:', round(np.mean([(test_pred[i]-y_test[i])**2 for i in range(len(test_pred))]),2))\n", "intent": "Next we check the **MSE** on training/test datasets. We can see the **MSE** shows much lower than that from baseline:\n"}
{"snippet": "y_pred_logistic = logisticClassifier.predict(X_test[:,featureIndices])\n", "intent": "Get the predictions of this classifer on the test data:\n"}
{"snippet": "metrics.f1_score(y_test, y_pred_logistic)\n", "intent": "Evaluate using the F1-score:\n"}
{"snippet": "y_pred_linear_SVM = linearSVMPipeline.predict(X_test[:,featureIndices])\n", "intent": "Now get the model's predictions on the test data and calclate the F1-score:\n"}
{"snippet": "y_pred=(lr.predict_proba(X_test)[:,0]+dt.predict_proba(X_test)[:,0]+rf.predict_proba(X_test)[:,0]+xgb.predict_proba(X_test)[:,0]+knn.predict_proba(X_test)[:,0])/5\n", "intent": "I will take the majority votes of all the models.\n"}
{"snippet": "y_predict_dt=dt.predict(X_test)\nprint(y_predict_dt.shape)\n", "intent": "as decison tree accuracy is good we will use that.\n"}
{"snippet": "knn.predict([[3, 5, 4, 2]])\n", "intent": "<h3>Predict the response for a new observation</h3>\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "<h3> creating onther observation to predict with 2 arrays</h3>\n"}
{"snippet": "knn.predict(X_new)\n", "intent": "<h3>Here predict the response for a new observations</h3>\n"}
{"snippet": "score = model_simple2_ml.evaluate(x_test_feature_2, y_test_class, verbose=0)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_simple2_ml.predict_proba(x_test_feature_2)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "score = model_simple100_ml.evaluate(x_test_feature_100, y_test_class, verbose=0)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_simple100_ml.predict_proba(x_test_feature_100)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "score = model_single_dl.evaluate(x_test_flatten, y_test_class, verbose=1)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_single_dl.predict_proba(x_test_flatten)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "score = model_multi_dl.evaluate(x_test_flatten, y_test_class, verbose=1)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_multi_dl.predict_proba(x_test_flatten)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "score = model_simple_conv.evaluate(x_test_conv, y_test_class, verbose=1)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_simple_conv.predict_proba(x_test_conv)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "score = model_pooling_conv.evaluate(x_test_conv, y_test_class, verbose=1)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_pooling_conv.predict_proba(x_test_conv)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "score = model_dropout_conv.evaluate(x_test_conv, y_test_class, verbose=1)\n", "intent": "**Step 5: Check the performance of the model**\n"}
{"snippet": "model_dropout_conv.predict_proba(x_test_conv)[0]\n", "intent": "**Step 6: Make & Visualise the Prediction**\n"}
{"snippet": "user_input = \"hey can I get more examples\"\ngrams = vec2.transform(extract_grams(user_input, [2]))\nprint(grams, \"\\n\")\nprint(svm2.predict(grams))\n", "intent": "And finally we accept user input, and give back a classification label:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss+=style_weights[i]*torch.sum(torch.pow(gram_matrix(feats[style_layers[i]])-style_targets[i],2))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        layer = style_layers[i]\n        current_feat = feats[layer]\n        diff = tf.reduce_sum((style_targets[i] - gram_matrix(current_feat)) ** 2)\n        style_loss += style_weights[i] * diff\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint accuracy_score(y_test, prediction)\n", "intent": "* Above cell is for classifier\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out my CNN model on the test dataset of dog images.  Ensure that my test accuracy is greater than 1%.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out my model on the test dataset of dog images and the test accuracy is 51%.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions: \", lin_reg.predict(some_data_prepared))\nprint(\"Labels: \", list(some_labels))\n", "intent": "Compare some predicted values with original ones\n"}
{"snippet": "housing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Check Root Mean Squared Error Linear Regression Model\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Cross Validation Score - check rmse scores a few times\n"}
{"snippet": "class Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1), dtype=bool)\n", "intent": "Lets create Never5Classifier, its method \"predict\" returns only 0s.\n"}
{"snippet": "precision_score(y_train_5, y_train_pred)\n", "intent": "precision = TP/(TP+FP)\n"}
{"snippet": "recall_score(y_train_5, y_train_pred)\n", "intent": "recall = TP/(TP+FN)\n"}
{"snippet": "f1_score(y_train_5, y_train_pred)\n", "intent": "Combine precision and recall to one metric called F1\n"}
{"snippet": "precision_score(y_train_5, y_train_forest_pred_90)\n", "intent": "Scores are better for Random Forest Classifier\n"}
{"snippet": "y_pred = model.predict(x_np) \nsns.regplot(x_np, y_np, fit_reg=False, color='red')\nsns.regplot(x_np, y_pred, fit_reg=False, color='blue')\n", "intent": "Plot the predicted values (red) and actual values (blue) on the same graph\n"}
{"snippet": "model_dir = './models'\nwith open(path.join(model_dir, 'simple_linear.pkl'), 'rb') as f:\n    simple_linear = pickle.load(f)\nsqft_input = np.array([sqft]).reshape(1,1)\npredicted_price_sl = simple_linear.predict(sqft_input).squeeze()\nprint(\"simple linear predicted price: ${}M\".format(predicted_price_sl/1e6))\n", "intent": "Compare with our simple linear model from before\n"}
{"snippet": "y_pred = acc_linreg.predict(X)\nmetrics.r2_score(y, y_pred)\n", "intent": "Let's confirm the R-squared value for our simple linear model using `scikit-learn's` prebuilt R-squared scorer:\n"}
{"snippet": "mean_squared_errors = np.abs(\n    cross_val_score(lr,transformed_pca_x.ix[:,:11],y,\n                    cv=50,scoring='mean_squared_error'))\nroot_mean_squared_errors = map(np.sqrt,mean_squared_errors)\nprint \"50-fold mean RMSE: \", np.mean(root_mean_squared_errors)\nprint \"50-fold std RMSE: \", np.std(root_mean_squared_errors)\n", "intent": "So it looks like this gives us pretty terrible performance.\n* How many components do we need to keep until we achieve very similar performance?\n"}
{"snippet": "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"mean_squared_error\"))\nrmse_cross_val_scores = map(np.sqrt, cross_val_scores)\nprint \"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores)\nprint \"Std 10-fold rmse: \", np.std(rmse_cross_val_scores)\n", "intent": "And now let's run the whole pipe through the `cross_val_score` object:\n"}
{"snippet": "print(metrics.classification_report(y_test, rf_predict_test, labels = [1,0]))\n", "intent": "Classification Report\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Generate our predictions for the withheld test data\n"}
{"snippet": "print(classification_report(y_test, predictions))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predictions)))\n", "intent": "Now that we have the predicted outcome (survived / died) for each passenger in the test dataset, let's evaluate the performance of our model.\n"}
{"snippet": "X_test_scaled = scaler.transform(X_test)\npredictions = svmmodel.predict(X_test_scaled)\nprint(classification_report(y_test, predictions))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predictions)))\n", "intent": "Let's evaluate our model, does it out-perform Logistic Regression?\n"}
{"snippet": "x = reg.predict(X_train)\ny = reg.predict(X_train)-y_train\nx1 = reg.predict(X_test)\ny1 = reg.predict(X_test)-y_test\nsns.regplot(x, y)\nsns.regplot(x1,y1);\n", "intent": "But it is easier with Seaborn:\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "Now let's generate some new data and predict the label\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit,\n                            target_names=faces.target_names))\n", "intent": "Looks like out of this sample, our sample did not get any wrong. We can get a better understanding the performance using a classification report.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)\n", "intent": "To learn about the accuracy of our model , lets generate the accuracy score\n"}
{"snippet": "new_ages = np.linspace(Wages.age.min(), Wages.age.max(), num=1000)\npredictions = estimate.predict(exog=dict(age = new_ages))\nsimpleTable, data, column_names = summary_table(estimate, alpha=0.05)\nprint('column_names: ', column_names)\npredicted_mean_ci_low, predicted_mean_ci_high = data[:,4:6].T\n", "intent": "We now make out-of-sample predictions and get the standard errors of the predictions (i.e. Confidence Interval for the mean).\n"}
{"snippet": "Y_pred = model.predict(X_test)\nprint(\"training accuracy :\",accuracy_score(Y_test,Y_pred))\nY_pred_ = model.predict(X_train)\nprint(\"test accuracy :\",accuracy_score(Y_train,Y_pred_))\n", "intent": "Now we will print the accuracy of the classifer in predicting the class of training examples and test examples.\n"}
{"snippet": "homogeneity_score(labels_pred = labels, labels_true = y)\n", "intent": "Homogeneity score is a measure of how homogenous the clusters are.\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n", "intent": "Performance metrics\n"}
{"snippet": "blend1 = (xgbpred+rfpred+etpred)/3\nmulticlass_log_loss(yte.values, blend1.values)\n", "intent": "Let's try giving each model equal weights:\n"}
{"snippet": "scores = cross_val_score(lr, X, y['registered[True]'], cv=20, scoring='accuracy')\n", "intent": "`cross_val_score` will return an array of scores. One for each cross valication (set the number with `cv`).\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy) \n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy) \n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions=lm.predict(X_test)\n", "intent": "** predict off the test values **\n"}
{"snippet": "from sklearn import metrics\nprint('MAE: ',metrics.mean_absolute_error(y_test, predictions))\nprint('MSE: ',metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ',np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "** evaluate model **\n"}
{"snippet": "def cv_res(clf, X, y):\n    return np.mean(cross_val_score(clf, X, y, cv=10, scoring='roc_auc', n_jobs=-1))\n", "intent": "This is the function to test our models:\n"}
{"snippet": "prediction=clf_rf.predict(X_test)\n", "intent": "Random forest is our best option.Let's get the results for the test data.\n"}
{"snippet": "prediction=clf_rf.predict(X_test)\nprediction\n", "intent": "Random forest is our best option.Let's get the results for the test data.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = list()\nfor x in test_X:\n    yhat = model_persistence(x)\n    predictions.append(yhat)\ntest_score = mean_squared_error(test_y, predictions)\nprint('Test MSE: %.3f' % test_score)\n", "intent": "<h2>Step 4: Make and Evaluate Forecast</h2>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy1 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy1)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "excludes = range(32)\nget_train_loss(M, criterion, train_dataset, excludes)\n", "intent": "- Test get_train_loss\n"}
{"snippet": "excludes = range(32)\nget_train_loss(model, criterion, train_dataset, excludes)\n", "intent": "- Test get_train_loss\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"PattayaStreet.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "sensitivity = TP / float(FN + TP)\nprint(sensitivity)\nprint(metrics.recall_score(y_test, y_pred_class))\n", "intent": "**Sensitivity**: When the actual value is positive, how often is the prediction correct?\n"}
{"snippet": "precision = TP / float(TP + FP)\nprint(metrics.precision_score(y_test, y_pred_class))\n", "intent": "**Precision**: When a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(dectree, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\n", "intent": "AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(ranfor, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(svc, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(nb, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_scores_roc = cross_val_score(logreg, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\ncv_scores_roc.mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_scores_roc = cross_val_score(lda, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\ncv_scores_roc.mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_scores_roc = cross_val_score(qda, features_final, asd_classes, cv=10, scoring='roc_auc').mean()\ncv_scores_roc.mean()\n", "intent": "AUC Score: AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "score = model.evaluate(X_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"\\n Testing accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. \n"}
{"snippet": "eval_results = model.evaluate(\n    input_fn=dataset_input_fn(\"test\"))\nprint(\"Accuracy: %.2f\" % eval_results['accuracy'])\n", "intent": "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:\n"}
{"snippet": "test_predictions =\\\n    list(model.predict(input_fn=dataset_input_fn(\"test\")))\ntest_predictions_classes = np.array(\n    [p['class_ids'][0] for p in test_predictions])\nprint(\"Classification Report\\n=====================\")\nprint(classification_report(\n    newsgroups['test_target'], test_predictions_classes))\n", "intent": "We can even use the same tools from scikit-learn that we use for any other model, once we have the array with predictions\n"}
{"snippet": "x = data[[\"TV\", \"Radio\"]]\nprint np.sqrt(-cross_val_score(lm, x, y, cv=10, scoring=\"mean_squared_error\")).mean()\n", "intent": "This is the model with newspaper, next is model without newspaper\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(bnb,data,target,cv=10)\n", "intent": "Single holdout grouping shows little indication that there's overfitting present in the model. \n"}
{"snippet": "scores = cross_val_score(lm, X_train, y_train, cv=10)\nprint(\"Accuracy scores (cross-validation, k=10):\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard deviation:\", scores.std())\n", "intent": "Using 10-fold cross-validation, we find that the mean of the model accuracy is approx. 0.65 (with standard deviation of 0.065).\n"}
{"snippet": "trainPredict = model.predict(trainX)\n", "intent": "Gives poor performance, model has an average error of 23 passengers (000's') on the training and 48(000's) on test\n"}
{"snippet": "model.load_weights(\"weights.best.hdf5\")\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"Created model and loaded weights from file\")\ndataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\nX = dataset[:,0:8]\nY = dataset[:,8]\nscores = model.evaluate(X, Y, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Use the same model as above and load in the weights from the file stored above, use this model to make predictions on the dataset \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncvs_lr_sm = cross_val_score(model_lr_sm, X_sm, y_sm, cv=5, n_jobs=3).mean()\n", "intent": "Perform a kFold Cross validation on the model to see if the model is overfitting the data. Applying SMOTE can sometimes overfit the model.\n"}
{"snippet": "pred_lr_sm = model_lr_sm.predict(X_test)\n", "intent": "> Validation accuracy is 86.37%\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = Rnd_forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = knn_model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = logistic.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(y_test,y_pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = svm_clf.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score( truth , preds, average = 'micro' )\n", "intent": "* Uncalibrated roc auc score:\n"}
{"snippet": "mean_score = 0\nfor subject in xrange(1,13):\n    mask = (subjects == subject)\n    truth_subject = truth[mask]\n    preds_subject = preds[mask]\n    score = roc_auc_score( truth_subject , preds_subject, average = 'micro' )\n    print 'subject', subject, 'score:', score\n    mean_score += score\nprint '\\n Mean score:', mean_score/12.0\n", "intent": "* roc auc score averaging over subjects: \n"}
{"snippet": "vgg_predictions = [np.argmax(dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(vgg_predictions)==np.argmax(test_targets, axis=1))/len(vgg_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import seaborn as sns\ndef get_batch(array_size, n_samples=100):\n    x = np.random.rand(n_samples, array_size)\n    y = x.argsort()\n    return x, y\ndef evaluate(y_pred, y_actual):\n    score = np.mean(y_pred.round() == y_actual)*100\n    return score\n", "intent": "See the accompanying blog post on my [home page](http://georgep.xyz).\n"}
{"snippet": "print(np.mean(((bos.PRICE - lm.predict(X)) ** 2)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,pred))\nprint(confusion_matrix(y_test,pred))\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "def get_reward_min(smiles, predictor, invalid_reward=0.0, get_features=get_fp):\n    mol, prop, nan_smiles = predictor.predict([smiles], get_features=get_features)\n    if len(nan_smiles) == 1:\n        return invalid_reward\n    return np.exp(-prop[0]/3 + 3)\n", "intent": "Next we will minimize the pIC50 fpr JAK2.\nThe reward function we will use here is \n$$\nR(s) = \\exp(\\dfrac{-predictor(s)}{3} + 3) \n$$\n"}
{"snippet": "residuals = regr.predict(regression_x_train) - regression_y_train\n", "intent": "We might want to set the intercept to zero because there will be no synthesis if there is no $p_I$.\n"}
{"snippet": "residuals = regr.predict(regression_x_train) - regression_y_train\n", "intent": "Plot the residuals to discover any remaining structure:\n"}
{"snippet": "residuals = (regr.predict(regression_x_train) - regression_y_train) / regr.predict(regression_x_train)\n", "intent": "$$T = (0.0453 + \\sigma \\cdot \\varepsilon) \\cdot L $$\n"}
{"snippet": "predictions = result.predict(regression_data).to_dict()\n", "intent": "Predicted ```ribo_speeds```:\n"}
{"snippet": "eval_results = dnnc.evaluate(input_fn=test_input_fn)\n", "intent": "evaluating results on the test set:\n"}
{"snippet": "def wasserstein_real_loss(y_true, y_pred):\n    return K.mean(y_true*y_pred)\n", "intent": "$$\\mathbb{E}_{x\\sim\\mathbb{P}_r}[D(x)]$$\n"}
{"snippet": "def wasserstein_fake_loss(y_true, y_pred):\n    return K.mean(y_true*y_pred)\n", "intent": "$$\\mathbb{E}_{x\\sim\\mathbb{P}_g}[D(\\tilde{x})]$$\n"}
{"snippet": "print 'Random Forest:\\n', metrics.classification_report(y, est.predict(X))\n", "intent": "Below we can see that the model appears to only make a correct prediction with its first guess for the two most frequent outcomes, 'NDF' and 'US'.\n"}
{"snippet": "ndcg_score(y_valid, est.predict_proba(X_valid))\n", "intent": "Below we can see that the xgboost model predicts more classes than the random forest model.\n"}
{"snippet": "train_yhat = regr.predict(train_x)\naccuracy = train_yhat==train_y\nprint(\"Accuracy: %0.1f%%\" % (sum(accuracy)/len(train_x)*100))\n", "intent": "Now that we have our model trained, how accurate is the model?\n"}
{"snippet": "y_pred = lr.predict(X_test)\nct = pd.crosstab(y_pred, y_test)\nprint('Confusion Matrix | Logistic Regression Predictions')\nct\n", "intent": "Since we have five classes, we'll generate a confusion matrix to visualize whether hashtags have been consistently predicted:\n"}
{"snippet": "print('Number of component classes:', len(tweets.comp_class.unique()))\nprint('Average silhouette score of component classes:',\n      silhouette_score(lsa_matrix, tweets.comp_class))\n", "intent": "Let's start by looking at the average silhouette score of all component classes.\n"}
{"snippet": "clf.predict(['You are a f** cu*t!'])\n", "intent": "Good, so it's not saying this is rude.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ntr_pred=clf.predict(X=X)\ncm = confusion_matrix(Y, tr_pred)\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "a=auto_numeric['engine-power']\nb = dt.predict(X)\ncoefficients = np.polyfit(a, b, 1)\nprint ('Price='+str(coefficients[0])+'*Engine Power + '+str(coefficients[1]))\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "print('Coefficient of Determination:',r2_score(y, y_pred))\nrms = np.sqrt(mean_squared_error(y, y_pred))\nprint('Root mean squared error:', rms)\nprint('Mean Absolute Error:', mean_absolute_error(y, y_pred))\nprint('Correlation Coefficient:',np.corrcoef(y, y_pred)[1][0])\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "print('Coefficient of Determination:',r2_score(y, newy_pred))\nmultirms = np.sqrt(mean_squared_error(y, newy_pred))\nprint('Root mean squared error:', multirms)\nprint('Mean Absolute Error:', mean_absolute_error(y, newy_pred))\nprint('Correlation Coefficient:',np.corrcoef(y, newy_pred)[1][0])\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\ny_pred = [0, 1, 0, 0]\ny_true = [0, 1, 0, 1]\nprint('[[tn fn]\\n [fp, tp]]')\nprint(confusion_matrix(y_true, y_pred))\nprint()\nprint('Recall =', recall_score(y_true, y_pred))\nprint('Precision =', precision_score(y_true, y_pred))\nprint('F1 =', f1_score(y_true, y_pred))\n", "intent": "$$ F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} $$\n"}
{"snippet": "from sklearn.metrics import log_loss\ny_true = [0, 0, 1, 1]\ny_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\nlog_loss(y_true, y_pred)\n", "intent": "$$ l(y, \\hat{y}) = - \\frac{1}{m} \\sum_{i=1}^m y^{(i)} log(\\hat{y}^{(i)}) + (1 - y^{(i)})log(1 - \\hat{y}^{(i)}) $$\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_absolute_error(y_true, y_pred)\n", "intent": "$$ MAE(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^m \\mid y^{(i)} - \\hat{y}^{(i)} \\mid $$\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_squared_error(y_true, y_pred)\n", "intent": "$$ MSE(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - \\hat{y}^{(i)})^2 $$\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_valid) \nr2_score(y_valid,tuned_forest_predictions)\n", "intent": "Make a prediction for test data.\n"}
{"snippet": "Y_pred_prob = clf.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(Y_test, Y_pred_prob[:, 1])\nunder_roc = roc_auc_score(Y_test, Y_pred_prob[:, 1])\nunder_roc\n", "intent": "Calculate ROC and Area under ROC\n"}
{"snippet": "def translate(sentence):\n    sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n    sentence = pad([sentence], length=21)\n    prediction = logits_to_text(model.predict(sentence)[0], french_tokenizer)\n    return prediction\nprint('he saw a old yellow truck')\nprint('Translation:')\nprint(translate('he saw a old yellow truck'))\nprint('Actual:')\nprint('il a vu un vieux camion jaune')\n", "intent": "Lastly, let's create a function that can take in an unprocessed English sentence and return the French translation.\n"}
{"snippet": "predictions = [np.argmax(model_Xception.predict(np.expand_dims(feature, axis = 0))) \n                                  for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import fbeta_score\nimport warnings\narch = resnet34\ndef f2(preds, targs, start = 0.17, end = 0.24, step = 0.01):\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        return max([fbeta_score(targs, (preds > th), 2, average = 'samples')\n                   for th in np.arange(start, end, step)])\nmetrics = [f2]\n", "intent": "Metrics on which competition are based on. We'll also be using a resnet 34 model.\n"}
{"snippet": "RN50_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(RN50_predictions)==np.argmax(test_targets, axis=1))/len(RN50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "features = cephs_and_dist.loc[:, ['Period','VminusI', 'Z']]\ntarget = cephs_and_dist.loc[:, 'm_H'] - cephs_and_dist.loc[:, \n                                                           'mu']\nerror = cephs_and_dist.loc[:, 'TotalError']\npredictions = linear_regression.predict(features);\n", "intent": "Now we can make predictions for the records where $m_{\\alpha, i} - \\mu_i$ is unknown:\n"}
{"snippet": "nn.predict(X_train.iloc[[0]])\n", "intent": "Now let's test our neural network by checking to see how it performs on predicting the first row of data.\n"}
{"snippet": "scores = cross_val_score(nn_iris, features, labels, cv = 30)\n", "intent": "Now, we will use  the `cross_val_score` function to measure the accuracy of the function across different iterations.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions) == np.argmax(test_targets, axis=1))/ len(InceptionV3_predictions)\nprint('Test Accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred = grid.predict(X_test.ravel())  \np_prob = grid.predict_proba(X_test.ravel()) \n", "intent": "Top 3 categories are the top 3 categories with the highest probability.\n"}
{"snippet": "def topk_scored(model,step_name X_test, k):\n    results = []\n    p_prob = model.predict_proba(X_test)\n    for pred in p_prob:\n        idxs = np.argsort(pred)[::-1]\n        results.append(model.best_estimator_.named_steps[step_name].classes_[idxs][:k])\n    return results\n", "intent": "The function below brings all the steps above so it can be applied to the X_test set.\n"}
{"snippet": "def eval_keras(input):\n    preds = model.predict(input, batch_size=128)\n    predict = np.argmax(preds, axis=2)\n    return (np.mean([all(real == p) for real, p in zip(labels_test, predict)]), predict)\n", "intent": "To evaluate, we don't want to know what percentage of letters are correct but what percentage of words are.\n"}
{"snippet": "preds = model.predict_proba(X_test_scaled)\n", "intent": "Finally, here we predict labels of the test data\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print msg\n", "intent": "- Based on the model we created from above, I will evalute them using K-fold cross validation.\n"}
{"snippet": "from sklearn import metrics\nprint(\"Neigh's Accuracy: \"), metrics.accuracy_score(y_testset, pred)\n", "intent": "Now let's compute neigh's prediction accuracy. We can do this by using the metrics.accuracy_score function\n"}
{"snippet": "print(\"Neigh23's Accuracy:\"), metrics.accuracy_score(y_testset,pred23)\nprint(\"Neigh90's Accuracy:\"), metrics.accuracy_score(y_testset,pred90)\n", "intent": "Let's do the same for the other instances of KNeighborsClassifier.\n"}
{"snippet": "_, acc = learner.evaluate(10, test=True)\nprint(f'Accuracy on test set: {acc:.6f}')\n", "intent": "Compute average accuracy for 10 epochs from the test set.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, l in enumerate(style_layers):\n        gram_feats = gram_matrix(feats[l])\n        style_loss += style_weights[i] * tf.reduce_sum((gram_feats - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "target_names = ['healthy trees', 'unhealthy trees']\nprint (metrics.confusion_matrix(y_test, logit_test_pred))\nprint (metrics.classification_report(y_test, logit_test_pred, target_names=target_names))\nmlhelp.classification_accuracy(logit_test_pred, y_test);\n", "intent": "Not so great anymore, right? When we look at the confusion matrix what's the likely culprit:\n"}
{"snippet": "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\nprint(\"Test loss = \", test_loss)\nprint(\"Test accuracy = \", test_acc)\n", "intent": "Evaluate the model on the test data set.\n"}
{"snippet": "predicted_simple_model_train = simple_model.predict(train_matrix_word_subset)\ntrain_data['predicted_simple_model_train'] = predicted_simple_model_train\naccuracy_train_simple = round(float(sum(train_data['predicted_simple_model_train'] == train_data['sentiment']))/len(train_data.index),2)\n", "intent": "Now, compute the classification accuracy of the **simple_model** on the **train_data**:\n"}
{"snippet": "num_false_positives = sum(model_5.predict(validation_data.drop('safe_loans', 1)) > validation_data['safe_loans'])\nnum_false_positives\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "num_false_negatives = sum(model_5.predict(validation_data.drop('safe_loans', 1)) < validation_data['safe_loans'])\nnum_false_negatives\n", "intent": "Calculate the number of **false negatives** made by the model.\n"}
{"snippet": "y_pred_class = knn.predict(X_test)\nprint((metrics.accuracy_score(y_test, y_pred_class)))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "y_pred_train = model.predict(features_train)\npred_index = np.argmax(y_pred_train, axis=1)\nreal_index = np.argmax(y, axis=1)\nwrong = []\nfor ix, pred in enumerate(pred_index):\n    if pred != real_index[ix]:\n        wrong.append(ix)\n", "intent": "Training Result Analysis\n"}
{"snippet": "y_pred = model.predict(features_test, batch_size=64, verbose=1)\n", "intent": "Use the trained model to predict the test images\n"}
{"snippet": "xgb_train = xgb.DMatrix(x_train, label=y_train)\nxgb_test = xgb.DMatrix(x_test, label=y_test)\nxgb_pars = {'min_child_weight': 80, 'eta': 0.5, 'colsample_bytree': 0.9, \n            'max_depth': 20,'subsample': 0.9, 'lambda': 1., 'nthread': -1, \n            'booster' : 'gbtree', 'silent': 1,'eval_metric': 'rmse', 'objective': 'reg:linear'}\nmodel = xgb.train(xgb_pars, xgb_train, maximize=False, verbose_eval=10)\npredict = model.predict(xgb_test)\n", "intent": "Try with more complicated model\n"}
{"snippet": "logisticRegr.predict(test_img[0].reshape(1,-1))\n", "intent": "Uses the information the model learned during the model training process\nThe code below predicts for one observation\n"}
{"snippet": "logisticRegr.predict(test_img[0:10])\n", "intent": "The code below predicts for multiple observations at once\n"}
{"snippet": "print \"Accuracy: \", spam_detector.score(messages_test, label_test)\npredictions = spam_detector.predict(messages_test)\nprint confusion_matrix(label_test, predictions)\nprint '(row=expected, col=predicted)'\nprint classification_report(label_test, predictions)\n", "intent": "Evaluation on Test Set\n"}
{"snippet": "examples = ['Ban chung cu cao cap gia re', 'toi nay di choi de']\nexample_counts = transformer.transform(examples)\npredictions = spam_detector.predict(example_counts)\npredictions\n", "intent": "Predict a new message\n"}
{"snippet": "y_hat = model.predict(x_test) \nr2_vanilla = r2_score(y_test,y_hat) \ny_test = y_test.reshape((y_test.shape[0],1)) \nresidual = y_test - y_hat \nmae_vanilla      = np.mean((residual**2)**0.5) \nprint(\"R-Squared = \", r2_vanilla)\nprint(\"MAE       = \", mae_vanilla)\n", "intent": "Evaluating the model according to the established metrics.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def evaluate(model, y):\n    acc = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(acc, tf.float32))\n    return accuracy\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "print(clf.predict(test_person))\n", "intent": "Predicting the outcome of Ms. Test Person\n"}
{"snippet": "clf.predict([[160,0]])\n", "intent": "*** Notice that we want to predict something that is not in the training data ***\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n    return f1_score(target.values, y_pred,  average='weighted')\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Run the code cell below to initialize three helper functions for training and testing the four supervised learning models. \n"}
{"snippet": "pred_biomass = regr_rf.predict(all_training_data)\n", "intent": "We know apply the Random Forest model to the predictor variables to retreive biomass\n"}
{"snippet": "y_pred_class = clf.predict(x_test);\nclf.predict_proba(x_test);\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake)\n    g_loss = ls_generator_loss(score_fake)\n    print(g_loss,g_loss_true)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    j = 0\n    for i in style_layers:\n        gram = gram_matrix(feats[i])\n        style_loss += style_weights[j] * torch.sum((gram-style_targets[j])**2)\n        j+=1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def regularization_L2_softmax_loss(reg_lambda, weight1, weight2):\n    weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)\n    weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)\n    return weight1_loss + weight2_loss\n", "intent": "While not necessary, this step also helps to prevent overfitting. We'll also implement some additional overfit steps in the later steps.\n"}
{"snippet": "def checkAccuracy(predy, actualy):\n    con_mat = confusion_matrix(trainy_pred, trainy)\n    print(con_mat)\n    precision = precision_score(trainy_pred, trainy, average=None)\n    recall = recall_score(trainy_pred, trainy, average=None)\n    accuracy = accuracy_score(trainy_pred, trainy)\n    print('Precision is {} Recall is {} Accuracy is {}'.format(precision, recall, accuracy))\n", "intent": "In this step we are printing confusion metrix, precision, recall and accuracy of model.\n"}
{"snippet": "test_input_fn = create_test_input_fn()\npredictions = estimator.predict(test_input_fn)\ni = 0\nfor prediction in predictions:\n    true_label = census_test_label[i]\n    predicted_label = prediction['class_ids'][0]\n    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == 50: break\n", "intent": "The Estimator returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples.\n"}
{"snippet": "kmeans.predict([[-100000, -1000, -100, -10, -1000, -10, -100, -1000]])\n", "intent": "If we have people with *all* different choices and there are only two clusters possible, they should be put in different clusters. Let's verify. \n"}
{"snippet": "scores = cross_val_score(clf,X,y, cv=10)\nprint(\"Cross-Validation Score: {}\\u00B1{:.0%}\\n\".format(int(100*scores.mean()), scores.std()))\n", "intent": "The accuracy is similar to the previous classifiers.\n"}
{"snippet": "print('\\nConfusion Matrix')\ny_pred = pd.Series(clf.predict(X_test), name='Predicted').replace({0: 'Cancelled', 1: 'Kept'})\ny_act = pd.Series(y_test.values, name='Actual').replace({0: 'Cancelled', 1: 'Kept'})\ndisplay(pd.crosstab(y_act, y_pred, margins=True))\nprint('\\nClassification Report')\nprint(classification_report(y_act, y_pred)) \n", "intent": "It also has trouble correctly labelling the \"cancelled\" titles compared to the \"kept\" titles.\n"}
{"snippet": "print('\\nConfusion Matrix')\ny_pred = pd.Series(clf.predict(X_test), name='Predicted').replace({0: 'Cancelled', 1: 'Kept'})\ny_act = pd.Series(y_test.values, name='Actual').replace({0: 'Cancelled', 1: 'Kept'})\ndisplay(pd.crosstab(y_act, y_pred, margins=True))\nprint('\\nClassification Report')\nprint(classification_report(y_act, y_pred)) \n", "intent": "The confusion matrix though shows that the classifier barely manages to classify any of the \"Cancelled\" data sets. \n"}
{"snippet": "scores = cross_val_score(knn,X_train_scaled,y_train, cv=5)\nprint(\"Cross validation scores: {}\".format(scores))\nprint(\"Average cross-validation scroe: {:.2f}\".format(scores.mean()))\n", "intent": "Cross validation scores\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, KNN_predicted))\n", "intent": "Classification accuracies report for KNN regression\n"}
{"snippet": "scores = cross_val_score(logit,X_train,y_train, cv=5)\nprint(\"Cross validation scores: {}\".format(scores))\nprint(\"Average cross-validation scroe: {:.2f}\".format(scores.mean()))\n", "intent": "Cross Validation scores\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, logit_predicted))\n", "intent": "Classification Accuracies Report for Logit model\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, LinSVC_predicted))\n", "intent": "Classification Accuracies Report for Linear SVM\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, KernelSVC_predicted))\n", "intent": "Classification Accuracies report for Kernelized SVM\n"}
{"snippet": "    preds = model.evaluate(X_test, Y_test)\n    print (\"Loss = \" + str(preds[0]))\n    print (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "print_score('NMF (original)', X, df3['|sim_img0-2-3|'] )\nprint_score('NMF Transformed', X_nmf, df3['|sim_img0-2-3|'] )\n", "intent": "Ok. Let's also try without NMF decomposition (just similarity via dot product of original feature vectors). But first assess scores.\n"}
{"snippet": "prediction=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(Y_test,prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "import seaborn \nfrom talk_utils import plotter\nt = np.linspace(x.min(), x.max())\nplotter(t, reg.predict(t.reshape(-1, 1)), x, y)\n", "intent": "We use the model to make a prediction and then plot that prediction:\n"}
{"snippet": "scores = model.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "<h1>Evaluating the model</h1>\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "With our model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "auroc_score = roc_auc_score(y_test, y_pred)\nprint ('Mean AUROC Score:', auroc_score)\nfor i in range(0, 6):\n    print (dependent_vars[i], ':', roc_auc_score(y_test[:, i], y_pred[:, i]))\n", "intent": "Analyze AUROC scores and confusion matrix to get a better idea of the results, since there is class imbalance\n"}
{"snippet": "test_loss, test_acc = model.evaluate(x_test, y_test)\nprint('Test accuracy:', test_acc)\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "predictions = model.predict(x_test)\npredictions[0]\n", "intent": "With the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "img = x_test[0]\nprint(img.shape)\nimg = (np.expand_dims(img,0))\nprint(img.shape)\npredictions_single = model.predict(img)\nprint(predictions_single)\nnp.argmax(predictions_single[0])\n", "intent": "Finally, use the trained model to make a prediction about a single image. \n"}
{"snippet": "score = model.evaluate(test_Resnet50,test_targets, verbose=0)\nprint(\"Test accuracy %.4f%s\"%(score[1]*100,\"%\"))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "class regression_model:\n    def __init__(self):\n        self.b = 0\n        self.i = 0\n    def train(self, x, y):\n        self.b = 0 \n        self.i = 0 \n    def predict(self, x):\n        return self.b * x + self.i\n", "intent": "Now that we can calculate the slope and intercept formulas, let's create our linear regression model from scratch and make a new prediction.\n"}
{"snippet": "My_Resnet50_predictions = [np.argmax(My_Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(My_Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(My_Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.load_weights('Model_2.weights.best.hdf5')\n(loss, accuracy) = model.evaluate(X_test, y_test, batch_size=16, verbose=0)\nprint('Accuracy on test data: {:.2f}%'.format(accuracy * 100))\n", "intent": "Evaluating `Model-2` on the test data.\n"}
{"snippet": "model.load_weights('Model_3.weights.best.hdf5')\n(loss, accuracy) = model.evaluate(X_test, y_test, batch_size=16, verbose=0)\nprint('Accuracy on test data: {:.2f}%'.format(accuracy * 100))\n", "intent": "Evaluating `Model-3` on the test data.\n"}
{"snippet": "model.load_weights('Model_3.weights.best.hdf5')\n(loss, accuracy) = model.evaluate(bench_video, bench_target, batch_size=16, verbose=0)\nprint('Accuracy on test data: {:.2f}%'.format(accuracy * 100))\n", "intent": "So, now we have 216 videos (24 videos for each of the 9 person). We will load our model and evaluate the performance of the model on this data.\n"}
{"snippet": "y_hat = best_rfc.predict(X_test)\n", "intent": "Predictions of X_test\n"}
{"snippet": "roc = roc_auc_score(y_test, best_rfc.predict_proba(X_test)[:,1])\nprint \"AUC Score: \", roc\n", "intent": "AUC =/= accuracy. AUC is under the blue curve. AUC is 1 if the model is perfect. All true positives and no false positives.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions) == np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\ny_pred = svc_1.predict(x_train)\nprint(metrics.classification_report(y_train, y_pred))\nprint (metrics.confusion_matrix(y_train, y_pred))\n", "intent": "Let's measure precision and recall on the evaluation set, for _each class_. \n"}
{"snippet": "cv = KFold(len(y_train), 5, shuffle=True, random_state=0)\nscores = cross_val_score(svc_1, x_train, y_train, cv=cv)\nprint (scores)\nprint (\"Mean score: {0:.3f} (+/-{1:.3f})\".format(\n        np.mean(scores), sem(scores)))\n", "intent": "Try again with a linear SVM kernel and show a classification report as above.\n"}
{"snippet": "trainer.load('/home/yrinc/workspace/simple-faster-rcnn-pytorch/model/pth/chainer_best_model_converted_to_pytorch_0.7053.pth')\nopt.caffe_pretrain=True \n_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\nvis_bbox(at.tonumpy(img[0]),\n         at.tonumpy(_bboxes[0]),\n         at.tonumpy(_labels[0]).reshape(-1),\n         at.tonumpy(_scores[0]).reshape(-1))\n", "intent": "You'll need to download pretrained model from [google dirve](https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY) \n"}
{"snippet": "print('Model Accuracy: ', best_logit.score(X_test, y_test))\npred = best_logit.predict(X_test)\nprint('Model AUC Score: ', roc_auc_score(y_test, pred))\n", "intent": "The training accuracy is 0.945 and AUC score is 0.953 for the final model.\n"}
{"snippet": "grid_svm.predict([\"Illyasviel von Einzbern\"])\n", "intent": "Emmmmm incorrect, my name is not predicted correctly\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint( confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "It is not better according to accuracy, but that is not the measure for which it was optimized.\n"}
{"snippet": "from sklearn import metrics\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "class Network(Network):    \n    def do_backward(self, target, type_loss):\n        if type_loss == \"MSE\":\n            d_loss = self.d_MSE_loss(target)\n            grad = self.output_node.compute_backward(d_loss)\n        else:\n            raise ValueError(\"Only MSE loss is implemented for now.\")\n", "intent": "We can now implement the wrapper for the backward pass, which takes as input both the target vector and the type of the loss.\n"}
{"snippet": "reconstruction_loss_all = rec_loss(reconstruction, data)\nnow = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\nprint('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\nprint('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.data[0]))\n", "intent": "Now, let's assess its quality by calculating the reconstruction error over the entire dataset:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(MYOWN_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_test_predicted = model.predict(X_test.as_matrix(), verbose=0)\n", "intent": "Now let's take a look at the missclassified observations.\n"}
{"snippet": "score = model.evaluate(X_test_matrix, Y_test, verbose=0)\nprint(model.metrics_names)\nprint(score)\n", "intent": "After training the model we need to evaluate it.\n"}
{"snippet": "Y_test_predicted = model.predict(X_test_matrix, verbose=0)\n", "intent": "Now let's take a look at the missclassified observations.\n"}
{"snippet": "y_pred_train = knn.predict(X)\n", "intent": "We can then use the trained classifier to predict the target of the training data\n"}
{"snippet": "knn.predict(new_data)\n", "intent": "Make a prediction of the category of this observation\n"}
{"snippet": "knn.predict(new_data)\n", "intent": "and if we now have the knn classifier predict the species it predicts virginica for the data as we would expect.\n"}
{"snippet": "def answer_eight():\n    from sklearn.metrics import accuracy_score\n    X_train, X_test, y_train, y_test = answer_four()\n    predicted = answer_seven()\n    return accuracy_score(predicted, y_test)\nanswer_eight()\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "expected = dataset.target\npredicted = model.predict(dataset.data)\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "Use the model to make predictions and evaluate the predictions:\n"}
{"snippet": "preds= ResNets.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "wrong_vals = X_test.assign(Wrong = np.where(y_test != rf.predict(X_test),1,0))\n", "intent": "What did we get wrong with our randomforest?\n"}
{"snippet": "probabilities = tc.predict_proba(messages)\n", "intent": "Now can simply classify and get the class probability for each message. \n"}
{"snippet": "def get_score(fit, X_test, y_test):\n    r2 = r_squared(y_test, fit.predict(X_test))\n    mse = np.mean((y_test - fit.predict(X_test))**2)\n    print \"R2: \", r2\n    print \"MSE: \", mse\n", "intent": "And we wrap this is in a function that retrieves the mean squared error as well as the $R^2$ for a model\n"}
{"snippet": "print \"Score for simple random forrest:\"\nget_score(forest_1, X_test, y_test)\nprint \"Score for complex random forrest:\"\nget_score(forest_2, X_test, y_test)\n", "intent": "Having fitted the model allows us to calculate the $R^2$ as well as the mean squared error.\n"}
{"snippet": "get_score(lr, X_test, y_test)\n", "intent": "And we are then able to calculate the $R^2$ and the MSE as previously\n"}
{"snippet": "get_score(regr_ada, X_test, y_test)\n", "intent": "And compute the scores\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.\n    for l in range(len(style_layers)):\n        gram_l = gram_matrix(feats[style_layers[l]])\n        print(style_weights[l])\n        style_loss += style_weights[l] * (tf.reduce_sum((gram_l - style_targets[l])**2))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "classifiers = [(svc, 'Support Vector Machine'),\n                (lr, 'Logostic Regression'),\n                (knn, 'k-Nearest Neighbors'),\n                (tr, 'Decision Tree'),\n                (rf, 'Random Forest'),\n                (gb, 'Gradient Boosting')]\nfor clf, label in classifiers:\n    print(30*'_', '\\n{}'.format(label))\n    clf.grid_predict(X, Y)\n", "intent": "It remains only to examine the predictions of the different classifiers that have been trained earlier\n"}
{"snippet": "x_test = np.array(['bad'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint (confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(\"\\n\")\nprint (classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "confusion_matrix(Y_test, logreg.predict(X_test_scaled))\n", "intent": "Now let's try to look at model performance:\n"}
{"snippet": "fpr, tpr, _ = roc_curve(Y_test, hyperLR.predict(X_test_scaled))\nauc(fpr, tpr)\n", "intent": "Compared to the untuned model's accuracy of 0.8288, the model with hyperparameter tuning has better results.\n"}
{"snippet": "def predict_proba(X, w_):\n", "intent": "Let's look at the output of our model on the test data, and compute basic accuracy.\n"}
{"snippet": "print('The first 5 rows of our test data are: \\n', Iowa_test[Iowa_Predictors])\nprint('The predictions of the selected data are: \\n', Iowa_DT_model.predict(Iowa_test[Iowa_Predictors]))\n", "intent": "Lets test the prices of the houses present in the Test data to see how our model fares so far in its default state.\n"}
{"snippet": "test_Survived = pd.Series(voting_c.predict(test_2), name='Survived', dtype='int')\nresult = pd.concat([test_id, test_Survived], axis=1)\nresult.sample(5)\n", "intent": "Predict the test set with trained model and create a list of predictions~\n"}
{"snippet": "model.predict()\n", "intent": "Can predict future points using\n"}
{"snippet": "def sized_train_predict(clf, X_train, y_train, X_test, y_test, n_list):\n    log_dict = {}\n    for i in range(len(n_list)):\n        print \"------------- start train and predict --------------\"\n        train_predict(clf, X_train[:n_list[i]], y_train[:n_list[i]], X_test, y_test)\n        print \"-------------- train and predict done --------------\"\n        print \"\"\n        log_dict[i] = {n_list[i]:train_predict(clf, X_train[:n_list[i]], y_train[:n_list[i]], X_test, y_test)}\n    return log_dict\n", "intent": "In order to be D.R.Y, writing a helper function that extends `train_predict`. \n"}
{"snippet": "logistic_perf = sized_train_predict(clf_A, X_train, y_train, X_test, y_test, [100,200,296])\n", "intent": "**TODO: Execute the 'train_predict' function for each classifier and each training set size**\n"}
{"snippet": "score = model.evaluate(x_test,y_test,verbose=0)\nprint 'test accuracy',score[1]\n", "intent": "calculate the Model Accuracy\n"}
{"snippet": "score = model2.evaluate(x_test,y_test,verbose=0)\nprint 'test accuracy',score[1]\n", "intent": "check the accuracy of the model on the test set \n"}
{"snippet": "corrcoeff_analyser = corr_coeff.CorrelationCoefficient(time_series=tsr)\ncorrcoeff_data = corrcoeff_analyser.evaluate()\ncorrcoeff_data.configure()\nFC = corrcoeff_data.array_data[..., 0, 0]\n", "intent": "<p><div style=\"text-align: justify\"><font size=\"4.5\" face=\"time roman\">and create and evaluate the analysis:</font></div></p>\n"}
{"snippet": "print(sklearn.metrics.precision_score(test_data_df['logit_predict'], test_data_df['category'], average = 'weighted')) \nprint(sklearn.metrics.recall_score(test_data_df['logit_predict'], test_data_df['category'], average = 'weighted')) \nprint(sklearn.metrics.f1_score(test_data_df['logit_predict'], test_data_df['category'], average = 'weighted')) \n", "intent": "Better than what I was expecting!\n"}
{"snippet": "data = [\n    go.Histogram(\n        x=clf_Forest.predict_proba(churn_test[Feature_names])[:,1]\n    )\n]\npy.iplot(data)\n", "intent": "So we can see that at a threshold of 0.3, it is offering low cost.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\ny_train_predict = cross_val_predict(sgd_model, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_predict)\n", "intent": "Using Confusion Matrix\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_5, y_train_predict)\nrecall_score(y_train_5, y_train_predict)\n", "intent": "Using Precision and Recall\n"}
{"snippet": "y_scores = sgd_model.decision_function([digit])\nthreshold = 200000\ndigit_prepared = (y_scores > threshold)\ny_scores = cross_val_predict(sgd_model, X_train, y_train_5, cv=3, method='decision_function')\n", "intent": "Precision vs Recall Tradeoff\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = linear_model.predict(housing_prepared)\nlinear_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions))\nlinear_rmse\n", "intent": "Measuring RMSE for the model using sklearn.metrics.mean_squared_error\n"}
{"snippet": "predicted = super_learner.predict(X_test)\n", "intent": "Evaluate the trained classifier\n"}
{"snippet": "scores = cross_val_score(super_learner, X_valid, y_valid, cv=10)\ndisplay(scores)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "print( np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "print( np.mean(((bos.PRICE - lm.predict(X)) ** 2)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "testset=np.array(returnVec).reshape(1, -1)\npredict(testset)\n", "intent": "**Predict result**  \n"}
{"snippet": "print('R^2 =',metrics.explained_variance_score(y_test,predictions))\n", "intent": "**Explained Variance Score R^2**  \nMeasurement of how much variance your model explains\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Grab predictions off test set.\n"}
{"snippet": "svc_iris.predict([[1, 2, 3, 4]])[0]\n", "intent": "Something to investigate later.\n"}
{"snippet": "c=[(47,140)]\nx=clf.predict(c)\nx\n", "intent": "where predicted dummy variable is True for boys\n"}
{"snippet": "yts_pred = regr.predict(Xts)\nRSS_ts = np.mean((yts_pred - yts) ** 2 ) / (np.std(yts) ** 2)\nprint (\"RSS_Test = {:f}\".format(RSS_ts))\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "test_files = [im for im in os.listdir(TEST_DIR)]\ntest = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\nfor i, im in enumerate(test_files): \n    test[i] = read_image(TEST_DIR+im)\ntest_preds = model.predict(test, verbose=1)\n", "intent": "Finishing off with predictions on the test set. Scored LB 1.279 \n"}
{"snippet": "final_tresh = tresholds[scores[:, 2].argmax()]\ny_hat_test = (model.logpdf(test.drop('Class', axis=1).values) < final_tresh).astype(int)\nprint('Final threshold: %d' % final_tresh)\nprint('Test Recall Score: %.3f' % recall_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test Precision Score: %.3f' % precision_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test F2 Score: %.3f' % fbeta_score(y_pred=y_hat_test, y_true=test['Class'].values, beta=2))\ncnf_matrix = confusion_matrix(test['Class'].values, y_hat_test)\nplot_confusion_matrix(cnf_matrix, classes=['Normal','Anormal']\n                      , title='Confusion matrix')\n", "intent": "Finally, we report the test recall, precision and \\(F_2\\)-score, along with the confusion matrix.\n"}
{"snippet": "final_tresh = tresholds[scores[:, 2].argmax()]\ny_hat_test = (model.predict(test.drop('Class', axis=1).values) < final_tresh).astype(int)\nprint('Final threshold: %d' % final_tresh)\nprint('Test Recall Score: %.3f' % recall_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test Precision Score: %.3f' % precision_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test F2 Score: %.3f' % fbeta_score(y_pred=y_hat_test, y_true=test['Class'].values, beta=2))\ncnf_matrix = confusion_matrix(test['Class'].values, y_hat_test)\nplot_confusion_matrix(cnf_matrix, classes=['Normal','Anormal'], title='Confusion matrix')\n", "intent": "Finaly, we pick the threshold with the best validation \\(F\\)-score and produce a final evaluation on the test set.\n"}
{"snippet": "tresholds = np.linspace(-400, 0, 100)\ny_scores = gmm.score_samples(valid.drop('Class', axis=1).values)\nscores = []\nfor treshold in tresholds:\n    y_hat = (y_scores < treshold).astype(int)\n    scores.append([recall_score(y_pred=y_hat, y_true=valid['Class'].values),\n                 precision_score(y_pred=y_hat, y_true=valid['Class'].values),\n                 fbeta_score(y_pred=y_hat, y_true=valid['Class'].values, beta=2)])\nscores = np.array(scores)\nprint(scores[:, 2].max(), scores[:, 2].argmax())\n", "intent": "Now, also using the validation set, we tune a threshold that will be used to classify data as normal or anomalous.\n"}
{"snippet": "final_tresh = tresholds[scores[:, 2].argmax()]\ny_hat_test = (gmm.score_samples(test.drop('Class', axis=1).values) < final_tresh).astype(int)\nprint('Final threshold: %f' % final_tresh)\nprint('Test Recall Score: %.3f' % recall_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test Precision Score: %.3f' % precision_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test F2 Score: %.3f' % fbeta_score(y_pred=y_hat_test, y_true=test['Class'].values, beta=2))\ncnf_matrix = confusion_matrix(test['Class'].values, y_hat_test)\nplot_confusion_matrix(cnf_matrix, classes=['Normal','Anormal'], title='Confusion matrix')\n", "intent": "Finally, with the trained and tuned model, we produce a final evaluation on the test set.\n"}
{"snippet": "tresholds = np.linspace(-.2, .2, 200)\ny_scores = model.decision_function(valid.drop('Class', axis=1).values)\nscores = []\nfor treshold in tresholds:\n    y_hat = (y_scores < treshold).astype(int)\n    scores.append([recall_score(y_pred=y_hat, y_true=valid['Class'].values),\n                 precision_score(y_pred=y_hat, y_true=valid['Class'].values),\n                 fbeta_score(y_pred=y_hat, y_true=valid['Class'].values, beta=2)])\nscores = np.array(scores)\nprint(scores[:, 2].max(), scores[:, 2].argmax())\n", "intent": "Using our validation set, we tune a threshold for our score.\n"}
{"snippet": "final_tresh = tresholds[scores[:, 2].argmax()]\ny_hat_test = (model.decision_function(test.drop('Class', axis=1).values) < final_tresh).astype(int)\nprint('Final threshold: %f' % final_tresh)\nprint('Test Recall Score: %.3f' % recall_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test Precision Score: %.3f' % precision_score(y_pred=y_hat_test, y_true=test['Class'].values))\nprint('Test F2 Score: %.3f' % fbeta_score(y_pred=y_hat_test, y_true=test['Class'].values, beta=2))\ncnf_matrix = confusion_matrix(test['Class'].values, y_hat_test)\nplot_confusion_matrix(cnf_matrix, classes=['Normal','Anormal'], title='Confusion matrix')\n", "intent": "Using the optimal threshold, we report a final test score.\n"}
{"snippet": "tresholds = np.linspace(0, 6, 100)\nscores = []\nfor treshold in tresholds:\n    y_hat = (y_scores_valid.mean(axis=1) > treshold).astype(int)\n    scores.append([recall_score(y_pred=y_hat, y_true=valid['Class'].values),\n                 precision_score(y_pred=y_hat, y_true=valid['Class'].values),\n                 fbeta_score(y_pred=y_hat, y_true=valid['Class'].values, beta=2)])\nscores = np.array(scores)\nprint(scores[:, 2].max(), scores[:, 2].argmax())\n", "intent": "And then we use our validation set to tune a thsershold for the score.\n"}
{"snippet": "  cv_scores = []\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        cv_scores.append(log_loss(val_y, preds))\n        print(cv_scores)\n        break\n", "intent": "Without CV statistic,to score get 0.5480 by SRK. And CV statistic get 0.5346 In fact ,you \nneed to turn down the learning rate and turn up run_num\n"}
{"snippet": "testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\nadvpredictions = advancedmodel.predict(advancedtest)\n", "intent": "And again like last time, let's transform our test data and make some predictions!\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    results[name] = scores\n", "intent": "Now run all the classifiers, using 5-fold cross validation.\n"}
{"snippet": "accuracy_score(target_val, rf_predictions)\n", "intent": "And a nice touch with sklearn (and the use of train_test_split method) is that you can conveniently set aside a part of the \n"}
{"snippet": "classifier.evaluate(\n    input_fn=generate_labelled_input_fn([TEST_FILE], BATCH_SIZE),\n    steps=100\n)\n", "intent": "Let's see how the trained classifiers fares on our test data.\n"}
{"snippet": "predictions = classifier.predict(\n    generate_prediction_input_fn([image_array]),\n    predict_keys=['probabilities', 'classes']\n)\n", "intent": "Note that the estimator's `predict` method returns a generator.\n"}
{"snippet": "PredProb=neigh.predict_proba(X_test)\npred=np.asmatrix(PredProb)\npred.columns=('high','low','medium')\ns=np.asmatrix(pd.get_dummies(y_test))\ndef f(x):\n    return sp.log(sp.maximum(sp.minimum(x,1-10**-5),10**-5))\nf=np.vectorize(f)\npredf=f(pred)\nmult=np.multiply(predf,s)\nprint('log loss =',np.sum(mult)/-len(y_test))\n", "intent": " I looked into building out the log loss function to see how much error there is in the predictions\n"}
{"snippet": "def calculate_loss(model):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    return 1./num_examples * data_loss\n", "intent": "**loss function** implementation:\n"}
{"snippet": "def predict(model, x):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    return np.argmax(a2, axis=1)\n", "intent": "To **predict** the class, use forward propagation as defined above and returns the class with the highest probability.\n"}
{"snippet": "def log_likelihood(model, x, y):\n    prob = model.predict_log_proba(x)\n    rotten = y == 0\n    fresh = ~rotten\n    return prob[rotten, 0].sum() + prob[fresh, 1].sum()\nll_train = log_likelihood(fitted_model,TrainInput,TrainOutput)\nll_test = log_likelihood(fitted_model,TestInput,TestOutput)\nprint('Log-Likelihood on Training dataset: ' ,ll_train)\nprint('Log-Likelihood on Testing dataset: ' ,ll_test)\n", "intent": "---\nGiven a fitted model, we can compute the log-likelihood of our data as a way to assess the performance of your model. \n"}
{"snippet": "names = [\"No loss\", \"Losses\"]\ntarget_names =  ['class {}({})'.format(clsnum, clsname) for clsnum, clsname in enumerate(names)]\nprint(metrics.classification_report(y_test, y_pred,target_names=target_names))\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n", "intent": "Let's plot the the confusion matrix\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "batch = mnist.test.next_batch(10000)\ntest_images = batch[0].reshape([-1, 28, 28, 1])\ntest_labels = batch[1]\ndef evaluate(result_tensor, data_placeholder):\n    feed_images = np.reshape(test_images, [-1, *data_placeholder.shape[1:]])\n    result = result_tensor.eval(\n        feed_dict={data_placeholder: feed_images})\n    return np.mean(result == test_labels)\n", "intent": "Read the 10000 mnist test points\n"}
{"snippet": "myModel_predictions = [np.argmax(myModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(myModel_predictions)==np.argmax(test_targets, axis=1))/len(myModel_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = linModel.predict(X = X_test)\nprint (\"RMSLE Value For Linear Regression: \", rmsle(y_test,preds))\n", "intent": "Now we test our model on our training set\n"}
{"snippet": "for column in training_data_target.columns:\n    err = np.sqrt(mean_squared_error(training_data_target[column],training_result[column]))\n    print('Error in ' + column, err)\n", "intent": "Root Mean squared errors on train and test\n"}
{"snippet": "y_pred = knn.predict(X)\n", "intent": "<div class=\"alert alert-success\">\n    <b>EXERCISE</b>: use this classifier to *predict* labels for the data\n</div>\n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos.PRICE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print (\"predicted\", spam_detect_model.predict(tfidf4)[0])\nprint (\"expected\", messages.label[3])\n", "intent": "Let's try to classifying our single random message and checking how we do: \n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint (all_predictions)\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset. Let's begin by getting all the predictions:\n"}
{"snippet": "class_predict = log_model2.predict(X_test)\nprint (metrics.accuracy_score(Y_test, class_predict))\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        sq_diff = torch.pow( gram_matrix(feats[style_layers[i]]) - style_targets[i], 2)\n        style_loss += style_weights[i] * torch.sum(sq_diff)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "output=forest.predict(test_data).astype(int)\npredictions_file = open(\"titanicpredictions.csv\", 'wb')\nopen_file_object = csv.writer(predictions_file)\nopen_file_object.writerow([\"PassengerId\", \"Survived\"])\nopen_file_object.writerows(zip(test_ids, output))\npredictions_file.close()\n", "intent": "We then apply training to the test data set and ouput the results to a CSV file:\n"}
{"snippet": "print('Score: %.2f' % sklearn.metrics.mean_squared_error(y_predicted, y_actual))\n", "intent": "One can see, the classifier is sensitive to tuning parameters and it can be easily overfitted.\n"}
{"snippet": "test_data['actualDonatedAmount'] = 2**test_data['donatedamount']\ntest_data['predictedDonatedAmount'] = 2**lm.predict(test_data[X])\nsns.lmplot( 'actualDonatedAmount','predictedDonatedAmount', test_data)\n", "intent": "**Converting the logged values back to donated values ** \n"}
{"snippet": "from sklearn.metrics import classification_report\nreport = classification_report(test_pred.Y, test_pred.y)\nprint(report)\n", "intent": "Removing C and Upwardly Urban didn't change the models accuracy.\n"}
{"snippet": "scores = cross_val_score(custom_logreg1.model, X_train_s, y_train, cv=5, scoring='roc_auc')\n", "intent": "All features are significant at p < 0.05, except two.\n"}
{"snippet": "print preds[:10,:], '\\n'\ncounts = np.apply_along_axis(np.bincount, 1, preds, minlength=8, weights=acc)\npred_y_dev_ensemble = np.argmax(counts, axis=1)\nprint pred_y_dev_ensemble[:10], '\\n'\nacc_ensemble = metrics.accuracy_score(y_dev, pred_y_dev_ensemble)\nprint acc_ensemble\n", "intent": "If we give equal weight to each model, the accuracy is less than 0.4 percentage points greater than the best model (AdaBoost).\n"}
{"snippet": "def mse_loss(x):\n    return tf.losses.mean_squared_error(labels=x, predictions=model.predict(x))\nloss_and_grads = tfe.implicit_value_and_gradients(mse_loss)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n", "intent": "Define the loss, gradients and optimizer for our model:\n"}
{"snippet": "column_combination6 = []\nsix_combs = list(combinations(x_columns, 6))\nresults6 = []\nfor combo in six_combs:\n    results6.append([combo, np.mean(cross_val_score(logreg, X.loc[:,combo], yv, cv=5))])\n", "intent": "Lets look for the variables, and score improvement, if six variables are searched through.\n"}
{"snippet": "net = SimpleNetwork()\nnet.train() \nfor batch_idx, (data, target) in enumerate(training_loader):\n    output = net(data)\n    loss = F.nll_loss(output, target)\n", "intent": "Now we know how to get a minibatch, let's start putting together a training framework:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.zeros(1)\n    for l in range(len(style_layers)):\n        loss += style_weights[l] * 2 * tf.nn.l2_loss(gram_matrix(feats[style_layers[l]])- style_targets[l])  \n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY, testPredict))\nprint('Test Score: %.2f RMSE' % (testScore))\n", "intent": "Calculate root mean squared error\n"}
{"snippet": "lr.predict_proba(X_test_std[0,:])\n", "intent": "    Mucho bueno indeeed!\n"}
{"snippet": "metrics.hamming_loss(y_test, prediction)\n", "intent": "Scikit-learn provides a [set of metrics](http://scikit-learn.org/stable/modules/classes.html\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndevtest_X, devtest_y = zip(*devtest_set_knn)\ndevtest_predictions = knn_classifier.predict(devtest_X)\naccuracy_score(devtest_y,devtest_predictions)\n", "intent": "You can also use `sklearn` to determine the accuracy of the classifier:\n"}
{"snippet": "predcit = arma_mod20.predict(start='1959-01-01', end='1995-01-01')\n", "intent": "<h3>check the first ten and last ten prediction and compare with the actual emission</h3>\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nmodel_path = 'saved_models/dog_model.h5'\nVGG19_model.save(model_path)\nprint('The best model is saved in \"{}\"'.format(model_path))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.evaluate(X_test_a,y_test_a)\n", "intent": "Let's evaluate the model performance on the test set\n"}
{"snippet": "predictions=model.predict(X_test_a) \n", "intent": "Let's observe the images which is misclassified by our model.\n"}
{"snippet": "local_results = bst.predict(dtest)\nconverted_local = [x > 0.5 for x in local_results]\nlocal = pd.Series(converted_local, name='local')\npd.crosstab(actual,local)\n", "intent": "Let's compare this with the confusion matrix of our local model.\n"}
{"snippet": "x_test = np.array(['I am awesome'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        im_gram_l = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((im_gram_l - style_targets[i])**2)\n    return style_loss    \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "np.argmax(m.predict(imgs_test_t[1717:1718,]))\n", "intent": "Ha predicho un 0. Cuadra.\n"}
{"snippet": "import numpy as np\nprint ('Residual sum of squares: %.2f' % np.mean((model.predict(X)- y) ** 2))\n", "intent": "<img src=\"Images/estimating_coefficients.png\" width=\"80%\">\n"}
{"snippet": "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)\n", "intent": "Note: this is the same operation as above but applied to the scaled actual values of Y instead of the predictions.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(scratch_cnn_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Scratch CNN trained w/o augmentation, Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('VGG16 model, Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "my_ResNet50_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in my_test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(my_ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(my_ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy on test data is about 6% less than the training data. This is definitely not a overfitted classifier but it is likely to be imporved.\n"}
{"snippet": "mse = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint mse / len(bos.PRICE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        g_l = gram_matrix(feats[style_layers[i]])\n        a_l = style_targets[i]\n        style_weight_l = style_weights[i]\n        style_loss += style_weight_l * tf.reduce_sum((g_l - a_l) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Rc=cross_val_score(lre,x_data[['horsepower']], y_data,cv=2)\nRc\n", "intent": "Double-click __here__ for the solution.\n<!-- Your answer is below:\nRc=cross_val_score(lre,x_data[['horsepower']], y_data,cv=2)\nRc[1]\n-->\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.12f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram=gram_matrix(feats[style_layers[i]])\n        deviation=tf.reduce_sum(tf.square(style_targets[i]-gram))\n        style_loss+=style_weights[i]*deviation\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = vgg.predict(image)\n", "intent": "This seems like a lot of steps, and I keep thinking there must be a better preprocessing image pipeline but I don't see it.\n"}
{"snippet": "probs = predict(parameters, test_data[features].T)\npreds = np.where(probs > 0.9, 1, 0)\ntn, fp, fn, tp = confusion_matrix(y_true = test_data[label].values.ravel(), \\\n                                  y_pred = preds.ravel()).ravel()\nprint ('(tn, fp, fn, tp) = ({}, {}, {}, {})'.format(tn, fp, fn, tp))\nprint ('precision = {}'.format(tp / (tp + fp)))\nprint ('recall = {}'.format(tp / (tp + fn)))\nprint ('accuracy = {}'.format((tp + tn) / float(len(test_data))))\n", "intent": "If we set our threshold to 0.9. The metrics of model performance is computed as follows.\n"}
{"snippet": "print('accuracy = ' + format(accuracy_score(y_test, y_test_preds)))\nprint('precision = ' + format(precision_score(y_test, y_test_preds)))\nprint('recall = ' + format(recall_score(y_test, y_test_preds)))\n", "intent": "Totally out without scaling the weight.\n"}
{"snippet": "logreg_score = logreg.predict(X_test)\nprint('Accuracy of logistic regression score= {:.8f}'.format(logreg.score(X_test, y_test)))\n", "intent": "The Precision, Recall and F1-Score values are too high for answer \"Yes\".\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = classifier.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \")\nprint(cm)\n", "intent": "Support Vector Machine model is formed by using *classifier()*  from the library *sklearn* and then we construct the model with *fit()* code.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = classifier.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \")\nprint(cm)\n", "intent": "- ***Chi-Square Test to Support Vector Machine Model:***\n"}
{"snippet": "y_pred = classifier.predict(X_test_scaled)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \")\nprint(cm)\n", "intent": "- ***Chi-Square Test to Support Vector Machine Model:***\n"}
{"snippet": "output = np.squeeze(p.forwardPredict(testData_SVI))\n", "intent": "Now run the test data through the model:\n"}
{"snippet": "yhat4 = adaboost.predict_proba(Xtest)\n", "intent": "Let's see how this classifier performs on the test data, compared to the other classifiers above.\n"}
{"snippet": "print metrics.accuracy_score(y_test, dt.predict(X_test))\nprint \"--------------------------------------------------------\"\nprint metrics.confusion_matrix(y_test, dt.predict(X_test)) \nprint \"--------------------------------------------------------\"\nprint metrics.classification_report(y_test, dt.predict(X_test))\nprint \"--------------------------------------------------------\"\nprint metrics.roc_auc_score(y_test, dt.predict(X_test))\n", "intent": "run the decison tree\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy on both the training data and test data is lower. However, this classifier is not overfit like the previous classifier.\n"}
{"snippet": "mean, std = scaled_features['cnt']\npredictions = network.run(X_test)*std + mean\nfor i in range(5):\n    print (\"Comparing prediction \nr2 = r2_score(y_test['cnt']*std + mean, predictions[0,])\nprint ('R2 Score: ', r2.round(3))\n", "intent": "Now, we re-run the predictions with the new model, and measure the accuracy.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test['Bookings'], y_hat_avg.moving_avg_forecast))\nprint(rms)\n", "intent": "We chose the data of last 2 months only. We will now calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test['Bookings'], y_hat_avg.SES))\nprint(rms)\n", "intent": "We will now calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test['Bookings'], y_hat_avg.Holt_linear))\nprint(rms)\n", "intent": "We will now calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "roc_auc_score(labels, deepbind_predictions)\n", "intent": "Let's check the auROC of deepbind predictions:\n"}
{"snippet": "c = \"toxic\"\n(y.\n drop(c, axis=1).\n apply(lambda x: pd.Series({\"score\": np.nan if (np.sum(x == 1) > np.sum(y[c] == 1)) else subset_score(y[c] == 1, x == 1)})))\n", "intent": "All categories are near subsets of toxic\n"}
{"snippet": "c = \"obscene\"\n(y.\n drop(c, axis=1).\n apply(lambda x: pd.Series({\"score\": np.nan if (np.sum(x == 1) > np.sum(y[c] == 1)) else subset_score(y[c] == 1, x == 1)})))\n", "intent": "Severely toxic comments are almost always obscene.  Threatening, insulting or hateful comments are typically obscene.\n"}
{"snippet": "c = \"insult\"\n(y.\n drop(c, axis=1).\n apply(lambda x: pd.Series({\"score\": np.nan if (np.sum(x == 1) > np.sum(y[c] == 1)) else subset_score(y[c] == 1, x == 1)})))\n", "intent": "Severely toxic or hateful comments are often insulting\n"}
{"snippet": "c = \"severe_toxic\"\n(y.\n drop(c, axis=1).\n apply(lambda x: pd.Series({\"score\": np.nan if (np.sum(x == 1) > np.sum(y[c] == 1)) else subset_score(y[c] == 1, x == 1)})))\n", "intent": "Threatening or hateful comments are rarely severely toxic\n"}
{"snippet": "c = \"identity_hate\"\n(y.\n drop(c, axis=1).\n apply(lambda x: pd.Series({\"score\": np.nan if (np.sum(x == 1) > np.sum(y[c] == 1)) else subset_score(y[c] == 1, x == 1)})))\n", "intent": "Threats are rarely hateful\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nypred = clf.predict(Xtest)\nprint(confusion_matrix(ytest, ypred))\n", "intent": "This single number doesn't tell us **where** we've gone wrong: one nice way to do this is to use the *confusion matrix*\n"}
{"snippet": "obs_3d_one = \nobs_3d_two = \nobs_3d_three = \nprediction_3d = classes_3d[knn_3d.predict([[obs_3d_one, obs_3d_two, obs_3d_three]])[0]]\nprint 'An observation with a feature-one value of {}, a feature-two value of '\\\n      '{}, and a feature-three value of {} is a member of {}'.format(obs_3d_one, obs_3d_two, \n                                                                     obs_3d_three, prediction_3d)\n", "intent": "**Predict Group Membership with Three Values**\nPopulate the observation values and predict group membership below.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.0\n    for i in range(len(style_layers)):\n        g = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * (g - style_targets[i]).pow(2).sum()\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(RL, X_train, y_train, cv=6)\nprint(scores)\nprint('According the validation scores look like all approaching 0.97, the model is robust.')\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "sgd_clf.predict([X_train[digit_index]])\n", "intent": "See if the image labelled as five is classified as a five:\n"}
{"snippet": "roc_auc_score(y_train_5, y_scores_forest)\n", "intent": "Also the AUC score is much better for the random forest:\n"}
{"snippet": "forecast = m.predict(futures)\n", "intent": "forecast using the extra-regressors as predictors\n"}
{"snippet": "MSE = np.mean((bos.PRICE - lm.predict(X))**2)\nprint(\"Mean squared error = {:.4f}\".format(MSE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "import pandas as pd\ntp = model_lr.predict(valid_x)\npred = np.array( [np.argmax( tp[i] ) for i in range(len(tp))] )\nct = pd.crosstab(valid_y, pred, rownames=[\"Actual\"], colnames=[\"Predicted\"], margins=False); ct\n", "intent": "To view the performance (on the validation set) it's convenient to use <a href=\"http://pandas.pydata.org\">pandas</a>.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.zeros([])\n    for i in range(len(style_layers)):\n        g = gram_matrix(feats[style_layers[i]])\n        a = style_targets[i]\n        style_loss += style_weights[i]*tf.reduce_sum((g-a)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = lin_reg.predict(X_test)\nlin_mse = mean_squared_error(y_test, predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "after we fit the model to the training set, we can measure its performance on the test set.\n"}
{"snippet": "results = np.zeros( (X_test.shape[0],10) ) \nfor j in range(nets):\n    results = results + model[j].predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n", "intent": "after the neural network is trained, run prediction\n"}
{"snippet": "re_mean = np.mean(bos.PRICE-lm.predict(X))\nn = len(bos.PRICE-lm.predict(X))\nprint (np.sum(((bos.PRICE - lm.predict(X)) - re_mean) ** 2) / n)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:2f}'.format(logreg.score(X_test,y_test)))\n", "intent": "** Predicting the test set results, and calculating the accuracy**\n"}
{"snippet": "n_folds = 10\ndef cross(model):\n    kf = KFold(n_folds, shuffle=True, random_state=12)\n    scores = (cross_val_score(model, train, y_train, scoring=\"accuracy\", cv = kf))\n    return(scores)\n", "intent": "- we'll create a method to perform cross validation for every model\n"}
{"snippet": "print(\"Predicting people's names on the test set\")\ny_pred = clf.predict(X_test_pca)\nprint(classification_report(y_test, y_pred, target_names=target_names))\n", "intent": "Quantitative evaluation of the model quality on the test set\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "yhat = model1.predict(x_test)\nx_test = x_test.reshape((x_test.shape[0], n_hours*n_features))\n", "intent": "Estimation & Prediction\n------\nNow let's used this trained model to output predicted values of the test variables:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nY_data_reshaped =  Y_data.as_matrix().reshape((Y_data.shape[0],))\nscores = cross_val_score(LDA_model, X_data, Y_data_reshaped, cv = 5, n_jobs = -1)\nprint(scores)\n", "intent": "Do the same with sklearn cross-validation package.\n"}
{"snippet": "scores = cross_val_score(LDA_model, X_data_new, Y_data_reshaped, cv = 5)\nprint(scores)\n", "intent": "Let's check if it is enough data for LDA to classify digits.\n"}
{"snippet": "thresh = 0.3\ny_predthresh_log01 = y_predproba_log01 > thresh\nprint(\"Precision score:\", precision_score(y_test, y_predthresh_log01))\nprint(\"Recall score:\", recall_score(y_test, y_predthresh_log01))\n", "intent": "Looks like we want our recall to be around that low 90%s. For that, we need to find the threshold...\n"}
{"snippet": "def predict(x, t, x_test, K=1):\n    w_hat = get_w_hat(x, t, K)\n    X_test = get_X(x_test, K)\n    return X_test*w_hat\n", "intent": "<div class=\"alert alert-info\">\nWrite a function that, when given `x`, `t` and `x_test`, computes `w_hat` and makes predictions at `x_test`.</div>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint (mean_squared_error(bos.PRICE, lm.predict(X))) \n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        index = style_layers[i]\n        gram = gram_matrix(feats[index])\n        loss += style_weights[i] * tf.reduce_sum((gram - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\ny_true, y_pred = y_test, SGD_model.predict(x_test) \nprint 'Mean absolute error of SGD regression was:'\nprint(mean_absolute_error(y_true, y_pred))\n", "intent": "Now let's see how it does on our test set. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = final_gb.predict(testdmat) \ny_pred\n", "intent": "Now let's use sklearn's accuracy metric to see how well we did on the test set. \n"}
{"snippet": "accuracy_score(y_pred, y_test), 1-accuracy_score(y_pred, y_test)\n", "intent": "Now we can calculate our accuracy. \n"}
{"snippet": "rf_pred = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "log_predict = log_pipe.predict(X_test)\n", "intent": "Priority for project is identifying as many swing counties as possible\n- Logistic Regression model selected based on this metric\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The model performed worse than the previous model but worse on the test set and training set. \n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy_Iv3 = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Iv3)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "for model in models:\n  score = model.evaluate(x_test, y_test_categorical, verbose=0)\n  print('Test loss:', score[0])\n  print('Test accuracy:', score[1])\n", "intent": "Get metrics for models on unseen data\n"}
{"snippet": "predicted_x = model.predict(x_test, normalize=False)\nresiduals = np.argmax(predicted_x,1)!=np.argmax(y_test_categorical,1)\nloss = sum(residuals)/len(residuals)\nprint(\"the validation 0/1 loss is: \",loss)\n", "intent": "Check the initial prediction for the VGG model. We can see that the absolute error is 0.0641, thus the accuracy is 0.9359.\n"}
{"snippet": "Res50_predictions = [np.argmax(Res50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Res50_predictions)==np.argmax(test_targets, axis=1))/len(Res50_predictions)\nprint(\"Test accuracy: {:.4f}\".format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "inception_model.load_weights('saved_models/weights.best.Inception.hdf5')\ninception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint(\"Test accuracy: {:.4f}\".format(test_accuracy))\n", "intent": "**Load weights and evaluate**\n"}
{"snippet": "xception_model.load_weights('saved_models/weights.best.Xception.hdf5')\nxception_predictions = [np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint(\"Test accuracy: {:.4f}\".format(test_accuracy))\n", "intent": "**Load weights and evaluate**\n"}
{"snippet": "score = model.evaluate(X_test, y_test, \n                       batch_size=32, verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Evaluate the model**\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "With the model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "num_samples = 64 \nnce = tf.nn.nce_loss(weights=nce_weights,\n                    biases=nce_biases,\n                    labels=train_labels,\n                    inputs=embed,\n                    num_sampled=num_samples,\n                    num_classes=vocabulary_size)\nloss = tf.reduce_mean(nce)\n", "intent": "Let's try to **predict** the **target word** using the **NCE** objective\n"}
{"snippet": "X_train_array = X_train.values\ny_train_array = y_train.values\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(mlp, X_train_array,y_train_array,cv =3, scoring = 'accuracy')\nf1_scores = cross_val_score(mlp, X_train_array,y_train_array,cv =3, scoring ='f1_macro')\nprint(\"Accuracy: \\n \", scores, \"\\n\", \"Accuracy: \\n \", f1_scores, \"\\n\", \"Mean Accuracy: %0.2f (+/- %0.2f) \\n\" % (scores.mean(), scores.std() * 2))\n", "intent": "<p><b> Validation Curve</b> error vs number of hidden nodes </p>\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "predicted = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(clf, X_test, y_test, cv=5)\nprint scores.mean(), 'Scores of 5 folds: ', scores\n", "intent": "`cross_val_score` predicts and scores labels for a test set using accuracy:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for style_layer, style_target, style_weight in zip(style_layers, style_targets, style_weights):\n        features = feats[style_layer]\n        style_feature = gram_matrix(features)\n        style_loss += content_loss(style_weight, style_feature, style_target)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_meta = test\ntest_x = test\ntest_meta[\"SVM\"] = \"\"\ntest_meta[\"KNN\"] = \"\"\ntest_meta[\"CART\"] = \"\"\ntest_meta[\"RF\"] = \"\"\ntest_meta[\"KNN\"] = knnModel.predict(test_x)\ntest_meta[\"CART\"] = cartModel.predict(test_x)\ntest_meta[\"RF\"] = rfModel.predict(test_x)\ntest_meta[\"SVM\"] = svmModel.predict(test_x)\n", "intent": "Let us now predict the test data from the ensemble model. I will first restructure the training data with the machine learning predictions\n"}
{"snippet": "import sklearn.mixture as mixture\nn_clusters = 4\ngmm = mixture.GMM(n_components=n_clusters, n_iter=1000, covariance_type='diag')\nlabels = gmm.fit_predict(syn_unmasked)\nclusters = []\nfor l in range(n_clusters):\n    a = np.where(labels == l)\n    clusters.append(syn_unmasked[a,:])\nprint len(clusters)\nprint clusters[0].shape\n", "intent": "Cluster number from previous homeworks where optimal cluster number was 4 (jay helped adapt)\n"}
{"snippet": "import sklearn.mixture as mixture\ndens_vec = (data[:, 4]/data[:, 3])\ndens_vec = (dens_vec-np.average(dens_vec))/np.std(dens_vec)\ndens_vec = np.reshape(dens_vec, (dens_vec.shape[0], 1))\nn_clusters = 5\ngmm = mixture.GMM(n_components=n_clusters, n_iter=1000, covariance_type='diag')\nlabels = gmm.fit_predict(dens_vec)\nz = int(midz)\nxsort = np.sort(np.unique(data[:, 0]))\nysort = np.sort(np.unique(data[:, 1]))\n", "intent": "First use density based clustering, taken from Assignment11_jay.ipynb part 1.\n"}
{"snippet": "regressions = [KNN(n_neighbors=29, algorithm='auto'),\n               RF(max_depth=10, max_features=1)]\nnames = ['KNN Regression', 'Random Forest Regression']\nfor i, reg in enumerate(regressions):\n    scores = cross_validation.cross_val_score(reg, X, Y, scoring='r2', cv=10)\n    print(\"R^2 of %s: %0.2f (+/- %0.2f)\" % (names[i], scores.mean(), scores.std() * 2))\n", "intent": "Conclude that for KNN, optimal number of neighbors is around 29. For random forest, optimal depth 10, features 2.\n"}
{"snippet": "scores = ms.cross_val_score(model, X, y ,cv=ms.KFold(n_splits=5, random_state=0))\n1 - scores\n", "intent": "- Use the function **cross_val_score** to implement 5-fold cross validation with a **Linear Discriminant Analysis** model. Report the test errors.\n"}
{"snippet": "print \"RF(600)classification report:\\n\",classification_report(y_test,yhat_rfc3)\nprint \"SVM(rbf)classification report:\\n\",classification_report(y_test,predicted2)\n", "intent": "Built a text report showing the main classification metrics for both models\n"}
{"snippet": "label = {0:'Sitting', 1:'Sitting down', 2:'Standing', 3:'Standing up', 4:'Walking'}\nsf['predicted_class'] = model.predict(sf)\ngl.evaluation.confusion_matrix(sf['class'].apply(lambda x: label[x]), sf['predicted_class'].apply(lambda x: label[x])).print_rows(num_rows=20)\n", "intent": "And for comparison sake, below is the confusion matrix using the model that achieved the highest accuracy (~99.7%).\n"}
{"snippet": "metrics.fbeta_score(y_test, y_pred_class, beta=1)\n", "intent": "<img src='F-beta.png'/>\n"}
{"snippet": "import random\ntmp = [random.randint(0, len(tmp_pdist)) for ii in range(0,tree_data['N_test'])]\ntmp_acc = accuracy_score(tree_data['test_labels'], tmp) * 100    \nprint 'Accuracy of:', tmp_acc, '%\\n'\n", "intent": ">   - Compare it to the random guess, what would you get if you'd guess a district randomly? \n"}
{"snippet": "import collections\nimport operator\ntmp = collections.Counter(SF_inputs['PdDistrict'])\ntmp = max(tmp.iteritems(), key=operator.itemgetter(1))\nprint 'largest value in dict:', tmp\ntmp = [tmp[0] for ii in range(0,tree_data['N_test'])]\ntmp_acc = accuracy_score(tree_data['test_labels'], tmp) * 100    \nprint 'Accuracy of:', tmp_acc, '%\\n'\n", "intent": ">   - And if you'd guess always one of the districts (for example the district with the most crimes)?\n"}
{"snippet": "lm_rfe_cv = -1*cross_val_score(lm, x_rfe, boston.iloc[:,-1], scoring='neg_mean_squared_error', cv= 5).mean()\nlm2_rfe_cv = -1*cross_val_score(lm, x2_rfe, boston.iloc[:,-1], scoring='neg_mean_squared_error', cv= 5).mean()\nlm3_rfe_cv = -1*cross_val_score(lm, x3_rfe, boston.iloc[:,-1], scoring='neg_mean_squared_error', cv= 5).mean()\n", "intent": "Fit a linear regression on the model. How can we go beyond the final variables remaining in RFE to further improve the performance of the model?\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\nresnet50_test_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "Prediction Accuracy : 73%\n"}
{"snippet": "input_fn = tf.estimator.inputs.numpy_input_fn( x={'images': mnist.test.images}, y=mnist.test.labels,\n                                              batch_size=batch_size, shuffle=False)\nmodel.evaluate(input_fn)\n", "intent": "Now, train the model with images dataset\n"}
{"snippet": "from pandas import datetime\nbegin_year = datetime(2009,1,1)\nend_year = datetime(2012,1,1)\nforecasted = arma_result.predict(start=begin_year, end=end_year)\nforecasted\n", "intent": "8\\. Compute prediction for years 2009-2012 and analyze their fit against actual values. (1 point)\n"}
{"snippet": "print 'Output Prediction'\nprint(model.predict_classes(training_data_input))\nprint 'Output Prediction Probabilities'\nprint(model.predict_proba(training_data_input))\n", "intent": "Let us find out how well trained our model is with the help of **predict_proba** and **predict_classes** functions. \n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        current_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((current_gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "- so for now we'll consider the root mean square method\n"}
{"snippet": "(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "- Now we compute the root mean squared error for this model\n"}
{"snippet": "human_score = human_model.evaluate(human_bottleneck_test, human_test_target, verbose=0)\ndog_score = human_model.evaluate(dog_bottleneck_test, dog_test_target, verbose=0)\nhuman_score[1] = human_score[1] * 100.00\ndog_score[1] = (1 - dog_score[1]) * 100\nprint('%.4f%% of the images in human_files_short have a detected human.' % human_score[1])\nprint('%.4f%% of the images in dog_files_short have a detected human.'  % dog_score[1])\n", "intent": "In this section I evaluate the model with dogs and humans_files_short sets\n"}
{"snippet": "accu = dogBreeds_Resnet50_model.evaluate(test_Resnet50, test_targets, verbose=0)\naccu = accu[1] * 100\nprint('Test accuracy: {0:.4f}%'.format(accu))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ypred = pipeline.predict_proba(xtest)\nypred\n", "intent": "we can also easily make predictions with the fit pipeline object -- this will take raw input data, apply feature selection, and score the records:\n"}
{"snippet": "intercept_array = np.ones(len(df))\nRM_array = df['RM'].values\nvalue_array = np.stack([intercept_array, RM_array], axis = 1)\npredicted_array = fitted_model.predict(value_array)\n", "intent": "- Alternatively, we could have used\n"}
{"snippet": "print \"My class predictions:\"\nprint predict_class(sentiment_model, sample_test_matrix )\nprint \"Class predictions according to  scikit-learn:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "def get_classification_accuracy(model, data, true_labels):\n    predicted_labels =  model.predict(data) \n    num_correct = sum ( np.equal(predicted_labels, true_labels) )\n    print \"num_correct using 'sum ( np.equal(predicted_labels, true_labels) )' = \", num_correct\n    print \"num_data\", data.shape[0]\n    accuracy = num_correct * 1.0/data.shape[0]\n    return accuracy\n", "intent": "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier.\n"}
{"snippet": "print \"Class predictions according to sklearn:\" \nprobs = sentiment_model.predict_proba(sample_test_matrix)\nprint probs[:,1] \n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from  scikit-learn.\n"}
{"snippet": "print round(big_model.evaluate(train_data)['accuracy'],2)\nprint round(big_model.evaluate(validation_data)['accuracy'],2)\n", "intent": "**19.** Now, let us evaluate **big_model** on the training set and validation set.\n"}
{"snippet": "big_model_train = big_model.predict(feature_train_matrix)\nprint \"\\n\",round(accuracy_score(response_train,big_model_train),2)\nbig_model_valid = big_model.predict(feature_valid_matrix)\nprint \"\\n\",round(accuracy_score(response_valid,big_model_valid),2)\n", "intent": "**19.** Now, let us evaluate **big_model** on the training set and validation set.\n"}
{"snippet": "round(evaluate_classification_error(my_decision_tree, test_data, target),2)\n", "intent": "**17.** Now, let's use this function to evaluate the classification error on the test set.\n"}
{"snippet": "print round(evaluate_classification_error(my_decision_tree, test_data, target),2)\n", "intent": "**17.** Now, let's use this function to evaluate the classification error on the test set.\n"}
{"snippet": "round( evaluate_classification_error(my_decision_tree_new, validation_set), 5)\n", "intent": "**20.** Now, let's use this function to evaluate the classification error of `my_decision_tree_new` on the **validation_set**.\n"}
{"snippet": "round(evaluate_classification_error(my_decision_tree_old, validation_set),5)\n", "intent": "**21.** Now, evaluate the validation error using `my_decision_tree_old`.\n"}
{"snippet": "print \"Validation data, classification error (model 1):\", \\\nevaluate_classification_error(model_1, validation_set)\nprint \"Validation data, classification error (model 2):\", \\\nevaluate_classification_error(model_2, validation_set)\nprint \"Validation data, classification error (model 3):\", \\\nevaluate_classification_error(model_3, validation_set)\n", "intent": "**24.** Now evaluate the classification error on the validation data.\n"}
{"snippet": "print \"Validation data, classification error (model 4):\", \\\nevaluate_classification_error(model_4, validation_set)\nprint \"Validation data, classification error (model 5):\", \\\nevaluate_classification_error(model_5, validation_set)\nprint \"Validation data, classification error (model 6):\", \\\nevaluate_classification_error(model_6, validation_set)\n", "intent": "**28.** Calculate the accuracy of each model (**model_4**, **model_5**, or **model_6**) on the validation set. \n"}
{"snippet": "print \"Validation data, classification error (model 7):\", \\\nevaluate_classification_error(model_7, validation_set)\nprint \"Validation data, classification error (model 8):\", \\\nevaluate_classification_error(model_8, validation_set)\nprint \"Validation data, classification error (model 9):\", \\\nevaluate_classification_error(model_9, validation_set)\n", "intent": "**31.** Now, let us evaluate the models (**model_7**, **model_8**, or **model_9**) on the **validation_set**.\n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv=5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "The model will be evaluated using a 5-fold cross validation with the accuracy, this metric is used in the competition leaderboard\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"yolo.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_test, y_test_pred)\n", "intent": "__Accuracy__ and __NDCG__\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_test, y_test_pred)\n", "intent": "__Accuracy__ and __NDCG__ (Just of recording)\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_test, y_test_pred)\n", "intent": "__Accuracy__ (just for recording)\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "<h3>Predict Testing Set</h3>\n"}
{"snippet": "print (('Mean Absolute Error (MAE): ') + str(metrics.mean_absolute_error(y_test, predictions)))\n", "intent": "<h3>Error Evaluation</h3>\n<hr>\n**Mean Absolute Error** (MAE): $$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print (('Mean Squared Error (MSE): ')+str(metrics.mean_squared_error(y_test,predictions)))\n", "intent": "<hr>\n**Mean Squared Error** (MSE): $$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print (('Root Mean Squared Error (RMSE): ')+str(np.sqrt(metrics.mean_squared_error(y_test,predictions))))\n", "intent": "<hr>\n**Root Mean Squared Error** (RMSE): $$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "print (('Variance: ')+str(metrics.explained_variance_score(y_test,predictions)))\n", "intent": "**Variance**: R Squared\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "<h3>Predictions</h3>\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "<h3>Predictions</h3>\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "<h3>Single Decision Tree Predictions</h3>\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\n", "intent": "<h3>Random Forest Predictions</h3>\n"}
{"snippet": "pred_dec = grid_search_dec.predict(x)\npred_dec\n", "intent": "In decision tree, the most important features are gender and pclass_3.\n"}
{"snippet": "print(classification_report(y_test_log, predict_log))\n", "intent": "Using the classification report method we will be assesing the performance of the model\n"}
{"snippet": "y_pred= neuralClassifier.predict(X_test)\npred_train= neuralClassifier.predict(X_train)\nprint(\"accuracy score for test set: \"+ str( sk.metrics.accuracy_score(y_test, y_pred , normalize=True)) )\nprint(\"Accuracy score for training set: \" + str( sk.metrics.accuracy_score(y_train, pred_train,normalize=True)) )\n", "intent": "Lets use \"Neural classifer and observe the accuracy\"\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross = (cross_val_score(ForestClassifier, X_train, y_train, cv=10))\nnp.mean(cross)\n", "intent": "We perform cross validation on our model. The model still reports mean cross validation score.\n"}
{"snippet": "y_pred= randomNeuralNet.predict(X_test)\npred_train= randomNeuralNet.predict(X_train)\nprint(\"accuracy score for test set: \"+ str( sk.metrics.accuracy_score(y_test, y_pred , normalize=True)) )\nprint(\"Accuracy score for training set: \" + str( sk.metrics.accuracy_score(y_train, pred_train,normalize=True)) )\n", "intent": "Accuracy of our model after randomized search for parameters\n"}
{"snippet": "logit_predict = logit_model.predict(X_test_tfidf)\nroc_auc_1 = roc_auc_score(y_test_bin, logit_predict)\nprint('Model 1. Standard logit model using tokenized only vectorization of features.')\nprint('ROC AUC score 1: ', roc_auc_1)\nprint('  ')\nprint ('Correctly predicts sample as:')\nprint('The movie is really bad, I will never recommend it',' ||| The movie was sad, but really great ending.  I will see it again')\nprint(logit_model.predict(vectorizer.transform(['The movie is really bad, I will never recommend it.','The movie was sad, but really great ending.  I will see it again.'])))\n", "intent": "For more info on this, see: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = model.predict(test_Ximp)\nprint(accuracy_score(test_y, predictions))\n", "intent": "Now that it is done, its time to predict using our model and determine the accuracy of our model.\n"}
{"snippet": "cross_val_score(clf, X_test, y_test, cv=5)\n", "intent": "This is where the course content stops\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    print(len(prediction))\n    prediction = np.argmax(prediction, axis = 2)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "x_test = np.array(['i will be successful'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "scores = []\nhus = [10, 25, 50, 100, 250, 500, 1000]\nsteps = np.arange(100, 2001, 100)\nfor step in steps:\n    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n                                        hidden_units=[10],\n                                        n_classes=3)\n    classifier.train(input_fn=train_input_fn, steps=step)\n    accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n    scores.append(accuracy_score)\n", "intent": "Try some different steps and hidden_units values to see the effect on the accuracy \n"}
{"snippet": "y_hat = svm_linear.predict(X)\nsvm_score = accuracy_score(y_hat, y)\nprint(svm_score)\n", "intent": "Use .predict to get prediction and accuracy_score to measure the score.\n"}
{"snippet": "y_hat = rbf_svm.predict(X)\nsvm_score = accuracy_score(y_hat, y)\nprint(svm_score)\n", "intent": "Use .predict and accuracy_score to compute svm score.\n"}
{"snippet": "trump_score = trump_model.evaluate(trump_input,trump_output, verbose=0)\nprint('trump score : ', trump_score)\nhilary_score = hilary_model.evaluate(hilary_input,hilary_output, verbose=0)\nprint('hilary score : ', hilary_score)\nobama_score = obama_model.evaluate(obama_input,obama_output, verbose=0)\nprint('obama score : ', obama_score)\nseinfeld_score = seinfeld_model.evaluate(seinfeld_input,seinfeld_output, verbose=0)\nprint('seinfeld score : ', seinfeld_score)\ngadot_score = gadot_model.evaluate(gadot_input,gadot_output, verbose=0)\nprint('gadot score : ', gadot_score)\n", "intent": "We do this in order to report the model accuracy on test data for each model\n"}
{"snippet": "from sklearn.metrics.cluster import adjusted_rand_score\nscore = adjusted_rand_score(y, y_cluster_kmeans)\nscore\n", "intent": "<a id='Comparing'></a>\n----\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake).data.numpy()\n    g_loss = ls_generator_loss(score_fake).data.numpy()\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss_per_layer = []\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        layer_style_loss = torch.sum((gram - style_targets[i]) ** 2)\n        style_loss_per_layer.append(style_weights[i] * layer_style_loss)\n    return np.sum(style_loss_per_layer)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    layer_losses = []\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        layer_loss = tf.reduce_sum((gram - style_targets[i]) ** 2)\n        layer_losses.append(layer_loss * style_weights[i])\n    return tf.add_n(layer_losses)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(test['species_name'], preds, average='weighted')\n", "intent": "$$ F_1 = 2 * \\frac{p * r}{p + r} $$\np = precision = (\nr = recall = (\n"}
{"snippet": "X_new = [[3,4,4,2], [4,4,3,2]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "Ty = train_np.shape[0]\ny_pred = list()\nfor y in range(Ty):\n    example = np.reshape(train_np_expanded[y,:,:], (1,context_length,1))\n    y_pred.append(model.predict(example).tolist()[0][0])\n", "intent": "Get the signal by giving the real-data sequence and predicting the next value.\n"}
{"snippet": "y_prediction = predict(X, W, b)\ncost_op = tf.reduce_sum(tf.pow(y_prediction-y, 2))\noptimizer = tf.train.RMSPropOptimizer(learning_rate=alpha).minimize(cost_op)\n", "intent": "Define Tensorflow operator for optimizing the cost.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(test['targets'], predicted))\n", "intent": "Now 84%! We can more information on accuracy from scikit-learn metrics tools.\n"}
{"snippet": "y_pred=classifier.predict(X_test)\ny_pred=(y_pred > 0.5)\n", "intent": "Predicting the Test result\n"}
{"snippet": "loss = model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1, sample_weight=None, steps=None)\nprint(loss)\n", "intent": "**Evaluate model on test data**\n"}
{"snippet": "scores = model.evaluate(x_test, y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "**4. Evaluate the model on Test data.**\n"}
{"snippet": "model.predict(0.07)\n", "intent": "We can then use the model to predict the **expected value** of the target variable for a particular value of $bmi = 0.07$:\n"}
{"snippet": "sepal_test = iris_test['sepal_length'].values.reshape(-1,1)\npetal_test = iris_test['petal_length']\npredicted_petal = model.predict(sepal_test)\n", "intent": "Now, let's use this model to predict the petal length using the sepal length from the test data:\n"}
{"snippet": "sum(model.predict(lal) == lalonde.treat)/len(lal)\n", "intent": "Here we check the performance of our model, we have an accuracy of 82% which is quite good for a very simple logistic regression.\n"}
{"snippet": "sum(model.predict(pa_reg) == pa.UN)/len(pa_reg)\n", "intent": "Here we check the performance of our model, we have an accuracy of 89% which is quite good for a very simple logistic regression.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop('median_house_value', axis=1)\ny_test = strat_test_set['median_house_value'].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "* Get the predictors and labels from test set, run pipeline and evaluate the model.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_2, cv=3, scoring='accuracy')\n", "intent": "* Evaluate the model using K-fold cross-validation with 3 folds:\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never2Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "* Accuracy seems good, but so does a classifier that always picks not 2 because 90% of the images are not 2:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(precision_score(y_train_2, y_train_pred))\nprint(recall_score(y_train_2, y_train_pred))\n", "intent": "* Scikit provides a bunch of functions for calculating said scores: ``precision_score`` and ``recall_score``.\n"}
{"snippet": "log_preds = learn.predict()\nlog_preds.shape\n", "intent": "* We can get predicitions for the validation set using the `predict` method of the `learn` object.\n  * Predictions are in log scale.\n"}
{"snippet": "log_preds, y = learn.TTA()\nprobs = np.mean(np.exp(log_preds), axis=0)\naccuracy_np(probs, y), log_loss(y, probs)\n", "intent": "* Didn't try unfreezing, because the dog breed set is actually a subset of ImageNet, so the conv layers are likely to be optimial.\n"}
{"snippet": "from sklearn.metrics import fbeta_score\nimport warnings\ndef f2(preds, targs, start=0.17, end=0.24, step=0.01):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return max([fbeta_score(targs, (preds>th), 2, average='samples')\n                    for th in np.arange(start,end,step)])\n", "intent": "* In the dogs & cats and dogbreeds datasets, each image is assigned to a single class:\n"}
{"snippet": "learn.save(f'{sz}')\nlearn.sched.plot_loss()\n", "intent": "* Can see the different cycles when plotting the loss:\n  * Note that each time it finds something better than the time before.\n"}
{"snippet": "ldclf = LinearDiscriminantAnalysis()\ncv_ld = cross_validation.cross_val_score(ldclf, dummies, target, cv=10)\nprint(\"Overall Accuracy of LDA: %0.2f (+/- %0.2f)\" % (cv_ld.mean(), cv_ld.std() * 2))\n", "intent": "linear discriminant analysis (LDA)\n"}
{"snippet": "prediction_artificial_df=kmeans1.predict(artificial_df)\n", "intent": "We predict the class of each datapoint\n"}
{"snippet": "encoded = encoder_model.predict(x)\n", "intent": "To use the encoder, simply pass our input data to its predict method:\n"}
{"snippet": "metrics.accuracy_score(highCrime, DT_prediction)\n", "intent": "1 b i.What are the training accuracy, precision, and recall for this tree \n"}
{"snippet": "def style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_height * img_width\n    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n", "intent": "The style loss is then the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.\n"}
{"snippet": "knn_score = r2_score(y_true,y_pre_knn)\nlinear_score=r2_score(y_true,y_pre_linear)\nridge_score=r2_score(y_true,y_pre_ridge)\nlasso_score=r2_score(y_true,y_pre_lasso)\ndecision_score=r2_score(y_true,y_pre_decision)\nsvr_score=r2_score(y_true,y_pre_svr)\ndisplay(knn_score,linear_score,ridge_score,lasso_score,decision_score,svr_score)\n", "intent": "Now we can get the accuracy score of each model.\n"}
{"snippet": "kfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Results: %.2f (%.2f) cosine_proximity\" % (results.mean(), results.std()))\nknn_score = r2_score(y_true,y_pre_knn)\nlinear_score=r2_score(y_true,y_pre_linear)\nridge_score=r2_score(y_true,y_pre_ridge)\nlasso_score=r2_score(y_true,y_pre_lasso)\ndecision_score=r2_score(y_true,y_pre_decision)\nsvr_score=r2_score(y_true,y_pre_svr)\ndisplay(knn_score,linear_score,ridge_score,lasso_score,decision_score,svr_score)\n", "intent": "The Results of deep learning regression are as folowing:\n"}
{"snippet": "kfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(estimator, X, y, cv=kfold)\nprint(\"Results: %.2f (%.2f) cosine_proximity\" % (results.mean(), results.std()))\n", "intent": "The Results of deep learning regression are as folowing:\n"}
{"snippet": "kfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Results: %.2f (%.2f) cosine_proximity\" % (results.mean(), results.std()))\n", "intent": "The Results of deep learning regression are as folowing:\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(data_X_test) - data_y_test) ** 2))\nprint('Coefficient of determination: %.2f' % regr.score(data_X_test, data_y_test))\n", "intent": "Display the results\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(data_X_test) - data_y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(data_X_test, data_y_test))\n", "intent": "Display the results\n"}
{"snippet": "pred = kmeans_model.predict(tf_idf)\n", "intent": "**Initializing cluster weights**\nWe will initialize each cluster weight to be the proportion of documents assigned to that cluster by k-means above.\n"}
{"snippet": "x_test = np.array(['i hate'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "accuracy_list = list()\nfor ts in test_split_list:\n    temp_scaled = scale_X.transform(ts[0])\n    res = mlp.predict(X=temp_scaled, seed=seed)\n    accuracy = (res[\"y_pred_cls\"][:,0] == ts[1][:,0]).sum() / float(len(res[\"y_pred_cls\"]))\n    accuracy_list.append(accuracy)\nprint(\"The average accuracy on imputed test sets: {}\".format(np.mean(accuracy_list)))\n", "intent": "Predict on each scaled test set and obtain average accuracy\n"}
{"snippet": "lof.contamination = 0.02 / 100\ninliers = lof.fit_predict(X_scaled)\n", "intent": "- LOF start at 0.02 % of datapoits and we are getting elbow shape at 0.02%\n- choose contamination = 0.02%\n"}
{"snippet": "labels1 = np.random.randint(0, 4,len(labels))\nlabels2 = [2]*len(labels)\nprint('RANDOM CLASS',accuracy_score(labels,labels1))\nprint('HIGH CLASS',accuracy_score(labels,labels2))\n", "intent": "**BASELINE PREDICTION**\n** WHEN THE LABELS ARE TAKEN AS RANDOM OR MAJORITY CLASS **\n"}
{"snippet": "nns = [3,4,5];\nclusterers = [hdbscan.RobustSingleLinkage(cut=0.125, k=(2**n)) for n in nns];\nclusterLabels = [clusterer.fit_predict(U) for clusterer in clusterers];\n", "intent": "Here's the meat of the code. Doing HDBScan on the signatures to try and tease out some groupings.\n"}
{"snippet": "loaded_model.predict(\"Although i told u dat i'm into baig face watches now but i really like e watch u gave cos it's fr u. Thanx 4 everything dat u've done today, i'm touched...\")\n", "intent": "Ensure that the results for loaded model are same as for the pre-saved model\n"}
{"snippet": "velocity_dnn = network.predict(X)\n", "intent": "Make predictions for test input samples\n"}
{"snippet": "bathy_dnn = network.predict(X)\n", "intent": "Make predictions for test input samples\n"}
{"snippet": "results = np.zeros((len(pred_river.ravel()), n_ens))\nfor i in range(n_ens):\n    X_r, Y_r = model_data.gen_test_data(vel=vel_red, dep=prof_red, noise=realization[:, i])\n    bathy_ens = network.predict(X_r)\n    results[:, i] = model_data.post_process(bathy_ens).ravel()\n", "intent": "Make predictions for all ensemble members\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nmodel_mse =  np.mean(-cross_val_score(reg, housing_features, \n                                      housing_prices, \n                                      cv=10, scoring='mean_squared_error'))\nprint model_mse\n", "intent": "**Answer: ** Let's get a cross-validation error for the model:\n"}
{"snippet": "baseline = np.repeat(np.mean(y_test), len(y_test))\nprint(f'Root Mean Square Error of baseline model is: \\\n${int(mean_squared_error(y_test, baseline)**0.5)}.')\n", "intent": "The RMSE is quite good, and similar for the training and test data. \n"}
{"snippet": "np.allclose(pred, logit.predict(X))\n", "intent": "Does this match predictions from sklearn?  Yes--\n"}
{"snippet": "logit.predict_proba(X)[:10]  \n", "intent": "The output of `predict_proba(X)` and `predict_log_proba(X)` are actually 2d arrays:\n"}
{"snippet": "np.allclose(log_odds, logit.predict_log_proba(X)[:, 1])\n", "intent": "Lastly, note that `predict_log_proba(X)` is _not_ a log odds; it is a log of probability:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt \nrms = sqrt(mean_squared_error(valid.Count,y_hat.naive))\nprint(rms)\n", "intent": "We will now calculate RMSE to check the accuracy of our model on validation data set.\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "- New observation are called \"out-of-sample\" data\n- uses the information it learned during model training process\n"}
{"snippet": "print 'The R$^2$ = {:.2f}'.format(r2_score(pred_good, y_test_good))\nprint 'The  RMSE = {:.2e}.'.format(mean_squared_error(pred_good, y_test_good))\n", "intent": "There are 247 examples for testing.\n"}
{"snippet": "print clf.predict(digits.data[-1:]) == digits.target[-1]\n", "intent": "Lets see if the one left out is predicted correctly now.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores2 = cross_val_score(regr2, X_train, y_train, cv=5, scoring = \"roc_auc\")\nscores2\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "print(\"Accuracy of predicting from decision tree trained on NN: {0:.2g}\".format(np.mean(clf_data_to_NN.predict(Xval_unscaled) == Yval)))\nprint(\"\\n(c.f. Accuracy of NN itself: {0:.2g})\".format(NN_accuracy))\n", "intent": "If we want to ensure we have an \"explanation\" for all predictions we can predict on the data directly from the decision tree trained on the NN.\n"}
{"snippet": "unique_rows, full_or_data, full_info = helper.process_single_cell_data_file(raw_scs_data)\nprint \"number of clusters  | Silhouette score\"\nfor i in range(2,max_number_of_clusters):\n    bmm = BMM(k_clusters=i\n                 , unique_rows=unique_rows, full_data_dict=full_or_data, full_info=full_info,\n              number_of_iterations=number_of_iterations)\n    data, predicted_labels = bmm.do_clustering()\n    predicted_labels = np.asarray(predicted_labels)\n    distance_matrix = np.matrix(helper.find_distance_matrix(unique_rows))\n    print str(i).ljust(20) + str(silhouette_score(distance_matrix, predicted_labels, metric=\"precomputed\"))\n", "intent": "Main method to create BMM instance and perform clustering using different parameters\n"}
{"snippet": "evaluate(test_labels, y_pred_test)\n", "intent": "**Test evaluation**\n"}
{"snippet": "print(linearkernel.predict(predict))\n", "intent": "Now we can see the results on the same data we had above:\n"}
{"snippet": "VGG19_predictions = [np.argmax(model_vgg19.predict(np.expand_dims(vggfeature, axis=0))) for vggfeature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "results = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg) \n", "intent": "Spotcheck each algorithm to find Classification Accuracy and Standard Deviation\n"}
{"snippet": "y_pred_fraud_record = lr.predict(X_fraud_record.values)\ncnf_matrix = confusion_matrix(y_fraud_record, y_pred_fraud_record)\nnp.set_printoptions(precision=2)\nprint(cnf_matrix)\nplot_confusion_matrix(cnf_matrix, ['0', '1'], title = 'Confusion Matrix - Fraud Records')\npr, tpr = show_data(cnf_matrix, print_res = 1);\n", "intent": "Applying the model on fraud data only and ploting the congusion matrix. \n"}
{"snippet": "y_pred_all = lr.predict(X.values)\ncnf_matrix = confusion_matrix(y, y_pred_all)\nnp.set_printoptions(precision=2)\nprint(cnf_matrix)\nplot_confusion_matrix(cnf_matrix, ['0', '1'], title = 'Confusion Matrix - Entire Dataset')\npr, tpr = show_data(cnf_matrix, print_res = 1);\n", "intent": "Applying the model on entire dataset and plotting the confusion matrix. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for lay_num in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[lay_num]])\n        A = style_targets[lay_num]\n        loss += style_weights[lay_num]*tf.reduce_sum((gram - A)**2)\n    return(loss)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "invs = (max(happy_index['score']) - min(happy_index['score']))\ngbr_error_scaled = - happy_index[\"score\"] + (gbr_cv.predict(X)*invs + min(happy_index['score']))\nhappy_index[\"delta\"] = gbr_error_scaled \nhappy_index[\"pred\"] = gbr_cv.predict(X) * invs + min(happy_index['score'])\nprint(happy_index.head(5))\n", "intent": "the data in the origonal space\n"}
{"snippet": "predictions=model.predict(X_test)\n", "intent": "<a id='predict1'></a>\n"}
{"snippet": "test_predictions = loaded_model.predict(X_test)\n", "intent": "<a id='predict2'></a>\n"}
{"snippet": "y_pred = (y_proba_val >= 0.5)\nprecision_score(y_test, y_pred)\n", "intent": "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:\n"}
{"snippet": "transfer_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in features_by_network[active_network]['test']]\ntest_accuracy = 100*np.sum(np.array(transfer_predictions)==np.argmax(test_targets, axis=1))/len(transfer_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prefix = \"I have lots of\".split()\nnoun = \"toys\"\nadjectives = [\"small\", \"green\", \"plastic\"]\ninputs = []\nfor adjs in itertools.permutations(adjectives):\n  words = prefix + list(adjs) + [noun]\n  inputs.append(words)\nload_and_score(inputs, sort=True)\n", "intent": "Same thing here for 'round'.\n"}
{"snippet": "y_pred = lr.predict(X_test)\ny_pred\n", "intent": "<p>Making predictions with our trained model onto our split test set</p>\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob))\n", "intent": "AUC is the percentage of the ROC plot that is underneath the curve\n"}
{"snippet": "sales_predictions = linreg.predict(features_test)\nsales_predictions[0:3]   \n", "intent": "**How do we interpret the TV coefficient (0.0466)?**\nFor every 1 TV, sales increase .0466 units\n"}
{"snippet": "from sklearn import metrics\n(10 + 0 + 20 + 10) / 4.\nmetrics.mean_absolute_error(labels_test, sales_predictions)\n", "intent": "**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n"}
{"snippet": "(10**2 + 0**2 + 20**2 + 10**2) / 4.\nmetrics.mean_squared_error(labels_test, sales_predictions)\n", "intent": "**Mean Squared Error (MSE)** is the mean of the squared errors:\n"}
{"snippet": "import numpy as np\nnp.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.)\nnp.sqrt(metrics.mean_squared_error(labels_test, sales_predictions))\n", "intent": "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors\n"}
{"snippet": "print(\"Train mean square error:\", np.mean((y_train - lm.predict(x_train)) ** 2))\nprint(\"Test mean square error:\", np.mean((y_test - lm.predict(x_test)) ** 2))\n", "intent": "We try to train our model based on all the ACT scores as we found out that by involving all the ACT scores, we can get a lower mean square error.\n"}
{"snippet": "print (fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response))\n", "intent": "**b.** Now that you've added the categorical data, let's see how it works with a linear model!\n"}
{"snippet": "clf.predict(X_test[0:1]).tolist()[0]\n", "intent": "For example, we can predict the class of the first point in the test set:\n"}
{"snippet": "gnb.predict_proba(X)\n", "intent": "You can get the probability estimates for the input vector X.\n"}
{"snippet": "data_missing_sun_hours['Sun_level_pred_knn'] = neigh.predict(test_data_scaled)\ndata_missing_sun_hours\n", "intent": "Now let's predict the sun_level using both KNN classifier and Naive Bayes classifier.\n"}
{"snippet": "auc = roc_auc_score(y_test, best_rfc.predict_proba(X_test)[:,1])\nprint (\"AUC Score: \", round(auc,3))\n", "intent": "<hr style=\"height:3px\">\n"}
{"snippet": "from sklearn import cross_validation\naccuracy_scores = cross_validation.cross_val_score(best_rfc, X, y, cv=10)\nauc_scores = cross_validation.cross_val_score(best_rfc, X, y, cv=10, scoring='roc_auc')\n", "intent": "<hr style=\"height:3px\">\n"}
{"snippet": "print(\"Baseline Accuracy: \",round(accuracy_score(X.y, np.zeros(X.shape[0]))*100,1))\nprint(\"Baseline AUC: \",roc_auc_score(X.y, np.zeros(X.shape[0])))\n", "intent": "<hr style=\"height:1px\">\n+ Predict all observations did not cancel. \n+ Set all y_hat equal to 0.\n+ This is the most common outcome.\n"}
{"snippet": "print(\"AUC:\",roc_auc_score(y, best_model.oob_prediction_))\n", "intent": "<hr style=\"height:2px\">\n"}
{"snippet": "encode_sample = encoder.predict(x_train[0:1])\n", "intent": "We encode the first image in the training dataset as an example.\n"}
{"snippet": "decode_sample = decoder.predict(encode_sample)\n", "intent": "Visualize the decoded image from encoded message:\n"}
{"snippet": "enc_images = encoder.predict(images.reshape(len(images),28,28,1)).reshape(len(images),g_output_size)\n", "intent": "Encode the dataset and forge into the size of input\n"}
{"snippet": "Charlie_predictions = [np.argmax(Charlie_model.predict(np.expand_dims(feature, axis=0))) \n                               for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(Charlie_predictions)==np.argmax(test_targets, axis=1)\n                          )/len(Charlie_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, predictions)\nprint('prediction accuracy: {}'.format(acc))\n", "intent": "Here, we first calculate the accuracy of this prediction which is the ratio of the correct predictions\nover the total number of predictions.\n"}
{"snippet": "print(\"Accuracy Score: %.2f\" % accuracy_score(y_test, predictions))\n", "intent": "The accuracy score is either the fraction (default) or the count (normalize=False) of correct predictions.\n"}
{"snippet": "print(\"Accuracy Score: %.2f\" % accuracy_score(y_test, y_pred))\n", "intent": "The accuracy score is either the fraction (default) or the count (normalize=False) of correct predictions.\n"}
{"snippet": "print(\"Accuracy Score: %.2f\" % accuracy_score(Y_test, predictions))\n", "intent": "The accuracy score is either the fraction (default) or the count (normalize=False) of correct predictions.\n"}
{"snippet": "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n", "intent": "The harmonic mean between homogeneity and completeness.\nScore between 0.0 and 1.0.\n* 1.0 stands for perfectly complete labeling.\n"}
{"snippet": "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n", "intent": "Shows how well defined the clusters are.\nScores\n* 1: Best\n* 0: indicates overlapping clusters\n* -1: Worst\n"}
{"snippet": "pred = regressor.predict(np.array(lstm_inp_stacked[0:300]))\nprint(\"PREDICTION\")\nfor vec in pred:\n    print(ix_to_char[np.where(vec == max(vec))[0][0]], end=\"\")\nprint(\"\\nACTUAL\")\nfor vec in lstm_inp_y[:300]:\n    print(ix_to_char[np.where(vec == max(vec))[0][0]], end=\"\")\n", "intent": "Just to evaluate model, I predict 300 characters from the training data itself.\n"}
{"snippet": "def recolor_images(reshaped_, center_colors, kmeans):\n    reshaped_ = reshaped_.copy()\n    nn_output = np.empty((reshaped_.shape[0], 1024))\n    print(nn_output.shape)\n    for index in range(reshaped_.shape[0]):\n        new_colors = kmeans.predict(reshaped_[index])\n        nn_output[index] = new_colors\n        j = center_colors[new_colors]\n        reshaped_[index] = j\n    return np.uint8(reshaped_), nn_output\n", "intent": "replacing colors in pictures\n"}
{"snippet": "regressor.load_weights(\"weights.best.hdf5\")\nprint(\"Best train loss :\", regressor.evaluate(nn_input, nn_output, verbose=0))\nprint(\"Best test loss  :\", regressor.evaluate(nn_input_test, nn_output_test, verbose=0))\n", "intent": "We load the best weights into the regressor and then evaluate on train and test accuracy\n"}
{"snippet": "scores = cross_val_score(pipeline,             \n                         df['message'],        \n                         df['label_binary'],   \n                         cv = 10,              \n                         scoring = 'accuracy', \n                         n_jobs=-1,            \n                         )\nprint scores\nprint '---'\nprint scores.mean(), scores.std()\n", "intent": "For this analysis I use cross-validation to test the stability of the model with different subsets of training data.\n"}
{"snippet": "def logistic(z):\n    return 1 / (1+np.exp(-z))\ndef nn(x,w):\n    return logistic(x.dot(w.T))\ndef nn_predict(x,w):\n    return np.around(nn(x,w))\ndef cost(y, t):\n    return - np.sum(np.multiply(t,np.log(y)) + np.multiply((1-t),np.log(1-y)))\n", "intent": "$\\sigma(z)=\\frac{1}{1+e^{-z}}$\n$\\xi(t_i,y_i) = -\\sum^{n}_{i=1} [t_i log(y_i) + (1 - t_i) log (1 - y_i)]$\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, ypred)\n", "intent": "Check accuracy score\n"}
{"snippet": "from sklearn import metrics\npred = model.predict([train_comment_seq_pad], batch_size=1024, verbose=1) \ny = train['identity_hate']\nfpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\nmetrics.auc(fpr, tpr)\n", "intent": "LB AUC: .9755\nThis is better than all of the previous models I tried including LSTM model1 above and logistic regression model in Part1.\n"}
{"snippet": "name = 'XGB'\nkfold = KFold(n_splits=10, random_state=seed)\ncv_results = cross_val_score(model, X_test, y_test, cv=kfold, scoring='accuracy')\nmsg = \"{}: {} ({})\".format(name, floatingDecimals(cv_results.mean()*100,2), floatingDecimals(cv_results.std()*100,2))\nprint(msg)\n", "intent": "**Make prediction on Test set**\n"}
{"snippet": "test_predictions = [np.argmax(XceptionModel.predict(np.expand_dims(feature,axis=0))) for feature in test_Xception]\naccuracy = 100*np.sum(np.array(test_predictions)==np.argmax(test_targets,axis=1))/len(test_predictions)\nprint(str(accuracy)+'%')\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf = model[winner]\nres = clf.predict(X_new)\nmt = confusion_matrix(y, res)\nprint(\"False positive rate : %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\nprint('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))\n", "intent": "Calculating the False positive and negative on the dataset\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ndef cross_validation_score(clf,training_set,n_folds = 5,return_score = False):\n    X,y = training_set.iloc[:,:-1],training_set.iloc[:,-1]\n    scores = list(cross_val_score(clf,X,y,cv = n_folds))\n    scores = sum(scores)/len(scores)\n    print(\"- Cross validation score of %s\" % scores)\n    if return_score:\n        return scores\n", "intent": "We are going to need to perform cross validation to assess the validity of our classifiers. \nFor that we are going to use the following function :\n"}
{"snippet": "accuracy_score(t_test,prediction)\n", "intent": "We can reach a **low accuracy of 92.2%** with a Logistic Regression from Sklearn\n"}
{"snippet": "accuracy_score(t_test,prediction)\n", "intent": "The **accuracy is not better** than sklearn's but it is **way faster**\n"}
{"snippet": "accuracy_score(t_test,prediction)\n", "intent": "A simple CNN model can **reach a high 99.2% accuracy**\n"}
{"snippet": "y_test_pred = logreg.predict(test)\n", "intent": "We predict the second half:\n"}
{"snippet": "withPT_predictions = [np.argmax(withPT_model.predict(np.expand_dims(feature, axis=0))) for feature in test_PT]\ntest_accuracyPT = 100*np.sum(np.array(withPT_predictions)==np.argmax(test_targets, axis=1))/len(withPT_predictions)\nprint('With pre trained CNN model %s, test accuracy: %.4f%%' % (ptCNN,test_accuracyPT))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "**Predicting Test Data**\nNow that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "predictions = [ridge_model.predict(i)[0] for i in testa250_dm]\n", "intent": "We use the ridge model to predict on our test data, then compare the predictions to the actual results.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50Data]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\nt=time.time()\nn_predict = 20\nprint('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\nprint('For these',n_predict, 'labels: ', y_test[0:n_predict])\nt2 = time.time()\nprint(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n", "intent": "I checked the score of the SVC by printing test accuracy and predicted the labels for 20 samples.\n"}
{"snippet": "print('Test Accuracy of Neural Net classifier = ', round(clf.score(X_test, y_test), 4))\nt=time.time()\nn_predict = 20\nprint('My Neural Net classifier predicts: ', clf.predict(X_test[0:n_predict]))\nprint('For these',n_predict, 'labels: ', y_test[0:n_predict])\nt2 = time.time()\nprint(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with Neural Net classifier')\n", "intent": "I checked the score of the Neural Net Classifier by printing test accuracy and predicted the labels for 20 samples.\n"}
{"snippet": "score = final_model.evaluate(x_test, y_test, verbose=0,)\nprint(\"Accuracy: \", score[1])\n", "intent": "Lastly we can evaluate this model on the test set just to double check that the model is still performing\n"}
{"snippet": "def rmse(prediction, ground_truth):\n    prediction = prediction[ground_truth.nonzero()]\n    ground_truth= ground_truth[ground_truth.nonzero()]\n    return sqrt(mean_squared_error(prediction, ground_truth))\n", "intent": "We only want to consider the error for movies that actually have ratings in the test dataset, so we filter everything else out using .nonzero()\n"}
{"snippet": "print(f'SGD: {log_loss(sgd_predict)}')\nprint(f'Logistic Regression: {log_loss(lr_predict)}')\nprint(f'LinearSVC: {log_loss(linear_svc_predict)}')\nprint(f'Random Forest: {log_loss(rf_predict)}')\n", "intent": "The lower the better for `log_loss`...\n"}
{"snippet": "print(f'SGD: {roc_auc_score(sgd_predict)}')\nprint(f'Logistic Regression: {roc_auc_score(lr_predict)}')\nprint(f'LinearSVC: {roc_auc_score(linear_svc_predict)}')\nprint(f'Random Forest: {roc_auc_score(rf_predict)}')\n", "intent": "The higher the better for `roc_auc_score`...\n"}
{"snippet": "uber_pred = model.predict(X_test) \n", "intent": "Lets start with the errors in prediction of training sets.\n"}
{"snippet": "svc_prediction = svc_model.predict(X_test)\n", "intent": "Now let's predict using the trained model. Call `predict` on `X_test`, and print the `confusion_matrix` and the `classification_report`.\n"}
{"snippet": "accuracy = np.round(accuracy_score(y_test1, rfc_predictions), decimals=2)\naccuracy\n", "intent": "Now, call `accuracy_score` on your test data and your predictions. Print these scores using `np.round` and 2 decimal places.\n"}
{"snippet": "rfe_prediction = rfe_model.predict(X_test1)\n", "intent": "Finally, let's evaluate our new reduced feature set model on our test data! You can call `.predict` directly on your `rfe_model`.\n"}
{"snippet": "rfe_accuracy = accuracy_score(y_test1, rfe_prediction)\nrfe_accuracy\n", "intent": "Finally, `print` the `accuracy_score` on your `predictions`.\n"}
{"snippet": "proba_prediction = logReg.predict_proba(X_test[20].reshape(1,-1))\nproba_prediction\n", "intent": "You can see the classifier predicted the song at index 20 as `dance`. How confident was it? We use the `predict_proba` method to find out!\n"}
{"snippet": "song_index = 20\nprint(logReg.predict(X_test[song_index].reshape(1,-1))) \nprint(knn.predict(X_test[song_index].reshape(1,-1))) \nprint(svm.predict(X_test[song_index].reshape(1,-1))) \n", "intent": "Now: Let us look at the prediction each classifier has for the song at index `20`.\n"}
{"snippet": "voter.predict(X_test[20].reshape(1, -1))\n", "intent": "Let's look at the prediction for song at index 20 !\n"}
{"snippet": "trainingData = (None, None)\nclass LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = {}\n        self.losses['loss'] = []\n        self.losses['val_loss'] = []\n    def on_epoch_end(self, epoch, logs={}):\n        self.losses['loss'].append(self.model.evaluate(trainingData[0], trainingData[1], verbose=0))\n        self.losses['val_loss'].append(logs.get('val_loss'))\n", "intent": "Train nSplit times to find best convergence\n"}
{"snippet": "prediction = logModel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "accuracy_MultinomialNB_train = accuracy_score( df[msk]['Sentiment'],\n                                              train_pred_MultinomialNB)\naccuracy_MultinomialNB_test = accuracy_score( df[~msk]['Sentiment'],\n                                             test_pred_MultinomialNB)\nprint (\"Train Accuracy: \",accuracy_MultinomialNB_train,\"\\nTest Accuracy:\",\n       accuracy_MultinomialNB_test)\nconfusion_MultinomialNB_train=confusion_matrix(df[msk]['Sentiment'],\n                                               train_pred_MultinomialNB)\nconfusion_MultinomialNB_test=confusion_matrix(df[~msk]['Sentiment'],\n                                              test_pred_MultinomialNB)\n", "intent": "Accuracy and Confusion Matrix: \n"}
{"snippet": "accuracy_svm_linear_train= accuracy_score( df[msk]['Sentiment'],\n                                          train_pred_svm_linear)\naccuracy_svm_linear_test= accuracy_score( df[~msk]['Sentiment'],\n                                         test_pred_svm_linear)\nprint (\"Train Accuracy: \",accuracy_svm_linear_train,\n       \"\\nTest Accuracy:\",accuracy_svm_linear_test)\nconfusion_svm_linear_train=confusion_matrix(df[msk]['Sentiment'],\n                                            train_pred_svm_linear)\nconfusion_svm_linear_test=confusion_matrix(df[~msk]['Sentiment'],\n                                           test_pred_svm_linear)\n", "intent": "Accuracy and Confusion Matrix:\n"}
{"snippet": "accuracy_mlp_train= accuracy_score( df[msk]['Sentiment'],\n                                   train_pred_mlp)\naccuracy_mlp_test= accuracy_score( df[~msk]['Sentiment'],\n                                  test_pred_mlp)\nprint (\"Train Accuracy: \",accuracy_mlp_train,\n       \"\\nTest Accuracy:\",accuracy_mlp_test)\nconfusion_mlp_train=confusion_matrix(df[msk]['Sentiment'],\n                                     train_pred_mlp)\nconfusion_mlp_test=confusion_matrix(df[~msk]['Sentiment'],\n                                    test_pred_mlp)\n", "intent": "Accuracy and Confusion Matrix:\n"}
{"snippet": "accuracy_dtrees_train= accuracy_score( df[msk]['Sentiment'],\n                                      train_pred_dtrees)\naccuracy_dtrees_test= accuracy_score( df[~msk]['Sentiment'],\n                                     test_pred_dtrees)\nprint (\"Train Accuracy: \",accuracy_dtrees_train,\n       \"\\nTest Accuracy:\",accuracy_dtrees_test)\nconfusion_dtrees_train=confusion_matrix(df[msk]['Sentiment'],\n                                        train_pred_dtrees)\nconfusion_dtrees_test=confusion_matrix(df[~msk]['Sentiment'],\n                                       test_pred_dtrees)\n", "intent": "Accuracy and Confusion Matrix:\n"}
{"snippet": "accuracy_forest_train= accuracy_score( df[msk]['Sentiment'],\n                                      train_pred_forest)\naccuracy_forest_test= accuracy_score( df[~msk]['Sentiment'],\n                                     test_pred_forest)\nprint (\"Train Accuracy: \",accuracy_forest_train,\n       \"\\nTest Accuracy:\",accuracy_forest_test)\nconfusion_forest_train=confusion_matrix(df[msk]['Sentiment'],\n                                        train_pred_forest)\nconfusion_forest_test=confusion_matrix(df[~msk]['Sentiment'],\n                                       test_pred_forest)\n", "intent": "Accuracy and Confusion Matrix:\n"}
{"snippet": "residual = model.resid\nfitted = model.predict()\n", "intent": "Then we investigate the prediction result of this model on the testing set: \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_predict))\n", "intent": "Checking accuracy score of predicted value\n"}
{"snippet": "evaluate_result = estimator.evaluate(input_fn=my_input_func(\"iris_test2.csv\",False,4))\nprint(\"Evaluation Result\")\nprint()\nfor key in evaluate_result:\n     print(\"{} :{}\".format(key,evaluate_result[key]))\n", "intent": "Ok, so now we have a trained model. How can we evaluate how well it's performing? \nFortunately, every Estimator contains an evaluate method:\n"}
{"snippet": "def print_it(y_pred,y_actual,params):\n    rmse=mean_squared_error(y_pred,y_actual)\n    print \"RMSE: %f, params: %s\" % (rmse,', '.join(str(p) for p in params),)\n", "intent": "Functions that will be reused.\n"}
{"snippet": "class YourNameMethodName():\n    def __init__(self, params=None):\n        'If params is None the method is initialized with default values.'\n        pass\n    def predict(X):\n        pass\n    def learn(X,y):\n        pass\n", "intent": "Please notice the special names that you should use for your classes:\n"}
{"snippet": "pred_val = list(gbm.predict(test_df[predictors])[0])\ntrue_val = list(test_df[target])\nprediction_acc = np.mean(pred_val == true_val)\nprint(\"Prediction accuracy: \", prediction_acc)\n", "intent": "The most important features are perimeter_worst, concave_points_mean, radius_worst, concave_points_worst.\nLet's now use the model for prediction.\n"}
{"snippet": "preds = clf.predict(val_df[predictors])\n", "intent": "Let's now predict the **target** values for the **val_df** data, using **predict** function.\n"}
{"snippet": "preds = clf.predict(val_df[predictors])\n", "intent": "Let's now predict the **target** values for the **val_df** data, using predict function.\n"}
{"snippet": "roc_auc_score(val_df[target].values, preds)\n", "intent": "Let's calculate also the ROC-AUC.\n"}
{"snippet": "roc_auc_score(val_df[target].values, preds)\n", "intent": "We also calculate area under curve (receiver operator characteristic).\n"}
{"snippet": "preds = clf.predict(val_fa_df[predictors_f])\n", "intent": "Let's now predict the **target** values for the **val_df** data, using **predict** function.\n"}
{"snippet": "roc_auc_score(val_fa_df[target].values, preds)\n", "intent": "We also calculate area under curve (receiver operator characteristic)\n"}
{"snippet": "pred_val = (best_gbm.predict(test_df[predictors])[0]).as_data_frame()\ntrue_val = (test_df[target]).as_data_frame()\nprediction_auc = roc_auc_score(pred_val, true_val)\nprediction_auc\n", "intent": "<a href=\"\nLet's use the best model to predict the target value for the test data.\n"}
{"snippet": "predicted=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rf_predicted=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, rf_predicted))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print('ARI=',adjusted_rand_score(y, y_cl),sep='')\n", "intent": "Note here that the Setosa cluster is labeled with 0s in the target, whereas in the clustering results, it is labeled with 1s. \n"}
{"snippet": "Y_pred = clf['gboost'].predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test,Y_pred))\nfrom sklearn.metrics import confusion_matrix\nmatrix = confusion_matrix(Y_test,Y_pred)\nprint(matrix)\n", "intent": "As we can see that we are getting highest accuracy of 95.36 %.Not Bad.\nLet's explore some of the metrics with Gradient Boosting\n"}
{"snippet": "def predict(parameters, X):\n    A2, cache = forward_propagation(X, parameters)\n    predictions = np.where(A2 > 0.5, 1, 0)\n    return predictions\n", "intent": "Once there is a trained neural network, the final component is a function that can make predictions based on forward propagation.\n"}
{"snippet": "prediction = ['ham' if i == 0 else 'spam' for i in prediction]\naccuracy_score(y_test,prediction)\n", "intent": "But first it's necessary to convert data to the same notation:\n"}
{"snippet": "print(classification_report(y_test, prediction, target_names = [\"Ham\", \"Spam\"]))\n", "intent": "As show above, the prediction is alright. Accuracy score is the same as obtained using built-in sklearn library.\n"}
{"snippet": "acc_train = round(100*accuracy_score(clf.predict(xtrain), ytrain),2)\nacc_test = round(100*accuracy_score(clf.predict(xtest), ytest),2)\n", "intent": "<div class=\"alert alert-success\">\n<b>\n</div>\n"}
{"snippet": "classifier.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nconfusion_matrix(y_test,classifier.predict(X_test))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,logmodel.predict(X_test)))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "y_pred_ploy2 = poly_2.predict(X2) \ny_pred_ploy3 = poly_3.predict(X3) \n", "intent": "**Model Evaluation**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))\n", "intent": "Let's calculate accuracy score which is the most popular metric for classification models:\n"}
{"snippet": "preds_sub = lmodel.predict(df_test)\npreds_sub = np.exp(preds_sub)\npreds_sub\n", "intent": "Predicting for test data:\n"}
{"snippet": "preds_sub = rfmodel.predict(df_test)\npreds_sub = np.exp(preds_sub)\npreds_sub\n", "intent": "Predicting for test data:\n"}
{"snippet": "preds_sub = xg_reg.predict(df_test)\npreds_sub = np.exp(preds_sub)\npreds_sub\n", "intent": "Predicting for test data:\n"}
{"snippet": "total_frames = input_X.shape[1]\nselected_index = total_frames - ar_order \nprint (selected_index)\nX_subsample = input_X[:,selected_index:selected_index+2]\norig_X, predicted_X,orig_predicted,orig_recon = predict(X_subsample,A,predict_states=1)\nprint (orig_X.shape)\nprint (predicted_X.shape)\nprint (orig_predicted.shape)\n", "intent": "A one-step forecast is a forecast of the very next time step in the sequence from the available data used to fit the model.\n"}
{"snippet": "cm = confusion_matrix(y_test, voteclf2.predict(X_test), labels = class_names)\nsns.heatmap(cm, annot = True, fmt = \"d\", xticklabels = class_names, yticklabels = class_names)\n", "intent": "Voteclf2 showed the best performance above all!\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Vanila Model Test accuracy: %.4f%%' % test_accuracy)\ndog_breed_predictions_b = [np.argmax(model_batch_normal.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy_b = 100*np.sum(np.array(dog_breed_predictions_b)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions_b)\nprint('Batch Normalized Model Test accuracy: %.4f%%' % test_accuracy_b)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "x_test = test[['fat_100g']]\ny_test = test['nutrition_score_uk_100g']\npredicted = model.predict(x_test)\nmodel.score(x_test, y_test)\n", "intent": "Use the trained model to see how well it performs against the test data\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat == yn)\nprint(\"Accuracy: {0:5.5f}\".format(acc))\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "df[\"yhat\"] = gridsearch.best_estimator_.predict(df[[\"rooms\", \"baths\", \"sqft\"]])\ndf.head(10)\n", "intent": "Let's make predictions and compare to our data.\n"}
{"snippet": "clf.predict([[3., 3.] , [-3., -3.]])\n", "intent": "Observations\n- support vectors in the table and figure\n- perfect classification of training data\n- equal distance \n- no points within margins\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg, X_train, y_train, cv=5)\nprint(scores)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\nprint(\"test accuracy: %f\", test_acc)\nprint(\"test loss\", test_loss)\n", "intent": "Print out results for test data\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nlin_mae = mean_absolute_error(y_pred, y_test)\nprint('Liner Regression MAE: %.4f' % lin_mae)\n", "intent": "Our model was able to predict the value of every house in the test set within $6164 of the real price.\nCalculate mean absolute error (MAE)\n"}
{"snippet": "y_nbc_pred_prob = gnb.predict_proba(X_test)[:, 1]\ny_nbc_pred_prob\n", "intent": "ROC curve can help you to choose a threshold that balances sensitivity and specificity in a way that makes sense for your particular context\n"}
{"snippet": "logistic_model.predict(np.array([[11.2, 2, .10], [11., 1.5, .9], [12., 6., 1]]))\n", "intent": "An f1_score of .75 is not great, but for such a simple model with only three variables it's far better than a random guess. \n"}
{"snippet": "logistic_model.predict_proba(np.array([[11.2, 2, .10], [11., 1.5, .9], [12., 6., 1]]))\n", "intent": "This means the first 'sample' is a white wine, while both the second and third are predicted as white. \n"}
{"snippet": "def evaluate(X_data, y_data):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "First we will define some useful functions to help us train the model.\n"}
{"snippet": "loss_density = scipy.ndimage.filters.gaussian_filter(loss_in_year(16), 11, mode='nearest') * extent\nX_test = np.vstack((loss_density[idx1,idx2],\n                    road_density[idx1,idx2],\n                    protectedareas[idx1,idx2],\n                    treecover_at_year(target_year-1)[idx1,idx2])).transpose()\ny_pred = clf.predict_proba(X_test)[:,1]\nprediction = np.zeros(extent.shape)\nprediction[idx1,idx2] = y_pred\nprediction_smoothed = scipy.ndimage.filters.gaussian_filter(prediction, 7, mode='nearest') * extent\nimshow(prediction_smoothed)\n", "intent": "Have a look at the deforestation forecast, with the most recent data available\n"}
{"snippet": "test_features = vectorizer.transform(test_data_list)\ntest_features = test_features.toarray()\nrf_pred = rf.predict(test_features)\n", "intent": "We will now test our model. To do that, we will first create bag of words feature for the test data and than predict the sentiment using our model.\n"}
{"snippet": "auc = roc_auc_score(y_test, y_score)\nf1 = f1_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nauprc = average_precision_score(y_test, y_score)\nprint(\"AUC score: \", auc)\nprint(\"F1 score: \", f1)\nprint(\"Accuracy score: \", accuracy)\nprint(\"AUPRC score: \", auprc)\n", "intent": "Now, let's compare the predicted outcome against the true labels. and plot the ROC curve.\n"}
{"snippet": "unnormalized_preds = list(map(unnormalize_price, eng_predictions))\nprint(mean_absolute_error(unnormalized_preds, eng_test_df['price']))\n", "intent": "Amazing! Our feature engineering reduced the error by about 20%!\n"}
{"snippet": "sample_predict_df = location_df.sample(frac=0.1)\ncluster_labels = kmeans.predict(sample_predict_df)\ncluster_labels\n", "intent": "we can also use the trained model to fit just a sample set\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test loss: ', score[0])\nprint('Test accuracy: ', score[1])\n", "intent": "Time to use that test data and see how our model does!\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nprint(\"\\nMean Absolute Error on the train data\",mean_absolute_error(y_train, predict_train),\"\\n\")\nprint(\"Mean Absolute Error on the test data\",mean_absolute_error(y_test, predict_test),\"\\n\")\n", "intent": "The function 'mean_absolute_error' in Sklearn will help us measure the MAE by simply providing the expected and predicted values.\n"}
{"snippet": "print(\"\\nCalculating MAE with Y_train:\", np.sum(np.absolute((y_train - model.predict(X_train))))/len(y_train),\"\\n\")\nprint(\"Calculating MAE with X_test, Y_test:\", np.sum(np.absolute((y_test - model.predict(X_test))))/len(y_test),\"\\n\")\n", "intent": "If you want to understand the math behind MAE, we can easily see how it is calculated by using the formula code below:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"\\nMean Squared Error on the train data\",mean_squared_error(y_train, predict_train),\"\\n\")\nprint(\"Mean Squared Error on the test data\",mean_squared_error(y_test, predict_test),\"\\n\")\n", "intent": "The function 'mean_squared_error' in Sklearn will help us measure the MSE by again providing the expected and predicted values.\n"}
{"snippet": "print(\"Calculate MSE with Y_train:\", np.mean((y_train - model.predict(X_train)) ** 2),\"\\n\")\nprint(\"Calculate MSE with X_test, Y_test:\", np.mean((y_test - model.predict(X_test)) ** 2),\"\\n\")\n", "intent": "We can easily calculate MSE by using the formula code below:\n"}
{"snippet": "from sklearn.metrics import r2_score\nprint(\"R-Squared on train data: \",r2_score(y_train, predict_train),\"\\n\")\nprint(\"R-Squared on test data: \",r2_score(y_test, predict_test),\"\\n\")\n", "intent": "The function 'mean_absolute_error' in Sklearn will help us measure the MAE by simply providing the expected and predicted values.\n"}
{"snippet": "kfold = model_selection.KFold(n_splits=5, random_state=5)\nevaluation_metric = 'r2'\ncross_val_output = model_selection.cross_val_score(model, X_test, y_test, cv=kfold, scoring=evaluation_metric)\nprint(\"R-Squared: \\nmean = %.3f standard deviation = %.2f\" % (cross_val_output.mean(), cross_val_output.std()),\"\\n\")\n", "intent": "We need to specify to Sklearn fucntion cross_val_score that we want to measure the 'r2'. \n"}
{"snippet": "y_pred = nb_classifier.predict(x_test)\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\nprint('Naive Bayes Test Accuracy: %.5f' % accuracy)\n", "intent": "After training, we can use the trained `nb_classifier` to predict the labels on the test data and evaluate the accuracy.\n"}
{"snippet": "y_pred = log_model.predict(x_test)\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\nprint('Logistic Regression Test Accuracy: %.5f' % accuracy)\n", "intent": "After training, we can use the built `log_model` to predict the labels on the test data and evaluate the accuracy.\n"}
{"snippet": "nb_trump_pred = nb_classifier.predict(trump_tfidf)\nnb_pos_count = 0\nnb_neg_count = 0\nfor res in nb_trump_pred:\n    if res == 0:\n        nb_neg_count += 1\n    else:\n        nb_pos_count += 1\nprint(\"Positive: \", nb_pos_count, \"   Negative: \", nb_neg_count)\n", "intent": "Predict the sentiment of Trump's tweets using the Naive Bayes classifier.\n"}
{"snippet": "log_trump_pred = log_model.predict(trump_tfidf)\nlog_pos_count = 0\nlog_neg_count = 0\nfor res in log_trump_pred:\n    if res == 0:\n        log_neg_count += 1\n    else:\n        log_pos_count += 1\nprint(\"Positive: \", log_pos_count, \"   Negative: \", log_neg_count)\n", "intent": "Predict the sentiment of Trump's tweets using the Logistic Regression classifier.\n"}
{"snippet": "preds = rf.predict(test_x)\nprint(accuracy_score(test_y, preds))\n", "intent": "This will create the prediction vector based on the classifier created above. The accuracy of the classifier is also noted below. \n"}
{"snippet": "def classify_test(train_x, train_y, test_x, test_y, clf):\n    clf.predict(test_x)\n    return clf.predict_proba(test_x)\n", "intent": "For each prediction, the classifier keeps track of its confidence level in its prediction. This and its code are displayed below.\n"}
{"snippet": "test_score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test accuracy: ', test_score[1])\nprint('Test loss: ', test_score[0])\n", "intent": "We then can evaluate the model using the evaluate() function on the test data. The function will return the test loss and accuracy.\n"}
{"snippet": "predicted_classes = model.predict(x_test)\npredicted_classes = np.argmax(np.around(predicted_classes), axis = 1)\ny_labels = np.argmax(y_test, axis = 1)\nincorrect_labels = np.where(predicted_classes!=y_labels)[0]\ncorrect_labels = np.where(predicted_classes==y_labels)[0]\nprint(\"Incorrect labels: \", len(incorrect_labels))\nprint(\"Correct labels: \", len(correct_labels))\n", "intent": "Using model.predict(), we can see how many labels are predicted correctly.\n"}
{"snippet": "classifier.train(input_fn=lambda:input_fn_train(train_x, train_y, 100), steps=1000)\naccuracy = classifier.evaluate(input_fn=lambda:input_fn_evaluate(test_x, test_y, 100))\nprint('\\nTest set accuracy: {accuracy:0.4f}\\n'.format(**accuracy))\npredictions = classifier.predict(input_fn=lambda:input_fn_predict(test_x.tail(), 100))\ntemplate = ('\\nPrediction is \"{}\" ({:.2f}%), expectation is \"{}\"')\nfor prediction, expectation in zip(predictions, test_y.tail()):\n    predicted_class = prediction['class_ids'][0]\n    probability = prediction['probabilities'][predicted_class]\n    print(template.format(predicted_class, 100 * probability, expectation))\n", "intent": "Now we train the model and evaluate it on the test set, showing you the overall accuracy as well as prediction for some sample.\n"}
{"snippet": "e = model.evaluate(X_test, Y_test, verbose=1)\nprint(\"Least squared error on test data:\")\nprint(e)\n", "intent": "Now that our model is trained on the training set, we will use the test set that we generated above to evaluate the model.\n"}
{"snippet": "print(accuracy_score(test_labels, prediction)*100)\n", "intent": "Just as expected ,the accuracy rate is the same even with the package\n"}
{"snippet": "test = Adaboost(benchmark1,50)\nprint (\"train the model\")\ntest.train(trainX, trainY)\ntest.predict(trainX, trainY)\ntest.predict(testX, testY)\n", "intent": "After implementation, we can use the same data to train the model and get the accuracy to veify that the implementation is correcct.\n"}
{"snippet": "print(\"*************Start evaluating model...*************\")\nacc = model.evaluate(x=[test_X,test_X], y=test_Y, batch_size=BATCH_SIZE)\nprint(\"test loss: {}, test accuacy: {}\".format(acc[0], acc[1]))\n", "intent": "Keras provides a ```model.evaluate()``` method for model evaluating. Here we set the batch size the same value as ```model.fit()```.\n"}
{"snippet": "feature_extracted_train = model_V3.predict(inception_model_input_train)\n", "intent": "By using the predict function we get the features we will use for training a small CNN model for both the test data as well as train data. \n"}
{"snippet": "predicted = logisticRegr.predict(X_test)\nprint(logisticRegr.predict(X_test))\nprint(y_test.index)\n", "intent": "- Step III: Predict the result on test set.\n"}
{"snippet": "predictions = rf.predict(test_features)\n", "intent": "Predict the max temperature based on the model defined above.\n"}
{"snippet": "predictList=classifier.predict(X)\nresult = predictList.tolist()\nx= (y_validation==result)\naccuracy = sum(x)/len(y_validation)\nprint('Training accuracy of the model is:',accuracy*100,'%')\n", "intent": "To evaluate our classifier we first test it by calculating training accuracy, validating on the training set\n"}
{"snippet": "print('MINORITY CLASSES:')\nprint(recall_score(yIETest, ieModel.predict(xIETest), pos_label='E'))\nprint(recall_score(yNSTest, nsModel.predict(xNSTest), pos_label='S'))\nprint(recall_score(yFTTest, ftModel.predict(xFTTest), pos_label='T'))\nprint(recall_score(yPJTest, pjModel.predict(xPJTest), pos_label='J'), end='\\n\\n')\nprint('MAJORITY CLASSES:')\nprint(recall_score(yIETest, ieModel.predict(xIETest), pos_label='I'))\nprint(recall_score(yNSTest, nsModel.predict(xNSTest), pos_label='N'))\nprint(recall_score(yFTTest, ftModel.predict(xFTTest), pos_label='F'))\nprint(recall_score(yPJTest, pjModel.predict(xPJTest), pos_label='P'))\n", "intent": "Instead of accuracy, which is a bad metric for imbalance dataset, we will be using **recall** and **f1 score**, to measure our model performance.\n"}
{"snippet": "def predict(matrix, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = matrix.mean(axis=1)\n        ratings_diff = (matrix - mean_user_rating)\n        pred = mean_user_rating + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        pred = matrix.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n    return pred\nitem_prediction = predict(matrix_sparse.T, item_similarity, type='item')\nuser_prediction = predict(matrix_sparse.T, user_similarity, type='user')\n", "intent": "* item_prediction matrix dim = (num of users, num of items)\n* user_prediction matrix dim = (num of users, num of items)\n"}
{"snippet": "Xception_predictions = [np.argmax(X_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "X0 = vectorizer.transform([\"This movie is not remarkable, touching, or superb in any way\"])\nclf.predict_proba(X0)\n", "intent": "The classifier predicts that the movie is \"Rotten\" with 99.76% probability based on the quote.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import make_scorer\nrmsle_scorer = make_scorer(rmsle, greater_is_better=False)\nkf = KFold(n_splits=20, shuffle=True, random_state=42)\ncv_results = cross_val_score(clf, x_test, y_test.tolist(), cv=kf, scoring=rmsle_scorer)\n", "intent": "In this section the testing is splitte in different folds and we review the RMSLE in each one.\n"}
{"snippet": "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n", "intent": "**Metrics for regression**\n"}
{"snippet": "print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n", "intent": "**Metrics for classification**\n"}
{"snippet": "dict(zip(feature_names, np.round(m1.evaluate([X_test, X2_test], [y for y in  Y_test], batch_size=256)[-11:], 3)))\n", "intent": "Let's evaluate the model against the test set, the same that was used in evaluating the Perceptron tagger above.\n"}
{"snippet": "print('R^2:', r2_score(y_test, pred))\nprint('MAE: ', mean_absolute_error(y_test, pred))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(g_loss, g_loss_true)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'].tolist(), answers['logits_fake'].tolist(),\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L = 0\n    for layer, target, weight in zip(style_layers, style_targets, style_weights):\n        gram_feature = gram_matrix(feats[layer])\n        L += weight * tf.reduce_sum((gram_feature - target) ** 2) \n    return L\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(classification_report(y_train, lr.predict(scale_train_feats), digits = 4))\n", "intent": "To get an idea of overfitting, we look at the training scores:\n"}
{"snippet": "y_pred_nb = nb.predict(X_test_dtm)\n", "intent": "Predicting using Naive Bayes. Note that we are passing X_test_dtm, and not X_test.\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred_nb)\n", "intent": "Naive Bayes gives accuracy score of around 85%.\nWe will see that Logistic Regression gives better accuracy score, but it's slower.\n"}
{"snippet": "y_pred_lr = lr.predict(X_test_dtm)\n", "intent": "Predicting with Logistic Regression.\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred_lr)\n", "intent": "We get accuracy score of 92%, much higher than Naive Bayes.\nSo looks like the trade off is between speed and accuracy.\n"}
{"snippet": "clf = CV_rfc.best_estimator_\npredictions = clf.predict(X_test)\nprint(metrics.accuracy_score(y_test, predictions))\n", "intent": "Now we set a random forest with the best parameters of the grid search.\nWe train and and finally test the random forest.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntotal_population = tp + fp + tn + fn\nprint accuracy_score(y, predicted)\nprint float(tp + tn) / total_population\n", "intent": "**Calculate the accuracy using the confusion matrix cells.**\n- Validate that it is the same as `metrics.accuracy_score`\n"}
{"snippet": "from sklearn.metrics import recall_score\nprint recall_score(y, predicted) \nprint float(tp) / (tp + fn)\n", "intent": "**Calculate the recall with the confusion matrix cells.**\n- Validate that this is the same as `metrics.recall_score`\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint precision_score(y, predicted) \nprint float(tp) / (tp + fp)\n", "intent": "**Calculate the precision using the confusion matrix cells.**\n- Validate that this is the same as `metrics.precision_score`\n"}
{"snippet": "def evaluate_error(model: training.Model) -> np.float64:\n    pred = model.predict(x_test, batch_size = 32)\n    pred = np.argmax(pred, axis=1)\n    pred = np.expand_dims(pred, axis=1) \n    error = np.sum(np.not_equal(pred, y_test)) / y_test.shape[0]    \n    return error\n", "intent": "One simple way to evaluate the model is to calculate the error rate on the test set.\n"}
{"snippet": "try:\n    all_cnn_weight_file\nexcept NameError:\n    all_cnn_model.load_weights(ALL_CNN_WEIGHT_FILE)\nevaluate_error(all_cnn_model)\n", "intent": "Since two models are very similar to each other, it is expected that the error rate doesn't differ much.\n"}
{"snippet": "try:\n    nin_cnn_weight_file\nexcept NameError:\n    nin_cnn_model.load_weights(NIN_CNN_WEIGHT_FILE)\nevaluate_error(nin_cnn_model)\n", "intent": "This is more simple than the other two, so the error rate is a bit higher.\n"}
{"snippet": "evaluate_error(ensemble_model)\n", "intent": "As expected, the ensemble has a lower error rate than any single model.\n"}
{"snippet": "test_data = get_flight_delays(df_test, carrier, id_airport, True)\ntest_data = test_data[['MEAN', 'heure_depart_min']].dropna(how='any', axis = 0)\nX_test = np.array(test_data['heure_depart_min'])\nY_test = np.array(test_data['MEAN'])\nX_test = X_test.reshape(len(X_test),1)\nY_test = Y_test.reshape(len(Y_test),1)\nfit.predict(X_test)\n", "intent": "We will train the first 3 weeks of January and compare the predicted result with the 4th week ground truth data.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, idx in enumerate(style_layers):\n        content_res = gram_matrix(feats[idx])\n        loss += style_weights[i]*torch.sum((content_res - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.zeros(shape = (1, ), dtype = tf.float32)\n    for i, idx in enumerate(style_layers):\n        cur = style_weights[i]*tf.reduce_sum((gram_matrix(features=feats[idx]) - style_targets[i])**2)\n        loss = loss + cur\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "m = glm_poissonexp[-1]\nthis_model_param = m.fit_\nyrhat = m.predict(scaler.transform(Xr))\nythat = m.predict(scaler.transform(Xt))\n", "intent": "Use one model to predict\n"}
{"snippet": "embedded = np.zeros((metadata.shape[0], 128))\nk=0\nfor i, m in enumerate(metadata):\n        img = load_image(m.image_path())\n        img = align_image(img)\n        img = (img / 255.).astype(np.float32)\n        embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n", "intent": "Embedding vectors can now be calculated by feeding the aligned and scaled images into the pre-trained network.\n"}
{"snippet": "y_hat = classifier.predict(X)\ny_hat\n", "intent": "We have trained a classifier and we predict the classes\n"}
{"snippet": "X_test_dict = X_dict\nX_test = vec.transform(X_test_dict)\ny_hat = classifier.predict(X_test)\ny_hat\n", "intent": "Should we use a test set, we only need to use `transform` to vectorize the set. Here we reuse the training set:\n"}
{"snippet": "X_test_dict = X_dict\nX_test = vec.transform(X_test_dict)\ny_hat = classifier.predict(X_test)\ny_hat\n", "intent": "Should we use a test set (here the training set), we only need to use `transform` to vectorize the set\n"}
{"snippet": "y_predicted = classifier.predict(X)\nprint(y_predicted)\nprint(classifier.predict([X[0]]))\nprint(classifier.predict(np.array([[35680, 2217]])))\n", "intent": "Here we apply the model to the training set\n"}
{"snippet": "scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\nprint('Score', scores.mean())\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (classifier, metrics.classification_report(y, y_predicted)))\n", "intent": "We use cross validation instead\n"}
{"snippet": "from sklearn import metrics\nprint(\"Predicting the POS in the test set...\")\ny_test_predicted = classifier.predict(X_test)\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n          % (classifier, metrics.classification_report(y_test, y_test_predicted)))\n", "intent": "And we predict the test set and measure the performance\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n    return f1_score(target.values, y_pred)\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Training and Evaluating Models\n------------------\n"}
{"snippet": "result=final_clf.predict(test_X)\n", "intent": "Make Prediction for Submission\n------------------------\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, feats, c_feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.\n    for i in range(len(style_layers)):\n        loss += tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2) * style_weights[i]\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "The metric we're using here, comparing the number of matches to the total number of samples, is known as the accuracy score\n"}
{"snippet": "score = model.evaluate(X, Y)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test accuracy %:', score[1]*100)\n", "intent": "We now evaluate our model and view the accuracy of the model (how our model performed against the data we trained it on).\n"}
{"snippet": "score = model.evaluate(x_test, y_test, batch_size=15)\n", "intent": "Do a quick check to see that our model has a reasonable level of accuracy, precision and recall\n"}
{"snippet": "x_to_predict = x_to_predict.reshape(1,NUM_OF_FEATURES) \nresult = model.predict(x_to_predict)\nprint(\"Probability that stock will rise in the next day: \" + str(result[0][0]))\n", "intent": "We can finally use the model to make a prediction:\n"}
{"snippet": "preds=aml.predict(test)\nfinal=test.cbind(preds)\nresults=h2o.as_list(final)\n", "intent": "Now after the best algo has been found , we will now implement it on the test set\n"}
{"snippet": "knn.predict(train1.head(10))\n", "intent": "Check on the the model, if working, then the train would be predicted completed\n"}
{"snippet": "result = knn.predict(test1)\n", "intent": "Predict test, then output to CSV. This may take about a minute.\n"}
{"snippet": "print(classification_report(Y_age_train, train_RSLTS['treePrediction'], target_names=['Child','Adult','Senior']))\n", "intent": "__Precision, Recall, F1 Score, and Accuracy__\n"}
{"snippet": "predictions = clf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(accuracy_score(y_test, predictions))\n", "intent": "**What performed better the random forest or the decision tree?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "import sklearn.metrics\nmutual_info_outcome = df.apply(lambda x: sklearn.metrics.mutual_info_score(df[\"outcome\"], x))\nprint (mutual_info_outcome)\nprint (\"'days_from_last_contact' attribute has the highest Mutual Information with 'outcome'\")\n", "intent": "Which attribute has the highest Mutual Information with the 'outcome' attribute?\n"}
{"snippet": "mutual_infos = dict()\nfor col in df:\n    if col != 'outcome': \n        mutual_infos[col] = mutual_info_score(df[col], df['outcome'])\n", "intent": "Which attribute has the highest Mutual Information with the 'outcome' attribute?\n"}
{"snippet": "noise = np.random.normal(0,1,test_y.shape[0])\nnoisy_test_y = test_y + noise\nprint(\"MSE for the test set is: \\n\", mse(mdl.predict(test_x), test_y))\nprint(\"MSE for the noisy test set is: \\n\", mse(mdl.predict(test_x), noisy_test_y))\n", "intent": "c) Add some noise (with mean=0, std=1) to the test set's y, and predict it again. What happened to the MSE? Why?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(mdl, train_x, train_y, cv=5))\n", "intent": "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat.\n"}
{"snippet": "noise = np.random.normal(0,1,test_y.shape[0])\nsignal = test_y + noise\nprint(\"Test MSE:\", mse(mdl.predict(test_x), test_y))\nprint(\"Test MSE with noise:\", mse(mdl.predict(test_x), signal))\nprint(\"ANSWER:The MSE got larger and this is because we have moved each data point in the test set in a way\\\nthat adds more spread to it around the linear relationship and this is exactly what MSE measures\")\n", "intent": "c) Add some noise (with mean=0, std=1) to the test set's y, and predict it again. What happened to the MSE? Why?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.0)\n    for l in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[l]])\n        loss += style_weights[l] * (tf.reduce_sum((G-style_targets[l])**2)) \n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "metrics.accuracy_score(train_labels1, predTrain1)\n", "intent": "Accuracy for the training set:\n"}
{"snippet": "metrics.accuracy_score(test_labels1, predTest1)\n", "intent": "Accuracy for the test set:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "<a id='Test the Model3'></a>\n[[ go back to the top ]](\nThe accuracy of our model on the test dataset is very low!\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "<a id='Test the Model5'></a>\n[[ go back to the top ]](\nTry out your model on the test dataset of dog images.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, l in enumerate(style_layers):\n        style_current = gram_matrix(feats[l])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(style_current - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "cnn_predictions = [np.argmax(cnn_model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_cnn]\ntest_accuracy = 100*np.sum(np.array(cnn_predictions)==\n                           np.argmax(test_targets, axis=1))/len(cnn_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "nnAccuracy = nnModel.evaluate(testFeatures, testLabels)[0].round(decimals=3)\nprint('The accuracy of the neural network is {}% on {} predictions.'.format((nnAccuracy*100), \n                                                                            len(testLabels)))\n", "intent": "To see if the neural network performs better than the majority baseline (61.8%), the accuracy is calculated using the testing data.\n"}
{"snippet": "data = input_data\nfor i in range(130):\n    input_data = data\n    data = np.round(model.predict(data))\n", "intent": "Run the convolutional network\n"}
{"snippet": "imgs = next(image_batches)\npreds = model.predict(imgs)\nlabels = np.vectorize(lambda x: classes[x])(np.argmax(preds, axis=1))\n", "intent": "Let's grab a batch of images, calculate the class probabilites and finally take the most probable class as a label for each image\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(y_test)\nprint(predictions)\naccuracy_score(y_test, predictions)\n", "intent": "Alright, we have the list of predictions, lets see how good our predictions are!!!\n"}
{"snippet": "print('{}\\n'.format(metrics.confusion_matrix(actual_labels, predicted_labels, labels=[1,-1,0])))\nprint(\"\\x1b[31m\\\" macro f1 score \\\"\\x1b[0m\")\nprint('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='macro')))\nprint(\"\\x1b[31m\\\" micro f1 score \\\"\\x1b[0m\")\nprint('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='micro')))\n", "intent": "See [Confusion Matrix](https://fr.wikipedia.org/wiki/Matrice_de_confusion) for more details\n"}
{"snippet": "print('{}\\n'.format(metrics.confusion_matrix(actual_labels, predicted_labels, labels=[1,-1,0])))\nprint(\"\\x1b[31m\\\" micro f1 score \\\"\\x1b[0m\")\nprint('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='macro')))\nprint(\"\\x1b[31m\\\" macro f1 score \\\"\\x1b[0m\")\nprint('{}\\n'.format(metrics.f1_score(actual_labels, predicted_labels, average='micro')))\n", "intent": "See [Confusion Matrix](https://fr.wikipedia.org/wiki/Matrice_de_confusion) for more details\n"}
{"snippet": "trainer.load('/home/bobo/PycharmProjects/torchProjectss/fasterbychenyun/simplefasterrcnn/chainer_best_model_converted_to_pytorch_0.7053.pth')\nopt.caffe_pretrain=True \n_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\nvis_bbox(at.tonumpy(img[0]),\n         at.tonumpy(_bboxes[0]),\n         at.tonumpy(_labels[0]).reshape(-1),\n         at.tonumpy(_scores[0]).reshape(-1))\n", "intent": "You'll need to download pretrained model from [google dirve](https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY) \n"}
{"snippet": "print(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(x_test) - y_test) ** 2))\nprint('R^2: %.2f' % regr.score(x_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "scores = cross_val_score(logr, X_train, y_train, cv=10)\nprint(scores)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "preds = results.predict(X_test)\n", "intent": "Remember the 20% of data we held back for testing purposes. Now we get to use it! We can generate predictions on our test data and score the results:\n"}
{"snippet": "mean_absolute_error(y_test, preds)\n", "intent": "We can evaluate our model's performance by calculating the Mean Absolute Error (MAE) using `sci-kit learn`:\n"}
{"snippet": "rf.predict(np.array([105.0, 2000000, 0.21, 1]).reshape(1,-1))\n", "intent": "Query an individual loan applicant\n"}
{"snippet": "input_value = np.array([35000,70000]).reshape(2,1)\nPredicted_value = predict(input_value, theta_est.ravel().reshape(2,1))\nprint(Predicted_value)\n", "intent": "Now use your final values for $\\theta$ and the `predict()` function to make predictions on profits in areas of 35,000 and 70,000 people.\n"}
{"snippet": "MSE = np.mean((y_test - model.predict(X_test)) ** 2)\nprint \"The Test Data MSE is: %0.3f\" % (MSE)\n", "intent": "Now, we evaluate our model against the test data using MSE.\n"}
{"snippet": "y_train_predict = lin_model.predict(X_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\nr2 = r2_score(Y_train, y_train_predict)\nprint(\"The model performance for training set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\n", "intent": "Model evaluation with RMSE and R^2\n"}
{"snippet": "y_test_predict = lin_model.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\nr2 = r2_score(Y_test, y_test_predict)\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\n", "intent": "Model evaluation for testing set\n"}
{"snippet": "best_xgb = clf.best_estimator_\nxgb_pred = best_xgb.predict(X_test)\nprint(\"Test error of XGBoost is: %.4f\" % mean_squared_error(y_test, xgb_pred))\n", "intent": "The selected alpha is 1.\n"}
{"snippet": "ev = regressor.evaluate(input_fn=lambda: input_fn(test_set), steps=1)\n", "intent": "> Next, see how the trained model performs against the test data set. Run evaluate, and this time pass the test_set to the input_fn:\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, I can use the CNN to test how well it identifies breed within our test dataset of dog images.  I will print the test accuracy below.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "I will try out my model on the test dataset of dog images. I will ensure that my test accuracy is greater than 60%.\n"}
{"snippet": "FullDF['Predict_logy'] = lcv.predict(X)\n", "intent": "Adding a column for log y predicted by the LassoCV model\n"}
{"snippet": "y_pred = log_model.predict(X_test)\n", "intent": "And once the model has been created, we can predict the Test dataset\n"}
{"snippet": "kfcv.predict_proba(personal_stats.reshape(1,-1))\n", "intent": "Looks like I survived!\nLet's look at my predicted probability of surviving:\n"}
{"snippet": "pred = logistic.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "del x_train, y_train\ngc.collect()\nx_test, x_test_filename = data_helper.preprocess_test_data(test_jpeg_dir, img_resize)\npredictions = classifier.predict(x_test)\n", "intent": "Before launching our predictions lets preprocess the test data and delete the old training data matrices\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for n in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[n]])\n        gram_target = style_targets[n]\n        style_loss += tf.reduce_sum((gram - gram_target) ** 2) * style_weights[n]\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(precision_score(y_test, y_pred, average=\"macro\"))\n", "intent": "The model performance in validate set and test set are similiar. It shows we applied the random oversampling method correctly.\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit\nn_samples = iris.data.shape[0]\ncv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\ncross_val_score(clf, iris.data, iris.target, cv=cv)\n", "intent": "** It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance: **\n"}
{"snippet": "preds = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)\ntest_labels = dtest.get_label()\ntest_labels[:10]\ntoppreds = np.argmax(preds,axis=1)\nprint(\"Accuracy: {}\".format(accuracy_score(test_labels,toppreds)))\n", "intent": "Get predictions on the test set and get the \"Traditional\" accuracy - how likely was the top prediction to match the label.\n"}
{"snippet": "bst2 = pickle.load(open(\"xgb-python.model\", \"rb\"))\npreds2 = bst2.predict(dtest, ntree_limit=bst2.best_ntree_limit)\n1-evalerror(preds2,dtest)[1]\n", "intent": "Verify that the reload and scoring functions work.\n"}
{"snippet": "k = sns.jointplot(bos.PRICE.values, lm.predict(X), data=bos, kind=\"reg\")\nk.set_axis_labels(\"Actual prices\", \"Predicted prices\")\n", "intent": "Below is the correlation between the prices predicted with the scikitlearn package:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy of the model on the train set is still much higher than in the test set, which suggests the model \nmay still be overfitting.\n"}
{"snippet": "predictions = model_rf.transform(test_data)\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n", "intent": "You can check your **model accuracy** now. Use **test data** to evaluate the model.\n"}
{"snippet": "predictions = []\nfor v, features in examples:\n    nb_instances = features.shape[0]\n    X = features.reshape((nb_instances, 1, 4096))\n    model.reset_states()\n    prediction = model.predict(X, batch_size=1)\n    prediction = prediction.reshape(nb_instances, 201)\n    class_prediction = np.argmax(prediction, axis=1)\n    predictions.append((v, prediction, class_prediction))\n", "intent": "Extract the predictions for each video and print the scoring\n"}
{"snippet": "predictions_def = []\nfor v, features in examples:\n    nb_instances = features.shape[0]\n    X = features.reshape((nb_instances, 1, 4096))\n    model_def.reset_states()\n    prediction = model_def.predict(X, batch_size=1)\n    prediction = prediction.reshape(nb_instances, 201)\n    class_prediction = np.argmax(prediction, axis=1)\n    predictions_def.append((v, prediction, class_prediction))\n", "intent": "Extract the predictions for each video and print the scoring\n"}
{"snippet": "print 'Classification Report:\\n', classification_report(y_test, y_pred)\n", "intent": "Or we use the classification report function:\n"}
{"snippet": "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=8)\nprint loss_and_metrics\n", "intent": "Let's evaluate the loss and accuracy on our test data:\n"}
{"snippet": "print 'Testing...'\ny_pred = model.predict(X_test, verbose = True, batch_size=8)\n", "intent": "Let's predict classes for our test data:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nlr_s=cross_val_score(best_lr,X=a_train,y=b_train,cv=5)\nsvc_s=cross_val_score(best_svc,X=a_train,y=b_train,cv=5)\nrf_s=cross_val_score(best_rf,X=a_train,y=b_train,cv=5)\nnn_s=cross_val_score(best_nn,X=a_train,y=b_train,cv=5)\n", "intent": "1. logistic regression - random forest\n2. random forest - SVM\n3. neural network - SVM\n4. random forest - neural network\n"}
{"snippet": "lprob=classifier.predict_log_proba(test_data)\nprob=classifier.predict_proba(test_data)\nrotten = test_y==0\nfresh = ~rotten\n", "intent": "**Your turn:** What is using this function as the score mean? What are we trying to optimize for?\n"}
{"snippet": "prediction = grid_xgr.best_estimator_.predict(X_test)\nsns.distplot(prediction)\n", "intent": "Let's plot predicted prices of train/test split data.\n"}
{"snippet": "predictions = cross_val_predict(grid_xgr.best_estimator_, \n                                df_train[numeric_features+cate_features], \n                                df_train['price_doc'], \n                                cv=cv)\nshow_results(y_test, \n             prediction, \n             df_train['price_doc'], \n             predictions)\n", "intent": "<a id=\"2.1\"></a>\nLet's cross-validate and check the normality of residuals.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_mod.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint classification_report(bank_target_test, knnpreds_test)\n", "intent": "For Classification Matrix & Confusion Matrix\n"}
{"snippet": "full_pred = mlp.predict(X)\npd.crosstab(Y, full_pred) \n", "intent": "The adjusted rand score is approximately 0.21, which indicates random labeling, and the large variance indicates that this model is overfitting.\n"}
{"snippet": "full_pred2 = mlp2.predict(X)\npd.crosstab(Y, full_pred2) \n", "intent": "The adjusted random score is slightly higher, variance decreased, showing there is less overfitting.\n"}
{"snippet": "full_pred3 = mlp3.predict(X)\npd.crosstab(Y, full_pred3) \n", "intent": "Adjusted rand score is lower, which is not good, and cross validation variance is remained the sames as the previous model.\n"}
{"snippet": "full_pred4 = mlp4.predict(X)\npd.crosstab(Y, full_pred4) \n", "intent": "Our adjusted rand scores increased, and the variance increased.  We will try some other adjustments in hyperparameters.\n"}
{"snippet": "full_pred5 = mlp5.predict(X)\npd.crosstab(Y, full_pred5) \n", "intent": "The adjusted randomn score (index), slightly decreased in respect to the previous model\n"}
{"snippet": "full_pred6 = mlp6.predict(X)\npd.crosstab(Y, full_pred6) \n", "intent": "The adjusted rand score is higher, and the variance also decreased.\n"}
{"snippet": "df_unknown = df[df['year'] == 2014]\ndf_unknown.index = list(range(0, len(df_unknown)))\ndf_unknown.reindex()\np_unknown = svm.predict(df_test[features])\nfor i in range(len(df_unknown)):\n    if p_unknown[i] == 1:\n        print(df_unknown['country'].iloc[i], 'will win!')\n", "intent": "Anyway let's use that to predict the winner!\n"}
{"snippet": "y_pred = rf.predict(X_test)\n", "intent": "Now we can make predictions on the test set and compute the accuracy:\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=5, method='decision_function')\n", "intent": "First we compute the decision function scores for all the training data:\n"}
{"snippet": "roc_auc_score(y_train_5, y_scores)\n", "intent": "...which is higher than \n"}
{"snippet": "cross_val_score(forest_clf, X_train, y_train_5, cv=3).mean()\n", "intent": "We also see that, unsurprisingly, the accuracy of the random forest is way higher than the SGD classifier:\n"}
{"snippet": "forest_clf.predict_proba([some_digit])\n", "intent": "We can get the list of probabilities that the random forest assigned to each class:\n"}
{"snippet": "sgd_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\nforest_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3)\n", "intent": "Let's compare the two classifiers confusion matrices:\n"}
{"snippet": "y_train_pred = lin_reg.predict(X)\n", "intent": "Note that we used `X` instead of `X_b` in `lin_reg.fit`. The bias term is taken care of automatically.\n"}
{"snippet": "y_val_predict_best = best_model.predict(X_val_poly_scaled)\n", "intent": "Predict with the best model:\n"}
{"snippet": "tree_clf.predict_proba([test_sample,])\n", "intent": "We use `predict_proba` to get the probabilities:\n"}
{"snippet": "y_pred = rf_clf_smaller.predict(X_test_smaller)\n", "intent": "It's interesting to check which movement classes the model was confused about:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, ypred)\n", "intent": "To determine the performance of the model, we'll use **accuracy_score**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nprint('R^2 score: %.2f' % r2_score(df.Sales, sales_estimates))\n", "intent": "In order to assess the model accuracy, we use the R-squared statistic.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, ypred)\n", "intent": "We can use `accuracy_score` to compare the labels from the test data set to the predictions from our fitted model.\n"}
{"snippet": "accuracy_score(ytest, ypred)\n", "intent": "* Let's have a look at the same diagnostics we used for LDA\n"}
{"snippet": "my_first_nn.evaluate(X_test, Y_test, verbose=0)\n", "intent": "And if you want to assess the performance of your model on the test dataset, use the [evaluate method](https://keras.io/models/model/\n"}
{"snippet": "import numpy as np\nfrom loss import SoftmaxCrossEntropy\nfrom utils.check_grads import check_grads_loss\nbatch = 10\nnum_class = 10\ninputs = np.random.uniform(size=(batch, num_class))\ntargets = np.random.randint(num_class, size=batch)\nlayer = SoftmaxCrossEntropy(num_class)\ncheck_grads_loss(layer, inputs, targets)\n", "intent": "In the file `loss.py`, implement the backward pass for `SodtmaxCrossEntropy`. Please test the gradients by yourself.\n"}
{"snippet": "scifi_uncoded['KNN_predict'] = KNN.predict(np.stack(scifi_uncoded['vect'], axis=0))\nscifi_uncoded[100:105]\n", "intent": "Surprisingly, KNN has a great performance on my data. I can extrapolate uncoded data base on it.\n"}
{"snippet": "scifi_uncoded['NN_predict'] = NN.predict(np.stack(scifi_uncoded['vect'], axis=0))\nscifi_uncoded[100:105]\n", "intent": "It looks that Neural Network works ok with my data. Let's extrapolate uncoded data based on it.\n"}
{"snippet": "xgb_cv_full = cross_val_score(XGBoostClassifier(), features, target, cv=4)\nprint(xgb_cv_full.mean(), xgb_cv_full.std())\n", "intent": "Such an awfull quality drop... Have any ideas why?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.)\n    i = 0\n    for l in style_layers:\n        style_loss += style_weights[i]*tf.reduce_sum(tf.square(gram_matrix(feats[l]) - style_targets[i]))\n        i += 1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = model.predict(test)\n", "intent": "We then use the model to predict and test performance on the test set. We set a label of 1 for predictions above a treshold of 0.5.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_label, predictions_labels, [0,1]))\n", "intent": "Prediction evaluation using the predicted labels and ground truth of the test set. (Support = Number of instances of each class for the set analised)\n"}
{"snippet": "svm_param = svm.svm_parameter('-t 2 -c {} -g {}'.format(best_pair[0], best_pair[1]))\nsvm_prob = svm.svm_problem(T_train_list, X_train_dict)\nmodel_rbf_best = svmutil.svm_train(svm_prob, svm_param)\np_label, p_acc, p_val = svmutil.svm_predict(T_test_list, X_test_dict, model_rbf_best)\n", "intent": "Use best parameters to train the model\n"}
{"snippet": "y_pred = model.predict(X_test)\ncorrect_preds = 0\nfor i in range(X_test.shape[0]):         \n    pred_list_i = [pred[i] for pred in y_pred]\n    val_list_i  = [val[i] for val in y_val]\n    matching_preds = [pred.argmax(-1) == val.argmax(-1) for pred, val in zip(pred_list_i, val_list_i)]\n    correct_preds = int(np.all(matching_preds))\ntotal_acc = (correct_preds / float(X_test.shape[0]))*100\nprint(total_acc)\n", "intent": "Calculate our whole-sequence accuracy\n"}
{"snippet": "xval_B = model_selection.cross_val_score(model_B, path_variables_values, path_target_values, \n                                         cv=KFold(n_splits=10, shuffle=True, random_state=1234))\nnp.average(xval_B),np.std(xval_B)\n", "intent": "remenber that order of columns are: left, right, straight, straight_left, straight_right, \nand they correspond to the value of output 1, 2, 3, 4, 5 \n"}
{"snippet": "print(mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictNew = model.predict(new)\nprint(predictNew)\n", "intent": "We will now use our model to predict the species for this new data.  We will use our latest model with k = 10.\n"}
{"snippet": "print classification_report(y_test, best_rfc.predict(X_test))\n", "intent": "    - Precision = tp/tp+fp\n    - Recall = tp/tp+fn\n"}
{"snippet": "benchmark_train = np.random.choice(range(1, 3), size=y_train.shape[0], replace=True)\nbenchmark_test = np.random.choice(range(1, 3), size=y_test.shape[0], replace=True)\nprint \"Benchmark training F1 score: {}\".format(f1_score(y_train, benchmark_train))\nprint \"Benchmark testing F1 score: {}\".format(f1_score(y_test, benchmark_test))\n", "intent": "In order to check that the testing scores are not a random fluke, I will choose labels randomly and test performance.\n"}
{"snippet": "loss = seq2seq.sequence_loss(dec_outputs, labels, weights, vocab_size)\n", "intent": "Build a standard sequence loss function: mean cross-entropy over each item of each sequence.\n"}
{"snippet": "predictions = lm.predict( X_test)\n", "intent": "** Use lm.predict() to predict off the X_test set of the data.**\n"}
{"snippet": "result_y_pred = reg.predict(X)\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y, result_y_pred))\n", "intent": "- Then we create predicted values for y.\n- Using the function below, we get a MSE of 21.90.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test,y_test_pred)\n", "intent": "To check if the model is reasobale we look at the explained variance score. \n"}
{"snippet": "print sqft_model.predict(np.array(house2[['sqft_living']]))\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "print my_feature_model.predict(np.array([map (lambda x : x[0], [bill_gates[key] for key in my_features])], dtype=float))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Bill_gates%27_house.jpg/2560px-Bill_gates%27_house.jpg\">\n"}
{"snippet": "preds = log_model.predict(X= train_features)\npd.crosstab(preds,titanic_train[\"Survived\"])\n", "intent": "Next, let's make class predictions using this model and then compare the predictons to the actual values:\n"}
{"snippet": "labels = dbscan.fit_predict(np.arange(X.shape[0]).reshape(-1, 1))\nprint \"FMI: %.5f\" % metrics.fowlkes_mallows_score(true_clones, labels)\nprint \"ARI: %.5f\" % (metrics.adjusted_rand_score(true_clones, labels))\nprint \"AMI: %.5f\" % (metrics.adjusted_mutual_info_score(true_clones, labels))\nprint \"NMI: %.5f\" % (metrics.normalized_mutual_info_score(true_clones, labels))\nprint \"Hom: %.5f\" % (metrics.homogeneity_score(true_clones, labels))\nprint \"Com: %.5f\" % (metrics.completeness_score(true_clones, labels))\nprint \"Vsc: %.5f\" % (metrics.v_measure_score(true_clones, labels))\n", "intent": "Is it better or worse than the result with everyone at the same time?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_3, cv=3, scoring='accuracy')\n", "intent": "Using `cross_val_score()` along with `sgd_clf`\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never3Clasifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X, y=None):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "Building a dumb classifier that predicts everything in the 'not 3' class\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')\n", "intent": "Let's evaluate the SGDClassifier now using `cross_val_score`\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n", "intent": "Let's start by looking at the confusion matrix.\n"}
{"snippet": "knn_clf.predict([some_digit.ravel()])\n", "intent": "`KNeighborsClassifier` supports multilabel classification and we train it using multiple labels.\nNow, let's make a perdiction.\n"}
{"snippet": "values = df.values\nX = values[:,0:8]\ny = values[:,8]\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=3, random_state=7)\nresult = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(result.mean())\n", "intent": "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values like LDA.\n"}
{"snippet": "from sklearn import linear_model, metrics\nnp.random.seed(5)\nregr_ridge = \nypred_ridge = cross_validate_regr(X_regr, y_regr, regr_ridge, folds_regr)\nprint(\"RMSE: %.3f\" % np.sqrt(metrics.mean_squared_error(y_regr, ypred_ridge)))\n", "intent": "** Question: ** use the [scikit learn implementation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n"}
{"snippet": "from sklearn import linear_model\nnp.random.seed(5)\nregr_lasso = \nypred_lasso = cross_validate_regr(X_regr, y_regr, regr_lasso, folds_regr)\nprint(metrics.mean_squared_error(y_regr, ypred_lasso))\n", "intent": "** Question: ** use the [scikit learn implementation]() to get the cross validated mean suared error and try to vary the $\\alpha$ value.\n"}
{"snippet": "clf_logreg_l1reg = \nypred_logreg_l1reg = cross_validate_clf(X_clf, y_clf, clf_logreg_l1reg, folds_clf)\nprint(\"without standardization:\", metrics.accuracy_score(y_clf,np.where(ypred_logreg_l1reg > 0.5, 1, 0)))\n", "intent": "**Question** compute the cross-validated predictions of the l1-regularized logistic regression with default parameters on our data.\n"}
{"snippet": "kfold = KFold(n_splits=10,random_state=2)\nresults = cross_val_score(xgb1, train_df, target_df, cv=kfold)\n", "intent": "We can also cross-validate - this is similar to train/test split, but run multiple times\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n", "intent": "Testing Accuracy of Linear Model\n"}
{"snippet": "y_unknown = xg.predict(X_unknown)\nwell_data['Facies'] = y_unknown+1\nwell_data.drop(['del_GR', 'del_ILD_log10', 'del_DeltaPHI', 'del_PHIND', 'del_PE',\n       'del_RELPOS', 'del_del_GR', 'del_del_ILD_log10', 'del_del_DeltaPHI',\n       'del_del_PHIND', 'del_del_PE', 'del_del_RELPOS'], axis=1)\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "predicted_labels = gbModel.predict(X_test)\n", "intent": "Now that the model has been trained on our data, we can use it to predict the facies of the feature vectors in the test set.  \n"}
{"snippet": "y_pred = gbModel.predict(X_blind)\nblind['Prediction'] = y_pred\n", "intent": "Now it's a simple matter of making a prediction and storing it back in the dataframe:\n"}
{"snippet": "y_unknown = gbModel.predict(X_unknown)\nwell_data['Facies'] = y_unknown\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "y_pred = vtclf.predict(X_blind)\nblind['Prediction'] = y_pred\n", "intent": "Now it's a simple matter of making a prediction and storing it back in the dataframe:\n"}
{"snippet": "y_unknown = vtclf.predict(X_unknown)\nvtclf.score(X_unknown)\nwell_data['Facies'] = y_unknown\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "jaccard_similarity_score(y_test, prediction)\n", "intent": "<b> Accuracy Score of the Model </b>\n"}
{"snippet": "predictions = gr_rfcml.best_estimator_.predict(X_test)\npred_prb = gr_rfcml.best_estimator_.predict_proba(X_test)      \n", "intent": "<b> Model Average Precision Score </b>\n"}
{"snippet": "dict_base = {}\nfor i in range(len(y.columns)):\n    col_true = y_test.iloc[:,i]\n    col_pred = prediction[:,i]\n    dict_base[y_test.columns[i]]= y_train.iloc[:,i].mean()\n    dict_pd[y_test.columns[i]] = accuracy_score(col_true, col_pred)\ncol_acc = pd.Series(dict_pd)\ncol_base = pd.Series(dict_base)\n", "intent": "<b> Baseline Accuracy for Each Label </b>\n"}
{"snippet": "predictions_for_70 = (mod_2.best_estimator_.predict_proba(X_test)[:,1]>0.7)*1\n", "intent": "<b>Results at the 0.7 Threshold</b>\n"}
{"snippet": "preds = model.predict(test_sample)\nprint (test_sample.shape)\nerrors = [i for i in range(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    query_img = test_sample[i]\n    _, result = model.kneighbors(query_img, n_neighbors=4)\n    show(query_img)\n    show(train[result[0],:], len(result[0]))\n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "preds = model.predict(test_sample)\nerrors = [i for i in range(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nerr_rate = float(len(errors))/len(preds)\nprint (err_rate)\n", "intent": "Now let's calculate the prediction error rates of the KNN classifier.\n"}
{"snippet": "list_predict = []\npredictions_prob = []\nfor i in range(len(mnist_X_test)):\n    predicted = NB.predict(mnist_X_test_flatten[i])\n    predictions_prob.append(NB.predict_prob(mnist_X_test_flatten[i]))\n    list_predict.append((mnist_y_test[i],predicted))\n", "intent": "Looking at the variance, we see that it is higher at the borders of the number because those are the regions that vary the most\n"}
{"snippet": "dog_breed_predictions = [np.argmax(selected_model.predict(np.expand_dims(feature, axis=0))) for feature in test_bottleneck]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions) == np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "sentence = Sentence('George Washington went to Washington .')\ntagger.predict(sentence)\nprint(sentence.to_tagged_string())\n", "intent": "We use the predict() method of the tagger on a sentence to add predicted tags to the tokens in the sentence:\n"}
{"snippet": "tagger = SequenceTagger.load('pos-multi')\nsentence = Sentence('George Washington lebte in Washington . Dort kaufte er a horse .')\ntagger.predict(sentence)\nprint(sentence.to_tagged_string())\n", "intent": "Flair offers access to multi-lingual models for multi-lingual text.\n"}
{"snippet": "text = \"This is a sentence. John read a book. This is another sentence. I love Berlin.\"\nfrom segtok.segmenter import split_single\nsentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\ntagger: SequenceTagger = SequenceTagger.load('ner')\ntagger.predict(sentences)\nfor i in sentences:\n    print(i.to_tagged_string())\n", "intent": "To tag an entire text corpus, one needs to split the corpus into sentences and pass a list of Sentence objects to the .predict() method.\n"}
{"snippet": "sentence = Sentence('This film hurts. It is so bad that I am confused.')\nclassifier.predict(sentence)\nprint(sentence.labels)\n", "intent": "We call the predict() method of the classifier on a sentence. This will add the predicted label to the sentence:\n"}
{"snippet": "scores = model.evaluate(X, Y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "The evaluation is available via the *evaluate* method. In our case we print out the accuracy:\n"}
{"snippet": "print(kmeans.predict([[0, 0], [4, 4]]))\n", "intent": "We can use the model now to make predictions about other datapoints:\n"}
{"snippet": "print(KNNClassifier.predict([[1.1, 0.9]]))\n", "intent": "We ask the classifier to suggest a class for an unseen vector:\n"}
{"snippet": "print(KNNClassifier.predict_proba([[2.9, 3.1]]))\n", "intent": "It can also give us the likelihoods for the probability of a data-point being in any of the classes:\n"}
{"snippet": "print(\"Prediction:\", classifier.predict(digits.data[-1:]))\nprint(\"Image:\\n\", digits.images[-1])\nprint(\"Label:\", digits.target[-1])\n", "intent": "We can use the *predict* method to request a guess about the last element in the *data* member:\n"}
{"snippet": "print(\"Prediction:\", classifier2.predict(iris.data[0:1]))\nprint(\"Target:\", iris.target[0])\n", "intent": "We can use this *unpickled* *classifier2* in the same way as shown above:\n"}
{"snippet": "scores = model.evaluate(X, Y)\nprint('{}: {}'.format(model.metrics_names, scores))\n", "intent": "Evaluate the model by printing the scores\n"}
{"snippet": "scores = model.evaluate(X, y)\nprint('{}: {}'.format(model.metrics_names[1], scores[1] * 100))\n", "intent": "Evaluate the model by printing the scores\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import fbeta_score\nprint(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\nprint(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\nprint(\"F =\", fbeta_score(y_test, clf.predict(X_test), beta=1)) \n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$ \n$$Recall = \\frac{TP}{TP + FN}$$\n$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n"}
{"snippet": "Resnet50_model.load_weights('saved_models/weights.best.ResNet50.hdf5')\nResnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)ODO: Calculate classification accuracy on the test dataset.\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_data_x = data[2]\ntest_data_y = data[3]\ntest_res = val_model.evaluate(test_data_x, test_data_y, batch_size=50, verbose=1)\nprint(test_res)\n", "intent": "We will use the same model wihout the dropout layers for testing as dropout is introduced only to prevent overfitting while training the model.\n"}
{"snippet": "model.load_state_dict(torch.load('tut2-model.pt'))\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n", "intent": "...and get our new and vastly improved test accuracy!\n"}
{"snippet": "model.load_state_dict(torch.load('tut4-model.pt'))\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n", "intent": "We get test results comparable to the previous 2 models!\n"}
{"snippet": "model.load_state_dict(torch.load('tut5-model.pt'))\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n", "intent": "Finally, let's run our model on the test set!\n"}
{"snippet": "print_report(pipe)\n", "intent": "We just made the task harder and more realistic for a classifier.\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "encoded_imgs_clean = autoencoder1[0].predict(x_train_clean)\nencoded_imgs_noisy = autoencoder1[0].predict(x_train_noisy)\n", "intent": "The encoder model obtained from the previous step is fed with the clean and noisy input.\n"}
{"snippet": "encoded_imgs_clean2 = autoencoder2[0].predict(encoded_imgs_clean)\nencoded_imgs_noisy2 = autoencoder2[0].predict(encoded_imgs_noisy)\n", "intent": "The encoder model obtained from the second autoencoder is fed with the clean and noisy output of the first autoencoder's feed forward step.\n"}
{"snippet": "y_predicted = gs_clf.predict(docs_test)\n", "intent": "Predict the outcome on the testing set and store it in a variable named y_predicted\n"}
{"snippet": "y_predicted = grid_search.predict(docs_test)\n", "intent": "Predict the outcome on the testing set and store it in a variable named y_predicted\n"}
{"snippet": "startTime = datetime.now()\nproba = cross_val_predict(estimator, X, dummy_y, cv=kfold, method='predict_proba')\nprint (datetime.now() - startTime)\n", "intent": "Predict probabilities for classes\n"}
{"snippet": "dogresnet50_predictions = [np.argmax(dogresnet50.predict(np.expand_dims(feature, axis=0))) for feature in dogresnet50_test]\ntest_accuracy = 100*np.sum(np.array(dogresnet50_predictions)==np.argmax(test_targets, axis=1))/len(dogresnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG19_predictions=[np.argmax(VGG19_model.predict(np.expand_dims(feature,axis=0))) for feature in test_VGG19]\ntest_accuracy=100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets,axis=1)/len(VGG19_predictions))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print metrics.classification_report(y, clf_tree_opt.predict(X_enc_df_full))\n", "intent": "Main metrics of optimal classifier (calculated from comparison of real and predicted values of target variable):\n"}
{"snippet": "clf_rf_opt = cv_srch.best_estimator_\nprint metrics.classification_report(y, clf_tree_opt.predict(X_enc_df_full))\nprint metrics.classification_report(y, clf_rf_opt.predict(X_enc_df_full))\n", "intent": "Main metrics comparison for two classifiers:\n"}
{"snippet": "print metrics.classification_report(y, pred_y, digits=4, target_names =[\"did not Survived\", \"Survived\"])\n", "intent": "Maint classification metrics:\n"}
{"snippet": "print(grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\njoblib.dump(rfc, model_file)\ny_test = grid_search.predict_proba(X_test)\n", "intent": "Predicting the target probabilities for the test dataset.\n"}
{"snippet": "print(grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\njoblib.dump(xgbc, model_file)\ny_test = grid_search.predict_proba(X_test)\n", "intent": "Predicting the target probabilities for the test dataset.\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    print(kfold)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n", "intent": "Evaluating each model\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melb_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "Let's find the MAE for our Melbourne model (```melb_model```).\n"}
{"snippet": "def r2_in_days(target, pred, n_days):\n    from sklearn.metrics import r2_score\n    r2s = []\n    for d in n_days:\n        if d == 'all':\n            r2s.append(r2_score(target, pred))\n        else:\n            r2s.append(r2_score(target[:d], pred[:d]))\n    return r2s\n", "intent": "The following is a utility function copied from Chapter 1 to evaluate test performance at different time horizons.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    layer_loss = [None] * len(style_layers)\n    for idx, layer in enumerate(style_layers):\n        w = style_weights[idx]\n        G = gram_matrix(feats[layer])\n        A = style_targets[idx]\n        layer_loss[idx] = w * tf.reduce_sum(tf.square(G - A))\n    style_loss = tf.add_n(layer_loss)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\ny_pred = cross_val_predict(bnb,training_data,target,cv=10)\nconfusion_matrix(target,y_pred)\n", "intent": "Will do cross validation. Naive Bayes is pretty good at avoiding overfitting.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'alpha': [0.0, 0.5, 1.0],\n              'binarize': [None, 0.2, 0.5, 0.7, 1.0],\n              'fit_prior': [True, False]\n}\ngrid = GridSearchCV(bnb, parameters, scoring='accuracy', cv=10, verbose=0)\ny_pred = cross_val_predict(grid,training_data,target,cv=10)\nconfusion_matrix(target,y_pred)\n", "intent": "Will use grid search to look at different parameters. \n"}
{"snippet": "print(classification_report(y_test, pred_y_sklearn))\n", "intent": "Notice the Lasso did predict people will leave. Need to look at other metrics.\n"}
{"snippet": "y_pred = clf.predict_proba(X_test)\n", "intent": " | **First alternative**| **Second Alternatve**  |\n | :-------------:| :-----: |\n |  2.4083371584  | **2.39597954685**|\n"}
{"snippet": "predictions = nb_detector.predict(msg_test)\nprint(confusion_matrix(label_test, predictions))\nprint(classification_report(label_test, predictions))\n", "intent": "And overall scores on the test set, the one we haven't used at all during training:\n"}
{"snippet": "print(svm_detector.predict([\"Hi mom, how are you?\"])[0])\nprint(svm_detector.predict([\"WINNER! Credit for free!\"])[0])\n", "intent": "So apparently, linear kernel with `C=1` is the best parameter combination.\nSanity check again:\n"}
{"snippet": "print('before:', svm_detector.predict([message4])[0])\n", "intent": "The loaded result is an object that behaves identically to the original:\n"}
{"snippet": "imputedage = knnclass.predict(AgeTestX)\nimputedage\n", "intent": "Now that we have a fitted model, let's predict the ages of our test set:\n"}
{"snippet": "best_threshold,best_f1_score = best_threshold_f1_score(regr,X_resample,y_resample)\ntest_f1 = test_f1_score(regr,best_threshold,X_test,y_test)\nprint('test f1 score:',test_f1,'train f1 score:',best_f1_score)\nresult_analyse(regr,best_threshold,X_resample,X_test,y_resample,y_test)\n", "intent": "* train and test f1 score\n* train and test confusion matrix\n* ROC curve\n"}
{"snippet": "best_threshold,best_f1_score = best_threshold_f1_score(regr,X_train,y_train)\ntest_f1 = test_f1_score(regr,best_threshold,X_test,y_test)\nprint('test f1 score:',test_f1,'train f1 score:',best_f1_score)\nresult_analyse(regr,best_threshold,X_train,X_test,y_train,y_test)\n", "intent": "* train and test f1 score\n* train and test confusion matrix\n* ROC curve\n"}
{"snippet": "x_train_pre=model_vgg16.predict(X_train)\n", "intent": "Know, we will use a pre-trained model to predict the 10 classes of this excercise, there are many of them but we will work with VGG16\n"}
{"snippet": "model.predict(X_pad)\n", "intent": "with the last trained model, we predict the sentiment of the reviews.\n"}
{"snippet": "(X_new = [[22587]] \n model.predict(X_new)\n", "intent": "Finally, we can view the attributes of our learned model:\n"}
{"snippet": "def knn_predict(x_test, X_train, y_train, k):\n    neighbors_idx = k_neighbors(x_test, X_train, k)\n    labels = []\n    for n in neighbors_idx:\n        labels.append(y_train.iloc[n])\n    most_common_label = Counter(labels).most_common(1)[0][0]\n    return most_common_label\n", "intent": "Now, use the k-nearest neighbors to vote on the class label of our test vector:\n"}
{"snippet": "print(\"Input Test Vector:\")\nprint(X_test.iloc[40, :])\nprint(\"Species: {0}\".format(y_test.iloc[40]))\nnn = knn_predict(X_test.iloc[40, :], X_train, y_train, 5)\nprint(\"\\nkNN Predicted Species: \")\nprint(nn)\n", "intent": "Finally, let's test our kNN implementation for k=5\n"}
{"snippet": "pred = regr.predict(pred)\nresults['price_pred'] = pred\n", "intent": "Create Lasso model prediction\n"}
{"snippet": "predictions = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nprecision = precision_score(y_train, predictions, average='macro')\nprint(\"Precision:\", precision)\n", "intent": "Now it get's a 91 % accuracy on average. Not bad ! \nLet's check the precision, recall and F-score:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for idx, layer in enumerate(style_layers):\n        style_loss += style_weights[idx] * (gram_matrix(feats[layer]) - style_targets[idx]).pow(2).sum()\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def age_score(df):\n    df['Age'] = df['Year Built']*df['Year Remod/Add']\nage_score(data)\nsns.regplot(data['Age'], data['SalePrice']);\nnp.corrcoef(data['Age'], data['SalePrice'])[0][1]\n", "intent": "Look at the combination of Year Built and Year Add/Remod\n"}
{"snippet": "def transform_features(df):\n    total_sf(df)\n    ordinal_bsmt_qual(df)\n    ordinal_neighborhood_qual(df)\n    ordinal_exterior_qual(df)\n    luxury_sum(df)\n    outside_sf(df)\n    qualitative_scoring(df)\n    age_score(df)\n    utilities(df)\n", "intent": "Master function to keep features for transforming collected, so they can be called later to transform the unseen test data\n"}
{"snippet": "for b in np.arange(int(train_batches.n/batch_size)):\n    imgs, labels = train_batches.next()\n    prob_pred = model.predict(imgs)\n    labs = [classes[i] for i in np.argmax(prob_pred, axis=1)]\n    plots(imgs, titles = labs)\n", "intent": "We have total 4 batches with each batch containing 4 images in the train directory. Lets predict the classes for these 16 images, batch by batch.\n"}
{"snippet": "X_test_prepared = full_pipeline.transform(X_test)\ny_test_prob = grid_search_rfc.predict_proba(X_test_prepared)[:,1]\n", "intent": "We will now take our top model with the Random Forest Classifier and evaluate it on our test set.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfinal_model = grid_search_en.best_estimator_   \ny_te_estimation = final_model.predict(X_te)\nfinal_mse = mean_squared_error(y_te, y_te_estimation)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance\n"}
{"snippet": "mean_squared_error(y, linear_regression.predict(X[:, np.newaxis]))\n", "intent": "$$ MSE(y, \\hat{y}) = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}-1}(y_i - \\hat{y}_i)^2$$\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\ny_pred_all = log_clf.predict(X_test)\nprint (\"No. of features = {}\".format(X_train.shape[1]))\nprint (\"Classification Report :\")\nprint (classification_report(y_true=y_test,y_pred=y_pred_all))\n", "intent": "Each original image was 28 X 28 pixel. It is flattened to a 1-D and this has 784 features.\n"}
{"snippet": "toolkit_leadscore = toolkit_model.predict(test,output_type='probability')\ntoolkit_roi = calc_call_roi(test, toolkit_leadscore, 0.2 )\nprint 'ROI for calling 20%% of highest predicted contacts: %.2f%%' % toolkit_roi\n", "intent": "After scoring the list by probability to purchase, the ROI for calling the top 20% of the list is:\n"}
{"snippet": "results = new_toolkit_model.evaluate(quadratic.transform(test))\nprint \"accuracy: %.5f, precision: %.5f, recall: %.5f\" % (results['accuracy'], results['precision'], results['recall'])    \n", "intent": "Next, we evaluate the new **AutoML Classifier**, **`new_toolkit_model`**, on the **`test`** data set.\n"}
{"snippet": "newtoolkit_leadscore = new_toolkit_model.predict(quadratic.transform(test),output_type='probability')\nnewtoolkit_roi = calc_call_roi(quadratic.transform(test), newtoolkit_leadscore, 0.2)\nprint 'ROI for calling predicted contacts: %.2f%%' % newtoolkit_roi\n", "intent": "As a last step of evaluating the **`newtoolkit_model`**, lets compute our ROI if we again contact the 20% of the leads that this new model scored.\n"}
{"snippet": "results = new_boostedtrees_model.evaluate(age_binning.transform(quadratic.transform(test)))\nprint \"accuracy: %.5f, precision: %.5f, recall: %.5f\" % (results['accuracy'], results['precision'], results['recall'])\n", "intent": "Next we evaluate the **`new_boostedtrees_model`** on the **test** data set.\n"}
{"snippet": "y_true, y_pred = y_test, gsearch1.predict(X_test)\nprint('RMSE: {}'.format(mean_squared_error(y_test, y_pred)))\nprint(classification_report(y_true, y_pred))\n", "intent": "_search = ['[', ',', ']']\nfor col in x5.columns:\n    for s in _search:\n        if s in col:\n            print(col, flush=True)\n"}
{"snippet": "for i in range(10000):\n    inp_, labels_ = mnist.train.next_batch(128)\n    session.run(optimizer, \n                feed_dict={labels:labels_, inp:inp_})\n    if i % 1000 == 0:\n        print(\"{:.1f}%, \".format(evaluate(pred, inp)*100), end=\"\")\n", "intent": "Training the network looks about the same as above\n"}
{"snippet": "y_predict = trainer.predict(Xt)\n", "intent": "Train and test the model\n<img src = \"nerprecision.png\">\n"}
{"snippet": "mean_squared_error(y, model.predict(X))\n", "intent": "Another measure of how good our model is is mean squared error:\n"}
{"snippet": "print(model.score(X, y))\nprint(mean_squared_error(y, model.predict(X)))\n", "intent": "How good is our model? Here is the R-squared and MSE:\n"}
{"snippet": "print(model.score(X, y))\nprint(mean_squared_error(y, model.predict(X)))\n", "intent": "Then our model would be worse, indicated by our R-squared going down and our MSE going up:\n"}
{"snippet": "print(classification_report(y, model.predict(X)))\n", "intent": "Wow! Did that really say 100% accuracy? Let's check our confusion matrix:\n"}
{"snippet": "bpr.evaluate(bpr.val_ratings, sample_size=1000)\n", "intent": "Let's check the performance of our BPR model using overall AUC and coldstart AUC:\n"}
{"snippet": "vbpr.evaluate(vbpr.val_ratings, sample_size=1000)\n", "intent": "Lets check overall validation auc, test auc and cold start auc:\n"}
{"snippet": "prediction = clf.predict_proba(df_input)\ndec_prediciton = []\nfor i in prediction :\n    dec_prediciton.append(i[1])\ncols = ['name', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1']\ndf_result = df_female_demo.loc[:, cols]\ndf_result['dec_prediction'] = dec_prediciton\ndf_result.sort_values(by = ['dec_prediction'], inplace = True, ascending= False )\ndf_result\n", "intent": "Next, Let's input above dataframe into our model.\n"}
{"snippet": "prediction = clf.predict_proba(df_input)\ndec_prediciton = []\nfor i in prediction :\n    dec_prediciton.append(i[0])\ncols = ['iid', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1']\ndf_result = df_male_drop.loc[:, cols]\ndf_result['dec_prediction'] = dec_prediciton\ndf_result.sort_values(by = ['dec_prediction'], inplace = True, ascending= False )\ndf_result.head(5)\n", "intent": "Next, Let's input above dataframe into our model.\n"}
{"snippet": "np.round(np.power(10,np.column_stack((reg.predict(Xtest),ytest))) - 1,decimals=0).astype(int)\n", "intent": "Sanity check on some records.\n"}
{"snippet": "y_predicted = model.predict(X_test)\naccuracy_score(y_test,y_predicted)\n", "intent": "Prediction and Analysis \n"}
{"snippet": "new_types = nn.predict(new)\nprint iris.target_names[new_types]\n", "intent": "Now the classification algorithm is set up and we can predict the class affiliations for our new data sample elements:\n"}
{"snippet": "logistic.predict(X.head(10))\n", "intent": "<span style=\"color:\n"}
{"snippet": "print(accuracy_score(gridsearch.predict(Xtestlr), ytestlr))\n", "intent": "No, GridsearchCV did not game the same value for C when all Cs were tested\n"}
{"snippet": "print(metrics.classification_report(expected, predicted)) \nprint(metrics.confusion_matrix(expected, predicted)) \n", "intent": "Summarize the fit of the model \n"}
{"snippet": "print(metrics.classification_report(expected, predicted)) \nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "Summarize the fit of the model \n"}
{"snippet": "print(classification_report(sst_train_y, sst_train_predicted_y))\n", "intent": "Since we have the labels, we can see how we did in reconstructing them:\n"}
{"snippet": "def f_score(cm, beta):\n    p = precision(cm)\n    r = recall(cm)\n    return (beta**2 + 1) * ((p * r) / ((beta**2 * p) + r))\n", "intent": "[F scores](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n"}
{"snippet": "def f1_score(cm):\n    return f_score(cm, beta=1.0)\n", "intent": "With `beta=1`, this is the __F1 score__, which gives equal weight to both categories:\n"}
{"snippet": "def macro_f_score(cm, beta):\n    return f_score(cm, beta).mean(skipna=False)\n", "intent": "The [macro-averaged F$_{\\beta}$ score](http://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "def weighted_f_score(cm, beta):\n    scores = f_score(cm, beta=beta).values\n    weights = cm.sum(axis=1)\n    return np.average(scores, weights=weights)\n", "intent": "[Weighted F$_{\\beta}$ scores](http://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "predictions = mod.predict_proba(X_test)\n_, pos_predictions = zip(*predictions)\n", "intent": "And use the classifier to make predictions; the second step here keeps just the probabilities for the positive class:\n"}
{"snippet": "def mean_squared_error(y_true, y_pred):\n    diffs = (y_true - y_pred)**2\n    return np.mean(diffs)\n", "intent": "The [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy with Resnet50: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "And print report and confusion matrix.\n"}
{"snippet": "preds = model.evaluate(test_dataset, test_labels)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model performs on the test set:\n"}
{"snippet": "predicted_symbols = model.predict(symbols)\n", "intent": "Carry out the prediction\n"}
{"snippet": "from sklearn.metrics import accuracy_score, classification_report\ny_pred = clf_gini.predict(X_test)\nprint(y_pred)\nprint (\"Accuracy is \", accuracy_score(y_test,y_pred)*100)\nprint (classification_report(y_test, y_pred))\n", "intent": "Test the Decision Tree by test dataset(30%)\nCalculate accuracy by comparing predicted value and actual value of test dataset\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(Y_test, Y_pred))\n", "intent": "**Question:** Compute the accuracy.\n"}
{"snippet": "tijdregel()\nmodel.load_weights(saved_model_file)\nflower_pred_proba = model.predict_proba(test_tensors, verbose=1)\nflower_predictions = list(np.argmax(flower_pred_proba, axis=1)) \nprint(\"Done!\")\n", "intent": "We try out our model on the test dataset of flower images.\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "ResNet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def accuracy(pred,lbls): \n    return metrics.r2_score(lbls, pred)\ndef error(pred,lbls):\n    E=0.0\n    for pt in range(NUM_ELPT):\n        E = E + np.sqrt(\n            np.sum(((lbls[:,pt] - pred[:,pt])*80)**2)/lbls.shape[0])\n    return E/NUM_ELPT\n", "intent": "Really quick define an accuracy function:\n"}
{"snippet": "def accuracy(pred,lbls): \n    return metrics.r2_score(lbls, pred)\ndef error(pred,lbls):\n    E=0.0\n    for pt in range(NUM_ELPT):\n        E = E + np.sqrt(\n            np.sum(((lbls[:,pt] - pred[:,pt])*80)**2 + ((lbls[:,pt+NUM_ELPT] - pred[:,pt+NUM_ELPT])*80)**2)/lbls.shape[0])\n    return E/NUM_ELPT\n", "intent": "Set up our own accuracy checker\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss_l = []\n    for i in range(len(style_layers)):\n        idx      = style_layers[i]\n        G        = gram_matrix(feats[idx])\n        L_s      = style_weights[i] * tf.reduce_sum((G - style_targets[i])**2)\n        style_loss_l.append(L_s)\n    Style_loss = tf.add_n(style_loss_l)\n    return Style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    print(student_output)\n    error = rel_error(correct, student_output)\n    print('Error is {:.4f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "rmse = sqrt(mean_squared_error(inv_y[:,0], inv_yhat[:,0]))\nprint('Test RMSE: %.3f' % rmse)\n", "intent": "for i in range(inv_yhat.shape[0]):\n    print('(predicted,actual) = ( %s , %s )' % (int(inv_yhat[i,0]),int(inv_y[i,0])))\n"}
{"snippet": "print X_test[lr.predict(X_test)>1000000].T.sort_values(by=604310).head()\nprint X_test[lr.predict(X_test)>1000000].T.sort_values(by=604310).tail()\n", "intent": "Two bad signs! Negative ages, which can't be correct, and someone coming in at 18 trillion years old.\n"}
{"snippet": "model.load_weights('dog.best.hdf5')\nfrom sklearn.metrics import accuracy_score\ny_pred = [np.argmax(y_hat) for y_hat in model.predict(x_test)]\ny_true = [np.argmax(y) for y in y_test]\nprint('Test accuracy: %.2f%%' % (accuracy_score(y_true, y_pred)*100))\n", "intent": "The weights corresponding to the best model are loaded and the accuracy of the model is calculated using the test images.\n"}
{"snippet": "naive_bayes = NaiveBayesClassifier.train\nclassifier = sentim_analyzer.train(naive_bayes, training_set)\nfor key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n    print('{0}: {1}'.format(key, value))\n", "intent": "We can now train our classifier on the training set, and subsequently output the evaluation results:\n"}
{"snippet": "testdmat = xgb.DMatrix(X_TEST)\ny_pred = final_gb.predict(testdmat) \nprint(y_pred)\npredicted = y_pred\npredicted[predicted > 0.5] = 1\npredicted[predicted <= 0.5] = 0\nX_TEST[\"REAL\"] = Y_TEST\nX_TEST[\"PRED\"] = predicted\nret = accuracy_score(predicted, Y_TEST), 1-accuracy_score(predicted, Y_TEST)\nprint(\"Accuracy is %s\" % ret[0])\n", "intent": "Now try the test data\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "Classification accuracy: percentage of correct predictions\n"}
{"snippet": "precision = TP / float(TP + FP)\nprint(precision)\nprint(metrics.precision_score(y_test, y_pred_class))\n", "intent": "precision or positive predictive value (PPV)\n"}
{"snippet": "logreg.predict_proba(X_test)[0:10, 1]\n", "intent": "For more complete list of metrics see:\nhttps://en.wikipedia.org/wiki/Precision_and_recall\n"}
{"snippet": "numpy.sqrt(\n    cross_val_score(\n        boost_model,\n        X=boston_housing_df[X_column_names],\n        y=boston_housing_df.medv,\n        cv=KFold(n=nb_samples,\n                 n_folds=5,   \n                 shuffle=True),\n        scoring=mse_score).mean())\n", "intent": "The multivariate Boosted Trees model has an estimated Cross Validation-estimated OOS RMSE of:\n"}
{"snippet": "boost_test_pred_probs = boost_model.predict_proba(X=get_dummies(X_test))\nboost_test_oos_performance = bin_classif_eval(\n    boost_test_pred_probs[:, 1], churn_test == 'yes',\n    pos_cat='yes', thresholds=selected_prob_threshold)\nboost_test_oos_performance\n", "intent": "Let's then evaluate the performance of the selected Boosted Trees model at the decision threshold determined above:\n"}
{"snippet": "log_reg_pred_probs = log_reg_model.predict_proba(\n    X=X_standard_scaler.transform(tabloid_test[X_var_names]))\nlog_reg_oos_performance = bin_classif_eval(\n    log_reg_pred_probs[:, 1], tabloid_test.purchase,\n    pos_cat=1, thresholds=selected_prob_threshold)\nlog_reg_oos_performance\n", "intent": "The expected performance of the model on the Test set at the decision threshold determined above is as follows:\n"}
{"snippet": "rf_pred = rf_model.predict(X=X_test)\nrf_oos_accuracy = (rf_pred == y_test).sum() / nb_test_samples\nrf_oos_accuracy\n", "intent": "We'll now evaluate the OOS performances of these 2 models on the Test set:\n"}
{"snippet": "linear_model.predict(test_case)\n", "intent": "Linear Model predicts price of:\n"}
{"snippet": "knn_model.predict(test_case)\n", "intent": "KNN Model with $k = 30$ predicts price of:\n"}
{"snippet": "knn_model.predict(test_case)\n", "intent": "Best-k KNN Model predicts price of:\n"}
{"snippet": "rmse(y_hat=rf_model.predict(used_cars_test_df[['mileage', 'year']]),\n     y=used_cars_test_df.price)\n", "intent": "The Test-set OOS RMSE is:\n"}
{"snippet": "rmse(y_hat=rf_model.predict(pandas.get_dummies(used_cars_test_df[predictor_names])),\n     y=used_cars_test_df.price)\n", "intent": "The Test-set OOS RMSE is:\n"}
{"snippet": "numpy.sqrt(\n    cross_val_score(\n        boost_model,\n        X=pandas.get_dummies(used_cars_train_df[predictor_names]),\n        y=used_cars_train_df.price,\n        cv=KFold(n=len(used_cars_train_df),\n                 n_folds=5,   \n                 shuffle=True),\n        scoring=mse_score).mean())\n", "intent": "Based on the Training set, a Boosted Trees ensemble with many predictors has the following estimated OOS RSME:\n"}
{"snippet": "X_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(iris_dataset['target_names'][prediction]))\n", "intent": "We can now use the fit to make a prediction for newly measured data.\n"}
{"snippet": "scores_mm = cross_val_score(pipe_mm, X, y, cv=10)\nscores_ca = cross_val_score(pipe_ca, X, y, cv=10)\nprint(f'MM: {scores_mm.mean():.3f} +/- {scores_mm.std(ddof=0):.3f}')\nprint(f'CA: {scores_ca.mean():.3f} +/- {scores_ca.std(ddof=0):.3f}')\n", "intent": "OK, so somewhat similar... It's hard to tell which model may be better, so let's increase the number of folds and save the results.\n"}
{"snippet": "mse = mean_squared_error(y_red_train, predictor.predict(X_red_train))\nprint ('Mean squared error of the red wine training set: %s' % mse)\nmse = mean_squared_error(y_red_test, predictor.predict(X_red_test))\nprint ('Mean squared error of the red wine test set: %s' % mse)\n", "intent": "Below is the calculated mean squared error.\n"}
{"snippet": "mse_rounded = mean_squared_error(y_red_train, np.round(predictor.predict(X_red_train)))\nprint ('Mean squared error of the rounded red wine training set: %s' % mse_rounded)\nmse_rounded = mean_squared_error(y_red_test, np.round(predictor.predict(X_red_test)))\nprint ('Mean squared error of the rounded red wine test set: %s' % mse_rounded)\n", "intent": "It seems that most of our error might be due to rounding. Let's round out our predictions and measure our new error! \n"}
{"snippet": "future_scores = [model.predict(df=test_set) for model in models]\nfutures.wait(future_scores)\n", "intent": "Like training jobs, scoring jobs can also be fired off in parallel for some sweet speedy scoring goodness\n"}
{"snippet": "loaded_model = ModelPipeline.from_existing(extra_trees_job_id, extra_trees_run_id)\nloaded_fut = loaded_model.predict(df=test_set)\nprint(loaded_fut.result()['state'])\nloaded_fut.table.head()\n", "intent": "- from_existing takes job_id and run_id, and reloads the trained model\n- I'll call .result() on my predict job to get it to block until finished\n"}
{"snippet": "prediction=classifier.predict(X_test)\naccuracy=accuracy_score(np.array(y_test),prediction)\nprint('The accuracy of our Random Forest model is {}'.format(accuracy))\n", "intent": "Now, making a prediction on the test set and checking the model's accuracy\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100 * np.sum(np.array(Resnet50_predictions) == np.argmax(test_targets, axis=1)) / len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "regret = 0\nfor user in range(n_users):\n    mean_egreedy_score = np.mean([get_egreedy_score(user, R_, mask_trin, gamma, model, epsilon) for _ in range(100)])\n    regret += get_best_score(user, R_, mask_trin, gamma) - mean_egreedy_score\n    break \nregret\n", "intent": "<mark>NOTE</mark> This only calculates regret <strong>for one user</strong> and <strong>without retraining</strong> for sake of speed.\n"}
{"snippet": "predicted_from_val = clf.predict(X_val)\nexpected_from_val = y_val\nmetrics.accuracy_score(expected_from_val, predicted_from_val)\n", "intent": "Step 6: Identify the model's accuracy score on the validation set.\n"}
{"snippet": "predicted_from_test = clf.predict(df_test[df_test.columns.tolist()[:-1]])\nexpected_from_test = df_test['activity']\nmetrics.accuracy_score(expected_from_test, predicted_from_test)\n", "intent": "Step 7: Identify the model's overall accuracy and OOB scores.\n"}
{"snippet": "print('An individual weighing 145 lbs who is 15 lbs underweight:')\nprint(gnb.predict([[145, 160, -15]]))\n", "intent": "Step 3: Now introduce some new individuals. What does the classifier predict their gender to be?\n"}
{"snippet": "predictions = classifier.predict(example_counts)\npredictions\n", "intent": "<h1>Time to predict</h1>\n"}
{"snippet": "clf2.predict(X_new)\n", "intent": "So there is a 90% chance that the new test sample is a setosa.\nThis is what the predict function would return.\n"}
{"snippet": "from sklearn import metrics\np = metrics.precision_score(y_test, y_pred)\nr = metrics.recall_score(y_test, y_pred)\nprint(p)\nprint(r)\n", "intent": "Another method to find Precision and Recall\n"}
{"snippet": "metrics.f1_score(y_test, y_pred)\n", "intent": "Now let's calculate the F-score\nF1 = 2PR / (P+R)\n"}
{"snippet": "print(metrics.classification_report(y_test, y_pred, target_names = ['Stars','QSOs']))\n", "intent": "Really handy function that does all this in one go -\n"}
{"snippet": "prediction = clf_regr.predict(dataset1)\nmse_scores_regr = cross_val_score(clf_regr, dataset1,dataset['ViolentCrimesPerPop'], scoring = 'neg_mean_squared_error' , cv=10)\nprint(\"fold value\")\nprint(mse_scores_regr)\nprint(\"MSE\")\nmse_scores_regr.mean()\n", "intent": "In this case MSE is the mean value of the squared deviations of the predictions from the true values.\n"}
{"snippet": "ridge_predict = clf_ridge.predict(dataset1)\nprint(\"MSE after prediction\")\nmean_squared_error(dataset['ViolentCrimesPerPop'], ridge_predict)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "mean_squared_error(dataset['ViolentCrimesPerPop'], pred)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "bottleneck_model_predictions = [np.argmax(mboo_model.predict(np.expand_dims(feature, axis=0))) \n                                for feature in test_bottleneck_model]\ntest_accuracy = 100*np.sum(np.array(bottleneck_model_predictions)==np.argmax(test_targets, axis=1))/len(bottleneck_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "evaluate_input_fn = get_input_fn('evaluation')\nmodel.evaluate(evaluate_input_fn)\n", "intent": "Remember, we need the evaluation input function when we test our model\n"}
{"snippet": "petalPredict = softmax_reg.predict([ checkPetal ])\npetalProb = softmax_reg.predict_proba([checkPetal])\nprint(\"DS9: Petal based for\", checkPetal,\n      \"results in Iris\", iris.target_names[petalPredict][0],\n      \"with \", round(petalProb[0][petalPredict][0]*100, 5),\n      \"% probability!\" )\nprint(petalProb)\n", "intent": "Result based on Petal\n--------------------------------\n"}
{"snippet": "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\nloss = seq2seq.sequence_loss(decode_outputs, labels, loss_weights, output_vocab_size)\noptimizer = tf.train.AdamOptimizer(1e-4)\ntrain_op = optimizer.minimize(loss)\n", "intent": "sequence_loss is cross-entropy on the soft max of the decode outputs.\n"}
{"snippet": "metrics.precision_score(y_true, y_pred)\n", "intent": "Turns out that precision isn't better than accuracy in our case. Let's check our math with\nscikit-learn:\n"}
{"snippet": "metrics.mean_squared_error(y_true, y_pred)\n", "intent": "For our convenience, scikit-learn provides its own implementation of the mean squared\nerror:\n"}
{"snippet": "metrics.explained_variance_score(y_true, y_pred)\n", "intent": "Let's verify our math with scikit-learn:\n"}
{"snippet": "metrics.r2_score(y_true, y_pred)\n", "intent": "The same value can be obtained with scikit-learn:\n"}
{"snippet": "y_pred = linreg.predict(X_test)\n", "intent": "In order to test the **generalization performance** of the model, we calculate the mean\nsquared error on the test data:\n"}
{"snippet": "ret, y_pred = lr.predict(X_train)\n", "intent": "Let's see for ourselves by calculating the accuracy score on the training set:\n"}
{"snippet": "_, y_pred = svm.predict(X_test)\n", "intent": "Call the classifier's `predict` method to predict the target labels of all data\nsamples in the test set:\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred)\n", "intent": "Use scikit-learn's `metrics` module to score the classifier:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Then we can calculate the performance of the algorithm using scikit-learn's\naccuracy_score metric:\n"}
{"snippet": "labels = agg.fit_predict(X)\n", "intent": "Fitting the model to the data works, as usual, via the `fit_predict` method:\n"}
{"snippet": "_, y_hat = mlp.predict(X)\n", "intent": "The same goes for predicting target labels:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_hat.round(), y)\n", "intent": "The easiest way to measure accuracy is by using scikit-learn's helper function:\n"}
{"snippet": "model.evaluate(X, y)\n", "intent": "After the training completes, we can evaluate the classifier as follows:\n"}
{"snippet": "_, y_hat_test = mlp.predict(X_test_pre)\naccuracy_score(y_hat_test.round(), y_test_pre)\n", "intent": "But, of course, what really counts is the accuracy score we get on the held-out test data:\n"}
{"snippet": "model.evaluate(X_test, Y_test, verbose=1)\n", "intent": "After training completes, we can evaluate the classifier:\n"}
{"snippet": "_, y_hat = rtree.predict(X_test.astype(np.float32))\n", "intent": "The test labels can be predicted with the `predict` method:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_hat)\n", "intent": "Using scikit-learn's `accuracy_score`, we can evaluate the model on the test set:\n"}
{"snippet": "knn.train(X, cv2.ml.ROW_SAMPLE, y)\n_, y_hat = knn.predict(X)\n", "intent": "Then we train the model and use it to predict labels for the data that we already know:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_hat)\n", "intent": "Finally, we compute the fraction of correctly labeled points:\n"}
{"snippet": "_, y_test_hat = knn.predict(X_test)\naccuracy_score(y_test, y_test_hat)\n", "intent": "When we test the model on the test set, we suddenly get a different result:\n"}
{"snippet": "gbm_train_preds = gbm.predict(x_train, num_iteration = gbm.best_iteration)\ngbm_test_preds = gbm.predict(x_test, num_iteration = gbm.best_iteration)\nprint gbm_train_preds.shape\nprint gbm_test_preds.shape\n", "intent": "Calculate predictions for both train and test sets, and then calculate MSE and RMSE for both datasets\n"}
{"snippet": "y_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\nacc_train = metrics.accuracy_score(y_train, y_pred_train)\nacc_test = metrics.accuracy_score(y_test, y_pred_test)\nprint 'Random Forest Classifier on Input Data: '\nprint 'Train accuracy: ' + str(acc_train)\nprint 'Test accuracy: ' + str(acc_test)\n", "intent": "Predict and calculate accuracy:\n"}
{"snippet": "frankenz=fz.FRANKENZ(N_members=10) \nmodel_obj,model_Nobj,model_ll,model_Nbands=frankenz.predict(p1,sqrt(v1),mask_train_d,p2,sqrt(v2),mask_test_d,\n                                                           impute_train=winbet_train,impute_test=winbet_test,\n                                                            ll_func=fz.loglikelihood_s)\n", "intent": "Let's not get too far ahead of ourselves though. Let's first compute the FRANKEN-Z sparse likelihoods.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nr2_score(y_test, y_pred, multioutput='variance_weighted') \n", "intent": "Measure the performance of the model above using R2 and MSE\n"}
{"snippet": "model.predict(X_test)\n", "intent": "Using the knn classifier, predict the class labels for the test set X_test.\n"}
{"snippet": "def permutationTesting():\n    null_cv_scoresdumb = cross_val_score(DummyClassifier(), X, y, cv=10)  \n    print(null_cv_scoresdumb)\n    meannull_cv_scoresdumb = np.mean(null_cv_scoresdumb)\n    print(meannull_cv_scoresdumb)\n", "intent": "to measure probablility of chance first score is mean accuracy of the null scores without perm targets, then the array, then the p-value. \n"}
{"snippet": "best_model_pred2 = model_f.predict(X_Final_Test)\n", "intent": "__Inception Module CNN with Functional API and Batch Normalization:__ _Predict_\n"}
{"snippet": "base_df['cluster'] = k_means.predict(base_df)\nbase_df.head(5)\n", "intent": "**Let's save the clusters to our dataframe.**\n"}
{"snippet": "threshold_df['cluster'] = k_means.predict(threshold_df) \nthreshold_df.head(5)\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">threshold_df</code>.**\n"}
{"snippet": "pca_df['cluster'] = k_means.predict(pca_df)\npca_df.head(5)\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">pca_df</code>.**\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint(\n    'F1 score in-sample (by SKLearn)', \n    f1_score(\n        y_true = nn_results['labels']['actual'], \n        y_pred = nn_results['labels']['predicted'], average='weighted'\n    )\n)\n", "intent": "Just checking the calculcation :)\n"}
{"snippet": "import sklearn.dummy\ndummy_classifier = sklearn.dummy.DummyClassifier(strategy='most_frequent')\nsklearn.cross_validation.cross_val_score(dummy_classifier,\n                                         X,\n                                         whole_df.junior,\n                                         cv=10,\n                                         scoring='accuracy')\n", "intent": "Lets compare with Dummy:\n"}
{"snippet": "sklearn.cross_validation.cross_val_score(logreg,\n                                         X,\n                                         DF.Y_pred,\n                                         cv=10,\n                                         scoring='accuracy')\n", "intent": "As you can see model works on real data but the coefficients are much worse than in original data set.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0010.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "predictions = model_5.predict(validation_data[features].as_matrix())\nnum_false_positives=0\nfor i in range(len(predictions)):\n    if predictions[i]==1 and validation_data[target].iloc[i]==-1:\n        num_false_positives+=1\nprint num_false_positives\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "predictions=model_5.predict(validation_data[features])\ncmat=graphlab.evaluation.confusion_matrix(validation_data[target], predictions)\ncmat\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nerror_all = []\nfor n in xrange(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(train_data[target], predictions)\n    error_all.append(error)\n    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].as_matrix(),\n                      y_pred=model.predict(test_matrix))\nrecall_with_default_threshold = recall_score(y_true=test_data['sentiment'].as_matrix(),\n                      y_pred=model.predict(test_matrix))\nprecision_with_high_threshold = precision_score(y_true=test_data['sentiment'].as_matrix(),\n                      y_pred=predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true=test_data['sentiment'].as_matrix(),\n                      y_pred=predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "def average_predictions(model, test_document, num_trials=100):\n    avg_preds = np.zeros((model.num_topics))\n    for i in range(num_trials):\n        avg_preds += model.predict(test_document, output_type='probability')[0]\n    avg_preds = avg_preds/num_trials\n    result = gl.SFrame({' topics':themes, 'average predictions':avg_preds})\n    result = result.sort('average predictions', ascending=False)\n    return result\n", "intent": "To get a more robust estimate of the topics for each document, we can average a large number of predictions for the same document:\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_vgg19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.5f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==\n                           np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "testResults = model.predict(testData[['YrSold', 'MoSold', 'LotArea', 'BedroomAbvGr']])\ntestVals = np.array([testData['Id'].values, testResults]).T\n", "intent": "As expected the error is a bit higher because we didn't test against the training data. Now let's test on the actual test data and submit.\n"}
{"snippet": "probs = logMod2.predict_proba(XTest)\nprobs\n", "intent": "So we only did a half of percentage point, which is only neglibile. Let's see probabilities as opposed to straight-up predictions.\n"}
{"snippet": "predictions_resnet50 = [np.argmax(model_resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(predictions_resnet50)==np.argmax(test_targets, axis=1))/len(predictions_resnet50)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cross_val_score(pipelineOptimalC, X, y, scoring=perplexity, cv=5).mean()\n", "intent": "An realistic estimate of the perplexity of the protocol encrypted by this pipeline is obtained by cross validation.\n"}
{"snippet": "activations = model3.predict(x_train[:,:,:,np.newaxis]) \nprint(activations.shape)\n", "intent": "Project the entire data set (10000 subjects) into the CNN feature space.\n"}
{"snippet": "nn.predict_proba(X_test[1:2])\n", "intent": "Below I will show a correct prediction along with the probabilities that the model came up with.\n"}
{"snippet": "test['predictions'] = random_forest_clf.predict( test[features] )\ntrain['predictions'] = random_forest_clf.predict( train[features] )\ntest['svm-predictions'] = svm_classifier.predict( test[features])\ntrain['svm-predictions'] = svm_classifier.predict( train[features])\n", "intent": "We'll need to to get the predictions from both classifiers, so we add columns to the test and training sets for the predictions.\n"}
{"snippet": "Y_test_rf = model_rf2.predict(X_test)\nY_test = model.predict(X_test)\n", "intent": "Let's perform prediction on the test data and save it to a file. We will use a random forrest model as stated above:\n"}
{"snippet": "prob = clf.predict_proba(Xtest)[:, 1]\narea = average_precision_score(ytest, prob)\nprint(\"Area under PR Curve\")\nprint(area)\n", "intent": "Now evaluate area under precision-recall (PR) curve\n"}
{"snippet": "pred = clf.predict(Xtest)\nprint(recall_score(ytest, pred))\n", "intent": "Determine the recall score based on 50% decision threshold\n"}
{"snippet": "prob = clf.predict_proba(Xtest)[:, 1]\narea = average_precision_score(ytest, prob)\nprint(\"Area under PR Curve\")\nprint(area)\n", "intent": " Area under the curve\n"}
{"snippet": "pred = clf2.predict(Xtest)\nprint(\"Test set accuracy:\")\nprint(np.mean(pred == ytest))\n", "intent": "Let's see how well Calvin's decision tree performs on the test set.\n"}
{"snippet": "area = average_precision_score(ytest, prob)\nprint(\"Area under PR Curve\")\nprint(area)\n", "intent": "The area under the PR curve for the test set is:\n"}
{"snippet": "pred = knn.predict(Xtest)\nprint(\"Accuracy: \")\nprint(np.mean(pred == ytest))\n", "intent": "And also evaluate the accuracy\n"}
{"snippet": "area = average_precision_score(ytestBS, predProb)\nprint(\"Area under PR Curve\")\nprint(area)\n", "intent": "The area under the PR curve for the test set is:\n"}
{"snippet": "pred = knn.predict(Xtest)\nprint(\"Accuracy: \")\nprint(np.mean(pred == ytestBS))\n", "intent": "And also evaluate the accuracy:\n"}
{"snippet": "pred = svm.predict(XtestSub)\nprint(\"Accuracy: \")\nprint(np.mean(pred == ytest))\n", "intent": "Predict on test set\n"}
{"snippet": "prob = svm.predict_proba(XtestSub)[:, 1]\narea = average_precision_score(ytest, prob)\nprint(\"Area under PR Curve\")\nprint(area)\n", "intent": "Evaluate area under the PR curve\n"}
{"snippet": "predProb = ada.predict_proba(XtestSub)[:, 1]\npr = average_precision_score(ytest, predProb)\nprint(\"Area under PR curve: \")\nprint(pr)\n", "intent": "Output the area under the PR curve\n"}
{"snippet": "pred = ada.predict(XtestSub)\nprint(\"Accuracy: \")\nprint(np.mean(pred == ytest))\n", "intent": "And evaluate the accuracy of the model\n"}
{"snippet": "prob = clf.predict_proba(Xtest)[:, 1]\npr = average_precision_score(ytest, prob)\nprint(\"Area under PR curve: \")\nprint(pr)\n", "intent": "Assess area under PR curve\n"}
{"snippet": "pred = nn.predict(Xtest)\nnp.mean(pred == ytest)\n", "intent": "Predict and assess accuracy\n"}
{"snippet": "prob = nn.predict_proba(Xtest)[:, 1]\npr = average_precision_score(ytest, prob)\nprint(\"Area under precision-recall curve: \")\nprint(pr)\n", "intent": "Output area under the curve\n"}
{"snippet": "pr = average_precision_score(ytest, probs)\nprint(\"Area under precision-recall curve: \")\nprint(pr)\n", "intent": "Let us evaluate the area under the precision-recall curve\n"}
{"snippet": "def classify(X_, w_):\n    return predictions\ndef compute_loss(X_, y_, w_):\n    return loss\ndef compute_grad(X_, y_, w_):\n    return gradient\n", "intent": "Apply the feature transformation function 'expand' to the data inside 'classify', 'compute_loss' and 'compute_grad' functions\n"}
{"snippet": "def classify(X_, w_):\n    return np.sign(np.dot(expand(X_), w_))\ndef compute_loss(X_, y_, w_):\n    return np.mean(np.maximum(0, 1-y_*np.dot(expand(X_), w_)))\ndef compute_grad(X_, y_, w_):\n    return np.sum((-expand(X_)*y_[:, np.newaxis])[y_*np.dot(expand(X_), w_) < 1], axis=0)\n", "intent": "Apply the feature transformation function 'expand' to the data inside 'classify', 'compute_loss' and 'compute_grad' functions\n"}
{"snippet": "plot_loss(fit)\n", "intent": "The loss decreases for both the training and test set. The neural network can predict the label of the test set\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Calculate the model accuracy\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n", "intent": "Calculate the model accuracy\n"}
{"snippet": "class Model:\n    def train(self, bad_lengths, good_lengths):\n        self._threshold = \n        return self\n    def predict(self, message):\n", "intent": "A model is an object which contains summary of the training data that allows us to do predictions on the test data.\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n", "intent": "Now we can validate our trainning session with the test set.\n"}
{"snippet": "plot_loss(net)\n", "intent": "This allows us to visualize the extent of overfit and whether the loss has converged.\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "We can now make predictions on both our training and testing sets.\n"}
{"snippet": "bnf_predictions = [np.argmax(bnf_model.predict(np.expand_dims(feature, axis=0))) for feature in test_bnf]\ntest_accuracy = 100*np.sum(np.array(bnf_predictions)==np.argmax(test_targets, axis=1))/len(bnf_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nprint 'R Squared Value: ' + str(r2_score(predictions, test_y))\nprint 'Mean Squared Error: ' + str(mean_squared_error(predictions, test_y))\n", "intent": "- Using mean squared error, we can find the accuracy of the linear regression model\n"}
{"snippet": "rsquared = r2_score(y_test, y_pred)\nrsquared\n", "intent": "**==>  Question : Does bill amount influence tip amount? (are they strongly linked?) **\n"}
{"snippet": "tip_data['est_tip'] =  ??? \ntip_data['est_tip'] = regr.predict(tip_data.bill.values.reshape(10,1)) \ntip_data\n", "intent": "**=>TODO: create a new pandas column called est_tip **\n"}
{"snippet": "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\nloss = tf.nn.seq2seq.sequence_loss(decode_outputs, labels, loss_weights, output_vocab_size)\noptimizer = tf.train.AdamOptimizer(1e-4)\ntrain_op = optimizer.minimize(loss)\n", "intent": "sequence_loss is cross-entropy on the soft max of the decode outputs.\n"}
{"snippet": "y_pred = []\nfor i in range(0,len(X_test)):\n\ty_pred.extend(classifier.predict(np.array(X_test[i]).reshape(1, -1)))\n", "intent": "We create a list of predictions `y_pred`.\n"}
{"snippet": "print(accuracy_score(y_test, y_pred))\n", "intent": "The list of actual gender classifications `y_test` is used to evaluate the predicted values `y_pred` and a simple accuracy is determined.\n"}
{"snippet": "from pymks.tools import draw_components\nstress_predict = model.predict(data_test, periodic_axes=[0, 1])\ndraw_components([model.fit_data[:, :3],\n                model.reduced_predict_data[:, :3]],\n                ['Training Data', 'Testing Data'])\n", "intent": "Now we want to draw how the samples are spread out in PCA space and look at how the testing and training data line up.  \n"}
{"snippet": "time_steps = 50\nfor steps in range(time_steps):\n    ch_sim.run(phi_sim)\n    phi_sim = ch_sim.response\n    phi_prim = prim_model.predict(phi_prim)\n    phi_legendre = leg_model.predict(phi_legendre)\n", "intent": "In order to move forward in time, we need to feed the concentration back into the Cahn-Hilliard simulation and the MKS models.\n"}
{"snippet": "for steps in range(time_steps):\n    ch_sim.run(phi_sim)\n    phi_sim = ch_sim.response\n    phi_prim = prim_model.predict(phi_prim)\n    phi_legendre = leg_model.predict(phi_legendre)\n", "intent": "Once again, we are going to march forward in time by feeding the concentration fields back into the Cahn-Hilliard simulation and the MKS models. \n"}
{"snippet": "y_predict = model.predict(X_new, periodic_axes=[0, 1])\n", "intent": "Now let's predict the stress values for the new microstructures. \n"}
{"snippet": "from scipy.stats.stats import pearsonr\ndef correlation_trees(clf, X):\n    c = []\n    for x1 in range(0, clf.estimators_.__len__()-1):\n        for x2 in range(x1+1, clf.estimators_.__len__()):\n            c.append( pearsonr(clf.estimators_[x1].predict(X), clf.estimators_[x2].predict(X) ) )\n    return np.average( c )\n", "intent": "Create function to compute average correlation among decision of all trees for a given classifier.\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "<a id='Evaluate'></a>\n[[ go back to the top ]](\nWith our model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "y_pred = regressor.predict(6.5)\nprint(y_pred)\n", "intent": "For an employee on an experience level of 6.5:\n"}
{"snippet": "pred = LinMod.predict(strat_train[cols])\n", "intent": "The training model then 'predicts' whether we should be long or short AUDUSD on a particular day\n"}
{"snippet": "pred = LinMod.predict(strat_valid[cols])\n", "intent": "We predict using the model trained on the training data and then \nto see if we are able to predict the subsequent return\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrmsecolumn = []\nfor column in df18:\n    try:\n        rmse = sqrt(mean_squared_error(np.array(df18[column]),df18['rank_order']))\n        rmsecolumn.append((rmse,column))\n    except ValueError:\n        print(column + ' ..whoeps')\nsorted(rmsecolumn)\n", "intent": "nog steeds niet interessant\n"}
{"snippet": "loss_train = regressor.evaluate(X_train, y_train,batch_size=32,verbose=1)\nloss_test = regressor.evaluate(X_test, y_test,batch_size=32,verbose=1)\nprint '\\nTraining Loss: ', loss_train\nprint 'Testing Loss: ', loss_test\n", "intent": "We evaluate the loss on testing data and training data\n"}
{"snippet": "inputs = X_test\ny_pred = regressor.predict(inputs)\nprint X_test.shape, len(y_test)\n", "intent": "We plot the real data against the predicted data for each\n"}
{"snippet": "loss_train = regressor.evaluate(X_train, y_train,batch_size=32,verbose=0)\nloss_test = regressor.evaluate(X_test, y_test,batch_size=32,verbose=0)\nprint 'Training Loss: ', loss_train\nprint 'Testing Loss: ', loss_test\n", "intent": "We evaluate the loss on testing data and training data\n"}
{"snippet": "inputs = X_test\ny_pred = regressor.predict(inputs)\nprint X_test.shape, len(y_test)\n", "intent": "We plot the real data against the predicted data for each model\n"}
{"snippet": "def vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n", "intent": "- https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.mean_absolute_error(y_pred,y_test))\n", "intent": "Mean absolute error is the average absolute value of the difference between the predicted and actual values:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_OH, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = reg.predict(X_test)\nprint(\"Variance score: {}\".format(r2_score(y_test, y_pred)))\n", "intent": "We can then evaluate the model performance ($R^{2}$-score) on the test set to *see how much variance in the model we are able to explain,*\n"}
{"snippet": "y_pred = gam.predict(X_test)\nprint(\"Variance score: {}\".format(r2_score(y_test, y_pred)))\n", "intent": "Evaluating the performance of our model on the test set we find an $R^{2}$ score of:\n"}
{"snippet": "y_pred = grid.predict(X_test)\nprint(\"Variance score: {}\".format(r2_score(y_test, y_pred)))\n", "intent": "Now that we have the best model from our grid search over the trainin set let see how it performs on the test set:\n"}
{"snippet": "def class_error(y, y_pred):\n    misclassified = 0\n    for i in range(len(y)):\n        if y[i] != y_pred[i]:\n            misclassified += 1\n    return float(misclassified) / len(y)\n", "intent": "Define a function for classification error:\n"}
{"snippet": "class_error(logistic.predict(train_X2), train_y2)\n", "intent": "Print the classification error on the training set:\n"}
{"snippet": "class_error(logistic.predict(valid_X2), valid_y2)\n", "intent": "Print the classification error on the validation set:\n"}
{"snippet": "def class_error(y, y_pred):\n    misclassified = 0\n    for i in range(len(y)):\n        if y[i] != y_pred[i]:\n            misclassified += 1\n    return float(misclassified) / len(y)\n", "intent": "Define a helper function to calculate classification error:\n"}
{"snippet": "class_error(pred_blend, valid_y2)\n", "intent": "print the classification error:\n"}
{"snippet": "def class_error(y, y_pred):\n    misclassified = 0\n    for i in range(len(y)):\n        if y[i] != y_pred[i]:\n            misclassified += 1\n    return float(misclassified) / len(y)\n", "intent": "A helper function to compute the classification error:\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nResNet50_test_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % ResNet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "results = m.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print \"%s: %s\" % (key, results[key])\n", "intent": "After the model is trained, we can evaluate how good our model is at predicting the labels of the holdout data:\n"}
{"snippet": "cross_validation.cross_val_score(clf_lsvc,xtrain_scale,y).mean()\n", "intent": "We can use cross-validation to estimate the accuracy of this classifier.\n"}
{"snippet": "Y_pred_gini = clf_gini.predict(X_test)\nY_pred_gini\n", "intent": "Let's see how both models do with the test sets.\n"}
{"snippet": "roc_auc_score(y,model.predict(x))\n", "intent": "We can see that our model has high bias, but not overfitting.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_bin, target_names=['unavailable', 'available']))\n", "intent": "[True Negative, False Positive]\n[False Negative, True Positive]\n"}
{"snippet": "real_case_pred = clf.predict(real_case)\n", "intent": "**Now it's time for prediction!** Fit the trained model on our real case test set.\n"}
{"snippet": "real_case_whole['price'] = price_hypo * 1.25\npred_adj_price = clf.predict(real_case_whole)\n1-sum(pred_adj_price)/91\n", "intent": "It appears that the booking rate actually decreases to 30%, as well as the total revenue. \n- Price increase\n"}
{"snippet": "model.predict(\"nuclear test in north korea\")\n", "intent": "It already works quite okay. However, when further inspecting the model, we find the following problem:\n"}
{"snippet": "model2.predict(\"nuclear test in north korea\")\n", "intent": "Rerun the predictions for `nuclear test in north korea` and `nuclear?? test in north korea??`. Can you explain the difference to the previous model?\n"}
{"snippet": "model2.predict(\"nuclear test in north korea\")\n", "intent": "Let's rerun the predictions for `nuclear test in north korea` and `nuclear?? test in north korea??`.\n"}
{"snippet": "y_submit = clfs[6].predict(X_submit)\nwrite_submission(y_submit, submission_filename='submission_ovo4.csv')\n", "intent": "This scored 0.77595. \nLet's try C=10 and submit.\n"}
{"snippet": "y_submit = clfs[4].predict(X_submit)\nwrite_submission(y_submit, submission_filename='submission_ovo5.csv')\n", "intent": "This scored :  0.76961\n"}
{"snippet": "y_submit = clfs[3].predict(X_submit)\nwrite_submission(y_submit, submission_filename='submission_ovo6.csv')\n", "intent": "This scored : 0.77293\n"}
{"snippet": "best_vect = best_parameters['vect']\nX_test_counts = best_vect.transform(X_test)\nX_test_tfidf = best_parameters['tfidf'].transform(X_test_counts)\nbest_predicted = best_parameters['clf'].predict(X_test_tfidf)\n", "intent": "Below we apply the best preprocessing transformations to our test data, and then predict the test results from our best trained classifier.\n"}
{"snippet": "parameters = all_parameters\nX, y, y_pred, classifier = random_forest_predict(df_players, parameters)\nprint('The accuracy is {0:.1f}%'.format(metrics.accuracy_score(y, y_pred)*100))\n", "intent": "Let's consider now all the 15 parameters and see what is going on.\n"}
{"snippet": "userArtistDataDF = userArtistDataRDD.filter(lambda x : int(x[1])!=1034635).toDF(['userID', 'artistID', 'playCount'])\nuserArtistDataDF = userArtistDataDF.withColumn('playCount', userArtistDataDF['playCount'].cast(LongType()))\nuserArtistDataDF = userArtistDataDF.groupBy('artistID').sum('playCount')\nuserArtistDataDF.show(1)\nartistsCounts = userArtistDataDF.rdd.map(lambda x : (int(x[0]),int(x[1]))).collectAsMap()\nbArtistsCounts = sc.broadcast(artistsCounts)\ndef getPlays(artistID):\n    return(bArtistsCounts.value.get(artistID,0))\nartistsClustersRDD = model_filtered.productFeatures().map(lambda x : [x[0], predict(x[1]), getPlays(x[0])])\nartistsClustersDF = artistsClustersRDD.toDF(['artistID', 'cluster', 'playCount'])\n", "intent": "Now that we have our artists clusters, time to prepare DataFrames to deal with the first group of users.\n"}
{"snippet": "from sklearn import metrics\nincome_pred = lm.predict(Age)\nprint('MSE:', metrics.mean_squared_error(income['Income'], income_pred))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "from sklearn.metrics import r2_score\npd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\nfor feature in feature_cols:\n    print \"'Grocery' and '\" + feature + \"'\"\n    print \"r2 score: {}\".format(r2_score(y_all, data[feature]))\n    print ''\n", "intent": "I will get a better understanding of the dataset by constructing a scatter matrix of each of the six product features present in the data.\n"}
{"snippet": "balanced_accuracy_score(y_test, y_pred)\n", "intent": "We can also use the `balanced_accuracy_score` when dealing with imbalanced sets :\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(classifier, data, target))\n", "intent": "One cross-validation gives spread out measures\n"}
{"snippet": "from sklearn import utils\nfor _ in range(10):\n    data, target = utils.shuffle(data, target)\n    print(cross_val_score(classifier, data, target))\n", "intent": "What if we try different random shuffles of the data?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(regressor, data, target))\n", "intent": "Explained variance vs Mean Square Error\n.......................................\nThe default score is explained variance\n"}
{"snippet": "print(cross_val_score(regressor, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1]))\n", "intent": "Along the Charles river\n"}
{"snippet": "print(cross_val_score(regressor, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1],\n                      scoring='neg_mean_squared_error'))\n", "intent": "Along the Charles river\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\nmost_frequent = DummyClassifier(strategy='most_frequent')\nprint(cross_val_score(most_frequent, digits.data, sevens))\n", "intent": "However, a stupid classifier can each good prediction wit imbalanced\nclasses\n"}
{"snippet": "print(cross_val_score(classifier, digits.data, sevens, scoring='recall'))\n", "intent": "**Recall**: Recall counts the fraction of class 1 actually detected\n"}
{"snippet": "print(cross_val_score(most_frequent, digits.data, sevens, scoring='recall'))\n", "intent": "Our recall isn't as good: we miss many sevens\nBut predicting the most frequent never predicts sevens:\n"}
{"snippet": "print(cross_val_score(most_frequent, digits.data, sevens,\n                      scoring='average_precision'))\n", "intent": "Naive decisions are no longer at .5\n"}
{"snippet": "random_choice = DummyClassifier()\nprint(cross_val_score(random_choice, digits.data, digits.target))\n", "intent": "The most frequent label is no longer a very interesting baseline\n"}
{"snippet": "print(cross_val_score(classifier, digits.data, digit_labels,\n                      scoring='roc_auc'))\n", "intent": "The ROC AUC can then be computed for each label\n"}
{"snippet": "print(cross_val_score(classifier, digits.data, digit_labels,\n                      scoring='average_precision'))\n", "intent": "as well as the average precision\n"}
{"snippet": "final_accuracy = accuracy_score(y_test, predictions)\nprint('Accuracy obtained with majority vote: {0:.2f}'.format(final_accuracy))\n", "intent": "d) Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model.\n"}
{"snippet": "import numpy as np\nxx, yy = np.mgrid[-3:3:.01, -3:3:.01]\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = log_reg.predict_proba(grid)[:].reshape(xx.shape)\n", "intent": "Plot decision boundary:\n"}
{"snippet": "probabilities = model.predict(validX)\nclasses = np.argmax(probabilities, axis=1)\n", "intent": "---\nLets use the model now to retrieve both the probabilities and the predicted classes on the validation set.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f% %' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(tmodel.predict(np.expand_dims(tensor, axis=0))) for tensor in test]\ntest_accuracy = 100 * np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "submission_set = get_data_set(train=False)\nsubmission_set.loc[:,0:7] = (submission_set.iloc[:, 0:7]-training_set_means)/training_set_sd\nsubmission_prediction = classifierFittedC.predict(submission_set)\noutput_result(\"outcome/OvR-regression.csv\",submission_prediction)\n", "intent": "The best model obtained in this analysis is use to make predictions for the submission set.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    gram_cur = []\n    for idx in style_layers:\n        gram_cur.append(gram_matrix(feats[idx].clone()))\n    style_loss = 0\n    for i in range(len(style_weights)):\n        style_loss += ((gram_cur[i] - style_targets[i])**2).sum() * style_weights[i]\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "xgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\n", "intent": "Here we will be predicting the values for both the XGBoost and Lasso model.\n"}
{"snippet": "x_test = np.array(['I won the math'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "def f1_score(y_true, y_pred):\n    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n    if c3 == 0:\n        return 0\n    precision = c1 / c2\n    recall = c1 / c3\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return f1_score \n", "intent": "<a id=\"compiler\"></a>\n"}
{"snippet": "test_x = np.array([df2[\"Open\"][0],df2[\"High\"][0],df2[\"Low\"][0]]).reshape(1,3)\npredicted_close_price = linear_mod.predict(test_x)\nprint (\"Predicted close price: {}\\nActual close price: {}\".format(predicted_close_price,df2[\"Close\"][0]))\n", "intent": "Now let's predict the closing price for April 17, 2017 by using the open, high and low prices for that day.\n"}
{"snippet": "test_x = np.array([df2[\"Open\"][0],df2[\"High\"][0],df2[\"Low\"][0]]).reshape(1,3)\npredicted_close_price = linear_mod.predict(test_x)\nprint \"Predicted close price: {}\\nActual close price: {}\".format(predicted_close_price,df2[\"Close\"][0])\n", "intent": "Now let's predict the closing price for April 17, 2017 by using the open, high and low prices for that day.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Answer: This classifier seems to have 96% accuracy on training data and 74% accuracy on the test set. It appears to be overfitting again\n"}
{"snippet": "def classify(features):\n    features=np.array([features])\n    prediction=log_model_hl.predict(features)\n    prediction=np.argmax(prediction[0])\n    if prediction == 1:\n        print('the banknote is forged')\n    else:\n        print('the banknote is real')\n", "intent": "we finally create a function that predicts whether a banknote is real depending on the model and check it on both a real and forged banknote:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_val)\nplot_confusion_matrix(confusion_matrix(y_val, y_pred), classes = range(10)) \n", "intent": "Here we'll plot the confusion matrix for our predictions on the validation dataset.\n"}
{"snippet": "mse_train = np.average((h - y)**2)\nr2_train = r2_score(y, h)\nmse_train, r2_train\n", "intent": "Let us check the accuracy parameters of training data.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(label_test, knn.predict(X_test))\n", "intent": "Let's look at the type of errors that these two classifiers made:\n"}
{"snippet": "print 'Case 1 prediction:', lm3.predict([1,68,64,1])\nprint 'Case 2 prediction:',lm3.predict([1,68,64,0])\n", "intent": "Now we can predict using certain values\n- Case 1: father 68'', mother 64'', male \n- Case 2: father 68'', mother 64'', female \n"}
{"snippet": "linreg = Linear_Regression(0.0000005, 0.1, True)\nX = df.drop(\"rings\", axis=1)\ny = df.rings\npredictions, costs = linreg.predict(X, y)\n", "intent": "Then, linear regression was conducted with a error threshold of $0.0000005$ and a learning rate of $0.1$, with normalized data:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = model_slim.predict(test_images)\nprediction\n", "intent": "Testing prediction printout and graphs.\n"}
{"snippet": "predictions = predict(parameters, X)\nprint('Accuracy: %d' % float((np.dot(Y, predictions.T) +\n                              np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "flist = [2,8]\nregr = feature_subset_regression(x,y,[2,8])\nprint \"w = \", regr.coef_\nprint \"b = \", regr.intercept_\nprint \"Mean squared error: \", mean_squared_error(y, regr.predict(x[:,flist]))\n", "intent": "Try using just features \n"}
{"snippet": "regr = feature_subset_regression(x,y,range(0,10))\nprint \"w = \", regr.coef_\nprint \"b = \", regr.intercept_\nprint \"Mean squared error: \", mean_squared_error(y, regr.predict(x))\n", "intent": "Finally, use all 10 features.\n"}
{"snippet": "preds = forest.predict(full_dev)\nprint(classification_report(dev_labels, preds))\n", "intent": "The full classification report:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\npredicted_home_prices\n", "intent": "The calculation of mean absolute error in the Melbourne data is\n"}
{"snippet": "predictions = iowa_model.predict(X)\nstep_4.check()\n", "intent": "Make predictions with the model's `predict` command. Save the predictions as first_preds.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "After building a model, the calculation of mean absolute error in the Melbourne data is:\n"}
{"snippet": "p_valid = KNN.predict(X_valid)\nprint(p_valid.shape)\n", "intent": "Step 2. Predicitng the validation set:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_valid, p_valid)\n", "intent": "Step3. Measuring the accuracy of the validation set:\n"}
{"snippet": "from sklearn.metrics  import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\np_valid = LR.predict(X_valid)\nprint(accuracy_score(y_valid, p_valid))\n", "intent": "Predicting the validation set and report the accuracy:\n"}
{"snippet": "target_names = ['Negative', 'Positive']\nprint(classification_report(y_valid, p_valid, digits = 3, target_names  = target_names))\n", "intent": "Printing clasification report (precision, recall, F1):\n"}
{"snippet": "target_names = ['Neg', 'Pos']\nprint(classification_report(y_valid, p_valid, digits = 3, target_names  = target_names))\n", "intent": "Printing clasification report (precision, recall, F1):\n"}
{"snippet": "eval_test = classifier.evaluate(input_fn=lambda:eval_input_fn(features=ids_test,\n                                                                    labels=y_test_ord,\n                                                                    batch_size=batch_size)\n                                     )\n", "intent": "Now we can evaluate our model on the test data.\n"}
{"snippet": "train_predicted_labels = model.predict(train)\n", "intent": "First, apply the model we just trained to the training data to get the predicted labels.\n"}
{"snippet": "print(\"Root Mean Squared Error (RMSE): {}\".format(math.sqrt(mean_squared_error(y_multi, cvp_multi))))\nprint(\"Mean Absolute Error(MAE): {}\".format(mean_absolute_error(y_multi, cvp_multi)))\nprint(\"Correlation Coefficient(CC): {}\".format(np.corrcoef(y_multi,cvp_multi)[0,1]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "ridge.pred = ridge.predict(trainX)\nprint(\"Training RMSE:\", np.sqrt(np.sum((ridge.pred - trainY)**2)))\n", "intent": "Compute the **training** RMSE of this fit:\n"}
{"snippet": "score = SegModel.evaluate(x_val, y_val, verbose=0)\nprint('Final Dice on validation set: {:.04f}'.format(1-score))\n", "intent": "After the training is complete, we evaluate the model again on our validation data to see the results.\n"}
{"snippet": "def get_next(text,token,model,fullmtx,fullText):\n    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n    tmp = token.texts_to_matrix(tmp, mode='binary')\n    p = model.predict(tmp)\n    bestMatches = p.argsort() [0][-20:]\n    bestMatch = np.random.choice(bestMatches,1)[0]\n    next_idx = np.min(np.where(fullmtx[:,bestMatch]>0))\n    return fullText[next_idx]\n", "intent": "This method is used for getting the next word of a given word.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Horrible predictions, first is of by some $70,000.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Micro-averaged precision = {:.2f} (treat instances equally)'\n      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\nprint('Macro-averaged precision = {:.2f} (treat classes equally)'\n      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))\n", "intent": "In micro: It is the weighted average. The class weight is defined by the class proportion. In macro: Each class has equal weight \n"}
{"snippet": "expected = test_labels\npredicted = classifier.predict(test_values)\n", "intent": "Now evaluate the model by predicting the value of the digit on the second half.\n"}
{"snippet": "cross_val_score(lr, X, y, cv=10, scoring='recall', verbose=5)\n", "intent": "Cross validate logistic regression with recall as scoring metric. Recall is defined as \nTrue Positives / (False Negatives + True Postives)\n"}
{"snippet": "y_pred_train=clf.predict(df_copy)\n", "intent": "Lets predict the F1-score and accuracy of this classification pipeline on training dataset just to understand if there are any obvious problems.\n"}
{"snippet": "y_pred_test=clf.predict(df_test)\n", "intent": "Now lets use our model to predict the output for the test data set and find the classification accuracy compared to the true output values\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('classification_report for testing dataset:')\nprint(classification_report(Target_labels_test, y_pred_test))\n", "intent": "Classification report will shed more light on the precision and Recall values for each target class. It will help us evaluate our model better.\n"}
{"snippet": "pred = clf.predict(test_d)\n", "intent": "And then, this algorithm is executed on test attributes.\n"}
{"snippet": "accuracy_score(pred, test_lab)\n", "intent": "To compare this prediction and the real data, The test data and labels are sent to this function.\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Creating predictions from the test set and classification report confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\npred = knn.predict(X_test)\n", "intent": "**Using the predict method to predict values.**\n"}
{"snippet": "predictions=logmodel.predict(x_test)\n", "intent": "** Predicting values for the testing data.**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "**Using the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Using the pipeline to predict from the X_test and create a classification report and confusion matrix.**\n"}
{"snippet": "predictions=model.predict(X_test)\n", "intent": "**Getting predictions from the model and creating a confusion matrix and a classification report.**\n"}
{"snippet": "grid_predictions=grid.predict(X_test)\n", "intent": "** Taking that grid model and creating some predictions using the test set and creating classification reports and confusion matrices for them.**\n"}
{"snippet": "su_model.predict('I love this restaurant', k=3)\n", "intent": "Find the first 3 labels as per probabilities.\n"}
{"snippet": "predXTrain = model.predict(XTrain)\npredXVal = model.predict(XVal)\nAxLine=YVal.index[0] \n", "intent": "**ETF Trading Predictions**\n"}
{"snippet": "trainPredict = model.predict(trainX_close)\ntestPredict = model.predict(testX_close)\ntestPredict_open = model.predict(testX_open)\nclasses_pre =preprocessing.classify(testPredict,testPredict_open)\nclasses_pre = np.asarray(classes_pre)\naccuracy = preprocessing.accuracy(test_classes[:len(classes_pre)],classes_pre)\nprint('Classification accuracy is :',accuracy)\n", "intent": "Predict future values and conclude classes, then compute error by comparing to True classes\n"}
{"snippet": "y_predicted = model.predict_proba(x_test)\n", "intent": "Using the trained model, predict for the test set: \n"}
{"snippet": "model.evaluate(X,y)\n", "intent": "let's make evaluation to find the loss and score:\n"}
{"snippet": "model1.evaluate(X,y,verbose=1)\n", "intent": "we will evaluate to find the loss and accurate:\n"}
{"snippet": "predictions = classify_model.predict(features)\n", "intent": "let's classify the generated data:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(predictions,target)\n", "intent": "using accuracy_score to calculate the prediction score:\n"}
{"snippet": "print(accuracy_score(predictions[8:16],target[8:16]))\n", "intent": "**Type 1: sender address - leapeturel@gmail.com**\n"}
{"snippet": "y_pred_train = best_knn.predict(X_train)\nprint \"Precision = %.1f%%, recall = %.1f%% and f1 score = %.1f%%.\" % (\n        precision_recall_fscore_support(y_pred_train, y_train, average='binary')[0]*100,\n        precision_recall_fscore_support(y_pred_train, y_train, average='binary')[1]*100,\n        precision_recall_fscore_support(y_pred_train, y_train, average='binary')[2]*100)\n", "intent": "As shown above, high scores across various measures of performance result primarily from little variation in the dependent variable.\n"}
{"snippet": "print(y_test.mean()) \nprint(model.predict(X_test).mean()) \n", "intent": "Odds ratios suggest that the likelihood of quitting increases with score ratio and decreases with attempt number.\n"}
{"snippet": "model_predicted = model.predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, model_predicted[:, 1]) \nauc = metrics.auc(fpr,tpr) \nprint(\"AUC: \", auc) \n", "intent": "The model tends to overpredict the positive class.\n"}
{"snippet": "scores = cross_val_score(gs.best_estimator_, train, y, cv=skf, n_jobs=-1)\nprint(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), 2*scores.std(ddof=1)))\n", "intent": "Hypertuning of the ensemble increased the performance of Decision Tree and KNN and decreased it for SVM.\n"}
{"snippet": "print model_5.predict_proba(sample_X)\nprint model_5.classes_\n", "intent": "For each row in the **sample_validation_data**, what is the probability (according **model_5**) of a loan being classified as **safe**? \n"}
{"snippet": "prediction = model_5.predict(validation_data_np_X)\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "precision_with_default_threshold= precision_score(y_true=test_data['sentiment'].to_numpy(), \n                                                  y_pred=predictions_with_default_threshold.to_numpy())\nrecall_with_default_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n                                                  y_pred=predictions_with_default_threshold.to_numpy())\nprecision_with_high_threshold= precision_score(y_true=test_data['sentiment'].to_numpy(), \n                                                  y_pred=predictions_with_high_threshold.to_numpy())\nrecall_with_high_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n                                                  y_pred=predictions_with_high_threshold.to_numpy())\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "probabilities = model.predict_proba(baby_test_matrix)\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "score = network.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Question 5: Evaluate your model** \n"}
{"snippet": "def cost_function(features, labels, weights, reg):\n    observations = len(labels)\n    predictions = predict(features, weights)\n    class1_cost = -labels*np.log(predictions)\n    class2_cost = (1-labels)*-np.log(1-predictions)\n    regularization_term = reg/(2*observations) * np.sum(np.square(weights))\n    cost = class1_cost + class2_cost + regularization_term\n    cost = cost.sum()/observations\n    return cost\nprint(f\"Cost of one iteration: {cost_function(X_,y,theta_,0.1)}\")\n", "intent": "These smooth monotonic functions (always increasing or decreasing) make it easy to calculate the gradient and minimize cost.\n"}
{"snippet": "model_predictions = [np.argmax(final_model.predict(np.expand_dims(feature, axis=0)))\n                     for feature in data[s_network]['test']]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print ('Mean Squared Error:',np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "Let's now use the trained model to predict whether the central pixel of each patch is a nerve or not:\n"}
{"snippet": "y_test_pred = nn.predict(X_test)\nacc = (np.sum(y_test == y_test_pred).astype(np.float) / X_test.shape[0])\nprint('Training Accuracy: {:.2f}.'.format(acc*100))\n", "intent": "Finally, let us evaluate the generalisation performance of the model by calculating the prediction accuracy on the test set:\n"}
{"snippet": "del cnn\ncnn2 = ConvNN(random_seed=123)\ncnn2.load(epoch=20, path=\"./tflayers-model/\")\nprint(cnn2.predict(X_test_centered[:10, :]))\n", "intent": "After the training is finished, the model can be used to do prediction on the test dataset as follows:\n"}
{"snippet": "preds = cnn2.predict(X_test_centered)\nprint(\"Test Accuracy: {:.2f}%.\".format(100*np.sum(y_test == preds)/len(y_test)))\n", "intent": "Finally, we can measure the accuracy of the test dataset as follows:\n"}
{"snippet": "score = classifier.evaluate(x=test_set[0], y=test_set[1])\nprint('Accuracy: {0:f}'.format(score['accuracy']))\n", "intent": "The last thing we want to do with this model is test the accuracy against some held-out test data set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "While the model is less accurate, it is not overfitting the data, so this classifier is probably better.\n"}
{"snippet": "y_pred = clf_gini.predict(X_test)\nprint(\"Accuracy is \", accuracy_score(y_test,y_pred)*100)\n", "intent": "We can now use this model to make some predictions and get the accuracy of the model:\n"}
{"snippet": "y_pred = svm.predict(X_test)\nprint(\"Accuracy is \", accuracy_score(y_test,y_pred)*100)\n", "intent": "We can now use this model to make some predictions and get the accuracy of the model:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = []\n    for idx, target, weight in zip(style_layers, style_targets, style_weights):\n        current = gram_matrix(feats[idx])\n        loss.append(weight * (current - target).pow(2).sum())\n    return torch.cat(loss).sum()\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions) * 100.0\nprint('Test accuracy: %.1f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = model.predict(X_test.as_matrix())\n", "intent": "The model can be used to predict the survival status of passengers in the test data.\n"}
{"snippet": "my_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in bottleneck_features[\"test\"]]\ntest_accuracy = 100*np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1))/len(my_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model1.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "print(body_reg.predict([ [5]]))\n", "intent": "We are also asked to make a prediction based on an input value.\n"}
{"snippet": "predictions = new_model.predict(test_data[0:25]).round()\nfor i in range(25):\n    if predictions[i]:\n        print('Test sample {} is poisonous.'.format(i))\n", "intent": "Lets run some predictions on the newly initiated model.\n"}
{"snippet": "rmse = sqrt(mean_squared_error(visitor_history[-12:], predictions))\nprint('Test RMSE: %.3f' % rmse)\n", "intent": "Did the LSTM work any better than the simple monthly average?\n"}
{"snippet": "precision = cross_val_score(clf, X_train, y, scoring='precision',cv=5,n_jobs=1)\naccuracy = cross_val_score(clf, X_train, y, scoring='accuracy',cv=5,n_jobs=1)\nrecall = cross_val_score(clf, X_train, y, scoring='recall',cv=5,n_jobs=1)\nf1 = cross_val_score(clf, X_train, y, scoring='f1',cv=5,n_jobs=1)\nroc_auc = cross_val_score(clf, X_train, y, scoring='roc_auc',cv=5,n_jobs=1)\n", "intent": "5 fold cross validation is choosen here and their metrics is calculated.\n"}
{"snippet": "print(clf.predict([[0, 1, 1]]))\n", "intent": "Predict the target value (and print it) for the passed data, using the fitted model currently in clf\n"}
{"snippet": "def BVEs(ys_, ys):\n    return np.array([[bias( ys_[:,ex], ys[ex] ), \n                      variance( ys_[:,ex] ), \n                      avg_error( ys_[:,ex], ys[ex])] for ex in range(len(ys))])\n", "intent": "The following two functions aggregate the values you computed (they will be useful in the final part of the exercise).\n"}
{"snippet": "metrics = model.evaluate(X_test, y_test_one_hot)\nmetric_value = metrics[1]\nprint()\nprint('Accuracy on test set: {:.2f} %'.format(100*metrics[1]))\n", "intent": "Accuracy on the test set:\n"}
{"snippet": "print(\"Evaluating model...\")\nroc, acc = evaluate(model)\n", "intent": "We evaluate the model by calculating the ROC and F-Score, the functions are defined above\n"}
{"snippet": "print(affinity.predict(42))\n", "intent": "Let's pick out a peak we know, and look at it more closely.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_bottleneck]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(model.predict(X_test))\n", "intent": "Now we can go ahead and make predictions and evaluate error by appending methods onto `model`. These _do not_ modify the `model` object!\n"}
{"snippet": "y_hat = mod.predict(x)\n", "intent": "Then we create the predicted values: \n"}
{"snippet": "from sklearn import metrics\nprint metrics.classification_report(y_test, y_pred)\nprint metrics.confusion_matrix(y_test, y_pred)\n", "intent": "We can see it more detailed.\n"}
{"snippet": "confusion_matrix(y_train_5, forest_clf.predict(X_train))\n", "intent": "Where each row represents a actual class and each column represents a predicted class.\n"}
{"snippet": "knnClf.predict([[3, 5, 4, 2]])\n", "intent": "Step 3:\n    * Predict the response for a new observation\nNew observations are called \"out-of-sample\" data\n"}
{"snippet": "from sklearn.metrics import f1_score, log_loss\ndef evaluate(labelsTrue,predictions1,predictions2):\n    if len(predictions1)>0:\n        f1 = f1_score(labelsTrue,predictions1,average=\"weighted\")\n        print(\"F1 score: \",f1)\n    if len(predictions2)>0:\n        logloss = log_loss(labelsTrue,predictions2, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n        print(\"Log loss for predicted probabilities:\",logloss)\n", "intent": "**Define function for evaluation and test it on the sgd classifier**\n"}
{"snippet": "def mae(yval_pred, yval):\n  val_mae = metrics.mean_absolute_error(yval_pred, yval)\n  return(val_mae)\n", "intent": "The following cell defines function **mae** (Mean Absolute Error), that we will use later to measure the accuracy of models.\n"}
{"snippet": "from scipy.stats import describe\ndescribe(rfr.predict(features))\n", "intent": "Is this value high or low?  Let's look at the distribution of predicted values for the data we trained the model with.\n"}
{"snippet": "pred = clf.predict(test_df[pred_variables])\nmetrics.accuracy_score(pred,test_df['churndep'])\n", "intent": "The accuracy of training set is 0.99\n"}
{"snippet": "predictions = lm.predict(X_test) \n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "for k in currencies:\n    scores = models[k].evaluate(xs_test, classes_test[k])\n    print(\"Currency %s. %s: %.2f%%\" % (k, models[k].metrics_names[1], scores[1]*100))    \n", "intent": "Evaluating the models for each currency\n---\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "**109 and 50 are the number of correct predictions. 8 and 4 are the number of incorrect predictions.**\n"}
{"snippet": "clf.predict([[5.0, 3.6, 1.3, 0.25]])\n", "intent": "Once we have learned from the data, we use our model to predict the most likely outcome on suneen data:\n"}
{"snippet": "predictions = clf.predict(X_test)\nprint(predictions)\n", "intent": "After fitting, the model can be used to predict the class of samples:\n"}
{"snippet": "probabilities = clf.predict_proba(X_test)\nprint(probabilities)\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "def target(**params):\n    l1_ratio = params['l1_ratio']\n    model = ElasticNet(alpha=0.1, l1_ratio=l1_ratio, random_state=seed)\n    scores = cross_val_score(model, X_train, y_train, scoring='r2', cv=5)\n    return scores.mean()\n", "intent": "Define target function and instantiate Bayesion Optimization object.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint (cross_val_score(bnb, data, target, cv=10))\n", "intent": "The difference is insignificant. That means the model is not overfitting. \n"}
{"snippet": "estimator = grid.best_estimator_\ny_pred = estimator.predict(test_df[column])\nresult = my_auc(test_df.Class, y_pred)\nprint(\"AUC=\" + str(result))\n", "intent": "No, after 100 estimators accuracy does not become better\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=0\n    for i in range(len(style_layers)):\n        G_Matrix_of_current=gram_matrix(feats[style_layers[i]])\n        style_loss+=style_weights[i]*tf.reduce_sum(tf.square(G_Matrix_of_current-style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "knn.predict_proba([[3, 5, 4, 2],])\n", "intent": "**Quick exercise**\n10 of you stand up - can we guess a characteristic about some candidates from your nearest-height neighbours?\n"}
{"snippet": "y_pred = linreg.predict(X_test)\nprint(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "See how well the model can predict with the mean squared error.\n"}
{"snippet": "Xception_predictions = [np.argmax(model_Xcep.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "start_time = timeit.default_timer()\npredicted = knn.predict(test_ext)\nelapsedPredict = timeit.default_timer() - start_time\nprint(\"KNN predict time: %.2f sec\" % (elapsedPredict))\n", "intent": "Make predictions on test data\n"}
{"snippet": "start_time = timeit.default_timer()\npredicted = knn.predict(X_test)\nelapsedPredict = timeit.default_timer() - start_time\nprint(\"KNN predict time: %.2f sec\" % (elapsedPredict))\n", "intent": "Make predictions on test data\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum(tf.pow(gram - style_targets[i], 2))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.zeros(1)\n    for i,l in enumerate(style_layers):\n        g = gram_matrix(feats[l])\n        style_loss = tf.add(style_loss, \\\n                            tf.multiply(\\\n                                tf.reduce_sum(tf.square(tf.subtract(g,style_targets[i]))), style_weights[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    print(student_output)\n    print(correct)\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "kmeans.predict([sl,pl])\nkmeans.predict([sw,pw])\n", "intent": "Various colors are given to the clusters. They have plotted again. plt.show() is used plot the same on graph. \n"}
{"snippet": "def calculate_and_print_wer_report(session, caption, tower_decodings, tower_labels, tower_total_losses, show_ranked=True):\n    items, mean = calculate_wer(session, tower_decodings, tower_labels, tower_total_losses)\n    if show_ranked:\n        print_wer_report(caption, mean, items=items)\n    else:\n        print_wer_report(caption, mean)\n    return items, mean\n", "intent": "Plus a convenience method to calculate and print the WER report all at once.\n"}
{"snippet": "with tf.device('/cpu:0'):\n    test_decodings, test_labels, test_total_losses = decode_batch(data_sets.test)\n    _, test_wer = calculate_and_print_wer_report(session, \"Test\", test_decodings, test_labels, test_total_losses)\n", "intent": "Now the trained model is tested using an unbiased test set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(km.labels_,college[:-1]['Cluster']))\nprint(classification_report(km.labels_,college[:-1]['Cluster']))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "def predict():\n    input_ = float(input(\"Input the population size/10000: \"))\n    print(\"For a population of \", input_, \", the profit is $\", ((input_ * 1.18864349) - 3.85208068) * 10000)\n", "intent": "Here, I hard coded the learned weights in the model that we defined earlier.\n"}
{"snippet": "test_label_predicted = clf.predict(X_test_counts)\nprint (\"test accuracy is \",np.mean(test_label_predicted == test.Label))\ntn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_true = test.Label,y_pred =test_label_predicted,).ravel()\nprint (sklearn.metrics.confusion_matrix(test_label_predicted, test.Label))\nprint (\"True positive rate is\", (tp)/(tp+fn))\n", "intent": "I observe that the test set performance is close to the dev set performance.\n"}
{"snippet": "def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content)) / 2\n", "intent": "The content loss is the (scaled, squared) Euclidean distance between feature representations of the content and combination images.\n"}
{"snippet": "def style_loss(style, combination, height, width):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = height * width\n    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n", "intent": "The style loss is then the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in train_tensors]\ntrain_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(train_targets, axis=1))/len(dog_breed_predictions)\nprint('Train accuracy: %.4f%%' % train_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=1)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "print(classification_report(Y_validation, predictions))\n", "intent": "*** Classification Report ***\n"}
{"snippet": "print(\"Accuracy score: \", accuracy_score(Y_generated, predictions_generated))\n", "intent": "*** Classification model accuracy on the generated speeches ***\n"}
{"snippet": "print(classification_report(Y_generated, predictions_generated))\n", "intent": "*** Classification Report ***\n"}
{"snippet": "y_train_predict=gbm2.predict(train)\n", "intent": "Predicting the train dataframe\n"}
{"snippet": "r2_score(y_train,y_train_predict)\n", "intent": "R2 score returns a 1 for a perfect or overfitted model\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for (i, g_target, weight) in zip(style_layers, style_targets, style_weights):\n        g_feature = gram_matrix(feats[i])\n        style_loss += weight * tf.reduce_sum(tf.squared_difference(g_feature, g_target))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_proba = digits_clf.predict(x_test)\ny_pred = np.argmax(y_proba, axis=-1)\n", "intent": "We can get the actual outputs of our neural network model using `predict`.\n"}
{"snippet": "best_clf.predict_proba([\"You are a nobody\"])\n", "intent": "Testing on random sentences\n"}
{"snippet": "rf_random = ran_search.best_estimator_\nprint(\"Cross-validation score for base RF\")\ncv_score(rf)\nprint(\"\\nCross-validation score for RF tuned by RandomizedSearchCV\")\ncv_score(rf_random)\nprint()\npprint(ran_search.best_params_)\n", "intent": "We're interested in seeing if there's any improvement between the untuned default RF model and our new one\n"}
{"snippet": "rf_best = grid_search.best_estimator_\nprint(\"Cross-validation score for untuned RF\")\ncv_score(rf)\nprint(\"\\nCross-validation score for RF tuned by RandomizedSearchCV\")\ncv_score(rf_random)\nprint(\"\\nCross-validation score for RF tuned by GridSearchCV\")\ncv_score(rf_best)\n", "intent": "How does this new model compare to the other model?\n"}
{"snippet": "rf_random = ran_search.best_estimator_\nprint(\"Cross-validation score for base RF\")\ncv_score(rf)\nprint(\"\\nCross-validation score for RF tuned by RandomizedSearchCV\")\ncv_score(rf_random)\nprint()\npprint(ran_search.best_params_)\n", "intent": "We're interested in seeing if there's any improvement between the untuned default RF model and our new one.\n"}
{"snippet": "rf_best = grid_search.best_estimator_\nprint(\"Cross-validation score for untuned RF\")\ncv_score(rf)\nprint(\"\\nCross-validation score for RF tuned by RandomizedSearchCV\")\ncv_score(rf_random)\nprint(\"\\nCross-validation score for RF tuned by GridSearchCV\")\ncv_score(rf_best)\n", "intent": "How does this new model compare to the RandomizedSearchCV model?\n"}
{"snippet": "rf_random = ran_search.best_estimator_\nprint(\"Cross-validation score for base RF\")\ncv_score(rf)\nprint(\"\\nCross-validation score for RF tuned by RandomizedSearchCV\")\ncv_score(rf_random)\nprint()\nprint(\"\\nParameters chosen by RandomizedSearchCV\")\npprint(ran_search.best_params_)\n", "intent": "We're interested in seeing if there's any improvement between the untuned default RF model and our new one.\n"}
{"snippet": "Lasso_train_score = cross_val_score(LassoSearch, x_train, y_train, cv=7)\nprint(\"Lasso: \", Lasso_train_score.mean())\nRidge_train_score = cross_val_score(RidgeSearch, x_train, y_train, cv=7)\nprint(\"Ridge: \",Ridge_train_score.mean())\nLasso_PCA_train_score = cross_val_score(LassoPCASearch, x_train, y_train, cv=7)\nprint(\"Lasso with PCA: \", Lasso_PCA_train_score.mean())\nRidge_PCA_train_score = cross_val_score(RidgePCASearch, x_train, y_train, cv=7)\nprint(\"Ridge with PCA: \", Ridge_PCA_train_score.mean())\nprint('OLS: ',Train_Global_games_ols)\n", "intent": "Now that we've calculated these scores, let's see them all together to compare. \n"}
{"snippet": "Lasso_test_score = cross_val_score(LassoSearch, x_test, y_test, cv=7)\nprint(\"Lasso: \", Lasso_test_score.mean())\nRidge_test_score = cross_val_score(RidgeSearch, x_test, y_test, cv=7)\nprint(\"Ridge: \",Ridge_test_score.mean())\nLasso_PCA_test_score = cross_val_score(LassoPCASearch, x_test, y_test, cv=7)\nprint(\"Lasso with PCA: \", Lasso_PCA_test_score.mean())\nRidge_PCA_test_score = cross_val_score(RidgePCASearch, x_test, y_test, cv=7)\nprint(\"Ridge with PCA: \", Ridge_PCA_test_score.mean())\nprint('OLS: ',Test_Global_games_ols)\n", "intent": "Now with the test data\n"}
{"snippet": "count_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\n", "intent": "Including the tree's axis-parallel decision boundaries and how the tree splits\n"}
{"snippet": "np.sum(xr.predict(pos_smote_rest)==1)\n", "intent": "How many of the 9k samples look like real positives to the model? Pretty small.. \n"}
{"snippet": "print \"CENTROID CLASSIFIER\"\nprint\nprint\"Test set accuracy: %f\" % clf_gs.score(X_test, y_test)\nprint\nprint(classification_report(y_pred = clf_gs.predict(X_test), y_true = y_test))\n", "intent": "---\nNow, comparing each classifier's accuracy, precision and recall:\n"}
{"snippet": "seed = 5\nfor name, model in algorithms.items():\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n    results[name] = cv_results\n    msg = \"Name: {0}, Results Mean: {1}, Results Std: {2}\".format(name, \n                                                                  round(cv_results.mean(), 3), \n                                                                  round(cv_results.std(), 3))\n    print(msg)\n", "intent": "Next, we'll iterate through the collection of algorithms and calculate build the models.\n"}
{"snippet": "x_test = iris_test[['sepal_length', 'sepal_width', \n                    'petal_length', 'petal_width']]\ny_test = iris_test[['species']].values.ravel()\npredictions = knn.predict(x_test)\nprint(predictions)\nprint(y_test)\n", "intent": "At this point, we can use the testing data to make predictions and just the efficacy of the model.\n"}
{"snippet": "print(accuracy_score(y_test, predictions))\n", "intent": "We can compute the accuracy of the predictions against the actual values.\n"}
{"snippet": "y_pred = grid.predict(X_test_norm)\n", "intent": "The grid Search also chose a k of 3 and we get a mean accuracy of avout 80%.  Let's predict values for the test set and see how this model fares.\n"}
{"snippet": "MSE_model = mean_squared_error(MSE_rf['yvals'], MSE_rf['ypred'])\nprint('The MSE of the random forest model is:',MSE_model)\n", "intent": "Now we will report the MSE of the random forest model and the MSE for each category:\n"}
{"snippet": "Y_pred = regressor.predict(X_test)\n", "intent": "Predicting the Test set results\n"}
{"snippet": "trn_features = model.predict(trn_data, batch_size=batch_size)\nval_features = model.predict(val_data, batch_size=batch_size)\n", "intent": "... and their 1,000 Imagenet probabilities from VGG16 -- these will be the *features* for our linear model:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)\nprobs[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "sentiments_by_char = defaultdict(list)\ndef sentiment_list (text_list):\n    scores = [sentiment_score(sentence) for sentence in text_list if sentiment_score(sentence) is not None]\n    if sum(scores) == 0: \n        return None\n    return scores \n", "intent": "First we need to create a new function which return a list of the sentiment score of each sentence\n"}
{"snippet": "ev = regressor.evaluate(input_fn=lambda: input_fn(test_set), steps=1)\n", "intent": "Next, see how the trained model performs against the test data set. Run evaluate, and this time pass the test_set to the input_fn:\n"}
{"snippet": "y = regressor.predict(input_fn=lambda: input_fn(prediction_set))\nprint (\"Predictions: {}\".format(str(y)))\n", "intent": "Finally, you can use the model to predict median house values for the prediction_set, which contains feature data but no labels for six examples:\n"}
{"snippet": "def loss(x, y, input_shape):\n    return cross_entropy_loss(y, forward(x, input_shape))\n", "intent": "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network.\n"}
{"snippet": "def loss(x, y, input_shape):\n    logits = forward(x, input_shape)\n    entropy = cross_entropy_loss(y, logits)\n    return entropy\n", "intent": "Using the `cross_entropy` layer and the `forward` function defined above, please implement the loss function for your network.\n"}
{"snippet": "predicted_ratings = predictions[test_matrix.nonzero()]\ntest_truth = test_matrix[test_matrix.nonzero()]\nmath.sqrt(mean_squared_error(predicted_ratings,test_truth))\n", "intent": "Let us consider only those ratings which are not zero in the test matrix and use them to find the error in our model\n"}
{"snippet": "y_test = df_in['actual']\ny_pred = df_in['pred']\ncr = (classification_report(y_test, y_pred, target_names=class_dictionary.values()))\nprint cr\n", "intent": "<p style=\"color:black ; text-align:left ; font-family: Verdana, serif;\"><b>Classification Report for the Test data</b></p>\n"}
{"snippet": "accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n", "intent": "<p style=\"color:black ; text-align:left ; font-family: Verdana, serif;\"><b>Overall Accuracy for the Test data</b></p>\n"}
{"snippet": "def GetFutureValuesToPredict(X, k):\n    X_Future = np.empty_like(X)\n    X_Future[:-k] = X[k:]\n    return X[:-k], X_Future[:-k]\n", "intent": "gets a vector of values and returns itslef + a offset of k indices forward.\nassumption: len(X) > k\n"}
{"snippet": "rms = sqrt(mean_squared_error(y_test, y_pred))\n", "intent": "Calculation of RMSE measure: \n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_test, predictedValues, target_names = ['spam', 'ham']))\n", "intent": "I just want to see a more concrete information about results of the winner model.\n"}
{"snippet": "def square_loss(h, y):\n    pass\nsquare_loss(np.array([3., 4., 5.]), np.array([2., 1, 1])) == 26/3\n", "intent": "2. Use `numpy.dot` to write the mean square loss function.\n"}
{"snippet": "xgb_train_pred = regr.predict(x_)\nxgb_train_pred = np.exp(xgb_train_pred)\nxgb_train_pred\n", "intent": "We will know run predictions for Sale Prices of train and test datasets respectively:\nFor train dataset:\n"}
{"snippet": "rf_train_pred = rf.predict(x_) \nrf_train_pred = np.exp(rf_train_pred) \nrf_train_pred \n", "intent": "We will know run predictions for Sale Prices of train and test datasets respectively:\nFor train dataset:\n"}
{"snippet": "np.mean(np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        gmc = gram_matrix(feats[style_layers[i]])\n        gms = style_targets[i]\n        loss += 2 * style_weights[i] * tf.nn.l2_loss(gmc - gms)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred_most_frequent,\n                            target_names=[\"not nine\", \"nine\"]))\n", "intent": "f1 score most frequent: 0.00\nf1 score dummy: 0.09\nf1 score tree: 0.55\nf1 score logistic regression: 0.89\n"}
{"snippet": "accuracy_score(y_test, pred)\n", "intent": "Accuracy score for Decision Tree:\n"}
{"snippet": "print(metrics.classification_report(y_test, pred))\n", "intent": "Classification Report for Decision Tree:\n"}
{"snippet": "accuracy_score(y_test, pred)\n", "intent": "Accuracy score for Random Forest Classifier:\n"}
{"snippet": "print(metrics.classification_report(y_test, pred))\n", "intent": "Classification Report for Random Forest Classifier:\n"}
{"snippet": "accuracy_score(y_test, pred)\n", "intent": "Accuracy score for Naive Bayes Classifier:\n"}
{"snippet": "print(metrics.classification_report(y_test, pred))\n", "intent": "Classification Report for Naive Bayes Classifier:\n"}
{"snippet": "accuracy_score(y_test, pred)\n", "intent": "Accuracy score for SVM Classifier:\n"}
{"snippet": "print(metrics.classification_report(y_test, pred))\n", "intent": "Classification Report for SVM Classifier:\n"}
{"snippet": "from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_predict)))\n", "intent": "Evaluating model with Mean Absolute Error, Mean Squared Error and Root Mean Squared Error\n"}
{"snippet": "y_2016 = df_2016['Happiness Score']\nX_2016 = df_2016.drop(['Happiness Score','Happiness Rank','Country','Region','Lower Confidence Interval','Upper Confidence Interval'],axis=1)\npredict_2016=lm.predict(X_2016)\n", "intent": "- Eliminating features in 2016 dataset to fit generic model.\n- Applied prediction on dataset\n"}
{"snippet": "y_2017 = df_2017['Happiness.Score']\nX_2017 = df_2017.drop(['Happiness.Score','Happiness.Rank','Country','Whisker.high','Whisker.low'],axis=1)\npredict_2017=lm.predict(X_2017)\n", "intent": "- Eliminating features in 2017 dataset to fit generic model.\n- Applied prediction on dataset\n"}
{"snippet": "predictions = nb_detector.predict(summaries_test)\nprint(confusion_matrix(sector_test, predictions))\nprint(classification_report(sector_test, predictions))\n", "intent": "And overall scores on the test set, the one we haven't used at all during training:\n"}
{"snippet": "print('before:', svm_detector.predict([summary4])[0])\nprint('after:', svm_detector_reloaded.predict([summary4])[0])\n", "intent": "The loaded result is an object that behaves identically to the original.\n"}
{"snippet": "pred = nb.predict(X_test)\n", "intent": "Time to see how the model did!\n**Calling predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint('Confusion Matrix: ','\\n'*2, confusion_matrix(y_test,pred),'\\n'*2)\nprint('Classification Report: ','\\n'*2, classification_report(y_test,pred))\n", "intent": "** Confusion matrix and classification report using predictions and y_test **\n"}
{"snippet": "pipe_pred = pipeline.predict(X_test)\n", "intent": "** Using the pipeline to predict from the X_test and building a classification report and confusion matrix.**\n"}
{"snippet": "from sin_model import Sin\ns = Sin()\ns.predict()\n", "intent": "With not much time under our belts, let's train something more\nlightweight than LSTM but hopefully more accurate than vanilla NN.\n"}
{"snippet": "from sklearn.metrics import classification_report\nmodel_prediction = classi.predict(model_features)\nprint(classification_report(model_groups,model_prediction))\n", "intent": "* It is shown that the second classifier model (classi2) presents the better F1 score for groups 2 and 3.\n"}
{"snippet": "result = model.evaluate(x = data.test.images, \n                        y = data.test.labels)\nfor name, value in zip(model.metrics_names, result):\n  print(name,value)\nprint(\"{0}: {1:.2%}\".format(model.metrics_names[1], result[1]))\n", "intent": "Once model has been trained, we can evaluate the result against test set.\n"}
{"snippet": "y_pred = model.predict(x=data.test.images)\ncls_pred = np.argmax(y_pred, axis=1)\nplot_example_errors(cls_pred)\n", "intent": "Plot mis-classified images from the test set\n"}
{"snippet": "??vgg.predict(imgs, True)\n", "intent": "Now use the existing model to predict the images from the batches\n"}
{"snippet": "testpred = pipeline.predict(X_test)\nprint(accuracy_score(testpred, y_test))\nprint(classification_report(testpred, y_test))\n", "intent": "The SVM works quite well already: we outperform the random baseline by a signficant margin.\n"}
{"snippet": "x_test = np.array(['come to play tennis'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "print(15*'-', 'Random forest without grid search and no CV', 15*'-', '\\n')\nRF_trainscore = forest.score(X_train, y_train)\nRF_score = forest.score(X_test, y_test)\nRF_pred = forest.predict(X_test)\nOutput(RF_trainscore, RF_score, RF_pred)\n", "intent": "This is improved greatly later with Grid Search and was left to show the improvement gained.\n"}
{"snippet": "r2_score_lasso = r2_score(y_test, y_pred_lasso)\nprint(lasso)\nprint(\"r^2 on test data : %f\" % r2_score_lasso)\n", "intent": "The next step is to calculate and print the score\n"}
{"snippet": "mseX = np.mean((df_data.GPRICE - lm.predict(X)) ** 2)\nprint (mseX)\n", "intent": "We now calculate the mean of the residual sum of squares.\n"}
{"snippet": "mseX_test = np.mean((Y_test - lm.predict(X_test)) ** 2)\nprint(mseX_test)\n", "intent": "We will now calculate the mean square errors for both the testing and the training data. We will then compare the residuals in a plot \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(model, xoil_temp, df.GasPrice)\nscores        \n", "intent": "Similarly for GasPrice vs OilPrice\n"}
{"snippet": "clf.predict(Xtestlr)\n", "intent": "After being fitted, the model can then be used to predict the class of samples:\n"}
{"snippet": "clf.predict_proba(Xtestlr)\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "y_1 = regr_1.predict(Xtestlr)\ny_2 = regr_2.predict(Xtestlr)\n", "intent": "We will now carry out prediction on the test data\n"}
{"snippet": "print \"MSE is {}\".format(np.round(mean_squared_error(df['medv'], ypred),2))\n", "intent": "The coefficient of determination is $R^2=0.74$ indicating a strong, positive linear relationship between medv and all predictors.\n"}
{"snippet": "ypred = model.predict(X=df[['lstat']])\n", "intent": "Their is some evidence of a non-linear relationship between medv and lstat.\n"}
{"snippet": "print \"MSE = {}\".format(mean_squared_error(df['medv'], ypred))\n", "intent": "The coefficient of determination $R^2=0.54$\n"}
{"snippet": "y_pred = np.random.randint(1, size=y_test.shape[0])\naccuracy_score(y_test, y_pred)\n", "intent": "Naive random prediction:\n"}
{"snippet": "outliers = bmi_life_model.predict([ [19], [40] ])\noutliers\n", "intent": "What about BMIs that are far from normal? How well does this model fit?\n"}
{"snippet": "pred = svm_model.predict(test_features)\n", "intent": "Make predictions on test data using the trained SVM model, generate the classification report and check the performance of the classifier. \n"}
{"snippet": "y_prediction = model.predict(X_test)\n", "intent": "We now apply the trained model to the test data (omitting the column PassengerId) to produce an output of predictions.\n"}
{"snippet": "print \"Mean absolute error:\", np.mean(abs(np.exp(clfforest.predict(Xtest)) - np.exp(ytest)))\nprint \"Standard deviation:\", np.std(abs(np.exp(clfforest.predict(Xtest)) - np.exp(ytest)))\n", "intent": "The difference between the accuracy on training data and test data suggests that we are overfitting. Let's look at the MAE:\n"}
{"snippet": "errorslr = abs(clflog.predict(Xtest) - ytest)\nprint np.mean(errorslr)\nprint np.std(errorslr)\n", "intent": "Now let's take a look at what the average error and compare that to the baseline.\n"}
{"snippet": "errorskn = abs(clfkn.predict(Xtest) - ytest)\nprint np.mean(errorskn)\nprint np.std(errorskn)\n", "intent": "Let's take another look at the average absolute error. \n"}
{"snippet": "n_samples = rows*cols\nflat_pixels = bands_data.reshape((n_samples, n_bands))\nresult = classifier.predict(flat_pixels)\nclassification = result.reshape((rows, cols))\n", "intent": "Let's apply the model to the night lights image for classification.\n"}
{"snippet": "lregtest_ols = tdr(sd_test[['Vmes','Vch','Vbb','Sa','Z','Vangover']].as_matrix(),sd_test['t'].as_matrix(),'ols',\"TestData_6var\")\nlregtest_ols.scaler = lregtrain_basic.scaler\nlregtest_ols.regr = lregtrain_basic.regr\nlregtest_ols.transform()\nlregtest_ols.predict()\nlregtest_ols.PlotPerformance(1,1000,'log',False)\n", "intent": "Now test the model on data that it was not trained on\n"}
{"snippet": "x = hh_submission.values\npred = forest.predict(x)\npred_prob = forest.predict_proba(x)\n", "intent": "Run predict_proba on array\n"}
{"snippet": "labels = training_data.map(lambda x: bestModel.predict(x))\n", "intent": "<div class=\"alert alert-info\">\nComplete the source code below to visualize the result of clustering.\n</div>\n"}
{"snippet": "print ('The residual sum of squares of the model =%0.4f'% np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "print('The mean squared error of the model:')\nprint(sklearn.metrics.mean_squared_error(bos.PRICE,lm.predict(X)))\nprint(round((np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos.PRICE)),10))\nprint(round(np.mean((bos.PRICE - lm.predict(X)) ** 2),10))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "kfold = KFold(n_splits=3)\ncv_results = cross_val_score(clf,Xtrain[imp_feat],ytrain, scoring='accuracy', cv=kfold )\nprint('accuracy: ', cv_results.mean())\n", "intent": "Let's take the raw data with just important features. Here we are still using RandomForestClassifier with default parameters.\n"}
{"snippet": "print(np.mean(sum((bos.PRICE - lm.predict(X)) ** 2)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "gender.predict([[0, 3, 25, 1, 35]])\n", "intent": "Predicting salary for someone meeting passed in criteria\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss_s = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        A = style_targets[i]\n        loss_s += style_weights[i] * tf.reduce_sum((G - A)**2)\n    return loss_s    \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn import metrics\ny_pred = knn.predict(X_test)\nprint(\"Model accuracy:\", metrics.accuracy_score(y_test, y_pred))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "print(\"Predicted value for x =\", xmin[0], \"is\", model.predict([xmin]))\nyhat = model.predict(X)\nyhat\n", "intent": "We can use the predict method on the model to apply the formula:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\npredicted = model.predict(X)\nprint(\"MSE:\", mean_squared_error(y, predicted))\nprint(\"R^2:\", r2_score(y, predicted))\n", "intent": "`sklearn` provides convenient functions to calculate these metrics\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_label,\n                            scoring=\"neg_mean_squared_error\", cv=10)\n", "intent": "* So it's 70612 +- 2875\n"}
{"snippet": "Y_train_pred = clf.predict(X_train)\nY_test_pred = clf.predict(X_test)\ntrain_error = 1 - accuracy_score(Y_train, Y_train_pred)\ntest_error = 1 - accuracy_score(Y_test, Y_test_pred)\nprint(\"Training Set Error Rate: \" + str(train_error))\nprint(\"Test Set Error Rate: \" + str(test_error))\n", "intent": "- **Test model performance**\n"}
{"snippet": "Y_exp_pred = clf.predict(X_exp)\nexp_error = 1 - accuracy_score(Y_exp, Y_exp_pred)\nprint(\"Experiment Error Rate: \" + str(exp_error))\n", "intent": "- **Calculate experiment error rate**\n"}
{"snippet": "predictions = dtr.predict(features)\n", "intent": "The decision tree is now trained and ready to make a prediction: \n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"norppa.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "model_score(y, preds, X, verbose = 1)\n", "intent": "Cool. We got the data and the model working. Now let's try to see the `model_score` function in action.\n"}
{"snippet": "_ = met.model_score(y,preds,X,verbose=1) \n", "intent": "Remember, we have to tell it where model score lives. Right now it lives under `met`'s house\n"}
{"snippet": "def mean_squared_error(x,y):\n    print(\"I'M MEAN SQUARED ERROR\")\n", "intent": "If I wanted them all to be available\n"}
{"snippet": "pred = model.predict(x_test)\naccuracy = metrics.accuracy_score(y_test, pred)\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, pred))\n", "intent": "Making predictions using the test data and calculating the accuracy.\n"}
{"snippet": "print(\"****** Test Data ********\")\nprint(test.shape)\nprint(train.shape)\npred = model.predict_classes(test)\nprint(metrics.classification_report(test_labels, pred))\nprint(\"Confusion Matrix\")\nprint(metrics.confusion_matrix(test_labels, pred))\n", "intent": "Use the test dataset to evaluate the model\n"}
{"snippet": "def accuracy(clf, x, y):\n    results = cross_val_predict(clf, x, y, cv=5)\n    return accuracy_score(y, results)\n", "intent": "Using cross validation \n"}
{"snippet": "score = model.evaluate(x_test, \n                       y_test, \n                       batch_size=128)\npredict = model.predict(x_test,\n                        batch_size=128)\n", "intent": "For special case, batch_size can be added in to keep the evaluate process same as training.\n"}
{"snippet": "test = preprocess(test)\ntest_pred = gs_xgb.predict(test)\nnp.sqrt(mean_squared_log_error(test['SalePrice'], test_pred))\n", "intent": "Indeed we can! Now, let's estimate the generalization error for this model:\n"}
{"snippet": "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n                       c_initializer = c_initializer):\n    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n    indices = np.argmax(pred, axis=-1)\n    results = to_categorical(indices, num_classes=78)\n    return results, indices\n", "intent": "**Step 3**: Implementing `predict_and_sample()`.\n"}
{"snippet": "diabetes_y_pred = regr.predict(diabetes_X_test)\n", "intent": "* Now assess how well your model has trained on the data by testing it on the testing data and finding the accuracy\n"}
{"snippet": "b_predictions = [np.argmax(b_model.predict(np.expand_dims(feature, axis=0))) for feature in test_b]\ntest_accuracy = 100*np.sum(np.array(b_predictions)==np.argmax(test_targets, axis=1))/len(b_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred_y = ds_model.predict(test_x)\nds_accuracy = accuracy_score(test_y, pred_y)\nprint('the accuracy of decision tree: %.4f' %ds_accuracy)\n", "intent": "make the prediction, compare with the real quality and calculate the accuracy.\n"}
{"snippet": "pred_y = rf_model.predict(test_x)\nrf_accuracy = accuracy_score(test_y, pred_y)\nprint('the accuracy of random forest: %.4f' %rf_accuracy)\n", "intent": "make the prediction and calculate the accuracy.\n"}
{"snippet": "predicted_test = clf.predict(X_test)\npredicted_train = clf.predict(X_train)\n", "intent": "different kernels and values of C did not improve the prediction\n"}
{"snippet": "accuracies['nn'] = score_test[argmax(score_train)]\nprint(\"accuracy on test data: \", accuracies['nn'])\nprint(r'R^2 on test data: ', r2_score(y_test, predicted_test))\n", "intent": "again, big time overfitting. k-fold would be better here fore validation.\n"}
{"snippet": "pred_train_forest, pred_train_dnn = best_forest.predict_proba(X_train), best_dnn.predict(X_train_dnn)\npred_test_forest, pred_test_dnn = best_forest.predict_proba(X_test), best_dnn.predict(X_test_dnn)\npredicted_input_train, predicted_input_test = concatenate((pred_train_forest, pred_train_dnn), axis =1), concatenate((pred_test_forest, pred_test_dnn), axis =1)\n", "intent": "outputs (class predictions) of forest and DNN are inputs of ensemble network\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for l in range(len(style_layers)):\n        gram_g = gram_matrix(feats[style_layers[l]])\n        loss += style_weights[l] * tf.reduce_sum((gram_g - style_targets[l])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nprint(len(y_train_pred))\nprint(y_train_pred)\n", "intent": ":    from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n"}
{"snippet": "resNet50_prediction = [np.argmax(resNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_RESNET50]\ntest_accuracy = 100 * np.sum(np.array(resNet50_prediction) == np.argmax(test_targets, axis=1)) / len(resNet50_prediction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for style_layer_gram_i, input_layer_i in enumerate(style_layers):\n        input_layer = feats[input_layer_i]\n        style_layer_gram = style_targets[style_layer_gram_i]\n        style_loss += style_weights[style_layer_gram_i] * tf.norm(gram_matrix(input_layer) - style_layer_gram)**2\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        curr_gram_matrix = gram_matrix(feats[style_layers[i]], True)\n        style_loss += style_weights[i] * 2 * tf.nn.l2_loss(curr_gram_matrix - style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "ResNet50_predictions = [np.argmax(top_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "val_rel_error = mre(val_y,pred_val)\nfor each_pair in zip(val_rel_error, val_distance_to_trn_centroid):\n    print each_pair\nprint r2_score(val_rel_error,val_distance_to_trn_centroid) \n", "intent": "Doing it manually, for results, see the Excel file in Results folder\n"}
{"snippet": "def model_stats(model, x_test, y_test, active):\n    accuracy = \"%.12f\" % (accuracy_score(y_test, model1.predict(x_test).astype(int)))\n    precision = \"%.12f\" % (precision_score(y_test, model1.predict(x_test).astype(int)))\n    recall = \"%.12f\" % (recall_score(y_test, model1.predict(x_test).astype(int)))\n    defaults = \"%d\" % (model1.predict(X_active).sum())\n    return [[accuracy, precision, recall, defaults]]\n", "intent": "Function to return stats for models\n"}
{"snippet": "X_test = testData.drop([\"Id\"], axis=1)\npredictions = lm.predict(X_test)\n", "intent": "Use the model to make sales predictions\n"}
{"snippet": "predY = np.choose(model.labels_, [1, 0, 2]).astype(np.int64)\nsm.accuracy_score(target, predY)\nsm.confusion_matrix(target, predY)\n", "intent": "Create a **confusion matrix** to determine True positives and False negatives\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfor i in range(1,10):\n    pred = [score > i / 100.0 for score in predictions]\n    print(\"Accuracy:\", accuracy_score(y_test, pred))\n    print(\"Precision:\", precision_score(y_test, pred))\n    print(\"Recall:\", recall_score(y_test, pred))\n    print(\"F1 Score:\", f1_score(y_test, pred))\n    print(\"Predicted Number of Spam:\", sum(pred), \"Actual Spam\", sum(y_test))\n", "intent": "Using a range of thresholds from 1 to 10% we can see if there is a level that gives a better F1 score.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict, cross_val_score\ncval_predict = cross_val_score(clf, train_prepared, y_train, cv=5, scoring=\"accuracy\")\ncval_predict\n", "intent": "Accuracy of the model using 5-fold cross validation on the training set.\n"}
{"snippet": "cross_val_score( svmdf2_fitted, X_test_scaled, y_test, cv = 5)\n", "intent": "The above classification report and confusion matrix shows that the precision, recall and overall accuracy score for the SVM were 100% accurate.\n"}
{"snippet": "print(\"Detailed classification Report RF on df3:\")\ny_true, y_pred = y_test, clf2_fitted.predict(X_test)\nprint(classification_report(y_true, y_pred))\n", "intent": "Print a detailed classification report\n"}
{"snippet": "cross_val_score( svmdf3_fitted, X_test_scaled, y_test, cv = 5)\n", "intent": "Validate SVM model performance with CV\n"}
{"snippet": "def do_predict(index_start, index_end):\n    return y_pred.eval({x: datasets[1][0][index_start:index_end,:].eval()})\ndef viz_range(index_start, index_end):\n    for i, p in zip(range(index_start, index_end), rc[0].apply(do_predict, index_start, index_end).result):\n        viz_prediction(datasets[1], i, p)\nviz_range(5, 8)\n", "intent": "We can also use `ipyparallel`'s remote apply capabilities to achieve a similar effect, with slightly different code.\n"}
{"snippet": "def squared_error(ys_orig, ys_line):\n    return sum((ys_line-ys_orig)**2)\ndef coefficient_of_determination(ys_orig,ys_line):\n    y_mean_line = [mean(ys_orig) for y in ys_orig]\n    squared_error_regr = squared_error(ys_orig,ys_line)\n    squared_error_y_mean = squared_error(ys_orig,y_mean_line)\n    return 1 - (squared_error_regr/squared_error_y_mean)\nr_squared = coefficient_of_determination(ys,regression_line)\nprint(r_squared)\n", "intent": "we use the parameter r to determine how good is our best fit\nr^2 = 1 - SE(y)/SE(y`)  where SE() is the squared error and y` is mean(y)\n"}
{"snippet": "period = np.random.rand() + 0.5\nshift = 3 * np.random.rand()\nw1 = np.sin(t / period - shift)\nclf.predict(w1.reshape(1, -1))\n", "intent": "Generate a new waveform. First one should give 0 --> sine wave, second one should give 1 --> rectangular wave\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc\ny_test_classes = {cls:[True if c == cls else False for c in y_test ] for cls in winning_rfc.classes_.tolist()}\nrfc_result = winning_rfc.predict_proba(X_test)\nrfc_classes = winning_rfc.classes_.tolist()\ny_predicted_probs = {cls:[item[rfc_classes.index(cls)] for item in rfc_result] for cls in rfc_classes}\n", "intent": "Understanding model class prediction performace\n"}
{"snippet": "[prob_fail3, prob_pass3] = lr.predict_proba([[1, 3.1]])[0]\nprint('Probability of student passing with 3.1 hours studying is ' + str(prob_pass3))\nprint('Probability of student failing with 3.1 hours studying is ' + str(prob_fail3))\n[prob_fail7, prob_pass7] = lr.predict_proba([[1, 7]])[0]\nprint('Probability of student passing with 7 hours studying is ' + str(prob_pass7))\nprint('Probability of student failing with 7 hours studying is ' + str(prob_fail7))\n", "intent": "The accuracy of our model is `0.75`, which makes it an \"okay\" model, but not great. \n"}
{"snippet": "predictions = reg.predict(test[predictors])\nnp.mean((predictions - test['cnt']) ** 2)\n", "intent": "The mean squared error metric makes the most sense to evaluate our error. MSE works on continuous numeric data, which fits our data quite well.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(grid_fit_TM.best_score_)\nbest_predictions = best_clf_TM.predict(X_test)\nprint(fbeta_score(best_predictions,y_test,0.5))\nprint(performance_TM)\nprint(best_clf_TM)\n", "intent": "What are the classifier parameters?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_pred)\n", "intent": "Finally, we compute the fraction of correctly labeled points:\n"}
{"snippet": "x = (np.array([6.9, 1.09, .06, 2.1, .0061, 12, 31, .99, 3.5, .44, 12])).reshape(1,-1)\nx_class = dtc.predict(x)[0]\nprint '\\nNew object attributes:'\nfor attr in attributeNames:\n    print attr[0]\nprint '\\nClassification result:'\nprint classNames[x_class]\n", "intent": "When you are changing the ``min_samples_split``, you change the number of decisions that is taken by the model.\n"}
{"snippet": "lr.predict_proba(X_test_std[:2,:])\n", "intent": "We can also predict the class-membership probaiblity of the samples via `predict_proba` method.\n"}
{"snippet": "x_testdf['t_predicted_prob'] = tree_grid.predict_proba(x_test)[:, 1]\nx_testdf['t_predictions'] = x_testdf['predicted_prob'].apply(map_predictions)\nx_testdf.head()\n", "intent": "<img src=\"Tree.png\">\n"}
{"snippet": "model.predict([6000])\n", "intent": "To use the trained model to classify new data\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification Accuracy:** What fraction of predictions got right our model?\n"}
{"snippet": "metrics.precision_score(y_test, y_pred_class)\n", "intent": "**Precision:** What proportion of positive identifications was actually correct?\n"}
{"snippet": "metrics.recall_score(y_test, y_pred_class)\n", "intent": "**Recall** (also called **true positive rate**): What proportion of actual positives was identified correctly?\n"}
{"snippet": "metrics.f1_score(y_test, y_pred_class)\n", "intent": "**Specificity** (also called **true negative rate**): What proportion of actual negatives was identified correctly?\n"}
{"snippet": "y_pred_prob = logreg.predict_proba(X_test)[:, 1] \n", "intent": "- By **default**, it predicts whatever class (column) has an estimate >= **0.5** (this is known as **treshold**).\n"}
{"snippet": "metrics.roc_auc_score(y_test, y_pred_class)\n", "intent": " **Area Under the Curve (AUC)**: useful as a single number summary of classifier performance.\n"}
{"snippet": "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean() \n", "intent": "**Cross-validated AUC**: average AUC score.\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y, y_pred)\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "y_pred = tree_model.predict(x_test)\n", "intent": "** Model Prediction **\n"}
{"snippet": "print('Accuracy Score:',accuracy_score(y_test,rf_pred))\nprint('\\n')\nprint('Confusion Matrix:')\nprint('\\n')\nprint(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint('Classification Report:')\nprint('\\n')\nprint(classification_report(y_test,rf_pred))\n", "intent": "**Model Evaluation**\n"}
{"snippet": "def calculate_loss(gImArr):\n    if gImArr.shape != (1, targetWidth, targetWidth, 3):\n        gImArr = gImArr.reshape((1, targetWidth, targetHeight, 3))\n    loss_fcn = K.function([gModel.input], [getTotalLoss(gModel.input)])\n    return loss_fcn([gImArr])[0].astype('float64')\n", "intent": "<h4>Step 7: Minimization of Loss:</h4> \n<p>Calculate total loss from our generated image to minimize the loss function.</p>\n"}
{"snippet": "dtest = xgb.DMatrix(test, feature_names=test.columns.values)\nypred = model.predict(dtest)\nlen(ypred)\n", "intent": "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"}
{"snippet": "whole_tsdf.ts_report()\n", "intent": "The [TimeSeriesDataFrame.ts_report](https://docs.microsoft.com/en-us/python/api/ftk.time_series_data_frame.timeseriesdataframe\n"}
{"snippet": "fig, ax = best_of_forecaster_prediction.show_error(err_name='MedianAPE', err_fun=calc_median_ape, performance_percent=(0.95, 1))\n", "intent": "Plot the grains with the bottom 5% performance, i.e. top 5% MedianAPE\n"}
{"snippet": "fig, ax = best_of_forecaster_prediction.show_error(err_name='MedianAPE', err_fun=calc_median_ape, performance_percent=(0, 0.05))\n", "intent": "Plot the grains with the top 5% performance, i.e. bottom 5% MedianAPE\n"}
{"snippet": "def total_variation_loss(x):\n    a = K.square(x[:, :image_height-1, :image_width-1, :] - x[:, 1:, :image_width-1, :])\n    b = K.square(x[:, :image_height-1, :image_width-1, :] - x[:, :image_height-1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n", "intent": "This gives a measure of noisiness for the generated image **G**.\n"}
{"snippet": "content_cost_v = content_cost(content_features, generated_features)\nstyle_costs_v = style_costs(style_features_list, generated_features_list)\ntotal_variation_loss_v = total_variation_loss(generated_v)\ncost = content_weight * content_cost_v\nfor index in range(5):\n    cost += style_weight * style_costs_v[index]\ncost += total_variation_weight * total_variation_loss_v\n", "intent": "***Total cost*** is the weighted sum of ***content cost***, ***style cost*** & ***total variation loss***.\n"}
{"snippet": "from sklearn.metrics import f1_score\ny_predict = clf.predict(X_train)                   \nprint 'Training f1 score = ', f1_score(y_train, y_predict, average = 'weighted')\nclass_labels = clf.get_classes()     \nprint 'Class labels=', class_labels\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "y_pred = classifier.predict(x_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\n", "intent": "<h3>Predicting x_test(Test Data) and generating confusion Matrix</h3>\n"}
{"snippet": "print(\"Precision: {}\".format(precision_score(y_test, y_pred)*100))\n", "intent": "<h3>Not Bad!</h3>\n<p>Got Better results this time!</p>\n"}
{"snippet": "flower = []\nfor index in neigh.predict(X_test):\n    if index==0:\n        flower.append('Setosa')\n    elif index==1:\n        flower.append('Versicolour')\n    elif index==2:\n        flower.append('Virginica')\nprint(neigh.predict(X_test))        \nprint(flower)\n", "intent": "See the original values and appended values for reference\n"}
{"snippet": "resid_gam = y_test - y_pred_gam\nresid_lin = y_test - altered_pred_y\ngam_q = np.percentile(resid_gam, 500/6.0)\nlin_q = np.percentile(resid_lin, 500/6.0)\nprint(\"gam_Q: \", gam_q)\nprint(\"lin_Q: \", lin_q)\ny_pred_gam     += gam_q\naltered_pred_y += lin_q\nprint(\"Linear adjusted real loss: \", real_loss(y_test, altered_pred_y))\nprint(\"Spline adjusted real loss: \", real_loss(y_test, y_pred_gam))\n", "intent": "Moving away from averages due to asymmetric loss functions. \n"}
{"snippet": "X_new = X_test.copy()\nX_new['client_prob'] = lmfit.predict_proba(X_test)[:,1]\nX_new['y_test'] = y_test\nX_new = X_new.sort_values(by='client_prob', ascending=False)\ngood_custos = X_new.head(1000)\nprint(\"Percent of clients with highest probability that actually say yes: %f\" % (np.mean(good_custos.y_test)*100.0))\n", "intent": "Using the logisitic model, find the 1000 clients that are most likely to score 'yes' (i.e have the highest probability that $Y=1$)\n"}
{"snippet": "from pygam import LogisticGAM\ngam = LogisticGAM().gridsearch(X_train,y_train,lam = np.logspace(-0.5, -0.25, 30))\nyhat_gam = gam.predict(X_test)\n", "intent": "Make a histogram of your logit scores\n"}
{"snippet": "from sklearn.calibration import calibration_curve\nx, y = calibration_curve(y_test, fit.predict_proba(X_test)[:,1], n_bins=no_bins)\n", "intent": "Plot a calibration curve and histogram of the probabilities from the logistic regression. \n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Let's now evaluate our final model:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_5_pred = cross_val_predict(sgd, X_train, y_train_5, cv=3)\n", "intent": "**Note**: cross_val_predict will concat all validation folds prediction into one array result.\n"}
{"snippet": "def predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n    return pred\n", "intent": "3.next step is to make predictions and evaluation\n"}
{"snippet": "a_smote_regular[\"Random Forest tuned model\"] = [log_loss(by_test_prelim_smote_regular,b_preds_prelim_smote_regular),log_loss(cy_test_prelim_smote_regular,c_preds_prelim_smote_regular)]\n", "intent": "Oversampling: smote regular\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Dog breed prediction accuracy on dog images: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "model_predictions = [np.argmax(Xception_model.predict(np.expand_dims(test_feature, axis=0))) for test_feature in test_Xception]\ntest_accuracy = 100 * np.sum(np.array(model_predictions) == np.argmax(test_targets, axis=1)) / len(model_predictions)\nprint('Classification accuracy on the test dataset: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now lets check how our  model performs with test data \n"}
{"snippet": "st = generate_df(iowa_data, '01/1/2016', '01/1/2017', '01/1/2016', '04/1/2016')\nX = st[variables]\ny = st['Total Sale']\npredictions = model.predict(X)\nprint \"r^2:\"+str(model.score(X, y))+\"\\t alpha:\"+str(rlmcv.alpha_)\n", "intent": "We can use the generated model to predict sales of 2016\n"}
{"snippet": "final_model = models_dict_ngrams['stemmed_words_SVC']\ncv_score = cross_val_score(final_model, train_features, tweet_sentiment_df['airline_sentiment'], scoring='accuracy', cv = 5)\ncv_score\n", "intent": "* Check cross validation score once again\n"}
{"snippet": "final_predictions = final_model.predict(test_features)\nids = test_df.tweet_id\nsubmission_df = pd.concat([ids, pd.Series(final_predictions)], axis = 1)\n", "intent": "* Get the final predictions for test data, create a data frame and label columns\n"}
{"snippet": "nodoc_vecs = code2emb_model.predict(encinp, batch_size=20000)\n", "intent": "Use the `code2emb` model to map the code into the same vector space as natural language \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    sum=0\n    for i,val in enumerate(style_layers):\n        sum+=tf.reduce_sum(style_weights[i]*tf.squared_difference(gram_matrix(feats[val]),style_targets[i]))\n    pass\n    return(sum)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfinal_model = grid_search_elastic_net.best_estimator_   \ny_test_estimation = final_model.predict(X_test_transform)\ny_test_estimation_transform = y_test_estimation.reshape(-1, 1)\nfinal_mse = mean_squared_error(Y_test_transform, y_test_estimation_transform)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance\n"}
{"snippet": "a= logist_reg.predict_proba(x_test[:1])[0]\nb = y_train.unique()\nb.sort()\nab = pd.Series(data=a,index=b)\nab.nlargest(3)\n", "intent": "the most probable products are\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,logist_reg.predict(x_test))\n", "intent": "The confusion matrix is the following:\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_data, test_labels, batch_size = 128)\nprint('test_accuracy', test_acc)\n", "intent": "Evaluate the network with network.evaluate. Testing the losses and the accuracy\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, v in enumerate(style_layers):\n        style_loss += style_weights[idx] * ((gram_matrix(feats[v]) - style_targets[idx])**2).sum()\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred_loanstatus= rfc.predict(X_test)\n", "intent": "*Prediting in X_test\n"}
{"snippet": "y_pred_loanstatus = rfc.predict(X_test)\n", "intent": "*Prediting in X_test\n"}
{"snippet": "y_pred_loanstatus= gbc.predict(X_test)\n", "intent": "*Prediting in X_test\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "x_test = np.array(['Thanks for your reading'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now try our own examples. Please make sure all the words we write are in the Glove embeddings.\n"}
{"snippet": "y_hat =rfc_best.predict(X_test)\n", "intent": "Here we got 92% accuracy\n"}
{"snippet": "print classification_report(y_test, rfc_best.predict(X_test))\n", "intent": "==>Precision and Recall\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(rfc_best, X, y, cv=10)\n", "intent": "--->We are splitting our data into 10 chunks\n"}
{"snippet": "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\nprint('Training Accurracy: {}'.format(scores[1]))\n", "intent": "We optimized the network weights on the training set accuracy, which we examine here. \n"}
{"snippet": "score = loaded_model.predict_proba(seq_array,verbose=1)\nprint(score.shape)\nprint(score)\n", "intent": "The model constructed from storage can be used to predict the probability of engine failure.\n"}
{"snippet": "train_pred = model.predict(X_train)\ntest_pred = model.predict(X_test)\nfrom sklearn.metrics import make_scorer, accuracy_score\ntrain_accuracy = accuracy_score(y_train, train_pred)\ntest_accuracy = accuracy_score(y_test, test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The testing accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nMAE_linearReg_split= mean_absolute_error(predicted_linearReg_split,Y_test)\nMSE_linearReg_split= mean_squared_error(predicted_linearReg_split,Y_test)\nR2_linearReg_split = r2_score(predicted_linearReg_split,Y_test)\n", "intent": "At the end we evaluate the square, absolute errors and R2 error.\n"}
{"snippet": "MAE_CV= mean_absolute_error(predict_CV,Target)\nMSE_CV= mean_squared_error(predict_CV,Target)\nR2_CV= r2_score(predict_CV,Target)\n", "intent": "At the end we evaluate the accurate of prediction calculating the error between the test datas and the predicted ones.\n"}
{"snippet": "predict = cross_val_predict(reg,DF_features,DF_target,cv=25)\n", "intent": "Predicting data with the test data with the model of random forest\n"}
{"snippet": "R2_score = r2_score(predictions_frame[\"AC_consump\"],predictions_frame[\"AC_consump_predicted\"])\nMean_absolute=mean_absolute_error(predictions_frame[\"AC_consump\"],predictions_frame[\"AC_consump_predicted\"])\nMean_absolute_square=mean_squared_error(predictions_frame[\"AC_consump\"],predictions_frame[\"AC_consump_predicted\"])\n", "intent": "Calculating the squared error, mean absolute error and mean squared absuluted error\n"}
{"snippet": "predict = linear_reg.predict(X_test)\n", "intent": "Predicting data with the test data\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nmean_squared_error_linearREG=mean_squared_error(Y_test,predict_linearAppliances)\nmean_absolute_error_linearREG=mean_absolute_error(Y_test,predict_linearAppliances)\nR2_score_linearReg= r2_score(Y_test,predict_linearAppliances)\nprint \"R2 index is \"+str(R2_score_linearReg)\nprint \"mean squared error is \"+str(mean_squared_error_linearREG)\nprint \"mean absolute error is \"+str(mean_absolute_error_linearREG)\n", "intent": "Let's find the metrics.\n"}
{"snippet": "mean_squared_error_CV=mean_squared_error(DF_target,predict_linearReg_CV)\nmean_absolute_error_CV=mean_absolute_error(DF_target,predict_linearReg_CV)\nR2_score_CV= r2_score(DF_target,predict_linearReg_CV)\nprint \"R2 index is \"+str(R2_score_CV)\nprint \"mean squared error is \"+str(mean_squared_error_CV)\nprint \"mean absolute error is \"+str(mean_absolute_error_CV)\n", "intent": "Let's find the metrics\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nmetric_R2_score = r2_score(y_test,prediction) \nmetric_mean_absolute_error = mean_absolute_error(y_test,prediction)\nmetric_mean_squared_error = mean_squared_error(y_test,prediction)\ncoeff_variation = np.sqrt(metric_mean_squared_error)/float(y_test.mean())\nprint \"The R2_score is \"+str(metric_R2_score)\nprint \"The mean absoulute error is \"+str(metric_mean_absolute_error)\nprint \"The mean squared error is \"+str(metric_mean_squared_error)\nprint \"Coefficient variation : \"+str(coeff_variation)\n", "intent": "Calculating the accuracy metrics of implemented machine learning model\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nR2_score_DF_RF_CV = r2_score(predict_DF_RF_CV[\"Indoor Room Temperature\"],predict_DF_RF_CV[\"Indoor Room Temperature Prediction_CV\"])\nmean_absolute_error_DF_CV = mean_absolute_error(predict_DF_RF_CV[\"Indoor Room Temperature\"],predict_DF_RF_CV[\"Indoor Room Temperature Prediction_CV\"])\nmean_squared_error_DF_CV = mean_squared_error(predict_DF_RF_CV[\"Indoor Room Temperature\"],predict_DF_RF_CV[\"Indoor Room Temperature Prediction_CV\"])\ncoeff_variation_DF_CV = np.sqrt(mean_squared_error_DF_CV)/predict_DF_RF_CV[\"Indoor Room Temperature\"].mean()\nprint \"\\nThe R2_score is: \"+str(R2_score_DF_RF_CV)\nprint \"The Mean absoulute error is: \"+str(mean_absolute_error_DF_CV)\nprint \"The Mean squared error is: \"+str(mean_squared_error_DF_CV)\nprint \"The Coefficient of variation is: \"+str(coeff_variation_DF_CV)\n", "intent": "Calculating the accuracy metrics of implemented machine learning model\n"}
{"snippet": "svc.predict(X[:10])\n", "intent": "Now we have a fit model, which can be used to make predictions about new data. We'll use the `predict` method to accomplish this.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_score = cross_val_score(svc, fmri_masked, targets)\nprint(cv_score)\n", "intent": "If all we want to do is score this model, note that `sklearn` has tools to perform cross-validation more succinctly:\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=240):\n    for i in range(n):\n        pair = pairs[i]\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n  with get_session() as sess:\n    d_loss, g_loss = sess.run(\n            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n  print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n  print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "model.predict(test)\n", "intent": "Now, you can use the test set to make label predictions\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        G, A = gram_matrix(feats[style_layers[i]]), style_targets[i]\n        style_loss += style_weights[i] * tf.reduce_sum((G - A)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ndog_breed_predictions_2 = [np.argmax(model2.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy of the Model: %.4f%%' % test_accuracy)\ntest_accuracy_2 = 100*np.sum(np.array(dog_breed_predictions_2)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions_2)\nprint('Test accuracy of Model 2 : %.4f%%' % test_accuracy_2)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "doggo_predictions = [np.argmax(doggo.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(doggo_predictions)==np.argmax(test_targets, axis=1))/len(doggo_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred = model.predict(x_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "Let's evaluate our model performance by calculating the residual sum of squares and the explained variance score (R^2).\n"}
{"snippet": "rfc_pred = rfc.predict(x_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n"}
{"snippet": "x_test_transformed = vectorizer.transform(x_test)\npred = model.predict(x_test_transformed)\n", "intent": "Time to see how our model did!\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test3.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "naive_predictions = np.ones(len(y_test))\nprint(\"Final accuracy score on the testing data: {:.4f}\"\\\n      .format(accuracy_score(y_test, naive_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\\n\"\\\n          .format(fbeta_score(y_test, naive_predictions, beta = 0.5)))\n", "intent": "Let's only focus on the historical data now to make some predictions.  I am going to include the data for the 1952-1960 elections.\n"}
{"snippet": "print(\"Unbalanced:\")\nprint(classification_report(LR.predict(X_test), y_test))\nprint(\"Balanced:\")\nprint(classification_report(LR_bal.predict(X_test), y_test))\n", "intent": "We got a pretty good accuracy score and there doesn't appear to be any overfitting which is good. Let's take a look\nat the classification report:\n"}
{"snippet": "y_train_predict = model_kmeans.predict(X_train)\ny_test_predict = model_kmeans.predict(X_test)\n", "intent": "Predict the labes both for the training and the test set.\n"}
{"snippet": "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_predict),'\\n'\n     \"Accuracy on test set:    \", accuracy_score(y_test, y_test_predict),)\n", "intent": "Check quality of prediction.\n"}
{"snippet": "y_train_predict = model_nb.predict(X_train)\ny_test_predict = model_nb.predict(X_test)\n", "intent": "Predict the labes both for the training and the test set. Note that predicting takes much longer than fitting the model.\n"}
{"snippet": "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_predict),'\\n'\n     \"Accuracy on test set:    \", accuracy_score(y_test, y_test_predict),)\n", "intent": "Check quality of prediction - pretty similar to K-Means.\n"}
{"snippet": "y_train_predict = model_logistic.predict(X_train)\ny_test_predict = model_logistic.predict(X_test)\n", "intent": "Predict the labes both for the training and the test set.\n"}
{"snippet": "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_predict),'\\n'\n     \"Accuracy on test set:    \", accuracy_score(y_test, y_test_predict),)\n", "intent": "Check quality of prediction, we notice some kind of overfitting, the accuracy on the training set is somewhat higher than on the test set. \n"}
{"snippet": "y_train_predict = model_random.predict(X_train)\ny_test_predict = model_random.predict(X_test)\n", "intent": "Predict the labes both for the training and the test set.\n"}
{"snippet": "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_predict),'\\n'\n     \"Accuracy on test set:    \", accuracy_score(y_test, y_test_predict),)\n", "intent": "Check quality of prediction. We notice significant overfitting on the training set with accuracy of 99.9%.\n"}
{"snippet": "y_train_predict = model_dense.predict(X_train)\ny_test_predict = model_dense.predict(X_test)\n", "intent": "Predict the labes both for the training and the test set.\n"}
{"snippet": "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_predict),'\\n'\n     \"Accuracy on test set:    \", accuracy_score(y_test, y_test_predict),)\n", "intent": "Check quality of prediction. We are now at 98% on the test sample, with some overfitting on the training set.\n"}
{"snippet": "y_train_predict = model_dense_val.predict(X_train)\ny_val_predict = model_dense_val.predict(X_val)\ny_test_predict = model_dense_val.predict(X_test)\n", "intent": "Predict the labes both for the training and the test set.\n"}
{"snippet": "y_test_predict_prob = model_dense_val.predict(X_test)\nerrors_predict_prob = y_test_predict_prob[errors]\nerrors_predict_label_2nd = np.argsort(-errors_predict_prob, axis=1)[:, 1]\n", "intent": "How about looking for those cases at the second likeliest digits, predicted by our model? \n"}
{"snippet": "from sklearn.feature_selection import f_regression\nf_test, pval = f_regression(X, lm.predict(X))\nprint('F-statistic: {}'.format(f_test[0]))\nprint('p-value: {}'.format(pval[0]))\n", "intent": "The linear fit is statistically significant at a significance level of 0.05. \n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(test_y, yhat)\nprint(\"Baseline MAE: {:.5f}\".format(mae))\n", "intent": "Now let's see what the mean _absolute_ error for this baseline model is\n"}
{"snippet": "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n    predictions = np.clip(y_pred, eps, 1 - eps)\n    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n    actual = np.zeros(y_pred.shape)\n    n_samples = actual.shape[0]\n    actual[np.arange(n_samples), y_true.astype(int)] = 1\n    vectsum = np.sum(actual * np.log(predictions))\n    loss = -1.0 / n_samples * vectsum\n    return loss\nprint(\"multiclass_log_loss() method loaded..\")\n", "intent": "Note: This may not need to be run each time, as the train_test_split ensures a balanced class distribution\n"}
{"snippet": "x_test = np.array(['I miss someone dear'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "cv = ShuffleSplit(n_samples, n_iter=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "* Giving cross-validation iterator as cv argument\n"}
{"snippet": "from sklearn import metrics\nprint(\"k=1 Accuracy: \")\nprint(metrics.accuracy_score(y_testset, y1))\nprint(\"k=23 Accuracy: \")\nprint(metrics.accuracy_score(y_testset, y23))\nprint(\"k=90 Accuracy: \")\nprint(metrics.accuracy_score(y_testset, y90))\n", "intent": "Awesome! Now let's compute neigh's <b>prediction accuracy</b>. We can do this by using the <b>metrics.accuracy_score</b> function\n"}
{"snippet": "from sklearn import metrics\nprint(\"RandomForests's Accuracy: \"), metrics.accuracy_score(y_testset, predForest)\n", "intent": "Let's check the accuracy of our model. <br>\nNote: Make sure you have metrics imported from sklearn\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy with transfer from VGG16: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "R50ext_predictions = [np.argmax(R50ext_model.predict(np.expand_dims(feature, axis=0))) for feature in test_R50ext]\nR50ext_test_accuracy = 100*np.sum(np.array(R50ext_predictions)==np.argmax(test_targets, axis=1))/len(R50ext_predictions)\nprint('Test accuracy with transfer from Resnet50: %.4f%%' % R50ext_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nResnet50_test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy with transfer from Resnet50: %.4f%%' % Resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def evaluate(x, y, expr, x_value, y_value):\n    psss\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint \"SUCCESS!\"\n", "intent": "Now try compiling and running a simple function:\n"}
{"snippet": "my_prediction = my_tree_one.predict(test_features)\n", "intent": "Making a prediction using the test set\n"}
{"snippet": "test_features = test[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\npred_forest = my_forest.predict(test_features)\nprint(len(pred_forest))\n", "intent": "Compute predictions on our test set features then print the length of the prediction vector\n"}
{"snippet": "predicted = model.predict(x_test)\nprint(\"Accuracy: %.2f%%\" % (np.mean(np.equal(np.argmax(predicted, axis=-1), np.argmax(y_test, axis=-1))) * 100))\n", "intent": "Keras evaluate function sometimes outputs incorrect results so we should check again using predicted output.\n"}
{"snippet": "docs_new = [\n    'God is love', \n    'OpenGL on the GPU is fast', \n    'The human visual cortex is a part of the brain'\n]\nX_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredicted = clf.predict(X_new_tfidf)\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, twenty_train.target_names[category]))\n", "intent": "Using the trained classifier we can now predict the category for some example sentences.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n    target_names=twenty_test.target_names))\n", "intent": "Print a more detailed performance analysis.\n"}
{"snippet": "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n", "intent": "Example classification (GridSearchCV behaves like a normal model)\n"}
{"snippet": "example = \"3rd of March 2001\"\nsource = string_to_int(example, 20, human_vocab)\nprediction = model.predict(np.array([source]))\nprediction = np.argmax(prediction[0], axis = -1)\noutput = int_to_string(prediction, inv_machine_vocab)\nprint(\"source:\", example)\nprint(\"output:\", ''.join(output))\n", "intent": "Try your simple NMT model on various examples using the code below.\n"}
{"snippet": "nb_predict_train = nb_model.predict(X_train)\nfrom sklearn import metrics\nprint (\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_train,nb_predict_train)))\n", "intent": "Performance on Training data\n"}
{"snippet": "print ('r2_score : {}'.format(metrics.r2_score(y_test,pred1)))\nprint ('Mean sqd error : {}'.format(metrics.mean_squared_error(y_test,pred1)))\nprint ('Explained variance score : {}'.format(metrics.explained_variance_score(y_test,pred1)))\n", "intent": "The predictions agree very well with the test data, except near the x limits (-400, 400). Let's check the r score and mean squared error. \n"}
{"snippet": "from sklearn.metrics import f1_score\ny_true = [0, 1, 2, 0, 1, 2]\ny_pred = [0, 2, 1, 0, 0, 1]\nf1_score(y_true, y_pred, average='macro')  \n", "intent": "p_micro*re_micro/(p_micro+re_micro)\n"}
{"snippet": "num_clusters = 9\nspectral_model = spectral(n_clusters=num_clusters)\ncluster_labels = spectral_model.fit_predict(card_vectors)\n", "intent": "What if I try without any labeling?\nI wasn't impressed with the results, so I may leave the code here but I'm not using it in graphs.\n"}
{"snippet": "print(\"Errors at specific experiments:\")\nfor n, temp in enumerate(temps):\n    t_pred = regr.predict([temp])\n    delta = (t_pred-t_diff[n])[0][0]\n    print(abs(round(delta,2)), end=\", \")\n", "intent": "**1. Verify that the model did indeed fit onto our original training data.**\n"}
{"snippet": "def predict_final(model, K, temp):\n    t_grad = [(temp[x+2] - temp[x])/(t[x+2]-t[x]) for x in range(K-5,K+5)]\n    t_grad_avg = sum(t_grad)/len(t_grad)\n    return model.predict([[t_grad_avg]]) + temp[K]\nK = 64 \nfor experiment in test.keys():\n    t, temp = test[experiment]\n    pred = round(predict_final(regr, K, temp)[0][0], 3)\n    print(\"   Actual:\", experiment, \"\\nPredicted:\", pred)\n    print(\"    Error:\", round(abs(float(experiment)-pred),3), end=\"\\n\\n\")\n", "intent": "**3. Make predictions using test data**\n"}
{"snippet": "test_seed = 2018 \nnp.random.seed(2018) \ntest_N = 10000\ntest_X = np.random.normal(size=3*test_N).reshape(test_N,3)\ntest_y = test_X.max(axis=1) + np.random.normal(scale=eps, size=test_N)\nprint(vanilla.evaluate(test_X, test_y, verbose=0))\nprint(augmented.evaluate(test_X, test_y, verbose=0))\nprint(ti_network.evaluate(s3.transform(test_X), test_y, verbose=0))\n", "intent": "Finally, to check we are not overfitting our data set, we check our performance on a test set is close to the Bayes' error.\n"}
{"snippet": "yhat2_rnn1 = model.predict(np.array(Xtest2))\nlen(yhat2_rnn1)\n", "intent": "I was so happy to see it run and see the accuracy increasing after each epoch!\nProbably would need more iterations\n"}
{"snippet": "for i in range(6):\n    print \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t | Recall EF{}:\\t\\t\".format(i),\\\n          round(recall_score(ytest3, yhat5_rf9, labels=['EF{}'.format(i)], average='macro'), 3)\n", "intent": "This is very similar to the best accuracy score previously obtained with SGD without the text length as a feature (0.784). Let's analyse the recall:\n"}
{"snippet": "accuracy_nlp['cntv8b - rf - text length feat'] = round(accuracy_rf9, 3)\nfor i, score in enumerate(scores_nlp[1:]):\n    score['cntv8b - rf - text length feat'] = \\\n    round(recall_score(ytest3, yhat5_rf9, labels=['EF{}'.format(i)], average='macro'), 3)\n", "intent": "- The recall of the weaker tornadoes has increased, but the one of the stronger ones has decreased.\n- This test is not going to be chosen.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n", "intent": "<p>Error on all predictions</p>\n"}
{"snippet": "for i in X_test0['p_item_product_lc'].unique():\n    for j in X_test0[X_test0['p_item_product_lc']==i]['p_units_lc'].unique():\n        x=X_test0[X_test0['p_item_product_lc']==i][X_test0['p_units_lc']==j]\n        print 'mean for',i,'in',j,':',np.mean(x['y_test']),'prediction error:',mean_absolute_error(x['y_test'], x['y_pred'])\n", "intent": "<p>Error broken down by product type</p>\n"}
{"snippet": "print (metrics.accuracy_score(y_test, predicted))\n", "intent": "Now we generate some evaluation metrics.\n"}
{"snippet": "model.predict_proba(np.array([1,1,1,1,1,0,0,22,2,3,3,3,4,3,5]))\n", "intent": "We can predict the probability of failing for a random student not present in the data set. \n"}
{"snippet": "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\nmodel = LogisticRegressionWithLBFGS.train(training,numClasses=2)\nlabelsAndPred = test.map(lambda p: (float(model.predict(p.features)),p.label))\n", "intent": "Logistic Regression\n"}
{"snippet": "print 'Random Forest:'\nprint(classification_report(target_test, rf_final.predict(features_test),\n                            target_names=['expired', 'completed']))\nprint\nprint 'Gradient Boosting'\nprint(classification_report(target_test, gb_final.predict(features_test),\n                            target_names=['expired', 'completed']))\n", "intent": "Now, we can compare random forest and gradiest boosting:\n"}
{"snippet": "N = [[1.9,  3.2,  1.3,  0.1],\n       [ 6.2,  3.1,  4.5,  1.6],\n       [ 6.9,  3.3,  5.9,  2.3]]\nkmeans.predict(N)\n", "intent": "After we have finished, we can user our fitted model to predict new values.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "* Proportion of correct predicitions\n* Common evaluation metric for classification problems\n"}
{"snippet": "em.predict(probs) + 1\n", "intent": "By using the em algorithm, specifying the number of components, we end up with an array of clusters.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n", "intent": "Done! You now have a working Linear Regression Model.  Let's try a few instances from the training set\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "It works, although not exactly accurate.  Lets measure this regression model's RMSE\n"}
{"snippet": "pred = forest.predict(data)\nmetrics.accuracy_score(pred, target)\n", "intent": "First, let's see how it behaves on the data it has already worked on:\n"}
{"snippet": "predi = forest.predict(data_holdout)\nmetrics.accuracy_score(predi, target_holdout)\n", "intent": "It is clear from the confusion matrix that our random forest is biased towards the *light skin* class.\nWe check the results on the data we held out:\n"}
{"snippet": "y_pred = clf.predict(X_test)\nmatrix = confusion_matrix(y_test,y_pred)\nprint(matrix)\n", "intent": "After that, we display the confusion matrix of our classification pipeline.\n"}
{"snippet": "def svm_viterbi_score(features_test,labels_test,log_trans,model):\n    log_lhood = log_lhood_svm(features_test,model)\n    state_seq = log_viterbi(log_lhood, log_trans)\n    new_score = float(len(np.squeeze(np.where(state_seq == labels_test))))/float(len(labels_test))\n    return new_score\n", "intent": "Accuracy score after viterbi filtering : model is SVM or GMM\n"}
{"snippet": "model.predict(x_test)\n", "intent": "We can see each prediction in the test data. The closer the score is to 1, the higher the probability that the review is positive.\n"}
{"snippet": "hidden_test_pred = model_mle.best_path([obs[0] for obs in seq_test])\nprint(classification_report(hidden_test_true, hidden_test_pred))\n", "intent": "... and try to recover sequence of hidden states knowing only observed states (using our best model)\n"}
{"snippet": "hidden_test_rand = np.random.choice(a=states, size=365)\nprint(classification_report(hidden_test_true, hidden_test_rand))\n", "intent": "Prediction quality is around $50\\%$. Seems to be low, but let's try with random prediction:\n"}
{"snippet": "hidden_test = model_true.best_path([obs[0] for obs in seq_test])\nprint(classification_report(hidden_test_true, hidden_test))\n", "intent": "... and using model with **true** parameters:\n"}
{"snippet": "tagger_crf = CRFPosTagger()\ntagger_crf.train(corpora_train)\ny_crf = [word for sent in tagger_crf.test(corpora_test) for word in sent]\nprint(classification_report(y_test, y_crf))\n", "intent": "We can conclude, that CRF outperforms Naive Bayes classifier (both use the same featureset):\n"}
{"snippet": "def print_metrics(true, pred):\n    print('Accuracy {:.4f}'.format(accuracy_score(true, pred)))\n    print('Precision {:.4f}'.format(precision_score(true, pred)))\n    print('Recall {:.4f}'.format(recall_score(true, pred)))\n    print('F-score {:.4f}'.format(f1_score(true, pred)))\n    print('MSE {:.4f}'.format(mean_squared_error(true, pred)))\n", "intent": "First of all, let's look at KNN classifier.\n"}
{"snippet": "test_data_x = test_data[input_data_columns];\npredicted_result = clf.predict(test_data_x)\n", "intent": "Now lets test this classifier on our test data\n"}
{"snippet": "logistic_predicted_results = logistic_clf.predict(test_data_x);\nsvm_predicted_results = svm_clf.predict(test_data_x);\nforest_predicted_results = forest_clf.predict(test_data_x);\n", "intent": "Lets predict results for each classifier now\n"}
{"snippet": "predicted_results = logistic_clf.predict(test_data_x);\n", "intent": "Lets predict the results for test_data now.\n"}
{"snippet": "probabilities = logistic.predict_proba(X)\nprobabilities\n", "intent": "After the model is trained, we can get the probability for each row to be treated or not.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "**Classification Accuracy** (evaluation metric)\n 1 prooportion of correct predicitons \n 2 Common evaluation metric for classification problems \n"}
{"snippet": "p = knn.predict([[20,4.3,5.3]])\ndic_fruitlabel_name[p[0]]\n", "intent": "- predict method in classifiers are useful for prediction \n"}
{"snippet": "print('Conf matrix for SVC rbf')\nprint(confusion_matrix(Y_test, clf_rbf.predict(X_test)))\nprint('Conf matrix for SVC linear')\nprint(confusion_matrix(Y_test, clf_lin.predict(X_test)))\nprint('Conf matrix for dummy most_fre')\nprint(confusion_matrix(Y_test, clf_dummy_1.predict(X_test)))\nprint('Conf matrix for dummy stratified')\nprint(confusion_matrix(Y_test, clf_dummy_2.predict(X_test)))\nprint('Conf matrix for dummy uniform')\nprint(confusion_matrix(Y_test, clf_dummy_3.predict(X_test)))\n", "intent": "|--|Pred_0|Pred_1|\n|:-|:-|:-|\n|Act_0|TN|FP|\n|Act_1|FN|TP|\n"}
{"snippet": "print('Dummy model Eval metrics with strategy as uniform:\\n',classification_report(Y_test, \n                                                                                   clf_dummy_3.predict(X_test),\n                                                                                  target_names = ['0', '1']))\nprint('Eval metrics for clf_rbf :\\n',classification_report(Y_test, clf_rbf.predict(X_test),\n                                                                                  target_names = ['0', '1']))\nprint('Eval metrics for clf_lin :\\n',classification_report(Y_test, clf_lin.predict(X_test),\n                                                                                  target_names = ['0', '1']))      \n", "intent": "- We can evaluate multiple models after scoring them on test set, like on precision, recall, f1; and also compare\nthem with a dummy classifier \n"}
{"snippet": "print('CF report of rbf clf \\n',classification_report(Y_test, clf_rbf.predict(X_test)))\n", "intent": "- micro averaging  \n- macro averaging  \n- weighted \n- sample\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors (in french: **Erreur Quadratique Moyenne**):\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "We now apply the trained model to the test data (omitting the column PassengerId) to produce an output of predictions.\n"}
{"snippet": "Y_dev_pred = model.predict(X_reg_dev)\nprint(Y_dev_pred[0:5])\n", "intent": "And get predictions for X_dev:\n"}
{"snippet": "cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=layer_2))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.5).minimize(cost)\nN_EPOCHS = 20000\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\nfor i in range(N_EPOCHS):\n    _, error = sess.run([optimizer,cost], feed_dict={x: inp, y_: target})\n    if i % 1000 == 0:\n        print(\"epoch\",i,\"error\",error)\n", "intent": "Again, we will use the same error function and optimizer we used in the previous example and train the network in the same way.\n"}
{"snippet": "score = model.evaluate(mnist.test.images, mnist.test.labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Finally we will evaluate the trained model on our test dataset and display the results.\n"}
{"snippet": "score = model.evaluate(test, real_vals, batch_size=16)\n", "intent": "Now we can evaluate our model's performance!\n"}
{"snippet": "results = model.predict(test)\nyay = 0\nnay = 0\nfor i in range(len(results)):\n    if round(float(results[i]))==round(float(real_vals[i])):\n        yay += 1\n    else: nay +=1\nprint('yay= ', yay, ' ,nay= ', nay, yay/(yay+nay), ' % correct predictions')\n", "intent": "This looks promising, but I want to run my own kind of analysis.  I want to round the values to see whether they hit or miss accordingly.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_prediction)\naccuracy\n", "intent": "Calculating accuracy using the **accuracy_score**\n"}
{"snippet": "from sklearn.metrics import log_loss\naccuracy = log_loss(y_test, y_prediction)\naccuracy\n", "intent": "Calculating accuracy using **log_loss**\n"}
{"snippet": "def cal_percentile(s,col):\n    score = percentileofscore(listing_train_for_query_df[col].values, s, kind='mean')\n    return score\ndef normalize_score(df,all_predictors):\n    df_normalized = df.copy()\n    for col in all_predictors:       \n        df_normalized[col]=df[col].apply(lambda x: cal_percentile(x,col))\n    return df_normalized\n", "intent": "* function for normalizing the predicted score to percentile\n* The calculation refers to train data set predicted scores. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = svc.predict(data)\naccuracy_score(clusters,predictions)\n", "intent": "><b>2c. COMPUTE THE THE PERCENTAGE OF POINTS THE ALGORITHM CORRECTLY CLASSIFIES\n"}
{"snippet": "X, y = create_dataset(traindf.drop([\"geom\"], axis=1),4,3) \nXtest, ytest = create_dataset(testdf.drop([\"geom\"], axis=1),4,3) \ntrain_validate_predict(X,y,Xtest,ytest)\n", "intent": "Now let's try by including centorid of each parcel as a feature\n"}
{"snippet": "from sklearn.metrics import log_loss\nyhat = two_layer_nn(x,w)\nprint(\"Log loss of the right model:\")\nprint(log_loss(y, yhat))\nyhat = two_layer_nn(x,w1)\nprint(\"Log loss of the wrong model:\")\nprint(log_loss(y, yhat))\nyhat = two_layer_nn(x,w2)\nprint(\"Log loss of the very wrong model:\")\nprint(log_loss(y, yhat))\n", "intent": "We often train neural network to classify using a cost function called cross entropy. Or log loss.\n"}
{"snippet": "print \"precision:\",precision_score(labels_test,pred)\nprint \"recall:\",recall_score(labels_test,pred)\n", "intent": "These are not even slightly good news:\n"}
{"snippet": "def dice_coef(y_true, y_pred):\n    intersection = K.sum(y_true * y_pred)\n    return (2. * intersection + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\ndef loss_fn_other(y_true, y_pred):\n    return K.mean(K.categorical_crossentropy(y_true, y_pred, from_logits=False), axis=1)\n", "intent": "- Define custom loss function to do calculate cross-entropy\n- Dice coefficient loss used to train model - is a similarity index\n"}
{"snippet": "def get_preds(t): return t.predict(X_valid)\nnp.mean(preds[:,0]), np.std(preds[:,0])\n", "intent": "```python\ndef parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n```\n"}
{"snippet": "very_different = ['metal', 'hip hop', 'jazz', 'christian', 'pop', 'reggae']\ndifferent_data = ready_data[ready_data['real_genre'].isin(very_different)]\nclass_df, test = fit_and_predict(different_data)\n", "intent": "Lets take only genres that are not that similar to each other.\n"}
{"snippet": "similar = ['r&b', 'pop']\nsimilar = ready_data[ready_data['real_genre'].isin(similar)]\nclass_df, test = fit_and_predict(similar)\n", "intent": "Pop and r&b are indeed hard to distinguish and normally singers have two of those genres assigned to them.\n"}
{"snippet": "from sklearn.base import BaseEstimator, ClassifierMixin\nclass MajorClassifier(BaseEstimator, ClassifierMixin):  \n    def __init__(self):\n        self.maxValue = 0\n    def fit(self,X,Y):\n        y = np.array(Y)\n        self.maxValue = max(set(y), key=y.tolist().count)\n    def predict(self,X):\n        ylist = [self.maxValue]*len(X)\n        return ylist\n", "intent": "* Majority classifier\n"}
{"snippet": "RESNET50_predictions = [np.argmax(resnetModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(RESNET50_predictions)==np.argmax(test_targets, axis=1))/len(RESNET50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(clf, X=digits.data, y=digits.target, cv=cv)\n", "intent": "We can do all this in one line using the `cross_val_score` method\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit, cross_val_score\nlabels = epochs.events[:, -1] - 2\ncv = ShuffleSplit(10, test_size=0.2, random_state=42)\nscores = cross_val_score(clf, epochs_data_train, labels, cv=cv, n_jobs=1)\nprint(scores)\n", "intent": "Let's use cross validation to evaluate our classifier\n"}
{"snippet": "print(classification_report(labels_test, labels_pred))\n", "intent": "f1, precision, and recall scores\n"}
{"snippet": "logreg = LogReg(LEARNING_RATE, EPOCHS, len(data_train[0]))\nbias_logreg, weights_logreg = logreg.train(data_train, labels_train)\ny_logistic = [round(logreg.predict(example)) for example in data_test]\n", "intent": "Training the logistic regression model\n"}
{"snippet": "net.evaluate(phone_number_data)\n", "intent": "Now that we got our numbers into the format the neural network actually accepts, it's time to see how it performs.\n"}
{"snippet": "def accuracy(y,yhat):\n    return sum(yhat==y)/len(y)\nyhat = blr.predict(Xb)\nprint('Accuracy of: ',accuracy(y,yhat))\n", "intent": "So how well did the training go?\n"}
{"snippet": "nn_bfgs = TLPVectorizedBFGS(n_output=10, \n                      n_features=X_train.shape[1], \n                      n_hidden=50, \n                      l2=0.1, \n                      l1=0.0, \n                      epochs=100,\n                      gtol=1e-9,\n                      random_state=1)\nyhat = nn_bfgs.predict(X_train)\nprint('Resubstitution acc:',accuracy_score(y_train,yhat))\n", "intent": "This is a nice test to show that mini-batching has better conversion properties for some datasets.\n"}
{"snippet": "dt_pred = dt.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=[]\n    for i, layer in enumerate(style_layers):\n        loss = torch.sum((gram_matrix(feats[layer]) - style_targets[i])**2)\n        style_loss.append(style_weights[i] * loss)\n    return np.sum(style_loss)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = []\n    for i, layer in enumerate(style_layers):\n        G = gram_matrix(feats[layer])\n        loss = style_weights[i] * tf.reduce_sum(tf.square(G - style_targets[i]))\n        style_loss.append(loss)\n    style_loss = tf.reduce_sum(style_loss)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    print(correct, student_output)\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "train_predict_control  = train_fit_control.predict(college_ind)\ntest_predict_control  = train_fit_control.predict(test_ind)\n", "intent": "Calculate the predictions, that is, the predicted y equals to the round values of predicted probability.\n"}
{"snippet": "train_proba_control = train_fit_control.predict_proba(college_ind)\ntest_proba_control = train_fit_control.predict_proba(test_ind)\n", "intent": "Calculate the probabilities $\\hat{p}$ by using the `predict_proba` method.\n"}
{"snippet": "print (\"Accuracy score:\", accuracy_score(test_target_iclevel, test_predict_iclevel))\n", "intent": "The following set of codes are about target variable `ICLEVEL`.\n"}
{"snippet": "train_predict_control  = dt_control.predict(college_ind)\ntest_predict_control  = dt_control.predict(test_ind)\n", "intent": "The following code makes the predictions for `CONTROL` target variable.\n"}
{"snippet": "train_proba_control = dt_control.predict_proba(college_ind)\ntest_proba_control = dt_control.predict_proba(test_ind)\n", "intent": "Then we calculate the predicted probabilities for the predictions.\n"}
{"snippet": "print (\"Accuracy score:\", accuracy_score(test_target_control, control_svm_pre_test))\n", "intent": "The following chunk is used to get the accuracy score for target variable `CONTROL` on the test set. \n"}
{"snippet": "preds = rf.predict(df_test)\n", "intent": "------------\nThat was quick... but now it's time to test how effective our method was. \nWe start by running our trained algorithm on the test data\n"}
{"snippet": "dog_breed_predictions = [np.argmax(my_cnn_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "for cls in y_df.columns:\n    scores = cross_val_score(pipe, df['Abstract Text'], y_df[cls], cv=10, scoring='roc_auc')\n    print(f'{cls}:')\n    print(\"AUC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    print('-'*100)\n", "intent": "Let's see how this model works on the other classes\n"}
{"snippet": "for cls in y_df.columns:\n    scores = cross_val_score(pipe, df['Abstract Text'], y_df[cls], cv=10, scoring='roc_auc')\n    print(f'{cls}:')\n    print(\"AUC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    print('-'*100)\n", "intent": "Lets see how it performs on all classes\n"}
{"snippet": "accuracy_value = accuracy_score(y_test, predictions) * 100 \nprint('The model is', accuracy_value, '% accurate.')\n", "intent": "- shows the pecentage of correct predictions\n"}
{"snippet": "def pred(model, crpimg):\n    imarr = np.array(crpimg).astype(np.float32)\n    imarr = np.expand_dims(imarr, axis=0)\n    out = model.predict(imarr)\n    max_index = np.argmax(out, axis=1)[0]\n    max_name = description[max_index,0]\n    print(max_index, max_name[0], out[0,max_index])\n", "intent": "Then we created a function called pred to predict the name.\n"}
{"snippet": "Grid_Search_Results = []\nfor i in range(100):\n    Grid_Search_Results.append(iterate_fit_predict())\nGrid_Search= np.asarray(Grid_Search_Results)\nGrid_Search_Results_Depth = np.asarray(Grid_Search.T[0], dtype=int)\nGrid_Search_Results_Price = np.asarray(Grid_Search.T[1], dtype=float)\n", "intent": "Iteration: Fit and Predict Model\n    (GridSearchCV Results)\n"}
{"snippet": "Grid_Search_Results = []\nfor i in range(1000):\n    Grid_Search_Results.append(iterate_fit_predict(city_data))\nGrid_Search= np.asarray(Grid_Search_Results)\nGrid_Search_Results_Depth = np.asarray(Grid_Search.T[0], dtype=int)\nGrid_Search_Results_Price = np.asarray(Grid_Search.T[1], dtype=float)\n", "intent": "Iteration: Fit and Predict Model\n    (GridSearchCV Results)\n"}
{"snippet": "MyResNet50_predictions = [np.argmax(MyResNet50_model.predict(np.expand_dims(i, axis=0))) for i in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(MyResNet50_predictions)==np.argmax(test_targets, axis=1))/len(MyResNet50_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    s_loss = tf.zeros(shape=[1])\n    for i in range(len(style_layers)):\n        l = style_layers[i]\n        gram = gram_matrix(feats[l])\n        loss = style_weights[i]*tf.reduce_sum(tf.squared_difference(gram,style_targets[i]))\n        s_loss = s_loss + loss\n    return s_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "cross_val_score(rfc, X_test, y_test,cv=5)\n", "intent": "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n"}
{"snippet": "from sklearn import metrics\nprint(f\"Neigh's Accuracy: {metrics.accuracy_score(y_test, pred)}\")\n", "intent": "Awesome! Now let's compute neigh's <b>prediction accuracy</b>. We can do this by using the <b>metrics.accuracy_score</b> function\n"}
{"snippet": "from sklearn import metrics\nprint(f\"Neigh23's Accuracy: {metrics.accuracy_score(y_test, pred23)}\")\nprint(f\"Neigh90's Accuracy: {metrics.accuracy_score(y_test, pred90)}\")\n", "intent": "Interesting! Let's do the same for the other instances of KNeighborsClassifier.\n"}
{"snippet": "from sklearn import metrics\nprint(f\"RandomForests's Accuracy: {metrics.accuracy_score(y_testset, predForest)}\")\n", "intent": "Let's check the accuracy of our model. <br>\nNote: Make sure you have metrics imported from sklearn\n"}
{"snippet": "print(np.mean((LinReg.predict(X_testset) - y_testset) ** 2) ** (0.5))\n", "intent": "And lastly, <b>RMSE</b>:<br>\n<img src = \"https://ibm.box.com/shared/static/hc55qagftbvl8yb306vcx78a738n2ylc.gif\", height = 100, align = 'left'>\n"}
{"snippet": "model_scores = {k: log_loss(Y_test, v.predict_proba(X_test)) for k,v in best_models.iteritems()}\nmodel_scores\n", "intent": "**Score** best versions of each model using log loss (cross entropy).\n"}
{"snippet": "print(clf.predict([0.58,0.76]))\n", "intent": "Next, we can predict and test. Let's print a prediction:\n"}
{"snippet": "prob_y_2 = clf_2.predict_proba(X)\nprob_y_2 = [p[1] for p in prob_y_2]\nprint( roc_auc_score(y, prob_y_2) ) \n", "intent": "Area Under ROC Curve (AUROC) is a general-purpose metric for classification\n"}
{"snippet": "for k,v in model_info.items():\n    print(mean_squared_error(y_test, v['grid_obj'].predict(np.array(X_test))))\n", "intent": "It is useful to compare to the predicted values for the test data.\n"}
{"snippet": "mean_squared_error(y_test, grid.predict(np.array(X_test)))\n", "intent": "Test set MSE of best model identified by grid search on the validation set:\n"}
{"snippet": "print('predicted: ',spam_detect_model.predict(tf4)[0])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "predictions = pl.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "print('Accuracy  (Train): %f'   % metrics.accuracy_score(y_train, pred_train))\nprint('Precision (Train): %f'   % metrics.precision_score(y_train, pred_train))\nprint('Recall    (Train): %f'   % metrics.recall_score(y_train, pred_train))\nprint metrics.classification_report(y_train, pred_train, labels=[1,0], target_names=[chosen_category[9:],'Other']) \n", "intent": "I finally evaluate our classifier\n"}
{"snippet": "pretrained_model_predictions = [np.argmax(pretrained_model.predict(np.expand_dims(feature, axis=0))) for feature in test_pretrained_model]\ntest_accuracy = 100*np.sum(np.array(pretrained_model_predictions)==np.argmax(test_targets, axis=1))/len(pretrained_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "bag_preds = baggingClassifier.predict(testing_data)\nrf_preds = randomForest.predict(testing_data)\nada_preds = adaBoost.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "import time\nt1 = time.time()\nprediction = knn_predict(digit_train, test, 10)\nprint('took', time.time() - t1)\n", "intent": "(j) Train your classifier with all of the training data, and test your classifier with the test data. Submit your results to Kaggle.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "It works, although the predictions are not exactly accurate. Let measure RMSE on the whole traning set\n"}
{"snippet": "X_test_dtm = vect.transform(X_test)\ny_pred_nb = nb.predict(X_test_dtm)\n", "intent": "<a id='section3g'></a>\n"}
{"snippet": "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\naccuracy_score(y_test, y_pred_nb)\n", "intent": "<a id='section3h'></a>\n"}
{"snippet": "from sklearn.model_selection import KFold, cross_val_score\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    score = cross_val_score(model, X_train_dtm, y_train, scoring=scoring, cv=kfold)\n    names.append(name)\n    results.append(score)\n", "intent": "<a id='section3f'></a>\nIn this step, we will build 6 models, and pick the one with the best accuracy score\n"}
{"snippet": "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\naccuracy_score(y_test, y_pred)\n", "intent": "<a id='section3h'></a>\n"}
{"snippet": "print \"F1 score for predicting all 'yes' on test set: {:.4f}\".format(f1_score(y_test,\n                                                                             ['yes'] * len(y_test),\n                                                                             pos_label='yes',\n                                                                             average='binary'))\n", "intent": "Comparing results of the models with benchmark such as predicting all \"yes\" values on the test set.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n", "intent": "Scikit-learn has a large variety of different performance metrics available. Note that the accuracy is found using the test data set.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for l, A, w in zip(style_layers, style_targets, style_weights):\n        G = gram_matrix(feats[l])\n        loss += w * torch.sum((G - A)**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "features_raw = data.drop('Defaulted', axis = 1)\ny_actual = y\ny_predict = np.ones(len(y))\naccuracy = accuracy_score(y_actual, y_predict)\nfscore = fbeta_score(y_actual, y_predict, beta=0.5) \nprint(\"Naive Predictor: [Accuracy score: {:.2f}, F-beta score: {:.2f}]\".format(accuracy, fscore))\n", "intent": "If we chose a model that always predicted an individual had default = 1, what would that model's accuracy and F-score be on this dataset?\n"}
{"snippet": "predict = fit.predict(gtab, te)\n", "intent": "We predict back the signal using the derived model parameters\n"}
{"snippet": "loss,accuracy=model.evaluate(X_test,y_test)\nprint('loss:',loss)\nprint('accuracy',accuracy)\n", "intent": "Now we can evaluate the model on the test data to get a sense of how we did.\n"}
{"snippet": "train_predictions = clf.predict(messages_tfidf)\nprint train_predictions\n", "intent": "Training data predictions\n"}
{"snippet": "test_predictions = clf.predict(test_messages_tfidf)\n", "intent": "Getting the predictions from the model with vectorized test data tweet messages (test_messages_tfidf).\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds=clf.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test,preds))\nprint(classification_report(y_test,preds))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "pred2=pipe.predict(xtest)\nprint(confusion_matrix(ytest,pred2))\nprint(classification_report(ytest,pred2))\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "if IN_JUPYTER:\n    print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\n    print(metrics.classification_report(y_test, y_pred))\n", "intent": "We now calculate a series of statistics that allow us to gauge how well the algorithm will perform on unseen data.\n"}
{"snippet": "note_predictions = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "preds=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rfc_preds = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,rfc_preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4))\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "preds = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "start = timer()\nsklearnOutput = knnClassifier.predict(testImages[0:100,:])\nend = timer()\nsklearnTime = end - start\nprint(sklearnOutput)\nprint(\"Processing Time using sklearn: \",sklearnTime,\" s\") \n", "intent": "Run speed test on 100 images\n"}
{"snippet": "start = timer()\nsklearnOutput = knnClassifier.predict(testImages)\nend = timer()\nsklearnTime = end - start\nprint(\"Processing Time using sklearn: \",sklearnTime,\" s\") \n", "intent": "Run accuray test over entire test set\n"}
{"snippet": "roc = roc_auc_score(y_test, best_rfc.predict_proba(X_test)[:,1])\nprint \"AUC Score: \", roc\n", "intent": "**AUC: Area Under the Curve**\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n", "intent": " **Part 1:** Implement K-Fold Cross Validation, with 10 folds, on your Breast Cancer Model\n"}
{"snippet": "print(\"Test point: ({}, {})\".format(X_test.iloc[0,0], X_test.iloc[0,1]))\nprint(\"Predicted species:\", knn[0].predict(X_test)[0])\nprint(\"Actual species:\", y_test.iloc[0])\n", "intent": "We can now make predictions using this model. For example, for the first test point:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = nb.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\n", "intent": "Then, we predict the output using the test set and compute the **confusion matrix** to evaluate the performance of the algorithm.\n"}
{"snippet": "y_pred_rfdtr= rfr_best.predict(X_test)\ny_pred_rfdtr\n", "intent": "Interestingly the coefficients on the variables that determine the sleep are positive in the random forest tree regression\n"}
{"snippet": "y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)\nloss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)\nfor i in range(10):\n  _, loss_value = sess.run((train, loss)) \n  print(loss_value)\n", "intent": "**Remember :**\n> 1) Define **loss function**\n> 2) Define **optimizer**\n> 3) Run **optimizer.minimize(loss)** (does not return anything)\n"}
{"snippet": "mse = tf.losses.mean_squared_error(Y_,Y)\ntf.summary.scalar(\"P1/MSE\",mse)\n", "intent": "** Mean Squared Error **\n"}
{"snippet": "def regularizer(wd):\n    fc1_w = tf.trainable_variables(scope='fc1/Weights')[0]\n    wd_fc1 = tf.multiply(tf.nn.l2_loss(fc1_w), wd, name='fc1/weight_loss')\n    tf.summary.scalar(\"fc1_decay\",wd_fc1)\n    fc2_w = tf.trainable_variables(scope='fc2/Weights')[0]\n    wd_fc2 = tf.multiply(tf.nn.l2_loss(fc2_w), wd, name='fc2/weight_loss')\n    tf.summary.scalar(\"fc2_decay\",wd_fc2)\n    return wd_fc1+wd_fc2\n", "intent": "Regularizing the weights of fc1 and fc2\n"}
{"snippet": "mae = metrics.mean_absolute_error(y_test, predictions)\nmse = metrics.mean_squared_error(y_test, predictions)\nrmse = np.sqrt(mse)\nprint('MAE is: ' + str(mae))\nprint('MSE is: ' + str(mse))\nprint('RMSE is: ' + str(rmse))\n", "intent": "Errors are largely normally distributed around zero.\n"}
{"snippet": "mae = metrics.mean_absolute_error(y_test, predictions)\nmse = metrics.mean_squared_error(y_test, predictions)\nrmse = np.sqrt(mse)\nprint('MAE is: ' + str(mae))\nprint('MSE is: ' + str(mse))\nprint('RMSE is: ' + str(rmse))\n", "intent": "Errors are normally distributed around zero.\n"}
{"snippet": "predicted = clf.predict(X_test_tfidf)\nprint(\"\\nAccuracy Training: \" + str(np.mean(predicted == labels_test)))\n", "intent": "Checking the model accuracy:\n"}
{"snippet": "print(\"Train\",f1_score(y_train_oh,\n                       ovr_tfidf_binary_linear_svc.predict(X_train),average= \"weighted\"))\nprint(\"CV\",f1_score(y_cv_oh,\n                       ovr_tfidf_binary_linear_svc.predict(X_cv),average= \"weighted\"))\n", "intent": "Weighted average of f1...\n"}
{"snippet": "f1_svm_test = dict(zip(genres,\nf1_score(y_test_oh,ovr_tfidf_binary_linear_weighted_SVC_reduced_c.predict(X_test),average=None)))\n", "intent": "Finally, we test our model on fresh test data:\n"}
{"snippet": "scores_wind = cross_val_score(lr, X_wind, y_wind, cv=5)\nprint(scores_wind, \"\\naverage =\", np.mean(scores_wind))\n", "intent": "Then, we implement a 5-fold CV procedure.\n"}
{"snippet": "scores_solar = cross_val_score(lr, X_solar, y_solar, cv=5)\nprint(scores_solar, \"\\naverage =\", np.mean(scores_solar))\n", "intent": "Then, we implement a 5-fold CV procedure:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"ashish.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "num_test = test.shape[0]\nX_test = test.reshape(num_test, *input_shape).astype('float32') / 255\ntest_predictions_dummies = final_model.predict_proba(X_test)\ntest_predictions = np.argmax(test_predictions_dummies, axis=1)\nprint(\"Predictions: {}\".format(test_predictions.shape))\n", "intent": "Let's quickly create a submission for kaggle:\n"}
{"snippet": "flower_classifier.predict(new_flower1)\n", "intent": "0 == > setosa\n1 == > versicolor\n2 == > virginica\n"}
{"snippet": "example=[\"amazon-instant-video-browser\",\"astronaut-notre-dame-bcs\",\"jobs-contently\",\"obama-boombox-israeli-radio-station-ad\",\"Google to launch Android SDK for wearables in two weeks\",\"US jobs growth last month hit by weather:Fed President Charles Plosser\",\"Apple versus Samsung case goes to California jury - WBAY\"]\ne=vectorizer.transform(example)\nnb.predict(e)\n", "intent": "How well did it do?\n"}
{"snippet": "tree = build_tree(x_train, y_train)\npred_train = predict(tree, x_train)\nacc_train = (y_train==pred_train).sum() / y_train.shape[0]\npred_test = predict(tree, x_test)\nacc_test = (y_test==pred_test).sum() / y_test.shape[0]\nprint('Training accuracy:', acc_train)\nprint('Test accuracy:', acc_test)\n", "intent": "Using the functions defined above build a tree and report the training and test accuracy.\n__(2 marks)__\n"}
{"snippet": "tree = build_tree(x_train, y_train)\ntrain = predict(tree, x_train)\ntest = predict(tree, x_test)\ntrain_accuracy = (y_train==train).sum()/y_train.shape[0]\ntest_accuracy = (y_test==test).sum()/y_test.shape[0]\nprint(train_accuracy)\nprint(test_accuracy)\n", "intent": "Using the functions defined above build a tree and report the training and test accuracy.\n__(2 marks)__\n"}
{"snippet": "def least_squares_error(x, y, w, c):\n    squared_error = np.sum((y - (w * x) - c) **2)\n    return squared_error\nprint('Squared error for w = 1.5, c = 0.5 is ', \n      least_squares_error(x_train, y_train, w=1.5, c=0.5))\n", "intent": "Write a function that calculates the least squared error on the training data for a particular value of the parameters $w$ and $c$.\n"}
{"snippet": "svm_predictions = svm_model.predict(test_setf)\n", "intent": " <h3>Prediction</h3>\n"}
{"snippet": "for index, row in quiz_matrix_pred.iterrows():\n    user_id = row[\"UserId\"]\n    for quiz_name,val in row.iteritems():\n        if quiz_name != \"UserId\":\n            quiz_matrix_pred.loc[quiz_matrix_pred[\"UserId\"] == user_id, quiz_name] =\\\n                round(algo.predict(user_id, quiz_name).est)\n", "intent": "Predicting the scores below\n"}
{"snippet": "pred = lr.predict(testing_set)\naccuracy = lr.test_accuracy(pred, testing_labels)\nprint(\"The accuracy is: %g\" % accuracy)\n", "intent": "We predict the labels of the testing set, and we print the accuracy of the classifier on this dataset.\n"}
{"snippet": "softmax = SoftmaxClassifier()\ntraining_labels = training_labels.astype(int)\ntesting_labels = testing_labels.astype(int)\nsoftmax.train(training_set, training_labels, alpha=1e-1, tol=-1.0, max_iter=10, batch_size=200, verbose=True)\ny_train_pred = softmax.predict(training_set)\ny_val_pred = softmax.predict(testing_set)\ntraining_accuracy = np.mean(training_labels == y_train_pred)\nvalidation_accuracy = np.mean(testing_labels == y_val_pred)\nprint(\"Training accuracy in M..0.....NIST is: %g\" % training_accuracy)\nprint(\"Testing accuracy in MNIST is: %g\" % validation_accuracy)\n", "intent": "Finally, we are going to train our softmax classifier on it, predict the labels on both the training set and testing set, and print the results\n"}
{"snippet": "softmax = SoftmaxClassifier()\ntraining_labels = training_labels.astype(int)\ntesting_labels = testing_labels.astype(int)\nsoftmax.train(training_set, training_labels, alpha=1e-1, tol=-1.0, max_iter=10, batch_size=200, verbose=True)\ny_train_pred = softmax.predict(training_set)\ny_val_pred = softmax.predict(testing_set)\ntraining_accuracy = np.mean(training_labels == y_train_pred)\nvalidation_accuracy = np.mean(testing_labels == y_val_pred)\nprint(\"Training accuracy in MNIST is: %g\" % training_accuracy)\nprint(\"Testing accuracy in MNIST is: %g\" % validation_accuracy)\n", "intent": "Finally, we are going to train our softmax classifier on it, predict the labels on both the training set and testing set, and print the results\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram_curr = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_curr - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print( 'Total Accuracy : ',np.round( metrics.accuracy_score( y_test, y_pred_df.predicted_final ), 2 ) )\nprint( 'Precision : ',np.round( metrics.precision_score( y_test, y_pred_df.predicted_final ), 2 ) )\nprint( 'Recall : ',np.round( metrics.recall_score( y_test, y_pred_df.predicted_final ), 2 ) )\ncm1 = metrics.confusion_matrix( y_pred_df.actual, y_pred_df.predicted_final, [1,0] )\nsensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\nprint('Sensitivity : ', round( sensitivity, 2) )\nspecificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\nprint('Specificity : ', round( specificity, 2 ) )\n", "intent": "Predicting *Defaults* as *No Defaults* have been minimized.\n"}
{"snippet": "model_data = df_2015_2016.append(df_2016_2017)\nmodel_data = model_data[['prev_fantasy_pts', 'fantasy_pts']]\npredict_data = df_2017_2018[['prev_fantasy_pts', 'fantasy_pts']]\nX_test = predict_data['prev_fantasy_pts'].values.reshape(-1,1)\ny_test = predict_data['fantasy_pts']\nall_pred = predict_data.prev_fantasy_pts.values\nrmse = np.sqrt(mean_squared_error(y_test, all_pred))\n", "intent": "* How effective is it to say last games' performance = this games' performance?\n"}
{"snippet": "all_pred = predict_data.fantasy_pts.values\nrmse = np.sqrt(mean_squared_error(y_test, all_pred))\n", "intent": "* Test effectiveness of optimization\n* Answers the question: \"If I know what's going to happen, can I win?\"\n"}
{"snippet": "print('Classification accuracy {:.1f}%'.format(100 * metrics.accuracy_score(labels_test, plabels_test)))\nprint('Classification F1-score {:.1f}%'.format(100 * metrics.f1_score(labels_test, plabels_test, average='weighted')))\n", "intent": "Get the overall accuracy (OA) and the weighted $F_1$ score\n"}
{"snippet": "class_labels = np.unique(np.hstack([labels_test, labels_train]))\nscores_test = model.predict_proba(features_test)\nlabels_binarized = preprocessing.label_binarize(labels_test, classes=class_labels)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor idx,lbl in enumerate(class_labels):\n    fpr[idx], tpr[idx], _ = metrics.roc_curve(labels_binarized[:, idx], scores_test[:, idx])\n    roc_auc[idx] = metrics.auc(fpr[idx], tpr[idx])    \n", "intent": "Calculate precision and recall rates, draw ROC curves and calculate AUC.\n"}
{"snippet": "df_data[\"pickup_cluster\"] = k_means.predict(df_data.loc[:,[\"pickup_latitude\",\"pickup_longitude\"]])\ndf_data[\"dropoff_cluster\"] = k_means.predict(df_data.loc[:,[\"dropoff_latitude\",\"dropoff_longitude\"]])\n", "intent": "This more successfully breaks up clusters of varying lengths.\n"}
{"snippet": "cohen_kappa_score(a1, a2)\n", "intent": "We can now invoke the library as follows to calculate the agreement between the two:\n"}
{"snippet": "cohen_kappa_score(a1, a1)\n", "intent": "This value represents high agreement. We can reach maximal agreement if the two assessments are identical:\n"}
{"snippet": "a3=[1,0,1,0,1,1,0,1]\ncohen_kappa_score(a1, a3)\n", "intent": "Now, let's see what happens for a third assessment that differs on three positions with the first one (the three last positions):\n"}
{"snippet": "def identify_one_book(test_text):\n    X_test_counts = count_vect.transform(test_text)\n    pred = clf.predict(X_test_counts)\n    return train_books[pred[0]]\n", "intent": "We have two functions here, one that takes a single snippet, and the other that takes a list of snippets\n"}
{"snippet": "roc_auc_score(y_test_int, prob_y_0) * 100\n", "intent": "Around 74% chance of the model built to \"rank\" class 1 as 1\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy_vgg16 = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_vgg16)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "print confusion_matrix(y_test, best_rfc.predict(X_test))\n", "intent": "Recall : \nOf the model identified as malignant, 96% of them actually are malignant.\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n", "intent": "Now let's use K-Fold Cross Validation method to overcome the overfitting of the 20%\nholdout\n"}
{"snippet": "print \"default hotspot\"\nprint(classification_report(default_hotspot, default_rfr.predict(df_x).round()))\nprint \"pp hotspot\"\nprint(classification_report(pp_hotspot, pp_rfr.predict(df_x).round()))\nprint \"balance hotspot\"\nprint(classification_report(balance_hotspot, balance_rfr.predict(df_x).round()))\n", "intent": "Results for the random forest classifier-- a binary classifier that simply determines whether an error is a hotspot or not. \n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    score_real = torch.from_numpy(score_real)\n    score_fake = torch.from_numpy(score_fake)\n    d_loss = ls_discriminator_loss(score_real, score_fake)\n    g_loss = ls_generator_loss(score_fake)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for idx, layer in enumerate(style_layers):\n        g_l = gram_matrix(feats[layer])\n        a_l = style_targets[idx]\n        loss += content_loss(style_weights[idx], g_l, a_l)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "investor2006 = investor['2006']\nneutral2006 = investor2006['Neutral']\nmsft2006two = msft.loc[neutral2006.index].interpolate()\nopen2006two = msft2006two.Open\nneutarray2 = np.array(neutral2006.tolist())\nneutarray2 = neutarray2.reshape(neutarray.size,1)\npred2006 = regressionmodel.predict(neutarray2)\n", "intent": "Model have been built with data from year 2005, let's try and make prediction and evaluate the results\n"}
{"snippet": "train_data_predictions = simple_model.predict(train_matrix_word_subset)\naccurate_predictions = np.sum(train_data['sentiment'] == train_data_predictions)\nprint(\"Accuracy for simple_model on train_data\", accurate_predictions/train_data_count)\n", "intent": "Now, compute the classification accuracy of the **simple_model** on the **train_data**:\n"}
{"snippet": "test_data_predictions = simple_model.predict(test_matrix_word_subset)\naccurate_predictions = np.sum(test_data['sentiment'] == test_data_predictions)\nprint(\"Accuracy for simple_model on test_data\", accurate_predictions/test_data_count)\n", "intent": "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:\n"}
{"snippet": "scores = model.evaluate(X_test, Y_test)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n", "intent": "We can then evaluate the accuracy of our model on the test set data using the `evaluate` function:\n"}
{"snippet": "y_pred = model.predict(X_ts)              \n", "intent": "- In this section, stock price is predicted with the help of X_test matrix and the model \n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n", "intent": "We obtained an array with the k scores. We can calculate the mean and the standard\nerror to obtain a final figure:\n"}
{"snippet": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel = LinearDiscriminantAnalysis()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n", "intent": "Linear discriminant analysis\n"}
{"snippet": "y_pred=clf.predict(X_test)\n", "intent": "Now that the classifier is fit, lets predict for 300k records in test and then predict for the training data as well\n"}
{"snippet": "print(sklearn.metrics.confusion_matrix(y_test,predictions))\nprint(sklearn.metrics.classification_report(y_test,predictions))\n", "intent": "Confusion Matrix and Report\n"}
{"snippet": "y_pred = best_regressor.predict(X_test)\nrmse=mean_squared_error(y_test,y_pred)\nr2=r2_score(y_test,y_pred)\nexplained_var_score=explained_variance_score(y_test,y_pred)\nprint(\"Root mean squared error:{} \\nR2-score:{} \\nExplained variance score:{}\".format(rmse,r2,explained_var_score))\n", "intent": "Q11. Generate predictions from the validation set, and output the above-mentioned scores.\n"}
{"snippet": "weights = tf.ones([batch_size, sequence_length]) \nseq_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nloss  = tf.reduce_mean(seq_loss)\noptm  = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\nprint (\"FUNCTIONS DEFINED.\")\n", "intent": "DEFINE TF FUNCTIONS\n"}
{"snippet": "fitted_point_spread = reg.predict(X)\nprint(fitted_point_spread)\n", "intent": "The fitted point spread can be viewed as follows:\n"}
{"snippet": "opt_RF_pred = opt_rf_model.predict_proba(df_test)[:, 1]\n", "intent": "The model was then fitted to the training and target data, and it can now be used to make predictions for the testing dataset df_test.\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n    prediction = model.predict([[source], s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "def run_xgboost(X_train, y_train, X_val):\n    param = {'silent': 1, 'eta': 0.1, 'max_depth': 3, 'subsample': 0.8, 'colsample_bytree': 0.5,\n             'objective': 'multi:softprob', 'eval_metric': 'mlogloss', 'num_class': 3, 'seed': 1001}\n    plst = param.items()\n    num_round = 1000\n    dtrain = xgb.DMatrix(X_train, label = y_train)\n    model = xgb.train(plst, dtrain, num_round)\n    dtest = xgb.DMatrix(X_val)\n    y_pred = model.predict(dtest, ntree_limit = model.best_ntree_limit)\n    return model, y_pred\n", "intent": "Model with Combined Features\n------------\nNow let's combine all the features we have created so far and train a xgboost model on them.\n"}
{"snippet": "tree_scores = cross_val_score(tree_clf, X, targets, cv=10)\n", "intent": "A perfect fit. Not surprising given the SVC; lets check for overfit with cross validation:\n"}
{"snippet": "knn_final_predictions = knn_5.predict(test_X)\n", "intent": "77.5% accuracy! Not bad! For good measure, let's also try our KNN classifer (which scored 84% on the training) on the test set:\n"}
{"snippet": "model.predict_proba(X_test)\n", "intent": "It is also possible to retrieve the probability associated with each prediction:\n"}
{"snippet": "print(classification_report(expected, predicted))\n", "intent": "La matriz anterior nos indica, que nuestro modelo tiene:\n- 112 True Negatives\n- 22 Falsos positivos\n- 21 Falsos negativos\n- 68 True positivos\n"}
{"snippet": "cv = KFold(X.shape[0], n_folds=5, shuffle=False, random_state=33)\nscores = cross_val_score(model, X, y, cv=cv)\nprint(\"Scores in every iteration\", scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Alternativa para separar el dataset de pruebas y el de entrenamiento\n"}
{"snippet": "print(classification_report(expected, predicted))\n", "intent": "[[115 19]\n [23  66]] in example, [[128 6] [80 9]] now\n"}
{"snippet": "print(classification_report(expected, predicted))\n", "intent": "[[115 19]\n [23  66]] in example, [[128 6] [80 9]] rbf, [[112 22] [21 68]] rbf norm\n"}
{"snippet": "print(classification_report(expected, predicted))\n", "intent": "[[115 19]\n [23  66]] in example, [[128 6] [80 9]] rbf, [[112 22] [21 68]] rbf norm, [[115 19] [22 67]] gaussiannb\n"}
{"snippet": "print(classification_report(expected, predicted))\n", "intent": "[[115 19]\n [23  66]] in example, [[128 6] [80 9]] rbf, [[112 22] [21 68]] rbf norm,\n [[103 31] [45 44]] gnb(pca)\n"}
{"snippet": "trn_features = model.predict(trn_data, batch_size = batch_size)\nval_features = model.predict(val_data, batch_size = batch_size)\n", "intent": "...and their 1,000 imagenet probabilties from VGG16--these will be the *features* for our linear model:\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size = bath_size)\nprobs = lm.predict_proba(val_features, batch_size = batch_size)\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "df1.loc[:,'bathrooms'] = df1.apply(lambda x: roundit(impute.predict(x[['bedrooms', 'sqft_living', 'grade']].values.reshape(1, -1))) if pd.isnull(x.bathrooms) else x.bathrooms, axis = 1)\n", "intent": "Now we predict the values and impute in the dataframe.\n"}
{"snippet": "y_predict = classifier.predict(X_test)\n", "intent": "After being fitted, the model can then be used to predict the class of samples:<a id='predict'></a>\n"}
{"snippet": "scores = cross_val_score(classifier, X, y, cv=cross_v)\naverage=np.mean(scores)\nprint(scores)\nprint(average)\n", "intent": "Croos Validation <a id='cross'></a>\n"}
{"snippet": "y_predict = classifier.predict(X_test)\n", "intent": "After being fitted, the model can then be used to predict the class of samples: <a id='predict'></a>\n"}
{"snippet": "scores = cross_val_score(classifier, X, y, cv=5)\naverage=np.mean(scores)\nprint(scores)\nprint(average)\n", "intent": "Croos Validation <a id='cross'></a>\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_y, \n                            predict_y, \n                            target_names=['Not Promoted', 'Promoted']))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$ \n$$Recall = \\frac{TP}{TP + FN}$$ \n$$F1 = \\frac{2TP}{2TP + FP + FN}$$ \n"}
{"snippet": "train_target_vecs = proc.transform(target_docs)\nhidden_states = embedding_model.predict(train_target_vecs[:, 1:])\n", "intent": "Embeddings for the target documents, which are natural language summaries (like docstrings):\n"}
{"snippet": "def vectorize_string(txt):\n  vec = proc.transform([txt])[:,1:]\n  emb = np.mean(embedding_model.predict(vec), axis=1)\n  return emb\n", "intent": "**3. Create Helper Function To Vectorize Queries.** we are going to use the language model to create \"sentence embeddings\" for our queries.  \n"}
{"snippet": "accuracy_score(y, y_hat)\n", "intent": "We have a prediction!  Let's see how it goes.\n"}
{"snippet": "print('Features Selected with ' + str(sel_feat))\nprint('Version ' + str(VERSION) + '; ' + str(sel_version),'\\n')\npred_train_all = logReg_m.predict(sm.add_constant(X_train_s))\nprint('Predicted probabilities of price going UP for whole Feature Set (Train) are: ')\ndisplay(pred_train_all[0:3])\nprint(\"\")\nprint('Response Vector (Train): ')\ndisplay(y_train_s[0:3])\n", "intent": "Multiple Logistic Regression 1 (pre-selected features with RandomForest in Chapter 2) \n"}
{"snippet": "print('Features Selected with ' + str(sel_feat))\nprint('Version ' + str(VERSION) + '; ' + str(sel_version),'\\n')\npred_train_all = logReg_mm.predict(sm.add_constant(X_train_s[sign_features]))\nprint('Predicted probabilities of price going UP for whole Feature Set (Train) are: ')\ndisplay(pred_train_all[0:3])\nprint(\"\")\nprint('Response Vector (Train): ')\ndisplay(y_train_s[0:3])\n", "intent": "Multiple Logistic Regression 1 (pre-selected features with RandomForest in Chapter 2) \n"}
{"snippet": "print('Recall for Forest fitted and response Vector predicted with one random_state in 3.2.1.:')\nprint(TP_o / (TP_o + FN_o))\nprint(metrics.recall_score(response_test, prediction_o),'\\n')\nprint('Recall for Forest fitted and response Vector predicted with multiple (i) random_states in 3.2.2.:')\nprint(TP_m / (TP_m + FN_m))\nprint(metrics.recall_score(response_test, prediction_m.resulting_prediction))\n", "intent": "Fraction of correctly predicted 'positives' ('price up')\n"}
{"snippet": "print('Precision for Forest fitted and response Vector predicted with one random_state in 3.2.1.:')\nprint(TP_o / (TP_o + FP_o))\nprint(metrics.precision_score(response_test, prediction_o),'\\n')\nprint('Precision for Forest fitted and response Vector predicted with multiple (i) random_states in 3.2.2.:')\nprint(TP_m / (TP_m + FP_m))\nprint(metrics.precision_score(response_test, prediction_m.resulting_prediction))\n", "intent": "How precise a positive (up) is predicted.\n"}
{"snippet": "pred_raw = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = model.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions))\n", "intent": "Now let's predict using the trained model.\n"}
{"snippet": "mod1 = trainCRFAndEvaluate(\n    X_train = X_train, \n    y_train = y_train_int,\n    X_test = X_test,\n    y_test = y_test_int,\n    labels = ['true', 'false'],\n    c1 = 1.0458554701644089,\n    c2 = 0.006237772066168241,\n    hyperparam_optim = False)\n", "intent": "Using training data and the parameters obtained in the previous step\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg, X, y, cv=5)\nscores   \n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "pred_panels = model.predict(panels, batch_size=6)\n", "intent": "Now we're going to make predictions on all the panels.\nBatch size of 6 is the largest I could go.\n"}
{"snippet": "def waldo_predict(img):\n    rimg = img_resize(img)\n    panels = split_panels(rimg)\n    pred_panels = model.predict(panels, batch_size=6)\n    pred_panels = np.stack([reshape_pred(pred) for pred in pred_panels])\n    return rimg, combine_panels(rimg, pred_panels)\n", "intent": "Now we can wrap the end to end process up in the following function.\n"}
{"snippet": "for i, full_img in enumerate(imgs):\n    full_img_r, full_pred = waldo_predict(full_img)\n    mask = prediction_mask(full_img_r, full_pred)\n    mask.save(FULL_PRED_PATH+'new_' +str(i)+'.png')\n", "intent": "And make predictions on all original images.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(features,axis=0))) for features in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = pd.Series(clf.predict_proba(X)[:, 1])\n", "intent": "First, let's score everyone using our model. Now, let's say that we will give loans to anyone with a greater than a $80\\%$ chance of paying it back\n"}
{"snippet": "from sklearn.metrics import classification_report, accuracy_score\nfor individual_predictions in [rf_predictions, svm_predictions, lr_predictions]:\n    print \"Accuracy:\", round(accuracy_score(yTest.astype(int), individual_predictions.astype(int)),2)\nprint classification_report(yTest.astype(int), ensembled_predictions.astype(int))\nprint \"Ensemble Accuracy:\", round(accuracy_score(yTest.astype(int), ensembled_predictions.astype(int)),2)\n", "intent": "And we could assess the performance of the majority voted predictions like so:\n"}
{"snippet": "Xception_predictions = [np.argmax(mymodel.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==\n                           np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions2 = network2.predict_classes(X_test)\nprint(classification_report(y_test, predictions2))\n", "intent": "It looks like SGD and gradient descent is not useful here, this only predict class 0 and 1\n"}
{"snippet": "RMSE = mean_squared_error(y,xgb3_L.predict(X))\nprint('RMSE:',MRSE)\n", "intent": "Root Mean Squared Error for our XGB Regressor Model\n"}
{"snippet": "dog_breed_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print scores\n    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores))\n", "intent": "This function will serve to perform and evaluate a cross validation:\n"}
{"snippet": "if not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n", "intent": "**saving the model**\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\n", "intent": "<center> \n<img src=\"img/precision.png\" style=\"max-width: 680px; display: inline\" />\n</center>\n"}
{"snippet": "from sklearn.metrics import recall_score\nprint(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\n", "intent": "<center> \n<img src=\"img/recall.jpeg\" style=\"max-width: 680px; display: inline\" />\n</center>\n"}
{"snippet": "crossval_revised = cross_val_score(revised_rf, X_train_selected, y_train, cv=10)\nprint('Cross-validation scores:', crossval_revised, '\\n')\nprint('Mean cross validation score:', np.mean(crossval_revised))\nprint('Cross validation standard deviation:', np.std(crossval_revised))\n", "intent": "Indeed, our test score has successfully exceeded our benchmark. We can move on to evaluating the model.\n"}
{"snippet": "plot_confusion_matrix(confusion_matrix(y_test, revised_rf.predict(X_test_selected)))\nprint(classification_report(y_test, revised_rf.predict(X_test_selected)))\n", "intent": "Our cross-validation scores are already better than our previous model. Let's check classification metrics:\n"}
{"snippet": "print(\"Accuracy Score:\", metrics.accuracy_score(y_test, predicted))\n", "intent": "We now evaluate our logistic regression using some common metrics for assessing model quality.\n"}
{"snippet": "print(\"Confusion Matrix:\\n\",metrics.confusion_matrix(y_test, predicted))\nprint(\"\\nClassification Report:\\n\",metrics.classification_report(y_test,predicted))\n", "intent": "We use a confusion matrix, classification report, and ROC curve to get a better view of the performance of our model.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nprint('Score: ', dt.score(X_train, y_train))\nprint('Cross validation score, 10-fold cv: \\n', cross_val_score(dt, X_train, y_train, cv=10))\nprint('Mean cross validation score: ', cross_val_score(dt,X_train,y_train,cv=10).mean())\n", "intent": "While we will, of course, make predictions on our test set, we treat that as a holdout set and first do some cross-validation on our training set.\n"}
{"snippet": "print(classification_report(y_test, ANN.predict(scaled_X_test), target_names=['Wine 1','Wine 2', 'Wine 3']))\n", "intent": "Our accuracy remains high even on the holdout set.\n"}
{"snippet": "print np.sum((bos.PRICE - lm.predict(X)) ** 2) \n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint \"The mean squared error is\", mse\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "for feature in features:\n    print('--------{0}----------'.format(feature))\n    print(classification_error(train, feature, 'target'))\n", "intent": "Now if we run for classification error again we get better results\n"}
{"snippet": "train_output = train['target'].as_matrix()\ntrain_predictions = predict(simple_model, train)\n", "intent": "First we extract the true results from the training set and use the model to make equivalent predictions.\n"}
{"snippet": "test_output = test_raw['target'].as_matrix()\ntest_predictions = predict(simple_model, test_raw)\ntest_num_error = len(test_output) - np.count_nonzero(test_output + test_predictions)\nerror_simple_test = test_num_error/float(len(test_output))* 100\nprint('Classification error: {0:.2f}%'.format(error_simple_test))\n", "intent": "Now calculate the classification error of the model on the test set.\n"}
{"snippet": "train_predictions_large = predict(large_model, train)\ntrain_num_error_large = len(train_output) - np.count_nonzero(train_output + train_predictions_large)\nerror_large_train = train_num_error_large/float(len(train_output))* 100\nprint('Classification error: {0:.2f}%'.format(error_large_train))\n", "intent": "Classification error on training set\n"}
{"snippet": "test_predictions_large = predict(large_model, test_raw)\ntest_num_error_large = len(test_output) - np.count_nonzero(test_output + test_predictions_large)\nerror_large_test = test_num_error_large/float(len(test_output))* 100\nprint('Classification error: {0:.2f}%'.format(error_large_test))\n", "intent": "Classification error on the test set\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "** Calculating the Mean Absolute Error, Mean Squared Error, and the Root Mean Squared Error. **\n"}
{"snippet": "y_pred = crf.predict(X_test)\nprint metrics.flat_f1_score(y_test, y_pred,\n                      average='weighted', labels=labels)\n", "intent": "Process the testing set and print out the f1-score.\n"}
{"snippet": "sorted_labels = sorted(\n    labels,\n    key=lambda name: (name[1:], name[0])\n)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "Print out prediction costs per label\n"}
{"snippet": "crf = rs.best_estimator_\ny_pred = crf.predict(X_test)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\nfor key, value in sorted(crf.state_features_.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n    print \"%s: %s\" % (key, value)\n", "intent": "Print the per-label scores and the features ordered by importance for the best model found by the grid search.\n"}
{"snippet": "sorted_labels = sorted(\n    labels,\n    key=lambda name: (name[1:], name[0])\n)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "Print out prediction scores per label\n"}
{"snippet": "mean_squared_error(cv_model.predict(prediction_xs), prediction_ys)\n", "intent": "and the prediction error is\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('test_accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "prediction = predict(X_test, parameters)\n", "intent": "...to get our performance on the untouched test set\n"}
{"snippet": "def compute_score(clf,X,y,scoring ='accuracy'):\n    xval = cross_val_score(clf,X,y,cv=5,scoring =scoring)\n    return np.mean(xval)\n", "intent": "<font size =3>\n    Creating a function for testing the score on cross validation\n</font>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = torch.empty(1, dtype=torch.float)\n    for i in range(len(style_layers)):\n        loss+=style_weights[i]*torch.sum((style_targets[i] - gram_matrix(feats[style_layers[i]]))**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "rss=np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "mse=np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(\"The Mean Squared Error is: {}\".format(mse))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_Xception = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Xception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def clf_report(model, dataset):\n    print \"Train accuracy: {}\".format(np.mean(dataset.tr_y==model.predict(dataset.tr_X)))\n    print \"Valid accuracy: {}\".format(np.mean(dataset.val_y==model.predict(dataset.val_X)))\n", "intent": "We use the following function to report training and testing accuracy:\n"}
{"snippet": "for i in range(0,32):\n    p = model.predict(np.array([x2[i,...]], dtype='float32'))\n    print(np.argmax(p), np.argmax(y2[i]))\n", "intent": "Here we inspect some internal layers in order to find potential strange behaviour like constant outputs or noise.\n"}
{"snippet": "new_data = np.array([100,0,50,0,0,0]).reshape(1,-1)\nnew_pred=model.predict(new_data)\nprint(\"The CLV for the new customer is : $\",new_pred[0])\n", "intent": "Let us say we have a new custoer who in his first 3 months have spent 100,0,50 on your website. Let us use the model to predict his CLV.\n"}
{"snippet": "test_name = \"test\"\ntest_predictions = learn.predict(is_test=True)\n", "intent": "We set the is_test flag to True to tell the model to predict on the test set.<br>\n"}
{"snippet": "def return_score():\n", "intent": "To find the average percent accuracy I will take a couple of samples and average the score\n"}
{"snippet": "from sklearn import metrics\npredicted = logit_model.predict(X_train)\nexpected = y_train\nprint(\"Classification report for classifier: %s\\n %s\\n\"\n     % (model, metrics.classification_report(expected, predicted)))\n", "intent": "This should have good accuracy since it was trained to this data\n"}
{"snippet": "pred = tfnet.return_predict(a)\n", "intent": "Do the prediction. Get the results\n"}
{"snippet": "generated_images = g.predict(\n    [noise, sampled_labels.reshape(-1, 1)], verbose=False, batch_size=64)\n", "intent": "Everything is ready to generate the images!\nEvaluate the generator **G** given the _noise_ and the _sampled labels_.\n"}
{"snippet": "predictions = predict(parameters, X)\nprint(np.sum(np.equal(np.squeeze(Y),np.squeeze(predictions)))/predictions.shape[1])\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "np.mean((Y_train.highest_bid - regressor_boost.predict(X_train))**2)\n", "intent": "Mean squared error on training set\n"}
{"snippet": "np.mean((Y_test.highest_bid - regressor_boost.predict(X_test))**2)\n", "intent": "Mean squared error on test set\n"}
{"snippet": "X_test.loc[:,'pred_price'] = regressor_boost.predict(X_test)\n", "intent": "Add predictions to data set\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Available metrics are: \\n{}\".format(model.metrics_names))\nprint(score)\n", "intent": "Now we can evaluate our model.\n"}
{"snippet": "print classification_report(y_test, y_pred)\nprint \"Time taken: \", end - start, \"seconds.\"\n", "intent": "Here is the classification report for logisitic regression model.\n"}
{"snippet": "print classification_report(y_test, y_pred)\nprint \"Time taken: \", end - start, \"seconds.\"\n", "intent": "Here is the classification report for scaled logisitic regression model.\n"}
{"snippet": "print classification_report(y_test, y_pred)\nprint \"Time taken: \", end - start, \"seconds.\"\n", "intent": "Here is the classification report for scaled logisitic regression in which the penalty parameter is tuned to optimise the accuracy.\n"}
{"snippet": "predictions = random_f.predict(X_test)\n", "intent": "Let's predict off the test values and evaluate our model.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now that we have fit our model, we can evaluate its performance by predicting off the test values\n"}
{"snippet": "predictions = lm.predict(X_test)\npredictions\n", "intent": "Now we can use the X_test dataset to make predictions and compare the predictions with the y_test\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Now we predict the values of the testing data.\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "Classification report for the model\n"}
{"snippet": "all_predictions = pipeline.predict(msg_test)\nprint(all_predictions)\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset. Let's begin by getting all the predictions:\n"}
{"snippet": "print(classification_report(label_test, all_predictions))\n", "intent": "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Now we can make predictions on our test data\n"}
{"snippet": "predictions = pipe.predict(X_test)\n", "intent": "Now use the pipeline to predict from the X_test and create a classification report and confusion matrix\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictionsTrain = kNN.predict(train_set)\nprint(pd.crosstab(predictionsTrain, class_set['Species']))\naccuracyTrain = kNN.score(train_set, class_set['Species'])\n", "intent": "Now that we fit the model let's predict the **Species** for the `train_set`.\n"}
{"snippet": "print('Best GS Score %.3f' % gs.best_score_)\nprint('Best GS Params %s' % gs.best_params_)\nypred = gs.predict(X_train)\ntrain_acc = (y_train == ypred).sum()/len(y_train)\nprint('\\nTraining accuracy: %.3f' % (train_acc))\nypred = gs.predict(X_test)\ntest_acc = (y_test == ypred).sum()/len(y_test)\nprint('\\nTesting accuracy: %.3f' % (test_acc))\n", "intent": "Now that we have our grid search fit to the parameters we can explore the results using the code below\n"}
{"snippet": "y_pred = decision_tree_classifier.predict(X_test)\nprint(\"Predicted class first element: {}\".format(y_pred[0]))\nprint(\"Real class first element: {}\".format(y_test[0]))\ncorrect_predictions = np.sum(y_pred == y_test)\nprint(\"Count number of predictions equal to real class: {:d}\".format(correct_predictions))\ntotal_predictions = y_pred.shape[0]\nprint(\"Total number of predictions: {:d}\".format(total_predictions))\naccuracy = correct_predictions / total_predictions\nprint(\"Accuracy {}/{}={:2f}\".format(correct_predictions, total_predictions, accuracy))\n", "intent": "100 % correct on training set! Is that good?\n"}
{"snippet": "predictions_ResNet50 = [np.argmax(model_ResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy_ResNet50 = 100*np.sum(np.array(predictions_ResNet50)==np.argmax(test_targets, axis=1))/len(predictions_ResNet50)\nprint('ResNet50 test accuracy: %.4f%%' % test_accuracy_ResNet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\nInceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\nResnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nVGG19_test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('VGG19 Test accuracy: %.4f%%' % VGG19_test_accuracy)\nInceptionV3_test_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('InceptionV3 Test accuracy: %.4f%%' % InceptionV3_test_accuracy)\nResnet50_test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Resnet50 Test accuracy: %.4f%%' % Resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy_inception = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_inception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = model_5.predict(validation_data[features].to_numpy())\nactuals = validation_data[target].to_numpy()\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n                            y_pred=predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n                            y_pred=predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n                            y_pred=predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n                            y_pred=predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "samplePoint = parsedTrainData.take(1)[0]\nprint samplePoint\nsamplePrediction = firstModel.predict(samplePoint.features)\nprint samplePrediction\n", "intent": "Now use the [LinearRegressionModel.predict()](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfinal_model = grid_search_e.best_estimator_   \ny_te_estimation = final_model.predict(X_te)\nfinal_mse = mean_squared_error(y_te, y_te_estimation)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance\n"}
{"snippet": "resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions) == \n                           np.argmax(test_targets, axis=1))/ len(resnet50_predictions)\nprint('\\nTest Accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('Mean Starting Salary', df['Actual'].mean())\n", "intent": "Here we can see the predicted starting salaries against actual salaries of the dataframe.\n"}
{"snippet": "haz_real_clf = clf.predict(haz_real_cut)\nnohaz_real_clf = clf.predict(nohaz_real_cut)\n", "intent": "** Classify asteroid datasets by cluster IDs **\n"}
{"snippet": "print(\"Accuracy is: {:.3}%\".format(accuracy_score(test_set_1[:, 0], w_pred_lab_1, normalize=True) * 100))\n", "intent": "        Accuracy Score\n"}
{"snippet": "target_names = ['Class 1', 'Class 2']\nprint(classification_report(test_set_1[:, 0], w_pred_lab_1, target_names=target_names))\n", "intent": "        Other metrics\n"}
{"snippet": "print(\"Accuracy is: {:.3}%\".format(accuracy_score(test_set_2[:, 0], w_pred_lab_2, normalize=True) * 100))\n", "intent": "        Accuracy Score\n"}
{"snippet": "print(classification_report(test_set_2[:, 0], w_pred_lab_2, target_names=target_names))\n", "intent": "        Other Metrics\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncmat = confusion_matrix(y_true=test_data['sentiment'],\n                        y_pred=model.predict(test_matrix),\n                        labels=model.classes_)    \nprint ' target_label | predicted_label | count '\nprint '--------------+-----------------+-------'\nfor i, target_label in enumerate(model.classes_):\n    for j, predicted_label in enumerate(model.classes_):\n        print '{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j])\n", "intent": "Using your tool, print out the confusion matrix for a classifier. For instance, scikit-learn provides the method confusion_matrix for this purpose:\n"}
{"snippet": "probabilities = model.predict_proba(baby_reviews_matrix)\nprobabilities\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "class_predict = log_model2.predict(X_test)\nprint (metrics.accuracy_score(Y_test,class_predict))\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "y_pred_class = knn.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred_class)\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "confusion_matrix(y_test, mnb.predict(X_test), target_names = ['b','e','m','t']).grid(False)\n", "intent": "An accuracy of 92.8% is pretty good. Let's see the confusion matrix to observe the model's behavior.\n"}
{"snippet": "confusion_matrix(y_test, sgd_search.best_estimator_.predict(X_test), target_names = ['b','e','m','t']).grid(False)\n", "intent": "The accuracy rose by ~2%. Let's evaluate this model's classifications.\n"}
{"snippet": "yvals = mlinreg.predict(cases)\n", "intent": "Next, we pass the test cases to the predict() method. This will give us the values of $y$ for different values of $(x_1,x_2)$.\n"}
{"snippet": "mse = metrics.mean_squared_error(y_hat,y)\nprint(\"Score (MSE): {}\".format(mse))\n", "intent": "$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2 $\n"}
{"snippet": "yhat = lr.predict(X = 5) \nprint(\"Predicted y: \", yhat)\n", "intent": "<b>Step 3:</b> Predict\n"}
{"snippet": "yhat = lr.predict(X)\nrsq = get_rsquared(y, yhat)\nprint(\"R^2: \", rsq)\n", "intent": "Compute the $R^2$ of the function\n"}
{"snippet": "pred = bst.predict(ddev, ntree_limit=bst.best_iteration)\ndev_data_w_pred = dev_data.copy()\ndev_data_w_pred['dev_label'] = dev_labels\ndev_data_w_pred['pred_label'] = (pred > 0.5).astype(int)\ndev_data_w_pred.head(5)\n", "intent": "**6.6 Understanding misclassified requests**\n"}
{"snippet": "pred = bst.predict(ddev, ntree_limit=bst.best_iteration)\ndev_data_w_pred = dev_data.copy()\ndev_data_w_pred['dev_label'] = dev_labels\ndev_data_w_pred['pred_label'] = (pred > 0.5).astype(int)\ndev_data_w_pred.head(5)\n", "intent": "**5.6 Understanding misclassified requests**\n"}
{"snippet": "new_post_vec = vectorizer.transform([new_post])\nnew_post_label = km.predict(new_post_vec)[0]\n", "intent": "As we know, we will first have to vectorize this post before we predict its label as follows:\n"}
{"snippet": "test_eval = cnn_model.evaluate(df_test_X, df_test_Y_one_hot, verbose=0)\n", "intent": "Lets evaluate our model over test data to see how it performs.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(model.predict(test_X), test_y))\n", "intent": "Scoring the accuracy with Root Mean Squared Error\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(test_y, model.predict(test_X))\nrmse = np.sqrt(mse)\nrmse\n", "intent": "And if we revisit the Root Mean Squared Error as our measure of model accuracy, we see that this is perhaps a respectable level of error.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint('Precision:', precision_score(y, preds))\nprint('Recall:', recall_score(y, preds))\n", "intent": "Both of these have simple implementations in `sklearn`\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint('F1 score:', f1_score(y, preds))\n", "intent": "Of course, it, too, is located in `sklearn.metrics`\n"}
{"snippet": "model.predict(test_X)\n", "intent": "And use that model to make a prediction on our test data\n"}
{"snippet": "import numpy as np\npredictions = model.predict(test_X)\n", "intent": "If we want to see how close we were, we compare against `test_y` and follow the same steps above.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(test_y, predictions)\nrmse = np.sqrt(mse)\nrmse\n", "intent": "Or we just use the `sklearn` implementation\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y, y_pred_probs)\n", "intent": "Pretty good for a dummy model on dummy data\n"}
{"snippet": "y_pred = model.predict(X)\n", "intent": "Now say we've decided on an acceptance threshold (in this case `.5`) and we want to take a look at our Confusion Matrix\n"}
{"snippet": "model.predict(x_train[0:5]).shape\n", "intent": "If we call `model.predict()` on one of our inputs, we obviously don't get back the multiclass prediction we're after.\n"}
{"snippet": "custom_model.evaluate(x_test, y_test)\n", "intent": "Huzzah, it's only kind of crap!\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nprint('\\nMetrics for the Linear Regression \\n')\nprint(\"Mean Absolute Error (MAE):\",mean_absolute_error(y_test, lin_predict))\nprint(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, lin_predict)))\nprint(\"Median Home price:\", Y.median())\n", "intent": "Now that we've predicted some home prices using the linear regression model, let's evaluate the accuracy of this model.\n"}
{"snippet": "print('\\nMetrics for the Lasso Regression \\n')\nprint(\"Mean Absolute Error (MAE):\",mean_absolute_error(y_test, las_predict))\nprint(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, las_predict)))\n", "intent": "Let's evaluate this model to see if the Lasso produces better results than ordinary least squares regression.\n"}
{"snippet": "print('\\nMetrics for the Ridge Regression \\n')\nprint(\"Mean Absolute Error (MAE):\",mean_absolute_error(y_test, rid_predict))\nprint(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, rid_predict)))\n", "intent": "Let's see how the l2-norm regularization of Ridge performs in predicting home prices, as opposed to Lasso\n"}
{"snippet": "lasso = Lasso(alpha = 0.00032)\nmses = cross_val_score(lasso, data[features], data[target], scoring=\"neg_mean_squared_error\", cv=kf)\nrmses = [np.sqrt(abs(mse)) for mse in mses]\nmean = np.mean(rmses)\nprint('RMSLES: ')\nprint(rmses)\nprint('\\nAverage RMSLE with LASSO: %.5f' % mean)\n", "intent": "Based of this plot, I am going to take alpha = 0.00032.\n"}
{"snippet": "print(np.sqrt(-cross_val_score(Treg, X, y, scoring = \"neg_mean_squared_error\", cv = 10).mean()))\nprint(mean_absolute_percentage_error(y_test, Treg.predict(X_test)))\n", "intent": "Decision Tree Regressor\n"}
{"snippet": "print(np.sqrt(-cross_val_score(Treg, X, y, scoring = \"neg_mean_squared_error\", cv = 10).mean()))\nprint(mean_absolute_percentage_error(y_test, Treg.predict(X_test)))\nprint(np.sqrt(mean_squared_error(np.exp(y), np.exp(Treg.predict(X)))))\n", "intent": "Decision Tree Regresor\n"}
{"snippet": "pred=[np.argmax(Xception_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_Xception]\ntest_accuracy=(100*np.sum(np.array(pred)==np.argmax(test_targets, axis=1)))/len(pred)\nprint(test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction_test = model.predict(img_test_norm)\nerror_test = 1 - accuracy_score(labels_red_test, prediction_test)\nprint ('The test error is %r ' %error_test)\n", "intent": "The missclasification rate using the test is:\n"}
{"snippet": "posterior = m.posterior_samples_f(X_test, full_cov=True, size=3)\nmeanYtest,a = m.predict(X_test,full_cov=True)\n", "intent": "On the test data set:\nd) Plot and evaluate the prediction error using the opmitized parameters. **2 points**\n"}
{"snippet": "import numpy as np\nfrom kmodes.kmodes import KModes\nkm = KModes(n_clusters=4, init='Huang', n_init=3, verbose=1)\nclusters = km.fit_predict(cluster_2016)\ncluster_2016['clusters'] = clusters\nprint(km.cluster_centroids_)\n", "intent": "* Here we apply **K-modes** algorithm as this dataset have both categorical value and numerical values.\n"}
{"snippet": "first_name_test_set = np.array(first_name_test_set)\nfn_test_prime = np.atleast_2d(first_name_test_set).T\nZ = clf.predict(fn_test_prime)\n", "intent": "Now I run a prediction for my test set\n"}
{"snippet": "net.evaluate()\nfrom sklearn.metrics import accuracy_score\ny_pred = np.argmax(net.forward(X_train), axis=1)\ny_true = np.argmax(Y_train, axis=1)\naccuracy_score(y_true, y_pred)*100\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_50]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_scaled = scale(test)\nhypotheses_size = clf_size.predict(test_scaled)\nhypotheses_toppings = clf_toppings.predict(test_scaled)\n", "intent": "***Classifying the test set using K-nearest neighbours***\n"}
{"snippet": "def evaluateAccuracy(clf,predictDF, truthDF):\n    correct_pred = 0\n    pred_x = clf.predict(predictDF)\n    for i in range(0,len(predictDF)):\n        if pred_x[i] == truthDF.iloc[i]:\n            correct_pred +=1\n    return (correct_pred/len(predictDF))\n", "intent": "1) Write a function that computes the accuracy of the predicted values\n"}
{"snippet": "confidence = knn.predict_proba(x_test)\nx_test2 = x_test.reset_index()\n", "intent": "3) The KNeighborsClassifier also has a [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "prediction = model.predict(df_test)\n", "intent": "Predict the loan status using the model:\n"}
{"snippet": "predictions = lm.predict(df_train)\n", "intent": "Note: we also evaluate the model on the df_test to avoid overfitting\n"}
{"snippet": "My_ResNet50_predictions = [np.argmax(My_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(My_ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(My_ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def compute_loss(X, y, w):\n    <your code here>\ndef compute_grad(X, y, w):\n    <your code here>\n", "intent": "The loss you should try to minimize is the Hinge Loss.\n$$ L =  {1 \\over N} \\sum_i max(0,1-y_i \\cdot \\vec w \\vec x_i) $$\n"}
{"snippet": "predicted = classifier.predict(X_test)\nprint(\"Predicted labels:\")\nprint(predicted[0:5])\nprint(\"Real labels:\")\nprint(y_test[0:5])\n", "intent": "We can now analyse how well the classifier generalizes to unseen data using the test set.\n"}
{"snippet": "rfc_pred = rfc.predict(x_test)\n", "intent": "** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "pred = KNN.predict(x_test)\n", "intent": "**Predicting values using your KNN model and X_test.**\n"}
{"snippet": "predictions= log_model.predict(x_test)\n", "intent": "** Predicting values for the testing data.**\n"}
{"snippet": "predictions = model.predict(x_test)\n", "intent": "**Get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print('Predicted:')\nprint(lm3.predict(df6).head())\nprint('\\n')\nprint('Actual:')\nprint(df6['property_crime'].head())\n", "intent": "The R-squared is also 1.0, which indicates possible overfitting.\n"}
{"snippet": "print('Predicted:')\nprint(rc_ny13_pvbl.predict(X_ny13_ncr).head())\nprint('\\n')\nprint('Actual:')\nprint(df_ny13_ncr['property_crime'].head())\n", "intent": "The R-squared is also 1.0, which indicates possible overfitting.\n"}
{"snippet": "scores_cv_ny14 = cross_val_score(lm_tts_ny13, X_ny14_ppmr_or, y_ny14_ppmr_or)\nprint('Cross validated scores:', scores_cv_ny14)\nprint('Average score:', scores_cv_ny14.mean())\n", "intent": "The NY 2014 score is lower compared to the NY 2013 model tested on itself.\n"}
{"snippet": "scores_cv_al13 = cross_val_score(lm_tts_ny13, X_al13_ppmr_or, y_al13_ppmr_or)\nprint('Cross validated scores:', scores_cv_al13)\nprint('Average score:', scores_cv_al13.mean())\n", "intent": "The AL 2013 score is much lower than the NY 2014 and 2013 scores.\n"}
{"snippet": "score = cross_val_score(knn, X, Y, cv=2)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\nscore_w = cross_val_score(knn_w, X, Y, cv=2)\nprint(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))\n", "intent": "Adding the other feature doesn't change the accuracy.\n"}
{"snippet": "player = [1, 1, 0, 1 , 2]\nknn.predict(player)\n", "intent": "Predict for a player with these statistics: 1 assist, 1 steal, 0 blocks, 1 turnover, 2 personal fouls\n"}
{"snippet": "def mean_score(t_obj):\n    t_score = tf.reduce_mean(t_obj) \n    return t_score\n", "intent": "We can even combine features from different layers by obtaining just the score of one and passing it to the other to jointly calculate the gradient\n"}
{"snippet": "net_pract.predict(X_pract)\n", "intent": "We now pass the training vector back through to see if we get something similar to ```T_pract```:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npreds_test = final_reg.predict(Xtest)\nprint(\"Test MSE for stacked ensemble: %.5f\" % mean_squared_error(ytest, preds_test))\n", "intent": "Now let's check the MSE on the test set.\n"}
{"snippet": "x = raw_valid.copy()\nx['pred_std'] = ens.plot_ci(X_valid[cols].values, y_valid, doPlot=False)[1].error.reshape(x.shape[0],1)\nx['pred'] = ens.predict(X_valid[cols].values)\n", "intent": "Above plot shows that we are least confident about predictions for **Year = 1984**\n"}
{"snippet": "preds = predict(net, md.val_dl)\n", "intent": "By decreasing learning rate, it improved a bit\n"}
{"snippet": "outputs = model.predict(dataset, steps=steps_per_epoch)\n", "intent": "For sake of simplicity, we predict on our training data. Never do this in practice!\n"}
{"snippet": "Resnet50_pre = [np.argmax(Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_pre)==np.argmax(test_targets, axis=1))/len(Resnet50_pre)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "np.mean((regr.predict(x_test) - y_test) ** 2)\n", "intent": "As seen above the point are away from the line that is the error. \nWe calculate the mean squared error for the above trained model.\n"}
{"snippet": "accuracy_score(y, pred)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(full_test_movies_ratings, full_predictions))\nprint(rms)\n", "intent": "dict = {1:2, 3:4, 5:6}\nprint(dict)\nif 3 in dict:\n    print(\"yo\")\nif 2+4 not in dict:\n    print(\"yee\")\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(test_movies_ratings, predictions))\nprint(rms)\n", "intent": "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. \n"}
{"snippet": "pred_labels = knn.predict(X_test)\npred_labels\n", "intent": "And you predict on new data:\n"}
{"snippet": "print 'Parameters: ', results.params\nprint 'Standard errors: ', results.bse\nprint 'Predicted values: ', results.predict()\n", "intent": "We can access the fit parameters like this:\n"}
{"snippet": "print classification_report(y, training_preds, labels=[0,1,2], target_names=['class 0', 'class 1', 'class 2'])\n", "intent": "The `classification_report` function will easily give us some other metrics:\n"}
{"snippet": "roc_auc_score(y_bin, bin_preds)\n", "intent": "And we can easily calculate the AUC:\n"}
{"snippet": "num_trees = []\ntrain_errs = []\nfor i, y_pred in enumerate(gbt.staged_predict(X_train)):\n    num_trees.append(i)\n    train_errs.append(zero_one_loss(y_train, y_pred))\ntest_errs = []\nfor i, y_pred in enumerate(gbt.staged_predict(X_test)):\n    test_errs.append(zero_one_loss(y_test, y_pred))\n", "intent": "GBT has a `staged_predict` function that shows you what prediction it would make after each tree in the ensemble sum.\n"}
{"snippet": "preds = svm.predict(X_test)\n", "intent": "Let's see how we do on a test set:\n"}
{"snippet": "kmeans.predict(digits_data)\n", "intent": "We can call the `predict` method, which will tell us which cluster center some new data is closest too:\n"}
{"snippet": "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n", "intent": "Cross validation on random forest:\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat == y)\nprint(\"Accuracy on training data = %f\" % acc)\n", "intent": "We can next plot the accuracy on the training data.  We see we get an accuracy better than simple linear classifier. \n"}
{"snippet": "yhat_ts = svc.predict(Xts)\n", "intent": "Measure the accuracy on the test data.  The prediction can take several minutes too -- SVMs are *very* slow!\n"}
{"snippet": "score, acc = model.evaluate(Xts, yts, verbose=1)\nprint(\"accuracy = %f\" % acc)\n", "intent": "We can test the performance on the test data set.\n"}
{"snippet": "decoded_imgs = autoencoder.predict(x_test_noisy)\nplot_image(3, 10, 32, 3, x_test, x_test_noisy, decoded_imgs)\n", "intent": "Let us see how the denoised images look. First row is original images, second row is noisy images, and third row shows corresponding denoised images.\n"}
{"snippet": "decoded_test_imgs = autoencoder.predict(x_test_noisy[:11])\nplot_image(3, 10, 32, 3, x_test, x_test_noisy, decoded_test_imgs)\n", "intent": "Let us see how the denoised images look with this model. We plot denoised images from both the testing set and the training set.\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(classification_report(y_test, y_hat, target_names=target_names))\nprint(\"Confusion matrix on the test data\")\nprint(confusion_matrix(y_test, y_hat, labels=range(n_classes)))\n", "intent": "We can use following tools to print classification performance.\n"}
{"snippet": "predictions = dtree_vanilla.predict(X_train)\nerrors = abs(predictions - y_train)\nprint(\"Mean Absolute Error for Train: \", round(np.mean(errors),2), \" degrees\")\npredictions = dtree_vanilla.predict(X_test)\nerrors = abs(predictions - y_test)\nprint(\"Mean Absolute Error for test: \", round(np.mean(errors),2), \" degrees\")\n", "intent": "Now let us measure the error between actual and predicted temperature\n"}
{"snippet": "IncepV3_predictions = [np.argmax(IncepV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_IncepV3]\ntest_accuracy = 100*np.sum(np.array(IncepV3_predictions)==np.argmax(test_targets, axis=1))/len(IncepV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "train_predict(lang1 = \"English\", lang2 = \"French\", \n                  path_lang1='./data/eng.txt', path_lang2='./data/frn.txt',\n                  maxlen = 4, step = 1,\n                  optimizer = 'adam', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=5)\n", "intent": "As seen in the ROC curve, there is a significant decline in accuracy when we use SGD. Let's try the adam optimizer next...\n"}
{"snippet": "train_predict(lang1 = \"English\", lang2 = \"French\", \n                  path_lang1='./data/eng.txt', path_lang2='./data/frn.txt',\n                  maxlen = 4, step = 1,\n                  optimizer = 'rmsprop', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=20)\n", "intent": "Now let's try each optimizer (rmsprop, sgd, adam) with epochs = 20, in that order.\n"}
{"snippet": "train_predict(lang1 = \"French\", lang2 = \"Dutch\", \n                  path_lang1='./data/frn.txt', path_lang2='./data/dut.txt',\n                  maxlen = 4, step = 1,\n                  optimizer = 'rmsprop', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=5)\n", "intent": "<b>French-Dutch:</b>\n"}
{"snippet": "train_predict(lang1 = \"English\", lang2 = \"Latin\", \n                  path_lang1='./data/eng.txt', path_lang2='./data/ltn.txt',\n                  maxlen = 4, step = 1,\n                  optimizer = 'rmsprop', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=5)\n", "intent": "<b>English-Latin:</b>\n"}
{"snippet": "train_predict(lang1 = \"French\", lang2 = \"Latin\", \n                  path_lang1='./data/frn.txt', path_lang2='./data/ltn.txt',\n                  maxlen = 4, step = 1,\n                  optimizer = 'rmsprop', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=5)\n", "intent": "<b>French-Latin:</b>\n"}
{"snippet": "train_predict(lang1 = \"Dutch\", lang2 = \"Latin\", \n                  path_lang1='./data/dut.txt', path_lang2='./data/ltn.txt',\n                  maxlen = 4, step = 1,\n                  optimizer = 'rmsprop', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=5)\n", "intent": "<b>Dutch-Latin:</b>\n"}
{"snippet": "train_predict(lang1 = \"English\", lang2 = \"French\", \n                  path_lang1='./data/eng.txt', path_lang2='./data/frn.txt',\n                  maxlen = 4, step = 1,\n                  preprocess_with_outerloop = True,\n                  optimizer = 'rmsprop', lr = 0.01,\n                  lstm_size = 128,\n                  batch_size = 128,\n                  num_epoch=5)\n", "intent": "We can invoke the above method with an extra parameter, <code>preprocess_with_outerloop</code> set to True.\n"}
{"snippet": "list_1_0 = clf.predict(X_test)\nlist_1_0\n", "intent": "Making predictions on the testing set of data\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n", "intent": "Let's decide what treshold should we use. Firstly, we gather the decision scores for each instance.\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\n", "intent": "Firstly, let's take a look at the confusion matrix.\n"}
{"snippet": "def GetProbabilities(clf, X_train, y_train, X_test, y_test):\n    def GetProbs(X_m,y_m):\n        Prediced_RFC_ = clf.predict(X_m)\n        CorrectlyPredicted_RFC_ = [[Prediced_RFC_[i]==y_m[i]] for i in range(0,len(y_m)-1)]\n        return np.sum(CorrectlyPredicted_RFC_)/len(X_m-1)\n    return GetProbs(X_train,y_train),GetProbs(X_test, y_test)\n", "intent": "Function to calculate probabily of correct prediction form a classifier:\n"}
{"snippet": "def pipeline_final(img):\n    out_scores, out_boxes, out_classes, image = predict(sess, img)\n    return image\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "probabilities = model.predict_proba(x)\nprobabilities\n", "intent": "Now, let's calculate the probabilities(this is the propensity score).\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for l in range(0, len(style_layers)):\n        f_gram = gram_matrix( feats[ style_layers[l]] )\n        sub = f_gram - style_targets[l]\n        sub = torch.mul( torch.pow( sub, 2 ), style_weights[l])\n        summed = torch.sum( sub )\n        style_loss += summed\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "sale_price = reg.predict(CLIENT_FEATURES[0])\nprint \"Predicted value of client's home: ${0:.3f}k\".format(sale_price[0])\n", "intent": "Now let's make a prediction on selling price for our client.\n"}
{"snippet": "restnet50_predict = [np.argmax(net50_model.predict(np.expand_dims(feature, axis=0)))\n                    for feature in test_net50\n                    ]\npredictlen=len(restnet50_predict)\ntest_accuracy = 100*(np.sum(np.array(restnet50_predict) == np.argmax(test_targets, axis = 1))/predictlen)\nprint('Accuracy of test is: %.3f%%' %test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print (\"Predicted %d, Label: %d\" % (classifier.predict(np.array([test_data[0]], dtype=float), as_iterable=False), test_labels[0]))\ndisplay(0)\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "resnet_pred = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_pred)==np.argmax(test_targets, axis=1))/len(resnet_pred)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nd_test = xgb.DMatrix(X_test)\np_test = bst.predict(d_test)\nnpround = np.vectorize(round)\np_test_ints = npround(p_test)\naccuracy = accuracy_score(y_test, p_test_ints)\nprint(\"Test Accuracy: \", accuracy)\n", "intent": "We have a **pretty good validation error of around 7% based only on word and character count**.\n"}
{"snippet": "p1 = ccGlassoPredict(model, YYos)\nprint( classification_report(CCos, p1 > 0.5, target_names=['Class 0', 'Class 1']) )\nplotROC(CCos, p1)\n", "intent": "Using stability selection to pre-learn the structure, 2 of the zeros in the first precision matrix are found.\n"}
{"snippet": "p1 = ccmrcePredict(*model, XXos, YYos)\nprint( classification_report(CCos, p1 > 0.5, target_names=['Class 0', 'Class 1']) )\nplotROC(CCos, p1)\n", "intent": "As for the regression coefficients, the learned precisions are a little dense here, but the learned values are close to ground truth.\n"}
{"snippet": "p1 = ccggmPredict(model, XXos, YYos)\nprint( classification_report(CCos, p1 > 0.5, target_names=['Class 0', 'Class 1']) )\nplotROC(CCos, p1)\n", "intent": "The learned precisions match the ground truth quite well.\n"}
{"snippet": "print(metrics.classification_report(y_test,y_test_pred))\n", "intent": "3) Display the classification report for this classifier.\n"}
{"snippet": "print(metrics.accuracy_score(y,y_pred_labels))\n", "intent": "3) Print out the classification report and the accuracy score.\n"}
{"snippet": "def accuracy(predictions, labels):\n    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\npredicted_classes = model.predict_classes(images, verbose=0)\nprob_classes = model.predict(images, verbose=0)\nprint(\"Prediction finished!\")\ntest_accuracy = (np.sum(predicted_classes == true_classes) / len(true_classes)) * 100.0\nprint(\"Test Accuracy: {:2.2f}%\".format(test_accuracy))\n", "intent": "Predict the sign type for each image and the probabilities\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision_score(y, prediction)\n", "intent": "Precision is defined by how many positive prediction prediction is really positive\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall_score(y, prediction)\n", "intent": "Recall is defined by how many positive class included in positive prediction\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y, prediction)\n", "intent": "F1 score is one way to combine precision and recall into single value, for easier comparison, in model selection, for example\n"}
{"snippet": "print(classification_report(y_test,d_prediction))\n", "intent": "**Decision Tree - df_1 ( all target variable including 'Unknown' )**\n"}
{"snippet": "print(classification_report(y_test_tg,d_prediction_tg))\n", "intent": "**Decision Tree - df_1_tg ( only KNOWN target variable )**\n"}
{"snippet": "print(classification_report(y_test,g_prediction))\n", "intent": "**Naive Bayes - df_1 ( all target variable including 'Unknown' )**\n"}
{"snippet": "print(classification_report(y_test_tg,g_prediction_tg))\n", "intent": "**Naive Bayes - df_1_tg ( only KNOWN target variable )**\n"}
{"snippet": "from sklearn_crfsuite import metrics\ny_pred = model.predict(X_test)\nprint(metrics.flat_accuracy_score(y_test, y_pred))\n", "intent": "To make predictions with the model:\n"}
{"snippet": "preds_a = model_a.predict(x_val)\npreds_b = model_b.predict(x_val)\npreds_c = model_c.predict(x_val)\npreds_d = model_d.predict(x_val)\nfinal_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)\nfinal_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d\n", "intent": "Different good models trained independently are likely to be good *for different reasons*.\n"}
{"snippet": "Resnet_50_predictions = [np.argmax(Resnet50_model.predict(\n    np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(Resnet_50_predictions)==np.argmax(\n    test_targets, axis=1))/len(Resnet_50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "How well does the OLS regression predict `Moving Time` observations that the model has not yet seen?\n"}
{"snippet": "ridge_predictions = ridge.predict(X)\n", "intent": "Did the ridge regression model give us any extra predictive precision?\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'],(1 - tocluster.labels_)))\nprint(classification_report(df['Cluster'],(1 - tocluster.labels_)))\n", "intent": "I create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.\n"}
{"snippet": "y_pred = lm.predict(X_test)\n", "intent": "Now that I have fit the model, I evaluate its performance by predicting off the test values.\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "It is possible to evaluate the model performance by calculating the residual sum of squares and the explained variance score (R^2).\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "I will create predictions from the test set so I will create a classification report and a confusion matrix.\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate the model.\n"}
{"snippet": "predictions = svc_model.predict(X_test)\n", "intent": "In order to get predictions from the my fitted model I can use the .predict() method from a SVC() object.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Now, to understand if I can improve my model I take that grid and I create some predictions using the test set.\n"}
{"snippet": "img = '7e83c129d2696dad7c9917aa5f31d44a'\nout = read_img(img, 'test/', (224, 224))\np_hat = model.predict(np.reshape(out, (1, 224, 224, 3)))\n", "intent": "Keras is fantastic - predicting with the ResNet50 model is one line.\n```python\nmodel.predict(matrix_img)\n```\n"}
{"snippet": "img  = read_img('orange','test',(224, 224))\niimg = np.reshape(img, (1, 224, 224, 3))\no_hat = model.predict(iimg)\n", "intent": "Just to make sure that ResNet50 has it's weights properly loaded. Since oranges were included in the original ImageNet Task, this should work!\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Once fit, we estimate the performance of the model on unseen reviews.\n"}
{"snippet": "print(\"AMI= \", adjusted_mutual_info_score(iris['target'], clabels))\n", "intent": "We can compare the true labels with the ones obtained using this clustering algorithm using for example the mutual information score\n"}
{"snippet": "print (adjusted_mutual_info_score(iris['target'], clabels))\n", "intent": "If we compute the mutual information score\n"}
{"snippet": "print (adjusted_mutual_info_score(iris['target'], clabels))\n", "intent": "In this case the mutual information scores higher for this criteria.\n"}
{"snippet": "print(adjusted_mutual_info_score(iris['target'], labels))\n", "intent": "Results are a little bit worse than hierarchical clustering\n"}
{"snippet": "print(\"AMI=\", adjusted_mutual_info_score(iris['target'], labels))\nprint(\"BIC=\", gmm.bic(iris['data']))\n", "intent": "These are the results of the AMI and the final BIC of the model\n"}
{"snippet": "lr.predict_proba(X_test_std[0:10,:])\n", "intent": "To predict probabilities:\n"}
{"snippet": "disease_pred = mnb_tot.predict(x)\n", "intent": "So the model is misclassifying some diseases, which are those dieases?\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(test_y, predicted_classes, target_names=target_names))\n", "intent": "We conclude by generating the classification report.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)) :\n        loss += ((style_targets[i] - gram_matrix(feats[style_layers[i]]))**2).sum() * style_weights[i]\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "results = model_selection.cross_val_score(model, X, Y)\nprint(results)\n", "intent": "__Running__ the model.\n"}
{"snippet": "Resnet_predictions = [np.argmax(myDogResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cv_scores = []\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        cv_scores.append(log_loss(val_y, preds))\n        print(cv_scores)\n        break\n", "intent": "Without CV statistic,to score get 0.5480 by SRK. And CV statistic get 0.5346 In fact ,you \nneed to turn down the learning rate and turn up run_num\n"}
{"snippet": "y_true = [100, 50, 30, 20]\ny_pred = [90, 50, 50, 30]\nprint metrics.mean_absolute_error(y_true, y_pred)\n", "intent": "*Mean Absolute Error: mean of the absolute value of errors*\n"}
{"snippet": "print metrics.mean_squared_error(y_true, y_pred)\n", "intent": "*Mean Squared Error: mean of squared errors*\n"}
{"snippet": "print np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "*Root Mean Squared Error: square root of above*\n"}
{"snippet": "ri_pred = linreg.predict(X)\n", "intent": "Why don't we make predictions for all of our X values and then connect them using a line?\n"}
{"snippet": "assorted_pred_prob = logreg.predict_proba(X)[:, 1]\n", "intent": "Logistic regression is doing something similar to linear regression with a threshold\n"}
{"snippet": "y_pred_class = logreg.predict(X_test)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "*Task 5 - Make predictions on the testing set and calculate the accuracy.*\n"}
{"snippet": "logreg.predict_proba([1, 0, 29, 0])[:, 1]\n", "intent": "If your age increases by one unit, \nPredict the probability of survival for Adam: first class, no parents or kids, 29 yrs old, male\n"}
{"snippet": "logreg.predict_proba([2, 0, 29, 0])[:, 1]\n", "intent": "Probability of survival is roughly 50%. His odds of survival are 1:1\nWhat about Bill in second class?\n"}
{"snippet": "logreg.predict_proba([1, 0, 29, 1])[:, 1]\n", "intent": "Now Susan, who is the same as Adam but female\n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "Seeing an ROC curve can help you choose a threshold that balances sensitivity and specificity in a way that makes sense for the particular context.\n"}
{"snippet": "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n", "intent": "And finally, get predictions for the test set and prepare a submission CSV:\n"}
{"snippet": "lr.predict(100)\n", "intent": "We can get predictions if we pass in values for total basement square feet. Let's make predictions for 100, 1,000, and 10,000 square foot basements.\n"}
{"snippet": "lr.predict(X)\n", "intent": "We can pass in our input data to get the predicted value for each house in our dataset.\n"}
{"snippet": "preds = np.stack([t.predict(X_test) for t in m.estimators_])\nnp.mean(preds[:,0]), np.std(preds[:,0])\n", "intent": "* Average of the prediction values\n* Variance of the tree predictions\n"}
{"snippet": "preds = np.stack([t.predict(X_test) for t in m.estimators_])  \nprint(preds)\nnp.mean(preds[:,0]), np.std(preds[:,0])\n", "intent": "* average of predicion value\n* variance\n"}
{"snippet": "def predict(network, X):\n", "intent": "To check the quality of our neural network, we will compute accuracy. To do this, implement the function that makes predictons using the model:\n"}
{"snippet": "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n", "intent": "Calculate the  accuracy of the model using the testing data.\n"}
{"snippet": "from sklearn.metrics import classification_report\ncr = classification_report(y_test, y_pred, \n                           target_names=iris.target_names)\nprint(cr)\n", "intent": "We can also create a classification report which will give other error metrics for each of the iris species.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc1 = accuracy_score(y_test, y_pred_cv1)\nacc2 = accuracy_score(y_test, y_pred_cv2)\nprint(acc1, acc2)\n", "intent": "Measure the accuracy of the models and compare the results.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_list = list()\nfor lab,pred in zip(['document-term', 'word2vec', 'combined'],\n                    [y_pred_cv, y_pred_embed, y_pred_comb]):\n    accuracy_list.append((lab, accuracy_score(y_test, pred)))\n", "intent": "Evaluate the error metrics.\n"}
{"snippet": "clf.predict(test[features])\n", "intent": "Apply classifier to test data using `clf.predict` and `clf.predict_proba`:\n"}
{"snippet": "preds = iris.target_names[clf.predict(test[features])]\n", "intent": "Evaluate classifier:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, model.predict(X_test))\ncm\n", "intent": "For evaluation of our model we can resort to confusion matrix.\n"}
{"snippet": "accuracy = model_selection.cross_val_score(model, X, Y, scoring='accuracy', cv=5) \nprint( \"Accuracy: \"+ str(round(100*accuracy.mean(), 2)))\nf1 = model_selection.cross_val_score(model, X, Y, scoring='f1_weighted', cv=5) \nprint(\"F1: \" + str(round(100*f1.mean(), 2)))\nprecision = model_selection.cross_val_score(model, X, Y, scoring='precision_weighted', cv=5) \nprint(\"Precision: \" + str(round(100*precision.mean(), 2)))\nrecall = model_selection.cross_val_score(model, X, Y, scoring='recall_weighted', cv=5)\nprint(\"Recall: \" + str(round(100*recall.mean(), 2)), \"\\n\")\n", "intent": "Let's take a look at metrics after fine-tuning\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred\n", "intent": "Now, let's make our prediction!\n"}
{"snippet": "y_pred = lin_regress.predict(input_fn=lambda: inputer(X, y, num_epochs=1, shuffle=False))\ny_pred = np.array([pred_row[\"predictions\"][0] for pred_row in y_pred])\nmse = metrics.mean_squared_error(y_pred, y)\nr2 = metrics.r2_score(y, y_pred)\nsum_stats = {\n    \"mse\" : mse,\n    \"rmse\" : math.sqrt(mse),\n    \"r2\" : r2\n            }\nprint(\"MSE: %0.3f\\tRMSE: %0.3f\\tR^2: %0.3f\" % (sum_stats[\"mse\"], sum_stats[\"rmse\"], sum_stats[\"r2\"]))\n", "intent": "We'll use MSE and RMSE to evaluate the effectiveness of the model. \n"}
{"snippet": "pred_fn =lambda : preprocess(my_feature, targets, num_epochs=1, shuffle=False)\nour_preds = our_lin_reg.predict(input_fn=pred_fn)\npred = np.array([item['predictions'][0] for item in predictions])\nmse = metrics.mean_squared_error(pred, targets)\nsqrt_mse = math.sqrt(mse)\nprint(\"Train MSE:\\t%0.3f\" % mse)\nprint(\"Train Root MSE:\\t%0.3f\" % sqrt_mse)\n", "intent": "This guide uses mean squared error as a measure of how well our model is doing\n"}
{"snippet": "pred = lin_regress.predict(input_fn=lambda: inputer(x, Y, num_epochs=1, shuffle=False))\npred = np.array([pred_row[\"pred\"][0] for pred_row in pred])\nsum_stats = {\n    \"mse\":metrics.mean_squared_error(pred, Y),\n    \"rmse\":math.sqrt(MSE)\n            }\nprint(\"MSE: %0.3f\\tRMSE: %0.3f\" %sum_stats[\"mse\"], sum_stats[\"rmse\"])\n", "intent": "We'll use MSE and RMSE to evaluate the effectiveness of the model. \n"}
{"snippet": "def contrastive_loss(y,d):\n    tmp= y *tf.square(d)\n    tmp2 = (1-y) *tf.square(tf.maximum((1 - d),0))\n    return tf.reduce_sum(tmp +tmp2)/batch_size/2\ndef compute_accuracy(prediction,labels):\n    return labels[prediction.ravel() < 0.5].mean()\n", "intent": "    - This needs to be modified for customized loss / distance computations\n"}
{"snippet": "def evaluate(accuracy_operation, X_data, y_data, batch_size):\n    num_examples = len(X_data)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, batch_size):\n        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "Evaluate how well the loss and accuracy of the model for a given dataset.\nYou do not need to modify this section.\n"}
{"snippet": "conf_mat_train = confusion_matrix(Y_train, clf.predict(X_train))\nconf_mat_train\n", "intent": "I chose 3 as number of features at each split 3, default is sqrt(\nhttp://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html\n"}
{"snippet": "print('SGD coefficients: \\n{}\\n'.format(t_sgd))\nprint('GD coefficients: \\n{}\\n'.format(t_gd))\nprint('SGD percent error: \\n{}\\n'.format(prcnt_err_sgd))\nprint('GD percent error: \\n{}'.format(percent_error(t_gd, linreg.coef_)))\n", "intent": "Law of diminishing returns is very obvious here\n"}
{"snippet": "predictions_DT_test = td_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_DT_test, td_classifier)\n", "intent": "- <h4> Model testing \n"}
{"snippet": "predictions_ada_test = ada.predict(x_test)\nprint_results(x_test, y_test, predictions_ada_test, ada)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_GradB_test = gradientBoost.predict(x_test)\nprint_results(x_test, y_test, predictions_GradB_test, classifier=gradientBoost)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_bagg_test = bagg.predict(x_test)\nprint_results(x_test, y_test, predictions_bagg_test, bagg)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_RF_test = rf_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_RF_test, rf_classifier)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_LR_test = logdown.predict(x_test)\nprint_results(x_test, y_test, predictions_LR_test, logdown)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_SVC_test = svm_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_SVC_test, svm_classifier)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_KNN_test = knn_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_KNN_test, knn_classifier)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "nb_predictions_test = gaussian_classifier.predict(x_test)\nprint_results(x_test, y_test, nb_predictions_test, gaussian_classifier)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "predictions_DT_test = td_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_DT_test, td_classifier)\n", "intent": "- <h4> Model testing </h4>\n"}
{"snippet": "predictions_ada_test = ada.predict(x_test)\nprint_results(x_test, y_test, predictions_ada_test, ada)\n", "intent": "- <h4> Model testing </h4>\n"}
{"snippet": "predictions_GradB_test = gradientBoost.predict(x_test)\nprint_results(x_test, y_test, predictions_GradB_test, classifier=gradientBoost)\n", "intent": "- <h4> Model testing </h4>\n"}
{"snippet": "predictions_bagg_test = bagg.predict(x_test)\nprint_results(x_test, y_test, predictions_bagg_test, bagg)\n", "intent": "- <h4> Model testing </h4>\n"}
{"snippet": "predictions_RF_test = rf_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_RF_test, rf_classifier)\n", "intent": "- <h4> Model testing </h4>\n"}
{"snippet": "predictions_SVM_test = svm_classifier.predict(x_test)\nprint_results(x_test, y_test, predictions_SVM_test, svm_classifier)\n", "intent": "- <h4> Model testing\n"}
{"snippet": "y_pred = regressor.predict(X_test)\nprint('Liner Regression R squared: %.4f' % regressor.score(X_test, y_test))\n", "intent": "Done! We now have a working Linear Regression model.\nCalculate R squared:\n"}
{"snippet": "pseudo_preds = model_res50_fc.predict(test_features, batch_size=batch_size)\n", "intent": "https://shaoanlu.wordpress.com/2017/04/10/a-simple-pseudo-labeling-function-implementation-in-keras/\n"}
{"snippet": "final_net.evaluate()\nanswers = final_net.forward(X_test)\nencoded_ans = np.array([answers.argmax(axis=1)==i for i in range(10)]).T\nfinal_net.train()\naccuracy_score(y_test, encoded_ans)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "prediction_rf = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,prediction_rf))\n", "intent": "**Now create a classification report from the results.**\n"}
{"snippet": "grid_pred = grid.predict(X_test)\n", "intent": "** Now lets make predictions using this new model**\n"}
{"snippet": "x_test = np.array(['A lot more room for improvements'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_true = out_test\ny_pred = clf.predict(in_test)\ncm = confusion_matrix(y_true, y_pred)\n", "intent": "> **Confusion Matrix**\n"}
{"snippet": "network_prediction_probabilities = network_model.predict(test_network, batch_size=32, verbose=0)\nnetwork_predict_labels = [np.argmax(p) for p in network_prediction_probabilities]\nnetwork_predict_labels = np.array(network_predict_labels)\ntest_labels = [np.argmax(test_targets, axis=1)]\npredict_match_test_count = np.sum(network_predict_labels == test_labels)\nnetwork_accuracy = 100 * (predict_match_test_count / len(network_predict_labels))\nprint('Network test accuracy: %.4f%%' % network_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = rfc_model.predict(X)\n", "intent": "Get the predictions.\n"}
{"snippet": "def predict(question,label_dict=label_dict,clf=model,cv=cv,tf=tf):\n    question=data_processing([question])\n    cv_vector=cv.transform(question)\n    tf_vector=tf.transform(cv_vector).toarray()\n    pred=clf.predict(tf_vector)\n    return [key for key in label_dict.keys() if label_dict[key]==pred][0]\n", "intent": "From the results, it might seem like we are overfitting on the model but we can't be sure as the test set is really small, so is\nthe training set.\n"}
{"snippet": "newsgroups_target_te_pred = clf.predict(newsgroups_data_te)\n", "intent": "Now, we predict the classes of our test data using the previously fitted classifier. \n"}
{"snippet": "acc_score = accuracy_score(newsgroups_target_te, newsgroups_target_te_pred)\nacc_score\n", "intent": "And we calculate the accuracy score which consists in calculating the amount of classes correctly assigned to the test data by the classifier.\n"}
{"snippet": "final_acc_score = accuracy_score(newsgroups_target_te, newsgroups_target_te_pred)\nfinal_acc_score\n", "intent": "And further compute the accuracy score by submitting the testing data to the classifier.\n"}
{"snippet": "acc_score = accuracy_score(newsgroups_target_te, newsgroups_target_te_pred)\n", "intent": "And we calculate the accuracy score which consists in calculating the amount of classes correctly assigned to the test data by the classifier.\n"}
{"snippet": "final_acc_score = accuracy_score(newsgroups_target_te, newsgroups_target_te_pred)\n", "intent": "And further compute the accuracy score by submitting the testing data to the classifier.\n"}
{"snippet": "probs = nn.predict_proba([X_test[test_example]])\nprint(probs)\n", "intent": "The model assigns the following probabilities for each class:\n"}
{"snippet": "print(\"Validation set evaluation:\")\ny_pred = model.predict(X_val)\nprint(classification_report(y_val, y_pred))\nprint(\"Test set evaluation:\")\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "Validate and evaluate:\n"}
{"snippet": "for X, y in zip(X_expand, y_train):\n    print('Actual steering angle {} model prediction {}'.format(y, model.predict(X)[0][0]))\n", "intent": "Training hits zero validation loss after epoch 5, i.e., it should have learned the data perfectly. Lets see how well the model predicts.\n"}
{"snippet": "print(predicted)\nacc = sklearn.metrics.accuracy_score(y_test, predicted)\nprint(acc)\n", "intent": "The new classifier created gives an accuracy of 82% which is slightly more then what received earlier but is very comparable.\n"}
{"snippet": "y_pred = LR.predict(X_test)\n", "intent": "Predict using testing data.\n"}
{"snippet": "from sklearn import metrics\nnp.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "Evaluate your model and Accuracy\n"}
{"snippet": "y_pred = LR.predict(final_data[1460:])\ngroundfloor = final_data['GrLivArea'][1460:]\n", "intent": "Using the testing data, final_data[1460:] , we will now predict the housing valuation.\n"}
{"snippet": "y_pred = LR.predict(X_test)\n", "intent": "Predict using testing data\n"}
{"snippet": "predict = clf.predict(x_test)\nprint(\"Our accuracy is: \" + str(accuracy_score(y_test, predict)*100) + \"%\")\n", "intent": "Now that we have created our classifier, we can test to see if it works!\n"}
{"snippet": "results = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    results[name] = scores\n", "intent": "On iterating over the classifiers, we have:\n"}
{"snippet": "forestPreds = randomForest.predict(Xtest)\npd.crosstab(Ytest, forestPreds, rownames=['Actual status'], colnames=['Predicted status'])\n", "intent": "Create cross table of actual and predicted\n"}
{"snippet": "print cross_val_score(randomForest, X, Y, cv = 10)\nprint cross_val_score(bagTree, X, Y, cv = 10)\n", "intent": "Initiate cross validation to verify we did not overfit on the test set.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "Now let's validate using k-fold cross validation\n"}
{"snippet": "results = network.evaluate(test_images, test_labels)\nprint(results)\n", "intent": "    Seems to be a good fit and decent accuracy. Let's test it\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nY_test_pred = []\nY_1 =  expit(np.dot(Phi_test, v_opt[1::]) + v_opt[0])\nfor i in range(len(Y_1)):\n    if Y_1[i] >= 0.5:\n        Y_test_pred.append(1)\n    elif Y_1[i] < 0.5:\n        Y_test_pred.append(0)\naccuracy_score(Y_test, Y_test_pred)\n", "intent": "This time we should get a better result for the accuracy on the test set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\npred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,pred)\nprint(classification_report(y_test,pred))\n", "intent": "Looks like tree model did well, but **False Negative** is still high.\n"}
{"snippet": "from sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscoresLoo = cross_val_score(regr, X, bos.PRICE, cv=loo, scoring='neg_mean_squared_error')\nprint(scoresLoo.mean())\n", "intent": "Now, let's try to calculate the mean squared error using \"Leave One Out\" method as well, to compare against the above.\n"}
{"snippet": "print(evaluate('Th', 200, temperature=0.1))\n", "intent": "Lower temperatures are less varied, choosing only the more probable outputs:\n"}
{"snippet": "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\nprint(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))\n", "intent": "Use the testing set to test your model, clearly calculating and displaying the error rate.\n"}
{"snippet": "y_predict = linreg.predict(X_test)\n", "intent": "... and make prediction using test data\n"}
{"snippet": "d_train = lgb.Dataset(X_train, label=y_train)\ngbm = lgb.train(lgb_params,d_train,100000,                  \n                    verbose_eval=10, \n                     feval=None)\ny_pred=gbm.predict(X_test)\nscore = roc_auc_score(y_test, y_pred)\nprint(score)                 \n", "intent": "train the best LGBM model and get the result\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_test, predicted,\n    target_names=test_set.target_names))\n", "intent": "Let's visualize the metics of the model.\n"}
{"snippet": "print sqft_model_statsmodels.predict(house2)[0]\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "score = binary_model.evaluate(binary_x_test, binary_y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "99% accuracy!\nLet's try it on our *test set* instead of our *validation set* now\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1), dtype=bool)\n", "intent": "Performance looks great! (above 90%). Lets make a small classifier to see if this is too good to be true.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n", "intent": "Count the number of times Class A is classified as Class B, and so forth.\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n", "intent": "In this case, lowering threshold increased recall, but would probably decrease accuracy.\n"}
{"snippet": "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\nprecision_score(y_train_5, y_train_pred_forest)\n", "intent": "ROC AUC is better for forest, and the curve looks better (tighter to upper left). Lets check out the precision and recall of the forest classifier.\n"}
{"snippet": "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n", "intent": "How do you evaluate a multi-label classifier? One approach is to calculate F1 for each label, and then compute the average.\n"}
{"snippet": "y_pred = bag_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "This means it is likely to do about this well on the test set.\n"}
{"snippet": "x_test_pred = np.argmax(classifier.predict(x_test[:100]), axis=1)\nnb_correct_pred = np.sum(x_test_pred == np.argmax(y_test[:100], axis=1))\nprint(\"Original test data (first 100 images):\")\nprint(\"Correctly classified: {}\".format(nb_correct_pred))\nprint(\"Incorrectly classified: {}\".format(100-nb_correct_pred))\n", "intent": "Evaluate the classifier performance on the first 100 original test samples:\n"}
{"snippet": "x_test_adv_pred = np.argmax(classifier.predict(x_test_adv), axis=1)\nnb_correct_adv_pred = np.sum(x_test_adv_pred == np.argmax(y_test[:100], axis=1))\nprint(\"Adversarial test data (first 100 images):\")\nprint(\"Correctly classified: {}\".format(nb_correct_adv_pred))\nprint(\"Incorrectly classified: {}\".format(100-nb_correct_adv_pred))\n", "intent": "And evaluate performance on those:\n"}
{"snippet": "x_test_robust_pred = np.argmax(robust_classifier.predict(x_test[:100]), axis=1)\nnb_correct_robust_pred = np.sum(x_test_robust_pred == np.argmax(y_test[:100], axis=1))\nprint(\"Original test data (first 100 images):\")\nprint(\"Correctly classified: {}\".format(nb_correct_robust_pred))\nprint(\"Incorrectly classified: {}\".format(100-nb_correct_robust_pred))\n", "intent": "Evaluate the robust classifier's performance on the original test data:\n"}
{"snippet": "x_test_pred = np.argmax(classifier.predict(x_test[:100]), axis=1)\nnb_correct_pred = np.sum(x_test_pred == np.argmax(y_test[:100], axis=1))\nprint(\"Original test data (first 100 images):\")\nprint(\"Correctly classified: {}\".format(nb_correct_pred))\nprint(\"Incorrectly classified: {}\".format(100-nb_correct_pred))\n", "intent": "Evaluate the classifier on the first 100 test images:\n"}
{"snippet": "x_test_adv_pred = np.argmax(classifier.predict(x_test_adv), axis=1)\nnb_correct_adv_pred = np.sum(x_test_adv_pred == np.argmax(y_test[:100], axis=1))\nprint(\"Adversarial test data (first 100 images):\")\nprint(\"Correctly classified: {}\".format(nb_correct_adv_pred))\nprint(\"Incorrectly classified: {}\".format(100-nb_correct_adv_pred))\n", "intent": "Evaluate the classifier on 100 adversarial samples:\n"}
{"snippet": "eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nnb_flag_adv = []\nnb_missclass = []\nfor eps in eps_range:\n    x_test_adv = attacker.generate(x_test[:100], eps=eps)\n    nb_flag_adv += [np.sum(np.argmax(detector(x_test_adv), axis=1) == 1)]\n    nb_missclass += [np.sum(np.argmax(classifier.predict(x_test_adv), axis=1) != np.argmax(y_test[:100], axis=1))]\neps_range = [0] + eps_range\nnb_flag_adv = [flag_original] + nb_flag_adv\nnb_missclass = [2] + nb_missclass\n", "intent": "Evaluate the detector for different attack strengths `eps`\n(**Note**: for the training of detector, `eps=0.05` was used)\n"}
{"snippet": "nb = DummyClassifier(strategy='most_frequent')\nfrom sklearn import cross_validation\nscores = cross_validation.cross_val_score(nb, counts, fixed_target, cv=10)\nprint scores\nprint scores.mean()\n", "intent": "Check CV with our baseline model\n"}
{"snippet": "scores = cross_val_score(linreg, data_2_scaled, y_1, cv = 6)\nprint(\"Cross validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n", "intent": "Linear Regression seems to have performed better than KNN Regression. Let us confirm this with cross validation.\n"}
{"snippet": "predictions = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Using the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:',metrics.mean_absolute_error(y_test,predictions))\nprint('MSE:',metrics.mean_squared_error(y_test,predictions))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,predictions)))\nprint('R^2:',metrics.explained_variance_score(y_test,predictions))\n", "intent": "Linear regression evaluation:\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Predicting values for the testing data.**\n"}
{"snippet": "predictions = svmc.predict(X_test)\n", "intent": "**Getting predictions from the model and creating a confusion matrix and a classification report.**\n"}
{"snippet": "predgrid = gsc.predict(X_test)\n", "intent": "** Creating some predictions using the test set with the gridsearch.**\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Creating predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predictions2 = rfc.predict(X_test)\n", "intent": "** Predicting the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,predictions2))\n", "intent": "**Now creating a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "**Creating predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Calling the new predictions, classification report and confusion matrix:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "knn.predict(id_test)\n", "intent": "We now use our trained model to predict the outcome based on the testing data (this data represents the \"future data sets\")\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        gram_current = gram_matrix(feats[layer])\n        style_loss = style_loss + style_weights[i]*(torch.pow(gram_current - style_targets[i], 2).sum())\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_predict = sentiment_model.predict(sample_test_matrix)\nprint(\"sentiment prediction:\", y_predict)\n", "intent": "Using **sciki-learn** (*predict*):\n"}
{"snippet": "def compute_predict(scores):\n    scores[scores>0] = 1\n    scores[scores<=0] = -1\n    return(scores)\n", "intent": "Using **my own function** (From previously computed scores!):\n"}
{"snippet": "my_y_predict =  compute_predict(np.copy(my_scores))\nprint(\"my_y_predict:\",my_y_predict)\n", "intent": "** Use np.copy() else my_scores are replaced by result of function compute_predict **\nIn Python, **object references are passed by value!!**\n"}
{"snippet": "y_predict_proba = sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "Using **sciki-learn** (*predict_proba*):\n"}
{"snippet": "def compute_predict_proba(scores):\n    proba = 1/(1+np.exp(-scores))\n    return(proba)\n", "intent": "Using **my own function** (From previously computed scores!):\n"}
{"snippet": "test_error = evaluate_classification_error(my_decision_tree, test_data, target)\nprint(\"test_error:\",round(test_error,2))\n", "intent": "Now, let's use this function to evaluate the classification error on the test set.\n"}
{"snippet": "print(\"Validation data, classification error (model 1):\", evaluate_classification_error(model_1, validation_set, target))\nprint(\"Validation data, classification error (model 2):\", evaluate_classification_error(model_2, validation_set, target))\nprint(\"Validation data, classification error (model 3):\", evaluate_classification_error(model_3, validation_set, target))\n", "intent": "Now evaluate the classification error on the validation data.\n"}
{"snippet": "print(\"Validation data, classification error (model 7):\", evaluate_classification_error(model_7, validation_set, target))\nprint(\"Validation data, classification error (model 8):\", evaluate_classification_error(model_8, validation_set, target))\nprint(\"Validation data, classification error (model 9):\", evaluate_classification_error(model_9, validation_set, target))\n", "intent": "Now, let us evaluate the models (**model_7**, **model_8**, or **model_9**) on the **validation_set**.\n"}
{"snippet": "prediction_proba = model_5.predict_proba(sample_validation_data[features])\n", "intent": "For each row in the **sample_validation_data**, what is the probability (according **model_5**) of a loan being classified as **safe**?\n"}
{"snippet": "error_all = []\nfor n in range(1, 31):\n    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n    error = 1.0 - accuracy_score(train_data[target], predictions)\n    error_all.append(error)\n    print(\"Iteration\",n,\"training error =\",error_all[n-1])\n", "intent": "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added.\n"}
{"snippet": "precision_with_default_threshold = precision_score(y_true,predictions_with_default_threshold)\nrecall_with_default_threshold = recall_score(y_true,predictions_with_default_threshold)\nprecision_with_high_threshold = precision_score(y_true,predictions_with_high_threshold)\nrecall_with_high_threshold = recall_score(y_true,predictions_with_high_threshold)\n", "intent": "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:\n"}
{"snippet": "y_baby_true = baby_reviews[\"sentiment\"]\nbaby_matrix = vectorizer.transform(baby_reviews['review_clean'])\nprobabilities = model.predict_proba(baby_matrix)[:,1]\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "w = np.random.normal(0, 1, data.shape[1])[np.newaxis] \nb = np.random.rand()*np.ones((data.shape[0],1)) \neta = 0.05\ny = predict(w, b, data)\ndelta_w = perceptronGradient(labels, y, data)\nw += eta*delta_w\nb += eta*(labels - y).sum()\n", "intent": "One iteration of batch training:\n"}
{"snippet": "mae = (abs(100-90) + abs(50-50) + abs(30+50) + abs(20-30))/4\nprint(mae)\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "Mean of the absolute values of the error\n"}
{"snippet": "mse = ((100-90)**2 + (50-50)**2 + (30-50)**2 + (20-30)**2)/4\nprint(mse)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "Mean of the square of the error\n"}
{"snippet": "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n", "intent": "Evaluate your performance in one line:\n"}
{"snippet": "classes = model.predict(x_test, batch_size=128)\n", "intent": "Or generate predictions on new data:\n"}
{"snippet": "print (treereg.predict([[2001, 200000, 4, 1]]))\n", "intent": "Using your treereg predictive model, predict the value of your car (or a friends car).\n"}
{"snippet": "pred_train = lreg.predict(X_train)\npred_test = lreg.predict(X_test)\n", "intent": "Perform prediction on both the training set and the test set.\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '3rd of March 2001']\ndef run_examples(examples):\n    for example in examples:\n        source = string_to_int(example, Tx, human_vocab)\n        prediction = model.predict(np.array([source]))\n        prediction = np.argmax(prediction[0], axis = -1)\n        output = int_to_string(prediction, inv_machine_vocab)\n        print(\"source:\", example)\n        print(\"output:\", ''.join(output))\nrun_examples(EXAMPLES)\n", "intent": "Try your simple NMT model on various examples using the code below:\n"}
{"snippet": "import regreg.api as rr\nloss = rr.squared_error(X, Y)\nloss\n", "intent": "For a given $X, Y$, here is the squared error loss\n"}
{"snippet": "y_pred_class = knn.predict(X_test)\nprint((metrics.accuracy_score(y_test, y_pred_class))) \n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n", "intent": "The average accuracy remains very close to the Logistic Regression model accuracy; hence, we can conclude that our model generalizes well.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "Precision= 314/(314+96)=0.77, Recall= 314/(314+85)=0.78\n"}
{"snippet": "results = model.predict(test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n", "intent": "Now we get the results and export the .csv file\n"}
{"snippet": "print(classification_report(y_test_raw, y_pred.argmax(axis=1)))\n", "intent": "Here we have the precision-recall report for each of the classes.\n"}
{"snippet": "pred_dt = clf_dt.predict(X_test.reshape(-1,28*28))\nprint('Predicted', len(pred_dt), 'digits with accuracy:', accuracy_score(y_test, pred_dt))\n", "intent": "We now run the trained tree through the test sets and get the prediction accuracy. \n"}
{"snippet": "print(classification_report(y_test, pred_dt))\n", "intent": "Here we have the precision-recall report for each of the classes.\n"}
{"snippet": "print(classification_report(y_test,pred_dt_best_depth))\n", "intent": "Here we have the precision-recall report for each of the classes.\n"}
{"snippet": "pred_dt_new = clf_dt_new.predict(X_test_new)\nprint('Predicted', len(pred_dt), 'digits with accuracy:', accuracy_score(y_test, pred_dt_new))\n", "intent": "Testing the accuracy we get 21.8%. This is is 11.8% bettern than randomly choosing a predicted class.\n"}
{"snippet": "print(classification_report(y_test,pred_dt_new))\n", "intent": "Here we have the precision-recall report for each of the classes.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_model = model.predict(Xtest) \naccuracy_score(ytest, y_model)\n", "intent": "Using the accuracy_score package the accuracy of the just created model is tested using the test data set. \n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "y_ground_truth = np.array([3, 5, 2, 1])\ny_predict = np.array([3, 5, 3, 1])\nerr_mce = sklearn.metrics.zero_one_loss(y_ground_truth, y_predict)\nprint(\"MCE = %2.3f\" % err_mce)\n", "intent": "----\nThe demo shows how to compute mean consequential error value with the 10-class toy sample on the slides.\n"}
{"snippet": "predicted_output = models[25].predict_proba(test_images)\npred_labels = models[25].predict(test_images)\nmisclassified = np.where(test_labels != pred_labels)\nmis_classified = np.asarray(misclassified)[0]\n", "intent": "Thus accuracy increases and the uncertainty decreases with an increase in the number of members in the ensemble.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "probabilities = train_and_predict()\n", "intent": "In this section, we predict probabilities of SWIRE objects and use these probabilities to find the predicted host galaxies of ATLAS objects.\n"}
{"snippet": "data_test[\"predicted_values_withoutpromotion\"] = result_withoutpromotion.predict(data_test)\ndata_test[\"predicted_values_withpromotion\"] = result_withoutpromotion.predict(data_test)\nactual_sales = data_test.groupby(\"Dept\").mean()[\"Weekly_Sales\"]\npredicted_sales_withoutpromotion = data_test.groupby(\"Dept\").mean()[\"predicted_values_withoutpromotion\"]\n", "intent": "<font color = 'green'>The above probability plot indicates that the residuals are normally distributed</font>\n"}
{"snippet": "user_pred = predict(train, user_dist, type='user')\n", "intent": "Generate predictive models for user-based collaborative filtering techniques\n"}
{"snippet": "def getMetrics(truth, pred):\n    discrete_pred = np.argmax(pred, axis=1)\n    results = {}\n    results['logloss'] = metrics.log_loss(truth, pred)\n    results['accuracy'] = metrics.accuracy_score(truth, discrete_pred)\n    results['confusion_matrix'] = metrics.confusion_matrix(truth, discrete_pred) \n    results['precision_recall_f1_support'] = metrics.classification_report(truth, discrete_pred)\n    return(results)\n", "intent": "To evaluate model performance, we create a wrapper function that will report various metrics.\n"}
{"snippet": "measurements = [71, 31, 88, 17.5]\nscaled = scaler.transform(measurements)\nmy_pbf = ridge.predict(scaled)+data['pbf_b'].mean()\nprint(\"My PBF = {:.2f}% +/- {:.2f}%\".format(my_pbf, -scores.mean()))\n", "intent": "Finally, I can predict my own body fat percentage. I'll use the CV error previously calculated as an estimate of the uncertainty.\n"}
{"snippet": "score = model.evaluate(test_data,test_labels)\nprint('test_loss: {}, test_acc:{}'.format(score[0],score[1]))\n", "intent": "And let's see how the model performs. Two values will be returned.\nLoss (a number which represents our error, lower values are better), and accuracy.\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred_test))\nprint('MSE:', metrics.mean_squared_error(y_test, pred_test))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred_test)))\n", "intent": "Accuracy of the model :Linear regression\n"}
{"snippet": " from sklearn import linear_model\nreg = linear_model.Lasso(alpha = 0.1)\nreg.fit (X_train,y_train)\npred_test = reg.predict(X_test)\nfrom sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred_test))\nprint('MSE:', metrics.mean_squared_error(y_test, pred_test))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred_test)))\n", "intent": " Machine learning algorithm : Lasso Regression for Evaluation, prediction\n"}
{"snippet": "print(classification_report(test_labels, predictions))\n", "intent": "Here, the following metrics will be introduced:\n- accuracy\n- f1\n- precision\n- recall\n"}
{"snippet": "train_op = tf.train.AdamOptimizer().minimize(tf.nn.l2_loss(model - Y))\n", "intent": "We then define our optimizer for the newly defined model:\n"}
{"snippet": "tf.nn.l2_loss(model - Y)\n", "intent": "We also note that in this case, the cost function adopted is the following:\n"}
{"snippet": "errors = []\nfor i in range(NUM_EPOCHS):\n    for start, end in zip(range(0, len(x_training), batch_size), range(batch_size, len(x_training), batch_size)):\n        sess.run(train_op, feed_dict={X: x_training[start:end], Y: y_training[start:end]})\n    cost = sess.run(tf.nn.l2_loss(model - y_validation), feed_dict={X:x_validation})\n    errors.append(cost)\n    if i%100 == 0: \n        print(\"epoch %d, cost = %g\" % (i, cost))\n", "intent": "Now we can launch the learning session:\n"}
{"snippet": "thresholds = {'AH': .5, 'DD': .5, 'TA': .5}\nfor m in moods:\n    print(m)\n    t = thresholds[m]\n    y_pred_thresh = (np.array(results_cv[m]['y_prob']) > t).astype(int)\n    print(y_pred_thresh.sum())\n    print(metrics.classification_report(results_cv[m]['y_true'], y_pred_thresh))\n", "intent": "Pick threshold for best precision (tp/(tp+fp)), minimize false positives.\n"}
{"snippet": "for m in tqdm_notebook(moods):\n    f = lambda _: clfs[m].predict_proba(_)\n    n_users['prob_' + m] = n_users['termdoc_' + m].progress_apply(f)\n    s_users['prob_' + m] = s_users['termdoc_' + m].progress_apply(f)    \n", "intent": "Scale termdoc matrix and compute probability of each class for each user.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    tmp = 0\n    for i in range(len(style_layers)):\n        g = gram_matrix(feats[style_layers[i]])\n        tmp += content_loss(style_weights[i], g, style_targets[i])\n    return tmp\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "final_prediction = sv_model.predict(w_test_set)\ntested_mse = mean_squared_error(w_test_labels, final_prediction)\ntested_rmse = np.sqrt(tested_mse)\ntested_rmse\n", "intent": "This looks really promising, the training took almost two hours, so this should work properly.\n"}
{"snippet": "for steps in range(time_steps):\n    ch_sim.run(phi_sim)\n    phi_sim = ch_sim.response\n    phi_prim = prim_model.predict(phi_prim)\n    phi_legendre = leg_model.predict(phi_legendre)\n", "intent": "Once again we are going to march forward in time by feeding the concentration fields back into the Cahn-Hilliard simulation and the MKS models. \n"}
{"snippet": "def classifier_boundary_2d(classifier,X):\n    step_size = 0.01\n    maxs  = X.max()\n    mins  = X.min()\n    x1,x2 = np.meshgrid(np.arange(mins.x1 -1,maxs.x1+1,step_size), np.arange(mins.x2-1,maxs.x2+1,step_size))\n    output = classifier.predict(np.c_[x1.ravel(),x2.ravel()])\n    output = output.reshape(x1.shape)\n    return x1,x2,output\n", "intent": "Plot classifier and test results using same function we developed in last chapter.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(ytrain,ypred_train,target_names=['class0','class1']))\n", "intent": "Can see above that it does not do a great job with the test data. Let's look at classification report:\nTrain data\n"}
{"snippet": "def map_boundary_2d(kmeansobj,data):\n    step_size = 0.01\n    maxs  = data.max()\n    mins  = data.min()\n    x1,x2 = np.meshgrid(np.arange(mins.x1 -1,maxs.x1+1,step_size), np.arange(mins.x2-1,maxs.x2+1,step_size))\n    output = kmeansobj.predict(np.c_[x1.ravel(),x2.ravel()])\n    output = output.reshape(x1.shape)\n    return x1,x2,output\nx1,x2,output = map_boundary_2d(kmeans,data)\n", "intent": "Now visualize the boundaries.\n"}
{"snippet": "ypred = pipe.predict(X)\nscore = pipe.score(X,y[0])\nprint('Score of classifier: ',score)\n", "intent": "Predict outputs for the training data and estimate it's performance:\n"}
{"snippet": "from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(ytest,ypred_test,target_names=['class0','class1','class2']))\nprint('\\nAccuracy score: ',accuracy_score(ytrain,ypred_train))\n", "intent": "Now look at test classification report:\n"}
{"snippet": "hidden_states = model.predict(X.reshape(-1,1))\n", "intent": "Run the predictor to get the hidden states:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"000030.jpg\") \n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "x_test = np.array(['not feeling happy']) \nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['15 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "EXAMPLES = ['1 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "dataset_xtrain = X[train]\ndataset_ytrain = y[train]\ndataset_xtest  = X[test]\ndataset_xtrain_normalized = (dataset_xtrain - dataset_xtrain.mean(axis=0)) / dataset_xtrain.std(axis=0)\ndataset_xtest_normalized = (dataset_xtest - dataset_xtest.mean(axis=0)) / dataset_xtest.std(axis=0)\nprediction_normalized = nn_classify(dataset_xtrain_normalized, dataset_ytrain, dataset_xtest_normalized)\nlabels_names=['1', '2', '3']\nC = skmetrics.confusion_matrix(y_true=y[test], y_pred=prediction_normalized)\nplot_confusion_matrix(C, labels_names)\nprint skmetrics.classification_report(y_true=y[test], y_pred=prediction_normalized)\n", "intent": "<p style=\"background-color:\n"}
{"snippet": "dtree_predictions = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rfc_predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, rfc_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    losses = []\n    for l, A, w in zip(style_layers, style_targets, style_weights):\n        G = gram_matrix(feats[l])\n        losses.append(w * tf.reduce_sum((G - A)**2))\n    return tf.add_n(losses, name='style_loss')\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table>\n    <tr>\n        <td><b>Cost after iteration 9000</b></td>\n        <td>0.218607</td> \n    </tr>\n</table>\n"}
{"snippet": "x_test_fs = x_test.loc[:, [column in ['lcavol', 'lweight'] for column in data.columns]]\ntest_preds_fs = model_fs.predict(x_test_fs)\nmetrics.mean_squared_error(y_test, test_preds_fs)\n", "intent": "That's a fair but worse than 0.4392 that we achieved with all the features.\nBut, let's check out the test set performance\n"}
{"snippet": "results = model.evaluate(X_test, y_test)\n", "intent": "Training and validation accuracy seems to improve after around 60 epochs\n"}
{"snippet": "results = model.evaluate(X_test, y_test)\n", "intent": "Training and validation accuracy improve instantaneously, but reach a plateau after around 30 epochs\n"}
{"snippet": "results = model.evaluate(X_test, y_test)\n", "intent": "Training and validation accuracy improve instantaneously, but reach plateau after around 50 epochs\n"}
{"snippet": "results = model.evaluate(X_test, y_test)\n", "intent": "Training and validation accuracy improve consistently, but reach plateau after around 60 epochs\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_true, y_pred)\n", "intent": "We can look at some metrics like the accuracy score:\n"}
{"snippet": "sklearn.metrics.accuracy_score(pl1_actual_y, pl1_pred_y)\n", "intent": "The `accuracy_score` gives as expected the exact same values as above with our manual case.\n"}
{"snippet": "df_pipeline_input_y_reconstruct = df_pipeline_input_y_categories.map(tmp_saleprice_category_to_median_mapping)\nminimum_reconstruction_error = \\\n    sklearn.metrics.median_absolute_error(df_pipeline_input_y, df_pipeline_input_y_reconstruct)\nminimum_reconstruction_error\n", "intent": "The error introduced alone because of the discretization of the sale price even if you predict the \"label\" correctly is:\n"}
{"snippet": "pl1_reconstruction_error = sklearn.metrics.median_absolute_error(df_pipeline_input_y, pl1_pred_y_mapped)\npl1_reconstruction_error\n", "intent": "And the resulting error is:\n"}
{"snippet": "np.sqrt(sklearn.metrics.mean_squared_error(df_pipeline_input_y, pl1_pred_y_mapped))\n", "intent": "There are other metrics to quantify the error, e.g.:\n"}
{"snippet": "sklearn.metrics.r2_score(df_pipeline_input.SalePrice, pl2_pred_y)\n", "intent": "Let's recreate this by hand. First we need to sub-select the real values from the input for which we have predictions:\n"}
{"snippet": "sklearn.metrics.median_absolute_error(df_pipeline_input.SalePrice, pl2_pred_y)\n", "intent": "We can also use other scoring functions of course:\n"}
{"snippet": "pl2.steps[-1][1].base_classifier.predict_proba(pl2.steps[-1][1].df.iloc[:,:-1].loc[[1],:])\n", "intent": "It does not make an awful lot of sense for a regressor, but we can also look at the label probabilities as follows:\n"}
{"snippet": "print('Adjusted Rand index: ' + str(metrics.adjusted_rand_score(y_actual, y_pred)))\nprint('Adjusted mutual information: ' + str(metrics.adjusted_mutual_info_score(y_actual, y_pred)))\nprint('Homogeneity score: ' + str(metrics.homogeneity_score(y_actual, y_pred)))\nprint('Completeness score: ' + str(metrics.completeness_score(y_actual, y_pred)))\nprint('V-Measure: ' + str(metrics.v_measure_score(y_actual, y_pred)))\nprint('FMI: ' + str(metrics.fowlkes_mallows_score(y_actual, y_pred)))\n", "intent": "On emergent categories on combined data (using the same data on which the model was fit)\n"}
{"snippet": "Resnet50predictions = [np.argmax(Resnet50model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50predictions)==np.argmax(test_targets, axis=1))/len(Resnet50predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred_bag=model_bag.predict(testing_data)\ny_pred_forest=model_forest.predict(testing_data)\ny_pred_adaboost=model_adaboost.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "y_pre_lin=mod_lin_reg.predict(X_test)\ny_pred_dt=mod_dt_reg.predict(X_test)\ny_pred_rf=mod_rf_reg.predict(X_test)\ny_pred_ada=mod_ada_reg.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum((actual-preds)**2)/len(actual)\nprint(mse(y_test, y_pred_dt))\nprint(mean_squared_error(y_test, y_pred_dt))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "index = np.where(log_classifier.classes_ == 1)[0][0]\nprint(log_classifier.classes_, index)\npred_road = np.zeros(train_road_mask.shape)\nfor coord, patch in Train_Data.iterate_raw_image_patches_with_coord(norm=True):\n    patch = patch.reshape([1,-1])\n    pred_road[int(coord[0]+width/2), int(coord[1]+width/2)] = log_classifier.predict_proba(patch)[0, index]\n", "intent": "Predict road prob masks\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    error = predictions - labels\n    abs_error = tf.abs(error)\n    return tf.where(tf.less_equal(abs_error, delta),\n                   0.5 * tf.square(error),\n                   0.5 * tf.square(error) + delta * (abs_error - delta))\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "def compute_loss(X, y, w):\n    X_ = expand(X)\n    right_val = tf.ones_like(y) - tf.multiply(y, tf.matmul(X_, tf.reshape(w, [6,1])))\n    return tf.reduce_mean(tf.maximum(tf.zeros_like(y), right_val))\ndef compute_grad(X, y, w):\n    X_ = expand(X)\n    yx = tf.multiply(y, X_)   \n    cond = tf.less(tf.multiply(w, yx), tf.ones_like(X_))\n    return tf.reduce_mean(tf.where(cond, -yx, tf.zeros_like(X_)), axis=0)\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "def predict(parameters, X):\n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2 > 0.5)\n    return predictions\n", "intent": "Using our model to predict results.\n"}
{"snippet": "model.predict(winePred[100:150,:], verbose=1) \n", "intent": "Predicts the output values of the 100-150th lines of the trained input data\n"}
{"snippet": "from sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\nf1score = f1_score(y_test, preds, average=\"micro\")\nlogloss = log_loss(y_test, proba, labels=GB.classes_)\nprint \"f1 score=\", f1score\nprint \"log loss=\", logloss\n", "intent": "Calculate performance metrics for our test set predictions.\n"}
{"snippet": "unknownBreedsTDM = vec.transform(df.loc[:,\"Breed\"])\nsizes = breedsClf.predict(unknownBreedsTDM.toarray())\ndf[\"Size\"] = sizes\ndf.loc[df[\"AnimalType\"]==\"Cat\", \"Size\"] = 1\ndf[[\"AnimalType\", \"Breed\", \"PureBreed\", \"Size\"]].head(20)\n", "intent": "Generate size estimates for our animals...cats are very uniformly sized and we set them to size 1.\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]] \nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)$$\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the square root of the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "best_result = 0 \nfor i in y.columns:\n    for j in RTL.columns:\n        if roc_auc_score(y[i],RTL[j]) > best_result:\n            best_params = (i, j) \n            best_result = f1_score(y[i],RTL[j])\n", "intent": "find the best values for label and RTL\n"}
{"snippet": "e = (clf.predict(test[:,0:2]) == test[:,2]).sum() \nprint 'number of correct classifications:',e\ne = e / 80.0\nprint 'percentage of correct classifications:',e\n", "intent": "<img src=\"Auswahl_2016-01-11_006.png\">\n"}
{"snippet": "e = (clf.predict(test[:,0:2]) == test[:,2]).sum() \nprint 'number of correct classifications:',e\ne = e / 80.0\nprint 'percentage of correct classifications:',e\n", "intent": "<img src=\"Auswahl_2016-01-11_011.png\">\n"}
{"snippet": "scores = []\nfor model in models:\n    cv_scores =  cross_val_score(model, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n    scores.append(cv_scores.mean())\nscores = np.asarray(scores)\nscores\n", "intent": "**Do a loop do to cross validation on all models.** Returns a numpy array of mean CV scores for each model.\n"}
{"snippet": "prop_score = logistic.predict_proba(X)\nprop_score\n", "intent": "Here is the propensity score (probability of belonging to the class 0 or 1)\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values.\n"}
{"snippet": "y_pred = rf.predict(X_test)\ntest_score = rf.score(X_test, y_test)\nprint('Testing Accuracy = {:.5f}'.format(test_score))\n", "intent": "Cross validation accuracy is very close to training accuracy, hence the model is no overfitting\n"}
{"snippet": "prob = rf.predict_proba(X_test)[:,1]\nprob[prob > 0.06] = 1\nprob[prob <= 0.06] = 0\n", "intent": "Rebuilding the random forest model reducing threshold to classify class 1 as low as 0.06\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(\"GBT\")\nprint(confusion_matrix(gbt_clf.predict(X_test), y_test))\nprint(\"Decision tree\")\nprint(confusion_matrix(tree_clf.predict(X_test), y_test))\nprint(\"kNN tree\")\nprint(confusion_matrix(grid.predict(X_test), y_test))\nprint(\"Random forest\")\nprint(confusion_matrix(rf_clf.predict(X_test), y_test))\n", "intent": "Creating confussion matrix\n"}
{"snippet": "predicted = model_test.predict(X_test)\nprint predicted\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "lstm_preds = lstm_best.predict(testing_seq)\ncnn_preds = cnn_best.predict(testing_seq)\n", "intent": "then predict the labels\n"}
{"snippet": "np.mean((regr.predict(x_test)-y_test)**2)\n", "intent": "$$J_\\theta=-0.1469\\cdot{\\theta}+341.05$$\n"}
{"snippet": "pred = model.predict(X)\n", "intent": "Next, let's call the <code style=\"color:steelblue\">.predict()</code> function.\n"}
{"snippet": "model.predict_proba(X[:10])\n", "intent": "Call <code style=\"color:steelblue\">.predict_proba()</code> on the first 10 observations and display the results.\n"}
{"snippet": "model.predict_proba(X)[0]\n", "intent": "Get the predictions for the first observation.\n"}
{"snippet": "model.predict_proba(X)[0][1]\n", "intent": "Get the probability of **just the positive class** for the first observation.\n"}
{"snippet": "pred = model.predict_proba(X[:10])\n[p[1] for p in pred]\n", "intent": "Use a simple list comprehension to extract a **list of only the predictions for the positive class**.\n"}
{"snippet": "threshold_df['cluster'] = K_means.predict(threshold_df)\nthreshold_df.head()\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">threshold_df</code>.**\n"}
{"snippet": "pca_df['cluster'] = K_means.predict(pca_df)\npca_df.head()\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">pca_df</code>.**\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test,y_pred)\n", "intent": "Now let's test the accuracy of the following model\n"}
{"snippet": "test_data_x = test_data[['Years of Service', '3PA', '3P%', '2P%', 'eFG%', 'FT', 'FTA', 'ORB', 'TRB', \\\n                                  'AST', 'STL', 'BLK', 'TOV', 'Teams', 'AllStars', 'MVPs']]\ntest_data_x[\"HOF?\"] = [x[1] for x in logreg.predict_proba(test_data_x)]\nprint test_data_x[test_data_x[\"HOF?\"] > 0.95][\"HOF?\"].sort_values(ascending=False)\n", "intent": "Reset for the probabilities\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred = nb.predict(Xte)\nprint(classification_report(Yte, pred, target_names=encoder.classes_))\nsns.heatmap(confusion_matrix(Yte, pred))\n", "intent": "**Multinomial Naive Bayesian** works the best. Lets run NB on our test data and get the confusion matrix and its heat map.\n"}
{"snippet": "predicted = hard_classifier.predict(transformed_test_tfidf)\n", "intent": "Predict the test data\n"}
{"snippet": "pred_lm_log = lm_for_houses.predict(X_test)\npred_lm = [math.exp(x) for x in pred_lm_log]\nprint 'Linear Regression: ', rmsle(pred_lm,[math.exp(x) for x in y_test])\npred_rf_log = rf_for_houses.predict(X_test)\npred_rf = [math.exp(x) for x in pred_rf_log]\nprint 'Random Forest: ', rmsle(pred_rf,[math.exp(x) for x in y_test])\npred_gb_log = gb_for_houses.predict(X_test)\npred_gb = [math.exp(x) for x in pred_gb_log]\nprint 'Gradient Boost: ', rmsle(pred_gb,[math.exp(x) for x in y_test])\n", "intent": "Run predictions and print out rmse\n"}
{"snippet": "houses_final = houses.copy()\npred_log = gb_for_houses.predict(houses_predict.drop(['price'],axis=1)) \npred = [round(math.exp(x)) for x in pred_log]\n", "intent": "Do final price prediction, make sure to convert back from log and round to whole number\n"}
{"snippet": "print(classification_report(y_test2, svm_lin_pred2))\n", "intent": "Overall accuracy increased by ~4%! (variation due to differences between each run). Let's evaluate on the test set\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nimport time\nrf_cv = cross_val_score(rf, x_train_scaled, y_train, cv=5)\nprint('Random forest CV score average: %s' %rf_cv.mean())\n", "intent": "Definitely overfitting.\n"}
{"snippet": "rf_test_pred = rf.predict(x_test_scaled)\nrf_test_accu = accuracy_score(rf_test_pred, y_test)\nprint('Random forest test accuracy: %s' %rf_test_accu)\n", "intent": "Cross val seems to indicated not overfitting to as high a degree as anticipated.\n"}
{"snippet": "x_test_95 = pca.transform(x_test_scaled)\npred_95 = rf_95.predict(x_test_95)\naccu_95 = accuracy_score(pred_95, y_test)\nprint('PCA w/ 95%% variance retained test accuracy: %s' %accu_95)\n", "intent": "Takes ~3x training time of unreduced data. \n"}
{"snippet": "test_y_hot = np_utils.to_categorical(test_y, 3)\nevaluation = logistic_regression.evaluate(test_X / 255.0, test_y_hot, verbose=1)[1]\nprint('\\nAccuracy: {:.2f}'.format(evaluation))\n", "intent": "After the training we can check the model performance on the test data using the method `.evaluate()`\n"}
{"snippet": "train_embeddings = model.predict(train_data)\n", "intent": "We now obtain the learned features from the VGG19. Depending of the computer this can be slow.\n"}
{"snippet": "test_embeddings = model.predict(test_data)\n", "intent": "To evaluate the model we need to apply the same transformation to the test data.\n"}
{"snippet": "pred = linear_regression.predict(train_X_norm)\npred = pred.reshape((pred.shape[0],))\nplot_points_regression(train_X_norm,\n                       train_y_norm,\n                       prediction=pred,\n                       title='Real estate prices prediction (test set)',\n                       xlabel=\"m\\u00b2\",\n                       ylabel='$')\n", "intent": "A nontrainded model starts with a random prediction. We always can use the model to predict by using the `.predict()` method.\n"}
{"snippet": "pred = linear_regression.predict(train_X_norm)\npred = pred.reshape((pred.shape[0],))\nplot_points_regression(train_X_norm,\n                       train_y_norm,\n                       prediction=pred,\n                       title='Real estate prices prediction (test set)',\n                       xlabel=\"m\\u00b2\",\n                       ylabel='$')\n", "intent": "After training, we can see the model's prediction quality \n"}
{"snippet": "predictions = tweet_model.predict(x_test)\n", "intent": "Now, let's generate our test predictions:\n"}
{"snippet": "test_data = (x_test, y_test)\nloss, accuracy = model.evaluate(test_data)\nprint('Test loss: {:.2f}'.format(loss))\nprint('Test accuracy: {:.2f}'.format(accuracy))\n", "intent": "Now let's evaluate our model in the test set and understand if our model is generalizing well.\n"}
{"snippet": "def content_loss(current_tensor, computed_ary):\n    _, height, width, number = computed_ary.shape\n    size = height * width * number\n    return tf.sqrt(tf.nn.l2_loss(current_tensor - computed_ary) / size)\n", "intent": "Difference (loss) between content layers of result image and content layers of content image.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets[:sample_set], axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nfrom extract_bottleneck_features import *\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_pred = logreg.predict(X_test)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "accuracy_score(Y_test, Y_pred)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "lables_predicted = logistic_mod.predict(test_images_flatten)\n", "intent": "Now execute the code in the cell below to predict the class of the images in the test data set. \n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2))/len(bos.PRICE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(LogReg, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))\n", "intent": "Cross validation generalize independent dataset, we use cross-validation to \"train\" our data to avoid overfitting.\n"}
{"snippet": "model0.predict(numpy.array(images1))\n", "intent": "And now we give these images to our model and take a look at what the filter has found. \n"}
{"snippet": "embedding_model.predict(numpy.array([[wordvec.vocab['python'].index]]))\n", "intent": "Looks good, right? But let's not waste our time when the computer could tell us definitively and quickly:\n"}
{"snippet": "no_vector_scores = no_vector_model.evaluate(x_test_new_padded, y_test)\nprint('loss: {} accuracy: {}'.format(*no_vector_scores))\n", "intent": "Assess the model. __This takes awhile. You might not want to re-run it.__\n"}
{"snippet": "validation_size = 1500\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))\n", "intent": "Here I extract a validation set, and measuring score and accuracy.\n"}
{"snippet": "predict_knn = knn_model.predict(X_test)\nprint round(metrics.accuracy_score(Y_test,predict_knn),4)\n", "intent": "So this is similar to the logistic regression number.  Let's see what we get out of the test dataset.\n"}
{"snippet": "model.predict(X[0])\n", "intent": "Performing some predictions\n"}
{"snippet": "x_test = np.array(['I do not love you'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "print(\"Ensemble ROC-AUC Score: %.3f\" % roc_auc_score(ytest, P.mean(axis=1)))\n", "intent": "To create an ensemble, we proceed as before and average predictions, and as we might expect the ensemble outperforms the baseline. \n"}
{"snippet": "include = [c for c in P.columns if c not in ['naive bayes']]\nprint('Truncated ensemble ROC-AUC score: %.3f' % roc_auc_score(ytest, P.loc[:, include].mean(axis=1)))\n", "intent": "We can try to improve the ensemble by removing the worst offender, say the Naive Bayes: \n"}
{"snippet": "with tf.name_scope(\"MSE\"):\n    y_true = tf.placeholder(\"float32\", shape=(None,), name=\"y_true\")\n    y_predicted = tf.placeholder(\"float32\", shape=(None,), name=\"y_predicted\")\n    mse = tf.losses.mean_squared_error(labels=y_true, predictions=y_predicted )\ndef compute_mse(vector1, vector2):\n    return mse.eval({y_true: vector1, y_predicted: vector2})\n", "intent": "Your assignment is to implement mean squared error in tensorflow.\n"}
{"snippet": "Resnet50_predictions =[np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = classifier.predict(test_x)\nprint(\"predcition: \", y_pred)\nprint(\"real value: \", test_y)\ncorrect = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\naccuracy = np.mean(correct)\nprint(\"Accuracy\", accuracy)\n", "intent": "we trained our ANN.\nNow time to predict the test batch to check our accuracy\n"}
{"snippet": "y_pred = regressor.predict(test_x)\nprint(\"predcition: \", y_pred)\nprint(\"real value: \", test_y)\ncorrect = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\naccuracy = np.mean(correct)\nprint(\"Accuracy\", accuracy)\n", "intent": "now predict the test data using trained lstm model\n"}
{"snippet": "y_pred = classifier.predict(test_x)\nprint(\"predcition: \", y_pred)\nprint(\"real value: \", test_y)\ncorrect = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\naccuracy = np.mean(correct)\nprint(\"Accuracy\", accuracy)\n", "intent": "now predict the test batch and check the accuracy of model\n"}
{"snippet": "prediction = model.predict(X_test)\n", "intent": "Now let's predict using the trained model. Call `predict` on `X_test`, and print the `confusion_matrix` and the `classification_report`.\n"}
{"snippet": "prediction_final = rfc.predict(X_test)\n", "intent": "Next, create a `RandomForestClassifier`, `fit` it to the training data, and call `predict` on the test features.\n"}
{"snippet": "np.round(accuracy_score(y_test, prediction_final), decimals=2)\n", "intent": "Now, call `accuracy_score` on your test data and your predictions. Print these scores using `np.round` and 2 decimal places.\n"}
{"snippet": "accuracy_score(y_test, y_pred)\nconfusion_matrix(y_test, y_pred) \n", "intent": "Print the `confusion_matrix`, by passing in `y_test` (actual output) and `predictions` (our predictions)\n"}
{"snippet": "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\nacc = accuracy_score(y_test, y_prediction)\nauc = roc_auc_score(y_test, y_prediction_proba[:, 1])\nfpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n", "intent": "Check accuracy of the models and other metrics \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(estimator, X, y, scoring='accuracy')\naverage_accuracy = np.mean(scores) * 100 \nprint(\"The average accuracy is {0:.1f}%\".format(average_accuracy))\n", "intent": "<h3>Running the algorithm</h3>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1)) / len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "[test_cost, test_acc] = model.evaluate(X_test, y_test)\nprint(\"Evaluation result on Test Data : Cost = {}, accuracy = {}\".format(test_cost, (test_acc*100)))\n", "intent": "Now let's see how we did!\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = list(model.predict(test_input_fn))  \ny_pred = [p['max'] for p in predictions]\nacc = accuracy_score(y_pred, test_y)\nprint(\"Accuracy on test set: {:.02%}\".format(acc))\n", "intent": "We can also evaluate the old-fashioned way, by calling `model.predict(...)` and working with the predicted labels directly:\n"}
{"snippet": "print(confusion_matrix(ytest_lr_33, clf_lr_33.predict(Xtest_lr_33)))\nprint(confusion_matrix(ytest_lr_20, clf_lr_20.predict(Xtest_lr_20)))\nprint(confusion_matrix(ytest_lr_50, clf_lr_33.predict(Xtest_lr_50)))\nprint(confusion_matrix(ytest_lr_50, clf_lr_20.predict(Xtest_lr_50)))\n", "intent": "This seems like a good mix of result though False Positive and Flase Negative are quite high.\n"}
{"snippet": "print(\"Accuracy Score ytest: %f\" % accuracy_score(clf_sgd_50.predict(Xtest), ytest))\nprint(\"Accuracy Score ytest: %f\" % accuracy_score(clf_sgd_33.predict(Xtest), ytest))\nprint(\"Accuracy Score ytest: %f\" % accuracy_score(clf_sgd_20.predict(Xtest), ytest))\n", "intent": "The above results look very different from [LogisticRegression Classifier](project_lr.ipynb\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The training and test accuracy cv_score are very close.\n"}
{"snippet": "rf_clf.predict_proba(X_test)[0:10]\n", "intent": "- Accuracy with Random Forest: 0.800 (30% test data)\n"}
{"snippet": "pr = best_ext.predict(test_preds)\nlabs = list(set(test_status))\nprint(labs)\nconfusion_matrix(y_true = test_status, y_pred = pr, labels = labs)\n", "intent": "This is worse than for our Random Forest model on the status variable, but let's look at the confusion matrix.\n"}
{"snippet": "pr_closed = best_ext_closed.predict(test_preds)\nlabs = list(set(test_closed_binary))\nprint(labs)\nconfusion_matrix(y_true = test_closed_binary, y_pred = pr_closed)\n", "intent": "This is a very low accuracy, let's look at the confusion matrix.\n"}
{"snippet": "resnet_predictions = [np.argmax(model_resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "result1 = model_1.evaluate(test_set)\n", "intent": "We can evaluate the performence of the classifier by evaluating it on the test dataset\n"}
{"snippet": "loss, acc = new_model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Check it's accuracy:\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2)) / len(bos.PRICE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "new_shape = (img.shape[0] * img.shape[1], img.shape[2] - 1)\nimg_as_array = img[:, :, :7].reshape(new_shape)\nprint('Reshaped from {o} to {n}'.format(o=img.shape,\n                                        n=img_as_array.shape))\nclass_prediction = rf.predict(img_as_array)\nclass_prediction = class_prediction.reshape(img[:, :, 0].shape)\n", "intent": "With our Random Forest classifier fit, we can now proceed by trying to classify the entire image:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nsklearn_rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(prediction)))\nprint (\"Sklearn calculated RMSLE: {}\".format(sklearn_rmsle))\n", "intent": "All good and great, how about we use the sklearn **Mean Squared Error** + **sqrt** and see if could get the same score?\n"}
{"snippet": "prediction2 = [1.4, 2.3, 3., 4.8, 2.1, 0.7]  \nsklearn_rmsle2 = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(prediction2)))\nprint (\"Sklearn calculated RMSLE: {}\".format(sklearn_rmsle2))\n", "intent": "We could see that they have the same score.\n"}
{"snippet": "cv_score = cross_validation.cross_val_score(forest, data, target2, scoring='roc_auc', cv=5)\n", "intent": "Computing the model's AUC using 5 fold cross validation\n"}
{"snippet": "import sklearn.model_selection as  model_selection\nkfold = model_selection.KFold(n_splits =10,random_state=1452345)\nscores = model_selection.cross_val_score(rf_model, X_train, Y_train, cv=kfold, scoring='accuracy')\nprint(\"Train accuracy %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std()*100))\nfrom sklearn.metrics import accuracy_score,roc_curve, auc\npreds = model_selection.cross_val_predict(rf_model, X_test, Y_test, cv=kfold)\nprint(\"Test accuracy %0.2f\" % (100* accuracy_score(Y_test, preds)))\n", "intent": "**Cross Validation with K=10 folds**\n"}
{"snippet": "from sklearn import model_selection\nscores =model_selection.cross_val_score(CV_log, X_train, Y_train, cv=kfold, scoring='accuracy')\nprint(\"Train accuracy %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std()*100))\nfrom sklearn.metrics import accuracy_score,roc_curve, auc\ncv_preds=list()\npreds = model_selection.cross_val_predict(CV_log, X_test, Y_test, cv=kfold)\nprint(\"Test accuracy %0.2f\" % (100* accuracy_score(Y_test, preds)))\n", "intent": "**Cross Validation with K=10 folds**\n"}
{"snippet": "pred_y_cr_pos = [swn_polarity(doc) for doc in X_cr_pos]\nprint ('In Theater Moviews Critic Review, Positive Reviews: ', accuracy_score(y_cr_pos, pred_y_cr_pos))\npred_y_cr_neg = [swn_polarity(doc) for doc in X_cr_neg]\nprint ('In Theater Moviews Critic Review, Negative Reviews: ', accuracy_score(y_cr_neg, pred_y_cr_neg))\nprint ('In Theater Moviews Critic Review, Summary: ', accuracy_score(np.hstack((y_cr_pos, y_cr_neg)), pred_y_cr_pos+pred_y_cr_neg))\n", "intent": "Compute the Accuracy of the SWN method (Using the Whole DataSet)\nHere we show the prediction accuracy separately, positive and negative.\n"}
{"snippet": "predictions = grid.predict(X_test)\n", "intent": "Prediction time! Fingers crossed!\n"}
{"snippet": "grid.predict(df[df.drop(['Date', 'Close_d+1'], axis=1).columns].loc[17012])\n", "intent": "Alright Alfred, time for a test - What is the closing price for SPY500 yesterday?\n"}
{"snippet": "y_predicted = logistic_model.predict(X)\ny_predicted\n", "intent": "Let's inspect the predictions from the last of these models:\n"}
{"snippet": "metrics.confusion_matrix(y, svc_model.predict(X))\n", "intent": "Again, look at the confusion matrix:\n"}
{"snippet": "test_predict = skgnb.predict(X_test)\ntest_matrix = confusion_matrix(y_test, test_predict)\nprint(test_matrix)\nprint(sum(y_test==0))\nprint(sum(y_test==1))\nprint(sum(test_predict==0))\nprint(sum(test_predict==1))\n", "intent": "Next, I'll use the trained Naive Bayes on the test data and look at the results. \n"}
{"snippet": "pred = tr.predict(features_test)\nprint(\"accuracy :\", round(accuracy_score(pred, labels_test),3))\nprint(\"precision :\", round(precision_score(labels_test, pred), 3))\nprint(\"recall :\", round(recall_score(labels_test, pred), 3))\n", "intent": "The out of the box accuracy of the decision tree algorithm is perfect on the training set. Lets see how the model performs on the test set.\n"}
{"snippet": "mean_absolute_error( trainY, trainY_ )\n", "intent": "**Mean Absolute Error (MAE)**\n"}
{"snippet": "mean_squared_error( trainY, trainY_ )\n", "intent": "**Mean Squared Error (MSE)**\n"}
{"snippet": "math.sqrt( mean_squared_error( trainY, trainY_ ) )\n", "intent": "**Root Mean Squared Error (RMSE)**\n"}
{"snippet": "acc = 0\ny_pred = neunet.predict(Xtest)\ncon_mat = np.zeros((output_dim, output_dim))\nfor i in range(len(y_pred)):\n    con_mat[y_pred[i], ytest[i]] += 1\n    if ytest[i] == y_pred[i]:\n        acc += 1\nacc = acc/len(y_pred)\nprint ('ACCURACY: ', acc)\nprint ('CONFUSION MATRIX: \\n', con_mat)\n", "intent": "Compute the accuracy.\n"}
{"snippet": "acc = 0\ny_pred = neunet2.predict(Xtest)\ncon_mat = np.zeros((output_dim, output_dim))\nfor i in range(len(y_pred)):\n    con_mat[y_pred[i], ytest[i]] += 1\n    if ytest[i] == y_pred[i]:\n        acc += 1\nacc = acc/len(y_pred)\nprint ('ACCURACY: ', acc)\nprint ('CONFUSION MATRIX: \\n', con_mat)\n", "intent": "Compute the accuracy.\n"}
{"snippet": "print(classification_report(y_test, y_pred_logreg, labels=[1, 0]))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\">\n"}
{"snippet": "test_point_index = 4\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 2\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "test_point_index = 2\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 2\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 2\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "from pymks.tools import draw_components_scatter\nstress_predict = model.predict(data_test)\ndraw_components_scatter([model.reduced_fit_data[:, :3],\n                         model.reduced_predict_data[:, :3]],\n                        ['Training Data', 'Testing Data'],\n                        legend_outside=True)\n", "intent": "Now we want to draw how the samples are spread out in PCA space and look at how the testing and training data line up.  \n"}
{"snippet": "y_predict = model.predict(X_new)\n", "intent": "Now let's predict the stress values for the new microstructures. \n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.9f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    num_style_layers = len(style_layers)\n    style_loss = torch.zeros(1).type(dtype)\n    for i in range(num_style_layers):\n        g_matrix = gram_matrix(feats[style_layers[i]])\n        diff = g_matrix - style_targets[i]\n        style_loss +=  style_weights[i]*(torch.norm(diff)*torch.norm(diff))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    content_image = 'styles/tubingen.jpg'\n    image_size =  192\n    tv_weight = 2e-2\n    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Error is {:.9f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "p = figure()\npred = ada_real.predict(X_test)\nx=np.arange(pred.shape[0])\np.circle(x,pred)\nshow(p)\n", "intent": "Test result, seems prediction is okay.\n"}
{"snippet": "def calc_acu_const(thre, mag = arg[0], ratio = arg[2], true=arg[4]):\n    thre = np.array(thre); mag = np.array(mag); ratio = np.array(ratio)\n    st = ratio >= thre; gl = ratio < thre\n    pred = np.ones(len(mag))\n    pred[gl] = 0\n    return -accuracy_score(true, pred)\ninitval = basinhopping(calc_acu_const,  [1.02079894], niter=500, T=1.0, stepsize=0.1)['x'] \nfmin(calc_acu_const, initval)\n", "intent": "Determining the best threshold by maximize the classification accuracy.  \n"}
{"snippet": "dist2_1 = calc_chi_from_thre(0.08606182,  0.39426386, arg[0], 1, arg[2], 1) \nchi2_1 = calc_chi_from_thre(0.08606182,  0.39426386, arg[0], arg[1], arg[2], arg[3]) \nrocDist2_1 = roc_auc_score(true, dist2_1) \nrocChi2_1 = roc_auc_score(true, chi2_1) \nprint 'ROC AUC score using distance is %1.4f' %rocDist2_1\nprint 'ROC AUC score using weighted distance is %1.4f' %rocChi2_1\nfprDist2_1, tprDist2_1, threshDist2_1 = roc_curve(arg[4], dist2_1)\nfprChi2_1, tprChi2_1, threshChi2_1 = roc_curve(arg[4], chi2_1)\nprint 'The informedness using distance is %1.4f' %np.max(tprDist2_1-fprDist2_1)\nprint 'The informedness using weighted distance is %1.4f' %np.max(tprChi2_1-fprChi2_1)\n", "intent": "Threshold = a$\\times$rKronMag + b\n"}
{"snippet": "predictions = kmeans.fit_predict(select_matrix)\nfirst_cluster = np.array(embed[np.where(predictions == 0)])\nsecond_cluster = np.array(embed[np.where(predictions == 1)])\nthird_cluster = np.array(embed[np.where(predictions == 2)])\n", "intent": "Visualizing the clustering\n"}
{"snippet": "predictions = keras_entity_recognizer.predict(df_test)\npredictions[['text', 'label', 'prediction', 'confidences']][:5]\n", "intent": "Apply the trained text classifier to predict entitites in the test dataset \n"}
{"snippet": "evaluator = keras_entity_recognizer.evaluate(predictions)\n", "intent": "Create the evaluator object by applying evaluate on the predictions\n"}
{"snippet": "loaded_keras_entity_recognizer = KerasEntityExtractor.load(pipeline_zip_file)\npredictions = loaded_keras_entity_recognizer.predict(df_test)\nevaluator2 = loaded_keras_entity_recognizer.evaluate(predictions)\nevaluator2.get_metrics('phrase_level_results')[['Recall', 'Prec.', 'F-score']]\n", "intent": "Load the scoring pipeline & evaluate performance of entity predictions on test data\n"}
{"snippet": "df_test = text_classifier.predict(df_test)\ndf_test.head()\n", "intent": "Apply the trained text classifier on the test dataset\n"}
{"snippet": "evaluator = text_classifier.evaluate(df_test)          \nevaluator.plot_confusion_matrix()\n", "intent": "The Evaluation module evaluates the accuracy of the trained text classifier on the test dataset.\n"}
{"snippet": "df_test = text_classifier.predict(df_test)\ndf_test.head()\n", "intent": "Apply the trained text classifier on the test dataset to generate class predictions:\n"}
{"snippet": "predictions = loaded_text_classifier.predict(df_test)\nloaded_evaluator = loaded_text_classifier.evaluate(predictions)\nloaded_evaluator.get_metrics('macro_f1')\n", "intent": "To evaluate a test dataset, apply the loaded text classification pipeline:\n"}
{"snippet": "def evaluate_error(model):\n    pred = model.predict(x_test, batch_size = 32)\n    pred = np.argmax(pred, axis=1)\n    pred = np.expand_dims(pred, axis=1) \n    error = np.sum(np.not_equal(pred, y_test)) / y_test.shape[0]    \n    return error\n", "intent": "One simple way to evaluate the model is to calculate the error rate on the test set.\n"}
{"snippet": "evaluate_error(all_cnn_model)\n", "intent": "Since two models are very similar to each other, it is expected that the error rate doesn't differ much.\n"}
{"snippet": "evaluate_error(nin_cnn_model)\n", "intent": "This is more simple than the other two, so the error rate is a bit higher.\n"}
{"snippet": "loss, accuracy = model.evaluate(test_x, test_y_onehot, verbose=1)\naccuracy\n", "intent": "We should have now model with random accuracy\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision = precision_score(y_true = test_data['sentiment'], \n                            y_pred = model.predict(test_matrix))\nprint \"Precision on test data: %s\" % round(precision, 2)\nprint \"Percentage of false positives on test data: %s\" % round((1 - precision), 2)\n", "intent": "[precision] = [\n            = [\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall = recall_score(y_true = test_data['sentiment'],\n                      y_pred = model.predict(test_matrix))\nprint \"Recall on test data: %s\" % round(recall, 2)\nprint \"Percentage of incorrect predictions of positives on test data: %s\" % round((1 - recall), 2)\n", "intent": "[recall] = [\n         = [\n"}
{"snippet": "baby_matrix = vectorizer.transform(baby_reviews['review_clean'])\nprobabilities = model.predict_proba(baby_matrix)[:,1]\n", "intent": "Now, let's predict the probability of classifying these reviews as positive\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's measure the model's RMSE (root mean squared error)\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for idx, layer in enumerate(style_layers):\n        style_loss += style_weights[idx] * tf.reduce_sum(tf.square(gram_matrix(feats[layer]) - style_targets[idx]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_train_pred_lr = lr.predict(X_train)\naccuracy_lr = len(y_train_pred_lr[y_train_pred_lr == y_train.values])/len(y_train)\nprint(\"Accuracy of logistic regression on train data is \", accuracy_lr*100, \"%\")\n", "intent": "**4b. Accuracy** \n- Can be used to make overall assessement\n- Ratio of total correct classification to the total number of classificaitons performed\n"}
{"snippet": "X_test, y_test = d_enc.iloc[idx_test].drop('response', axis=1), d_enc.iloc[idx_test]['response']\npredictions = est.predict(reorder(X_test).values.astype(np.float32))\ny_proba = predictions['overall_proba']\ny_proba_dem = predictions['demo_proba']\ny_proba_arr = predictions['protein_array_proba']\ny_pred = np.where(y_proba >= .5, 1, 0)\nscores = get_scores(y_pred, y_proba)\nprint('Hybrid model scores on test data:')\npd.Series(scores)\n", "intent": "Now check to see how well this model performs compared to our baselines from before:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(estimator=pipe_lr,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=1)\nprint('CV accuracy scores: %s' % scores)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n                                      np.std(scores))) \n", "intent": "Built in kfold cv evaluator: sklearn\n"}
{"snippet": "phrase = ['clinton', 'trump', 'bird', 'make america great again', 'her emails']\nvectorized = candidate_forest_model.named_steps['tfidfvectorizer'].transform(phrase)\nclasses = candidate_forest_model.named_steps['randomforestclassifier'].classes_\nprobs = candidate_forest_model.named_steps['randomforestclassifier'].predict_proba(vectorized)\nprint(classes, '\\n', probs)\n", "intent": "> Importance of Features per Candidate\n"}
{"snippet": "a = CNN_G.predict(noise(sz))\na.shape\n", "intent": "We train D a \"little bit\" so it can at least tell a real image from random noise.\n"}
{"snippet": "predictions = [np.argmax(peri_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "threshold = (0.5 - 0.166057850741)/0.00000120547524\nprint threshold\nlinreg.predict([277021.159936])\n", "intent": "y = mX + b\n0.5 = 0.00000120547524X + 0.166057850741\nX = (0.5 - 0.166057850741)/0.00000120547524\n"}
{"snippet": "print 'The root mean square error for power law fit =', np.sqrt(mean_squared_error(hy, fpower(hx, *popt)))\nprint 'The root mean square error for exponential fit =',np.sqrt(mean_squared_error(hy, fexp(hx, *popt2)))\n", "intent": "Using root mean squared error as the error metric\n"}
{"snippet": "adjusted_rand_score(base_df.cluster, threshold_df.cluster)\n", "intent": "(similarity measure between two clusterings)\n"}
{"snippet": "pred_Resnet = [np.argmax(model_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(pred_Resnet)==np.argmax(test_targets, axis=1))/len(pred_Resnet)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "holdout_predictions = lr.predict(holdout[columns])\n", "intent": "Then we will run the Testing data that we have saved in `holdout` earlier\n"}
{"snippet": "pred=dtree.predict(X_test)\n", "intent": "**Creating predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "prediction= rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predicting the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print('Classification Report')\nprint(classification_report(y_test,prediction))\n", "intent": "**Now create a classification report from the results.**\n"}
{"snippet": "vgg.predict(imgs,True)\n", "intent": "Let's predict the species of each image.\n"}
{"snippet": "print(get_roc_score(train, test))\n", "intent": "To spare you the graphs, right now I'll just calculate the ROC score:\n"}
{"snippet": "test_loss, test_acc = model_drop.evaluate(test_images, test_labels)\nprint('Test set loss:', test_loss)\n", "intent": "- Calcuate the test set loss and accuracy. How well does your model perform to the baseline from chapter 2.1 in the book?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(clf.predict(features_test), labels_test))\n", "intent": "How well is the classifier doing?\nAccuracy: correctly points/ total number of points\n"}
{"snippet": "myIndices = [10, 26, 50]\nfeatures_small_test = [features_test[i] for i in myIndices]\nprint \"for email 10,26,50 predicted: \",clf.predict(features_small_test),\" \"\n", "intent": "Predict labels authors for various email\n"}
{"snippet": "logreg.predict(1.4)\n", "intent": "We can also simply call `logreg.predict()` to make predictions, here we use 1.4 hours to show that we get the same result\n"}
{"snippet": "logreg.predict_proba(1.4)\n", "intent": "We can likewise call `logreg.predict_proba()` to return the predicted probabilities for each class (passing/failing)\n"}
{"snippet": "predict = nb.predict(text_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "pipe_prediction = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "mean_sq_error = (np.mean((bos.PRICE - lm.predict(X)) ** 2))\nmean_sq_error\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "clf.predict_proba(vectorizer.transform(['This movie is not remarkable, touching, or superb in any way']))\n", "intent": "No, because the \"not\" negates the whole sentiment of remarkable, touching, and superb. It clearly is a rotten review.\n"}
{"snippet": "RMSE_ridge_train = sqrt(np.mean((Y_train - ridge_reg.predict(X_train))**2))\nRMSE_ridge_test = sqrt(np.mean((Y_test - ridge_reg.predict(X_test))**2))\nprint('Root mean squared error for train data is: %.3f' %(RMSE_ridge_train))\nprint('Root mean sqaured error for test data is: %.3f' %(RMSE_ridge_test))\n", "intent": "Let us calculate the RMSE values for train and test data sets\n"}
{"snippet": "R2_ridge_grid_train = r2_score(ridge_grid.predict(X_train), Y_train)\nR2_ridge_grid_test = r2_score(ridge_grid.predict(X_test), Y_test)\nprint('R squared for train data is: %.3f' %(R2_ridge_grid_train))\nprint('R squared for test data is: %.3f' %(R2_ridge_grid_test))\n", "intent": "Let us compare the R2 scores for train and test data using GridSearchCV\n"}
{"snippet": "RMSE_ridge_grid_train = sqrt(mean_squared_error(ridge_grid.predict(X_train), Y_train))\nRMSE_ridge_grid_test = sqrt(mean_squared_error(ridge_grid.predict(X_test), Y_test))\nprint('Root mean squared error for train data is: %.3f' %(RMSE_ridge_grid_train))\nprint('Root mean sqaured error for test data is: %.3f' %(RMSE_ridge_grid_test))\n", "intent": "Let us compare the RMSE scores for train and test data using GridSearchCV\n"}
{"snippet": "RMSE_train = sqrt(np.mean((Y_train - lasso_reg.predict(X_train))**2))\nRMSE_test = sqrt(np.mean((Y_test - lasso_reg.predict(X_test))**2))\nprint('Root mean squared error for train data is: %.3f' %(RMSE_train))\nprint('Root mean sqaured error for test data is: %.3f' %(RMSE_test))\n", "intent": "Now let us calculate the RMSE scores for train and test data sets\n"}
{"snippet": "R2_lasso_grid_train = r2_score(lasso_grid.predict(X_train), Y_train)\nR2_lasso_grid_test = r2_score(lasso_grid.predict(X_test), Y_test)\nprint('R squared for train data is: %.3f' %(R2_lasso_grid_train))\nprint('R squared for test data is: %.3f' %(R2_lasso_grid_test))\n", "intent": "Let us compare the R2 scores for train and test data using GridSearchCV\n"}
{"snippet": "RMSE_lasso_grid_train = sqrt(mean_squared_error(lasso_grid.predict(X_train), Y_train))\nRMSE_lasso_grid_test = sqrt(mean_squared_error(lasso_grid.predict(X_test), Y_test))\nprint('Root mean squared error for train data is: %.3f' %(RMSE_lasso_grid_train))\nprint('Root mean sqaured error for test data is: %.3f' %(RMSE_lasso_grid_test))\n", "intent": "Let us compare the RMSE scores for train and test data using GridSearchCV\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nX = df.loc[: , ['balance','income','student']]\ny = df['default']\nscores = cross_val_score(regr, X, y, cv = 10)\nprint(scores)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "nn_pred = neural_net.predict(X_test)\n", "intent": "99.6% training accuracy\n"}
{"snippet": "r2score = r2_score(house_data['price'], lin_reg_predictions)\nr2score\n", "intent": "Seems like a fair fit, given the distribution of prices.\n"}
{"snippet": "r2score2 = r2_score(np.log(house_data['price']), lin_reg_predictions2)\nr2score2\n", "intent": "The residuals are normally distributed.\n"}
{"snippet": "kf = KFold(n_splits=10)\nX = house_data_processed\ny = np.log(house_data['price'])\nnonreg = np.mean(cross_val_score(lin_reg, X, y, cv=kf))\nelastic = np.mean(cross_val_score(elastic_net, X, y, cv=kf))\nprint(\"Non regularised 10 fold CV score: {}\".format(nonreg))\nprint(\"Elastic Net 10 fold CV score: {}\".format(elastic))\n", "intent": "Slight performance decrease; however effect on variance and bias is unclear.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nadvanced_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'zipcode','condition','grade','waterfront','view','sqft_above','sqft_basement','yr_built','yr_renovated','lat', 'long','sqft_living15','sqft_lot15']\nRMSE = mean_squared_error(test_data['price'], lm.predict(test_data['sqft_living'].reshape(4323,1)))**0.5\nprint 'simple RMSE:', RMSE\n", "intent": "<img src = images/rmse.png>\n"}
{"snippet": "def normalize(lst):\n    s = sum(lst)\n    return  [i/s for i in lst]\nnorm_preds = normalize(predictions)\npolarity = [1 if preds[0] - preds [1] > 0  else -1 for preds in norm_preds]\ntest['Predictions'] = polarity\ntest_temp = test[test.Sentiment != 0]\nprint(accuracy_score(test_temp.Sentiment,test_temp.Predictions))\nprint(classification_report(test_temp.Sentiment,test_temp.Predictions))\nprint(pd.crosstab(test_temp.Sentiment,test_temp.Predictions))\n", "intent": "Normalizing the prediction and only comparing positive and negative classes\n"}
{"snippet": "from sklearn.metrics import accuracy_score, classification_report\nprint(accuracy_score(training_df.Polarity,training_df.LEX))\nprint(classification_report(training_df.Polarity,training_df.LEX))\nprint(pd.crosstab(training_df.Polarity, training_df.LEX))\n", "intent": "*Same process but with a SVM classifier - the results not tested however*\n"}
{"snippet": "temp = ['I do not like SMRT', 'SMRT is just not bad']\npipeline.predict(temp)\n", "intent": "The problem of the dataset being not labelled properly can also be seen here\n"}
{"snippet": "mse = mean_squared_error(ytrue, yfit)\nprint(mse)\n", "intent": "Print out the misfit using the mean squared error.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipe, X, y, cv=5)\nscores \n", "intent": "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(collegedata['Cluster'],kmeans.labels_))\nprint(classification_report(collegedata['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print( np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simply the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint (confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier loses a little (~1%) in accuracy but it reduces overfitting by a lot.\n"}
{"snippet": "examples = ['i am too fat ', 'This girl is messing with me', 'You are my wife', \n            'I am looking forward to working in his team', 'it raining now but I am good', 'best shitty thing ever']\nx_test = np.array(examples)\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\npred = model.predict(X_test_indices)\nfor i in range(len(examples)):\n    print(x_test[i] +' '+  label_to_emoji(np.argmax(pred[i])))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "wgts = np.zeros(100)\nneval_pts = 1500000\neval_pts = np.random.randn(neval_pts,20)\nii,unordered_wgts = np.unique(kmeans_model.predict(eval_pts), return_counts=True)\nunordered_wgts = unordered_wgts/ float(unordered_wgts.sum())\nfor k,ic in enumerate(ii):\n    wgts[ic] = unordered_wgts[k]\n", "intent": "Compute the probability weights to be assigned to each cluster center\n"}
{"snippet": "mse = np.mean((dataset.PRICE - lm.predict(x_param)) ** 2)\nmse\n", "intent": "a. Review the following to estimate the *mean squared error*:\n"}
{"snippet": "residuals = dataset['PRICE'] - lm.predict(x_param)\nprint(\"Head of residual: {}\".format(residuals[:5]))\nprint(\"Mean of residual: {}\".format(np.mean(residuals)))\nprint(\"SD of residual: {}\".format(np.std(residuals)))\n", "intent": "b. Review the following to estimate the *residuals*:\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n", "intent": "We'll need to calculate root mean squared error for the data to figure out margin of error.\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat == y1)\nprint(\"Accuracy on training data = %f\" % acc)\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "y_test_pred = regr.predict(Xts)\nRSS_test = np.mean((y_test_pred-yts)**2)/(np.std(yts)**2)\nRsq_test = 1-RSS_test\nprint(\"RSS per sample = {0:f}\".format(RSS_test))\nprint(\"R^2 = {0:f}\".format(Rsq_test))\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "x_test = np.array(['feeling'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nprint(accuracy_score(test_labels, pred_test))\nprint(accuracy_score(valid_labels, valid_test))\nprint(f1_score(test_labels, pred_test, average='macro'))\nprint(f1_score(valid_labels, valid_test, average='macro'))\n", "intent": "Training on 5000 training Examples \n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nprint(accuracy_score(test_labels, pred_test))\nprint(accuracy_score(valid_labels, valid_test))\nprint(f1_score(test_labels, pred_test, average='macro'))\nprint(f1_score(valid_labels, valid_test, average='macro'))\n", "intent": "Training on all training Examples \n"}
{"snippet": "def show_report(model, X, y):\n    y_predicted = np.argmax(model.predict(X), axis=1)\n    y_true = np.argmax(y, axis=1)\n    print(\"Confusion matrix (rows: true, columns: predicted)\")\n    print(confusion_matrix(y_true, y_predicted))\n    print(\"\")\n    print(\"Classification report\")\n    print(classification_report(y_true, y_predicted))\n", "intent": "A function to display a confusion matrix and a classification report.\n"}
{"snippet": "show_report(model, X_dev, y_dev)\n", "intent": "model  and weights are saved \n"}
{"snippet": "test_predstest_pre  = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_predstest_pre)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr.predict(X_train_level2) \nr2_train_stacking =  r2_score(y_train_level2, train_preds) \ntest_preds =  lr.predict(X_test_level2) \nr2_test_stacking = r2_score(y_test, test_preds) \nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred),\n                                      mean_squared_error(y_test, y_test_pred)))\n", "intent": "$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^2\n$$\n"}
{"snippet": "import math\nprint('MSE train: %.3f, test: %.3f' % (math.sqrt(mean_squared_error(y_train, y_train_pred)),\n                                      math.sqrt(mean_squared_error(y_test, y_test_pred))))\n", "intent": "$$\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^2}\n$$\n"}
{"snippet": "X_grid = np.linspace(0, 20, 1000)\ny_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\ny_pred, y_std = y_pred.ravel(), y_std.ravel()\n", "intent": "The initial regressor is not very accurate.\n"}
{"snippet": "y_pred, y_std = optimizer.predict(X, return_std=True)\ny_pred, y_std = y_pred.ravel(), y_std.ravel()\nX_max, y_max = optimizer.get_max()\n", "intent": "Using *expected improvement*, the first five queries are the following.\n"}
{"snippet": "predictions = learner.predict(X_raw)\nis_correct = (predictions == y_raw)\npredictions\n", "intent": "Let's see how our classifier performs on the initial training set!\n"}
{"snippet": " def softmax_loss(x, y):\n    L_i = np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True)\n    loss = np.mean(-np.log([L_i[i, y[i]] for i in range(len(y))]))\n    dx = L_i\n    dx[range(len(y)),y] -= 1\n    dx /= len(y)\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"./img/loss.png\" width=\"300\">\n<img src=\"./img/log.png\" width=\"600\">\n"}
{"snippet": "df[\"pred_price\"] = clf.predict(df.values)\n", "intent": "Add the predicted price column\n"}
{"snippet": "print('Random chance score: ' + str(1/len(set(targets))))\nsvm_scores = cross_val_score(svm_classifier_naive, naive_BoW, targets_naive, cv=5)\nprint('SVM scores: ' + str(svm_scores))\ndt_scores = cross_val_score(dt_classifier_naive, naive_BoW, targets_naive, cv=5)\nprint('DT scores:  ' + str(dt_scores))\nrf_scores = cross_val_score(rf_classifier_naive, naive_BoW, targets_naive, cv=5)\nprint('RF scores:  ' + str(rf_scores))\nnn_scores = cross_val_score(nn_classifier_naive, naive_BoW, targets_naive, cv=5)\nprint('NN scores:  ' + str(nn_scores))\n", "intent": "                                       naive approach: uses everything\n"}
{"snippet": "svm_scores = cross_val_score(svm_classifier_tfidf, tfidf_BoW, targets_tfidf, cv=5)\nprint('SVM scores: ' + str(svm_scores))\ndt_scores = cross_val_score(dt_classifier_tfidf, tfidf_BoW, targets_tfidf, cv=5)\nprint('DT scores:  ' + str(dt_scores))\nrf_scores = cross_val_score(rf_classifier_tfidf, tfidf_BoW, targets_tfidf, cv=5)\nprint('RF scores:  ' + str(rf_scores))\nnn_scores = cross_val_score(nn_classifier_tfidf, tfidf_BoW, targets_tfidf, cv=5)\nprint('NN scores:  ' + str(nn_scores))\n", "intent": "                            TDF-IDF Comparison\n"}
{"snippet": "svm_scores_no_comment = cross_val_score(svm_classifier_no_comment, no_comment_BoW, targets_no_comment, cv=5)\nprint('SVM scores: ' + str(svm_scores_no_comment))\ndt_scores_no_comment = cross_val_score(dt_classifier_no_comment, no_comment_BoW, targets_no_comment, cv=5)\nprint('DT scores:  ' + str(dt_scores_no_comment))\nrf_scores_no_comment = cross_val_score(rf_classifier_no_comment, no_comment_BoW, targets_no_comment, cv=5)\nprint('RF scores:  ' + str(rf_scores_no_comment))\nnn_scores_no_comment = cross_val_score(nn_classifier_no_comment, no_comment_BoW, targets_no_comment, cv=5)\nprint('NN scores:  ' + str(nn_scores_no_comment))\n", "intent": "                                        naive approach: ignores comments\n"}
{"snippet": "svm_scores_op = cross_val_score(svm_classifier_op, op_BoW, targets_op, cv=5)\nprint('SVM scores: ' + str(svm_scores_op))\ndt_scores_op = cross_val_score(dt_classifier_op, op_BoW, targets_op, cv=5)\nprint('DT scores:  ' + str(dt_scores_op))\nrf_scores_op = cross_val_score(rf_classifier_op, op_BoW, targets_op, cv=5)\nprint('RF scores:  ' + str(rf_scores_op))\nnn_scores_op = cross_val_score(nn_classifier_op, op_BoW, targets_op, cv=5)\nprint('NN scores:  ' + str(nn_scores_op))\n", "intent": "                                    only operators and keywords used\n"}
{"snippet": "svm_scores_ansii = cross_val_score(svm_classifier_ansii, ansii_BoW, targets_ansii, cv=5)\nprint('SVM scores: ' + str(svm_scores_ansii))\ndt_scores_ansii = cross_val_score(dt_classifier_ansii, ansii_BoW, targets_ansii, cv=5)\nprint('DT scores:  ' + str(dt_scores_ansii))\nrf_scores_ansii = cross_val_score(rf_classifier_ansii, ansii_BoW, targets_ansii, cv=5)\nprint('RF scores:  ' + str(rf_scores_ansii))\nnn_scores_ansii = cross_val_score(nn_classifier_ansii, ansii_BoW, targets_ansii, cv=5)\nprint('NN scores:  ' + str(nn_scores_ansii))\n", "intent": "                           only operators, keywords, and ANSII functions used\n"}
{"snippet": "svm_scores_posix = cross_val_score(svm_classifier_posix, posix_BoW, targets_posix, cv=5)\nprint('SVM scores: ' + str(svm_scores_posix))\ndt_scores_posix = cross_val_score(dt_classifier_posix, posix_BoW, targets_posix, cv=5)\nprint('DT scores:  ' + str(dt_scores_posix))\nrf_scores_posix = cross_val_score(rf_classifier_posix, posix_BoW, targets_posix, cv=5)\nprint('RF scores:  ' + str(rf_scores_posix))\nnn_scores_posix = cross_val_score(nn_classifier_posix, posix_BoW, targets_posix, cv=5)\nprint('NN scores:  ' + str(nn_scores_posix))\n", "intent": "                           only operators, keywords, and Posix functions used\n"}
{"snippet": "svm_scores_op_funct = cross_val_score(svm_classifier_op_funct, op_funct_BoW, targets_op_funct, cv=5)\nprint('SVM scores: ' + str(svm_scores_op_funct))\ndt_scores_op_funct = cross_val_score(dt_classifier_op_funct, op_funct_BoW, targets_op_funct, cv=5)\nprint('DT scores:  ' + str(dt_scores_op_funct))\nrf_scores_op_funct = cross_val_score(rf_classifier_op_funct, op_funct_BoW, targets_op_funct, cv=5)\nprint('RF scores:  ' + str(rf_scores_op_funct))\nnn_scores_op_funct = cross_val_score(nn_classifier_op_funct, op_funct_BoW, targets_op_funct, cv=5)\nprint('NN scores:  ' + str(nn_scores_op_funct))\n", "intent": "                     only operators, keywords, Posix, and ANSII functions used\n"}
{"snippet": "regr.predict(95)\n", "intent": "**4. Extrapolate data:  If the ground temperature reached 95&deg; F, then at what approximate rate would you expect the crickets to be chirping?**\n"}
{"snippet": "print('predicted y value: ', sgd_clf.predict([X_test[111]]))\nprint('actual y value: ', y_test[111])\n", "intent": "With the trained model, the data scientist can predit the result of the test data.\n"}
{"snippet": "print (\"Loss, Accuracy\")\nmodel_top.evaluate(validation_data, validation_labels)\n", "intent": "Loss and accuracy :\n"}
{"snippet": "def style_loss(x, targ):\n    return K.mean((gram_matrix(x) - gram_matrix(targ))**2)\n", "intent": "Style loss is very similar to the content loss above, but it is the MSE between gram matrices instead of the raw activations.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(estimator=classification_pipeline, X=X_train, y=Y_train, cv=10, n_jobs=1)\nimport numpy as np\nprint \"CV Accuracy %.3f +/- %.3f\" % (np.mean(scores), np.std(scores))\n", "intent": "Instead of just a single test on test data, let's do a startified k-fold cross validation on training data\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint \"Precision: %.3f\" % precision_score(y_true=Y_test, y_pred=y_pred)\nprint \"Recall: %.3f\" % recall_score(y_true=Y_test, y_pred=y_pred)\nprint \"F1: %.3f\" % f1_score(y_true=Y_test, y_pred=y_pred)\n", "intent": "Now we will get the same statistics for the positive class\n"}
{"snippet": "predicted_price_non_std = slr.predict(np.array(num_rooms).reshape(len(num_rooms), 1))\nprint \"Predicted Price in $1000s using non-standardized data: %.3f\" % predicted_price_non_std\n", "intent": "Now let's check the prediction for non-standardized data. The prediction for both model will give the exact same results.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nMSE1 = mean_squared_error(y_true=y, y_pred=slr.predict(X))\nr2_1 = r2_score(y_true=y, y_pred=slr.predict(X))\nprint \"Mean Square Error for regression model using non-standardized feature: %.3f\" % MSE1\nprint \"Root Mean Square Error: \" , np.sqrt(MSE1)\nprint \"R2: \", r2_1\n", "intent": "Evaluate the performance of the regression model on training data. First, the non-standardized dataset.\n"}
{"snippet": "r2_scores = cross_val_score(forest, X_train, y_train, scoring='r2', cv=10)\nprint \"Cross validated R2 for Random Forest Regressor: \", r2_scores\nprint \"Mean cross validated R2: \", np.mean(r2_scores)\nprint \"STDV for cross validated R2: \", np.std(r2_scores)\n", "intent": "Notice that cross validated MSE scores are reported as negative numbers in sklearn\n"}
{"snippet": "yhat = lr.predict(X_te)\n(yhat != y_te).mean()\n", "intent": "We can then predict on the test set and see what the resulting 0-1 test error is.\n"}
{"snippet": "if not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n", "intent": "After we've fit a model as above, we can save its parameters to disk and then use it later on to make predictions, as below.\n"}
{"snippet": "print(\"Test accuracy = %3.3f\"%(model.evaluate(x_test_in, y_test_in)[1]))\n", "intent": "Also, lets evaluate the model using the test dataset! \n"}
{"snippet": "w, c = multiple_linear_regression_fit(x_train, y_train)\nr_squared, _ = multiple_linear_regression_score(w, c, x_test, y_test)\n", "intent": "> YOUR TURN NOW\n>Fit the training data and calculate the `r_squared` on the test set. Store the list of coefficients in `w` and the intercept in `c`.\n"}
{"snippet": "r_squared_train, _ = multiple_linear_regression_score(w, c, x_train, y_train)\nprint('R^2 score on train set:', r_squared_train)\n", "intent": "We run this on the training set as well..\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "With sklearn.metrics, errors can be easily calculated\n* MAE  - Mean Absolute Error\n* MSE  - Mean Squared Error\n* RMSE - Root Mean Squared Error. \n"}
{"snippet": "print(clf.predict(test_features))\nprint(test_target)\n", "intent": "Validate the values against the rows we set aside for testing\n"}
{"snippet": "weight_avg=logit_lv3_train_pred*0.5\nprint(auc_to_gini_norm(roc_auc_score(y_train, weight_avg)))\n", "intent": "**Average L3 outputs & Submission Generation\n**\n"}
{"snippet": "train_accuracy = 1 - np.sum(np.abs(clf.predict(X_train) - Y_train))/len(Y_train)\nprint('Accuracy on the train dataset is '+ str(train_accuracy))\n", "intent": "Ok, we fitted a classifier, but how should we evaluate performance? First let's look at the accuracy on the train dataset: it better be good!!!\n"}
{"snippet": "test_accuracy = 1 - np.sum(np.abs(clf.predict(X_test)-Y_test))/len(Y_test)\nprint('Accuracy on the test dataset is '+ str(test_accuracy))\n", "intent": "But what we want to know is how the method performs on the test dataset, whose labels we have not seen in training.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_score = clf.predict_proba(X_test)[:,1]\nroc_auc_score(Y_test, y_score)\n", "intent": "The ROC curve allows explore how the algorithm performs for different decison thresholds.\n"}
{"snippet": "show_classification_report(model1, X_test, y_test)\n", "intent": "**Simple RNN on the test set:**\n"}
{"snippet": "show_classification_report(model2, X_test, y_test)\n", "intent": "**LSTM on the test set:**\n"}
{"snippet": "show_classification_report(model3, X_test, y_test)\n", "intent": "**Conv1d + LSTM on the test set:**\n"}
{"snippet": "def r2Score(x,y):\n    z = metrics.r2_score(x, y)\n    return z\ndef rootmeanSqError(x,y):\n    final_mse = mean_squared_error(x, y)\n    final_rmse = np.sqrt(final_mse)\n    return final_rmse\n", "intent": "1. r2score -- to calculate the r2 score for predicted value\n2. rootmeanSqError -- to calculate the root mean score error for predicted value\n"}
{"snippet": "decisionTreeReg_M = decisionTree(False,car_train_M, price_train_M)\ndecisionTreeReg_Out_M = decisionTree_Predict(decisionTreeReg_M, car_test_M)\ndecisionTreeReg_r2_M = r2Score(decisionTreeReg_Out_M,price_test_M)\ndecisionTreeReg_rmse_M = rootmeanSqError(decisionTreeReg_Out_M,price_test_M)\ndecisionTreeReg_Out_I = decisionTree_Predict(decisionTreeReg_M, car_test_I)\ndecisionTreeReg_r2_I = r2Score(decisionTreeReg_Out_I,price_test_I)\ndecisionTreeReg_rmse_I = rootmeanSqError(decisionTreeReg_Out_I,price_test_I)\nprint(\"rmse: M--\",decisionTreeReg_rmse_M, \", I--\",decisionTreeReg_rmse_I )\nprint(\"r2: M--\",decisionTreeReg_r2_M, \", I--\",decisionTreeReg_r2_I)\n", "intent": " fitting the decision tree regression model on car_modified train data and predicting the price for both car_Modified test and car_Initial test\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=tf.constant(0.0)\n    for i in range(len(style_layers)):\n        G=gram_matrix(feats[style_layers[i]])\n        A=style_targets[i]\n        style_loss+=style_weights[i]*tf.reduce_sum((G-A)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "data, target = import_knowledge_dataset()\nbins_N = 8   \nfolds_number = [2, 4, 6, 8, 10]\nscores = cross_validation_test(data, target, StratifiedKFold(n_splits=9), False)\nprint(return_f1_macro_mean_score(scores))\n", "intent": "Wine - F1-Score, KFold & Discretization ways\n==================\n"}
{"snippet": "y_prob = gnb.predict_proba(X)\nprint(y_prob.shape)\ny_prob\n", "intent": "Building an ROC curve requires to use the probability estimates for the test data points *before* they are thresholded.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndef answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    pred = answer_seven()\n    return accuracy_score(y_test,pred)\nanswer_eight()\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "stacked=np.hstack([clf_bayes.predict_proba(X), clf_huber.predict_proba(X)])\n", "intent": "From the output probability of bayes and svm will be stacked column-wise\n"}
{"snippet": "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\nD_train_step = D_solver.minimize(D_loss, var_list=D_vars)\nG_train_step = G_solver.minimize(G_loss, var_list=G_vars)\nprint(\"OK\")\n", "intent": "Create new training steps so we instead minimize the LSGAN loss:\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\nprint(\"OK\")\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    result=tf.zeros([1])\n    for i in range(len(style_layers)):        \n        result+=style_weights[i]*tf.norm((gram_matrix(feats[style_layers[i]])-style_targets[i]))**2\n    return result    \n    pass\nprint(\"OK\")\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\nprint(\"OK\")\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "import numpy as np\nprint ('Residual sum of squares: %.2f' % np.mean((model.predict(X)- y) ** 2))\n", "intent": "<img src=\"Images/Resedual_Error.jpg\" width=\"80%\">\n"}
{"snippet": "y_pred_nb = model_nb.predict(X_test)\n", "intent": "Predict on the model using the X_test test data.\n"}
{"snippet": "print(\"Accuracy of Naive Bayes classifier on test set is\" , accuracy_score(y_test, y_pred_nb))\n", "intent": "Find accuracy of the model on test set.\n"}
{"snippet": "print(\"************** Naive Bayes Classification Report **************\")\nprint(classification_report(y_test, y_pred_nb))\n", "intent": "Classification Report showing Class wise Precision, Recall and F1 Score\n"}
{"snippet": "y_pred_knn = model_knn.predict(X_test)\n", "intent": "Predict on the model using the X_test test data.\n"}
{"snippet": "print(\"Accuracy of KNN classifier on test set is\" , accuracy_score(y_test, y_pred_knn))\n", "intent": "Find accuracy of the model on test set.\n"}
{"snippet": "print(\"************** KNN Classification Report **************\")\nprint(classification_report(y_test, y_pred_knn))\n", "intent": "Classification Report showing Class wise Precision, Recall and F1 Score\n"}
{"snippet": "pred_valid_svm = svm_model.predict(X_valid[selected_features])\n", "intent": "**Let's evaluate the model**\n"}
{"snippet": "pred_valid_dtree = dtree_model.predict(X_valid[selected_features])\n", "intent": "**Model Evaluation**\n"}
{"snippet": "pred_valid_rfc = rfc_model.predict(X_valid[selected_features])\n", "intent": "**Model Evaluation**\n"}
{"snippet": "pred_valid_knn = knn_model.predict(X_valid[selected_features])\n", "intent": "**Model Evaluation**\n"}
{"snippet": "print( clf.predict([[1,2,3,4],[8,7,6,5]]) )\n", "intent": "From here we can predict what class data points will end up in, as usual, using the predict method:\n"}
{"snippet": "points = [[0,0], [40,40], [30,30]]\nprint(knn.predict(points))\n", "intent": "Now we can predict any number of points and get the class they correspond with using the <i>predict</i> method on our model:\n"}
{"snippet": "print(model.predict([[6,1], [1,6]]))\n", "intent": "We can then predict points using the <i>predict</i> method:\n"}
{"snippet": "y_hat = mod.predict(x)\ny_hat\n", "intent": "Then we create the predicted values: \n"}
{"snippet": "list(clf.predict(iris[:3]))\n", "intent": "We will now try to predict the first 3 data points from the dataset using our classifier\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y, y_pred)\n", "intent": "- proportion of correct predictions \n- commmon evaluation metric for classification problems\n"}
{"snippet": "y_pred0 = np.zeros(len(y_pred))\nac = accuracy_score(y_test, y_pred0)\nprint(\"Name: %s, ac: %f\" % ('Zero Predictor', ac))\n", "intent": "How good is that?\nLet's compare to the constant predictor which always predicts zero.\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_tfidf1000[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf1000[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_tfidf1000[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf1000[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_feature[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_feature[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_feature[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_feature[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_tfidf1000[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf1000[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_tfidf1000[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_tfidf1000[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "result = clf_gini.predict(test_X)\nprint(accuracy_score(test_Y, result))\nprint(F1score(result, test_Y))\n", "intent": "Among the models tested, the Decision Tree model was the best so we will run it on the test set.\n"}
{"snippet": "score = model.evaluate(\n\tx=mnist.test.images,\n\ty=mnist.test.labels)\nprint(\"score = \", score)\n", "intent": "We can evaluate the performance of the model on the test set using `keras.models.Sequential.evaluate` method.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(Y_test,Y_pred))\n", "intent": "Not surprisingly the prevailing wage was by far the most important predictor. How did we score on Root Mean Squared Error?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(Y_test,Y_pred)\n", "intent": "Our score wasn't so bad:\n"}
{"snippet": "loss = tf.losses.mean_squared_error(labels=Y, predictions=Y_pred)\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate=0.15).minimize(loss)\n", "intent": "Really, we'd like to learn better values for beta and b so that our predictions get close to the real data.\n"}
{"snippet": "loss = tf.losses.huber_loss(labels=Y, predictions=Y_pred)\ntrain_op = (\n    tf.train.RMSPropOptimizer(learning_rate=0.5)\n        .minimize(loss)\n)\nsess.run(tf.global_variables_initializer())\nfor _ in xrange(20):\n    sess.run(train_op, feed_dict={x: X_train, Y: Y_train})\n    beta_np, b_np = sess.run([beta, b], feed_dict={x: X_train, Y: Y_train})\n    print(beta_np[0, 0], b_np[0, 0])\n", "intent": "Now that we have all the quick-and-easy built-in functions in play, we can start using some more advanced techniques:\n"}
{"snippet": "ypred_labels = lda.predict(xtest)\nall(ypred_labels == (ypred[:, 1] > 0.5))\n", "intent": "We also have a `predict` method that yields then class labels. Note that this is the same as making `predict_proba >= 0.5`:\n"}
{"snippet": "def sse(y,y_pred): return ((y-y_pred)**2).sum()\ndef loss(y,a,b,x): return sse(y, lin(a,b,x))\ndef avg_loss(y,a,b,x): return np.sqrt(loss(y,a,b,x)/n)\n", "intent": "And we define our loss function:\n"}
{"snippet": "a_guess = -1\nb_guess = -1\navg_loss(y, a_guess, b_guess, x)\n", "intent": "Next, we forget the values of a and b, and make our guess:\n"}
{"snippet": "avg_loss(y, a_guess, b_guess, x)\n", "intent": "The avg_loss reduces and our final guess for a and b are similar to the correct values (3, 8):\n"}
{"snippet": "lm.evaluate(x, y, verbose=0)\n", "intent": "The linear model created has set internal weights, which we can use to evaluate the loss function (MSE).\n"}
{"snippet": "score, acc = review.evaluate(x_test, y_test, batch_size=32, verbose=2)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n", "intent": "OK, Let's evaluate our model's accuracy:\n"}
{"snippet": "y_oh_hat = model.predict(x_test)\ny_hat = list(map(lambda x: key_map_inv[x], y_oh_hat.argmax(1)))\nprint_stats(y_test, y_hat)\ndisplay_misclassified_samples(x_test, y_test, y_hat, 12)\n", "intent": "Now, we'll test the model\n"}
{"snippet": "def predictTip(x_test):\n    return logitmodel.predict(x_test) *  linearmodel.predict(x_test)\nprint(\"Mean squared error : %f\" %metrics.mean_squared_error(Y_perc_test,predictTip(X_test)))\nprint(\"Rsquared score : %f\" %metrics.r2_score(Y_perc_test,predictTip(X_test)))\n", "intent": "We observer Payment Type has the maximum amount of influence on Tip Percentage. We now test our final predictTip \nalgorithm.\n"}
{"snippet": "RMS_error = math.sqrt(mean_squared_error(y_test, linear_reg.predict(X_test)))\nprint('RMS error of the following model is ',RMS_error)\n", "intent": "<H3>Accuracy Prediction of the Linear model using RMS. </H3>\n"}
{"snippet": "kmeans.predict(x_test)\n", "intent": "<h3>Predicting the cluster for each test example.</h3>\n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = (example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nmse = rss / bos.PRICE.count()\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictions_test = predict(test_x, test_y, parameters)\n", "intent": "**Expected Output**:\n<table> \n    <tr>\n        <td> **Accuracy**</td>\n        <td> 1.0 </td>\n    </tr>\n</table>\n"}
{"snippet": "pred_test = predict(test_x, test_y, parameters)\n", "intent": "<table>\n    <tr>\n    <td>\n    **Train Accuracy**\n    </td>\n    <td>\n    1.0\n    </td>\n    </tr>\n</table>\n"}
{"snippet": "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\n", "intent": "Calculate our loss and accuracy of our test data.\n"}
{"snippet": "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\nprint(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\nprint(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\n", "intent": "Tests our model against a single test instance and prints the results.\n"}
{"snippet": "labels = kmeans.predict(test_data)\n", "intent": "Ignore the warning here. The predictions are then simply calculated:\n"}
{"snippet": "def rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())\ny_train_pred = rf.predict(X_train)\ny_test_pred = rf.predict(X_test)\n", "intent": "We previously fit our model to our training data, let's see how our model does!\n"}
{"snippet": "scores = model3.evaluate(x_test_np, y_test_np, verbose=0)\ny_prob_np = model3.predict_proba(x_test_np)\nyhat = predict_with_cutoff(y_prob_np, 0.4734)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\nprint(classification_report(y_test_np, yhat))\nprint(scores)\n", "intent": "---\nCutoff at p=0.4734\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0018.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, predictions)\nprint(accuracy)\n", "intent": "Now, let's see the accuracy of our model by comparing our predicted values with the real values.\n"}
{"snippet": "inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_recall_fscore_support\ny_pred_tree = tree.predict(X_test)\ny_pred_log = logistic.predict(X_test)\nstatistic_tree = precision_recall_fscore_support(Y_test,y_pred_tree,average = 'micro')\nstatistic_log = precision_recall_fscore_support(Y_test,y_pred_log,average = 'micro')\nprint(\"tree\" + str(statistic_tree))\nprint(\"logistic\" + str(statistic_log))\n", "intent": "I need to find out the best suitable parameter. I'll creat a loop and process each given parameter. Finally, find the most suitable one.\n"}
{"snippet": "np.mean(cross_val_score(reg, X_train, y_train))\n", "intent": "Using *cross_val_score* gives a better estimate of how the model will perform on unseen data.\n"}
{"snippet": "from sklearn.metrics import matthews_corrcoef as mcc\ndef mcc_error(preds, train_data):\n    labels = train_data.get_label()\n    return 'mcc_error', mcc(preds.round(), labels), True\n", "intent": "LightGBM does not include MCC as a metric, but fortunately LightGBM makes it very easy to implement it yourself.\n"}
{"snippet": "pred_test = lg.predict(X_test)\nscore = mcc(y_test, pred_test)\nprint(score)\n", "intent": "This is a solid score. Let's see how it performs on the test data.\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = sum(yhat == y)\nprint(\"Accuracy (Fraction of true predictions): \", round(acc/len(yhat), 3))\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "def predict(x):\n    return line(x)\n", "intent": "Finally, make some predictions:\n"}
{"snippet": "y_pred = log_model.predict(X_test)\n", "intent": "Now we use the classifier to label our evaluation set. We can use either `predict` for classes or `predict_proba` for probabilities.  \n"}
{"snippet": "y_out = clf.predict(X_test)\nclf.predict_proba(X_test, check_input=True)\n", "intent": "We will generate a prediction score for our decesion tree classifier\n"}
{"snippet": "College_Data['Cluster_private'] = np.where(College_Data.Cluster == 0, 'No', 'Yes')\nCollege_Data.head()\nfrom sklearn.metrics import confusion_matrix\na = confusion_matrix(College_Data.Private,College_Data.Cluster_private)\nfrom sklearn.metrics import classification_report\nb = classification_report(College_Data.Private,College_Data.Cluster_private)\nprint(a)\nprint(b)\n", "intent": "** Confusion matrix**\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE:', metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(Y_test,predictions)))\nprint('Mean absolute percentage error:', (metrics.mean_absolute_error(Y_test,predictions)*100/(np.mean(Y_test))))\n", "intent": "The MAPE(Mean Absolute Percentage Error) is ~3.5%. Hence the model is a good fit\n"}
{"snippet": "Incep_predictions = [np.argmax(Incep_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(Incep_predictions)==np.argmax(test_targets, axis=1))/len(Incep_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = model.predict(x_test)\n", "intent": "We use our model to make predictions on the testing set. The model is predicting $(x_{0}, y_{0})$ of the electron for a given event.\n"}
{"snippet": "df = pd.concat([scoredf['headline_sentiment'], scoredf['first_sentiment']], axis=1)\npredicted_scores = ols_model.predict(add_constant(df), transform=False)\nlen(predicted_scores)\n", "intent": "Let's store our predicted values for all of our data in the dataframe just in case we want to use them later.\n"}
{"snippet": "pred = logistic.predict_proba(X)\npred\n", "intent": "The matrix 'pred' below shows the probability of each data point belonging to the treated (1st column) or control group (2nd column).\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    style_loss = 0;\n    for i in range(len(style_layers)):\n        local_gram =  gram_matrix(feats[style_layers[i]],True)\n        style_loss += style_weights[i]*tf.reduce_sum(tf.squared_difference(local_gram,style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    style_loss = 0;\n    for i in range(len(style_layers)):\n        local_gram =  gram_matrix(feats[i],True)\n        style_loss += style_weights[i]*tf.reduce_sum(tf.squared_difference(local_gram,style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "final_model = grid_search_gb.best_estimator_\nfinal_predictions = final_model.predict(test_X)\nfinal_mse = mean_squared_error(test_y, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n", "intent": "        5.2.2 Evaluate on the test set\n"}
{"snippet": "def my_own_scoring(regr, X_data, Y_data):\n    return np.mean(regr.predict(X_data) == Y_data)\ncross_val_score(regr, X_data, Y_data, cv = 5, scoring = my_own_scoring)\n", "intent": "There are many built-in scoring metrics. However, we can always define our own metric:\n"}
{"snippet": "def calculate_score(model, X, y):\n    mse = mean_squared_error(model.predict(X), y)\n    mae = mean_absolute_error(model.predict(X), y)\n    r2 = r2_score(model.predict(X), y)\n    return({'mse': mse, 'mae': mae, 'r2': r2})\n", "intent": "There is a function to calculate different scores.\n"}
{"snippet": "Y_pred = clf_rf.predict(X_test)\n", "intent": "** Predicting the result**\n"}
{"snippet": "print('AUC: {:.4f}'.format(roc_auc_score(y_test, y_pre_proba[:,1])))\nprint('F1: {:.4f}'.format(f1_score(y_test, y_pre)))\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pre))\nprint('Model Coefficient')\nprint(model.coef_[0])\n", "intent": "We use predict the class from test data and use various of evaluation metrices.\n"}
{"snippet": "preds = fore.predict_proba(X_test)\nclipped_preds = np.clip(preds, 0.05, 0.95)\ndf_sample_sub.Pred = clipped_preds\ndf_sample_sub.head()\n", "intent": "Create predictions using the logistic regression model we trained.\n"}
{"snippet": "preds = base2.predict(X_test)\n", "intent": "Create predictions using the logistic regression model we trained.\n"}
{"snippet": "preds = pipeline.predict_proba(X_test)[:,1]\nclipped_preds = np.clip(preds, 0.05, 0.95)\ndf_sample_sub.Pred = clipped_preds\ndf_sample_sub.head(100)\n", "intent": "Create predictions using the logistic regression model we trained.\n"}
{"snippet": "preds = base.predict(X_test)\n", "intent": "Create predictions using the logistic regression model we trained.\n"}
{"snippet": "prediction = loaded_model.predict(testimage)\nprint(prediction)\nif (prediction[0] == 1):\n        print(\"Cat\")\n        one = True\nelse:\n    print(\"Dog\")\n", "intent": "<img src=\"data/validation/cats/cat.1211.jpg\" />\n"}
{"snippet": "print('Coefficient: ', regressor.coef_[0][0])\nprint('Intercept: ', regressor.intercept_[0])\nprint(\"Mean squared error: %.10f\"\n      % np.mean((regressor.predict(X_test) - y_test) ** 2))\nprint('Variance score: %.10f' % regressor.score(X_test, y_test))\n", "intent": "Scatter plot to compare actual Diabetes vs predicted Diabetes for the test dataset.\n"}
{"snippet": "mols = list(dff.ix[dff['SCPN'] == 'US-20100056494-A1'].ROMol)\nsmarts,smi,img,mcsM = MCS_Report(mols,threshold=0.6,ringMatchesRingOnly=True)\nSVG(moltosvg(mcsM))\n", "intent": "OK, the structures seem consistent with a well defined MCS. Let's see if that's the case for two thresholds\n"}
{"snippet": "mols = list(dff.MurMol)\nsmarts,smi,img,mcsM = MCS_Report(mols,threshold=0.8,ringMatchesRingOnly=True)\nSVG(moltosvg(mcsM))\n", "intent": "And now we'll extract the MCS core:\n"}
{"snippet": "model.predict(x_test)\n", "intent": "***Loss:***     0.298\n***Accuracy:*** 88.78%    \n"}
{"snippet": "preds= model.predict(x)\n", "intent": "We can now run the pretrained network on the image and decode its prediction vector back to a human-readable format:\n"}
{"snippet": "print('Train R^2:', grid.score(X_train, y_train).round(4))\nprint('Test  R^2:', grid.score(X_test, y_test).round(4))\nprint('MAE:      ', metrics.mean_absolute_error(y_test, grid_pred).round(4))\nprint('MSE:      ', metrics.mean_squared_error(y_test, grid_pred).round(4))\nprint('RMSE:     ', np.sqrt(metrics.mean_squared_error(y_test, grid_pred)).round(4))\n", "intent": "*What are our new metrics scores with these optimized parameters?*\n"}
{"snippet": "print('Train R^2:', grid.score(X_train_scaled, y_train).round(4))\nprint('Test  R^2:', grid.score(X_test_scaled, y_test).round(4))\nprint('MAE:      ', metrics.mean_absolute_error(y_test, grid_pred).round(4))\nprint('MSE:      ', metrics.mean_squared_error(y_test, grid_pred).round(4))\nprint('RMSE:     ', np.sqrt(metrics.mean_squared_error(y_test, grid_pred)).round(4))\n", "intent": "*What are our new metrics scores with these optimized parameters?*\n"}
{"snippet": "y_hat = svm_linear.predict(X)\nprint(accuracy_score(y_hat, y))\n", "intent": "Let us compute the accuracy of the SVM with the linear kernel.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out the model on the test dataset of dog images.\n"}
{"snippet": "Z = knn.predict(xx_yy_grid)\nprint(xx.shape)\nprint(Z.shape)\nZ = Z.reshape(xx.shape)\nZ\n", "intent": "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.c_.html\n"}
{"snippet": "print('Random class-proportional (dummy)\\n', \n      classification_report(y_test, y_classprop_predicted, target_names=['not 1', '1']))\nprint('SVM\\n', \n      classification_report(y_test, svm_predicted, target_names = ['not 1', '1']))\nprint('Logistic regression\\n', \n      classification_report(y_test, lr_predicted, target_names = ['not 1', '1']))\nprint('Decision tree\\n', \n      classification_report(y_test, tree_predicted, target_names = ['not 1', '1']))\n", "intent": "`support` : The number of instances in the test set that has the true label.\n"}
{"snippet": "print( np.sum((bos.PRICE - lm.predict(X)) ** 2))\nlm.residues_\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nprint(\"mean squared error : \",mean_squared_error(bos.PRICE, Predicted))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "trainScore = model.evaluate(trainX, trainY, verbose=0)\nprint('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\ntestScore = model.evaluate(testX, testY, verbose=0)\nprint('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "evaluating our network score\n"}
{"snippet": "sc_per_centre = []\nnumber_of_cetroids = []\nfor i in range(2,20):\n    number_of_cetroids.append(i)\n    sc_per_centre.append(get_score(i))   \n", "intent": "* Report the silhouette score for several cluster numbers you tried. \n* Of these, which number of clusters has the best silhouette score?\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: {:.2f}'.format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n", "intent": "Performing 10 fold cross validation\n"}
{"snippet": "from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\ndef metric(observed, predicted):\n    rmse = mean_squared_error(observed,predicted)**0.5\n    mae = mean_absolute_error(observed,predicted)\n    print ('\\nRMSE: {}\\nMAE: {}'.format(rmse,mae))\ndef ridge_regressor(X_train,X_test,y_train,param):\n", "intent": "In the cell below we have functions that'll help us build the regression model and print the metrics\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "Now you can make predictions from the model:\n"}
{"snippet": "y_pred_cnn = model.predict_classes(X_test_cnn, batch_size=32, verbose=0)\nacc_cnn = accuracy_score(y_test, y_pred_cnn)\nprint ('CNN accuracy: ',acc_cnn)\n", "intent": "Again, the CNN with Keras performed best.\nLet's take a look at the predictions of the CNN:\n"}
{"snippet": "outcome = pipeline.predict_proba(obs)\noutcome\n", "intent": "Which in turn can be passed into predict_proba\n"}
{"snippet": "def predict(network, row):\n    outputs = forward_propagate(network, row)\n    return outputs.index(max(outputs))\n", "intent": "network = Initialized network\nrow = Unlabeled input image\n"}
{"snippet": "y_pred = cross_val_predict(sgd_clf, X, y, cv=3)\ndef part_d():\n    y_pred = cross_val_predict(sgd_clf, X, y, cv=3)\n    precision, recall, confusionmatrix = [precision_score(y, y_pred)], [recall_score(y, y_pred)], [confusion_matrix(y, y_pred)]\n    return [precision, recall, confusionmatrix]\n", "intent": "Compute precision, recall, and confusion matrix for the classifier in part c on the training set.\n"}
{"snippet": "def part_f():\n    y_pred = cross_val_predict(forest_clf, X, y, cv=3)\n    precision, recall, confusionmatrix = [precision_score(y, y_pred)], [recall_score(y, y_pred)], [confusion_matrix(y, y_pred)]\n    return [precision, recall, confusionmatrix]\n", "intent": "Compute precision, recall, and confusion matrix for the classifier in part e on the training set.\n"}
{"snippet": "y_train_pred=gs.predict(X_train)\ny_val_pred=gs.predict(X_val)\np1 = figure(plot_width=350, plot_height=350,title=\"training actual vs predicted\", x_axis_label='actual', \n            y_axis_label='predicted')\np2 = figure(plot_width=350, plot_height=350,title=\"Validation actual vs predicted\", x_axis_label='actual', \n            y_axis_label='predicted')\np1.scatter(y_train, y_train_pred),p2.scatter(y_val, y_val_pred);\n", "intent": "** Let's plot our results**\nNotice we can run predict straight off gridsearch as it will use the best estimator as the model\n"}
{"snippet": "temp_naive,actual=casual_observer(weather_grpd['T (degC)'].iloc[val_index:test_index],24)\ncasual_error=mean_absolute_error(actual,temp_naive)\nprint('simple prediction Error:',casual_error)\n", "intent": "We will use Mean Absolute Error function from Scikit learn on the validation data\n"}
{"snippet": "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\nprint score[1]\n", "intent": "Now that the model is trained, let's evaluate it on the test set:\n"}
{"snippet": "results = model.predict(test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n", "intent": "For those six case, the model is not ridiculous. Some of these errors can also be made by humans, especially for one the 9 that is very close to a 4.\n"}
{"snippet": "print(reg2.predict(house2[my_features]))\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.)\n    for style_layer, gram_target, style_weight in zip(style_layers, style_targets, style_weights):\n        gram_layer = gram_matrix(feats[style_layer]) \n        loss_layer = tf.reduce_sum(tf.pow(gram_layer-gram_target,2))\n        loss += (style_weight*loss_layer)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def evaluate(logits, labels):\n    equals = tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1))\n    return tf.reduce_mean(tf.cast(equals, tf.float32))\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "from __future__ import print_function\npred = fitted_models['gb'].predict(X_test)\nprint(confusion_matrix(y_pred=pred, y_true=y_test))\n", "intent": "We can observe how these models predicted whether or not an individual sought treatment. \n"}
{"snippet": "print('The model predicted ', np.round(accuracy_score(y_true=y_test, y_pred=pred)*100,2), \n      '% of the test data correctly.', sep = '')\nprint('The model predicted ', np.round(recall_score(y_pred=pred, y_true=y_test)*100, 2), \n      '% of all actual individuals who sought treatment.', sep = '')\nprint('The model predicted that an individual sought treatment correctly ', \n      np.round(precision_score(y_true=y_test, y_pred=pred)*100, 2), '% of the time', sep = '')\n", "intent": "From the confusion matrix, we can calculate important metrics for our model - specifically, accuracy, recall, and precision. \n"}
{"snippet": "train_predict = cross_val_predict(sgd_clf, train_data, train_target_6, cv=3)\ntrain_target_6.shape, train_predict.shape\n", "intent": "Use `cross_val_predict` with the `SGDClassifier` to create predictions for every row in the dataset `train_data`. \n"}
{"snippet": "cross_val_score(sgd_clf, train_data, train_target_6, cv=3, scoring=\"accuracy\")\n", "intent": "The `cross_val_score` function is used to score each of the three (3) validation sets.\n"}
{"snippet": "train_predict = cross_val_predict(sgd_clf, train_data, train_target, cv=3)\ntrain_target.shape, train_predict.shape\n", "intent": "Use `cross_val_predict` with the `SGDClassifier` to create predictions for every row in the dataset `train_data`. \n"}
{"snippet": "cross_val_score(sgd_clf, train_data, train_target, cv=3, scoring=\"accuracy\")\n", "intent": "The `cross_val_score` function is used to score each of the three (3) validation sets.\n"}
{"snippet": "train_predict  = train_fit.predict(train_data)\ntrain_predict[0:6]\n", "intent": "Calculate the predictions, $\\hat{y} = round(\\hat{p})$\n"}
{"snippet": "train_proba    = train_fit.predict_proba(train_data)\ntrain_proba.shape\n", "intent": "The probabilities are returned by the `predict_proba` method.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "Create a (small) dataframe `some_data`. Create the corresponding labels `some_labels`. Then transform the data and make predictions. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Now make predictions from the model created above using the `housing_prepared` data. \n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "From this model make predictions for the `housing_prepared` data. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ntree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\n", "intent": "First use the decision tree model created above.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Now use the linear regression model created above.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)   \nfinal_rmse\n", "intent": "We have been training our models using the `strat_train_set` dataframe. \nNow evaluate the best model on the `strat_test_set` dataframe.\n"}
{"snippet": "test_ndx = np.random.randint(0, train_data.shape[1], size=3)\nprint(\"Actual: \", train_target[test_ndx])\nprint(\"Predicted [ >=7, odd]: \")\nknn_clf.predict(train_data[test_ndx])\n", "intent": "Store three random row numbers in `test_ndx`. \nThen display the actual and predicted target values:\n"}
{"snippet": "sk_me.average_precision_score(test_target, test_predict)\n", "intent": "`average_precision_score` may require 1hot encoding\n"}
{"snippet": "test_predict = bag_clf.predict(test_data)\nprint(\"Bagging (with replacement sampling)\")\nprint(\"Accuracy score: \", accuracy_score(test_target, \n                                         test_predict))\n", "intent": "Create predictions from `test_data` and check their accuracy:\n"}
{"snippet": "test_predict = bag_clf.predict(test_data)\nprint(\"Pasting (without replacement)\")\nprint(\"Accuracy score: \", accuracy_score(test_target, \n                                         test_predict))\n", "intent": "Create predictions from `test_data` and check their accuracy:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_predict = bag_clf.predict(test_data)\nprint(\"Accuracy score: \", \n      accuracy_score(test_target, \n                     test_predict))\n", "intent": "Compare this to the model accuracy on the test dataset:\n"}
{"snippet": "sk_me.confusion_matrix(rnd_clf.predict(test_data),\n                       bag_clf.predict(test_data))\n", "intent": "The two sets of predictions are nearly identical:\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "    To predict the labels of new data: \n- .predict() method\n"}
{"snippet": "print(\"Training set:\")\npred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\nprint('Test set:')\npred_test = predict(X_test, Y_test, W, b, word_to_vec_map)\n", "intent": "Now, model has pretty high accuracy on the training set. Lets now see how it does on the test set.\n"}
{"snippet": "C = 5\ny_test_oh = np.eye(C)[Y_test.reshape(-1)]\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\npred = model.predict(X_test_indices)\nfor i in range(len(X_test)):\n    x = X_test_indices\n    num = np.argmax(pred[i])\n    if(num != Y_test[i]):\n        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())\n", "intent": "I got a test accuracy between 80% and 95%. Running the cell below to see the mislabelled examples. \n"}
{"snippet": "x_test = np.array(['not feeling happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "trying it on a custom example... Writing my own sentence below.\n"}
{"snippet": "svm_loss(scores, train_labels)\n", "intent": "Now let's calculate our loss for our previous scores.\n"}
{"snippet": "loss, d_scores = svm_loss(scores, train_labels)\nprint d_scores\n", "intent": "Now we can see what the gradiant of our scores is.\n"}
{"snippet": "predictions = []\nfor x in range(len(X_test)):\n    neighbours = get_neighbours(training_set=train, test_case=test[x][0], k=5)\n    majority_vote = majority(neighbours)\n    predictions.append(majority_vote)\nprint('The overall accuracy of the model is: ', str(accuracy_score(y_test, predictions)))\n", "intent": "Now run functions with test data\n"}
{"snippet": "def evaluate(df, topic, features, model):\n", "intent": "Define the regression pipeline:\n"}
{"snippet": "logisticRegr.predict(test_img[0].reshape(1,-1))\n", "intent": "Step 4. Predict the labels of new data (new images)\nUses the information the model learned during the model training process\n"}
{"snippet": "def getMisclassificationRate(clf, X_train, X_test, y_train, y_test):\n    label_pred_training = clf.predict(X_train)\n    label_pred = clf.predict(X_test)\n    mis_training = 1 - accuracy_score(y_train, label_pred_training)\n    mis_validation = 1 - clf.best_score_\n    mis_testing = 1 - accuracy_score(y_test, label_pred)\n    return [mis_training, mis_validation, mis_testing]\n", "intent": "Define some funcitons for multiple implementation later.\n"}
{"snippet": "pred = grid_search.best_estimator_.predict(X_test)\npred1 = grid_search1.best_estimator_.predict(X_test)\npred2 = grid_search2.best_estimator_.predict(X_test)\nacc = float(sum(np.equal(pred , y_test)))/len(y_test)\nprint('accuracy of best linear SVM = %s' % acc)\nacc1 = float(sum(np.equal(pred1 , y_test)))/len(y_test)\nprint('accuracy of best Gaussian SVM = %s' % acc1)\nacc2 = float(sum(np.equal(pred2 , y_test)))/len(y_test)\nprint('accuracy of best Random Forest = %s' % acc2)\n", "intent": "<h2>SEE HOW THE BEST MODELS FROM EACH CLASS PERFORM ON THE HOLDOUT SET</h2>\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score( rf , X_train , y_train , cv = 5)\nprint( \"the set of scores on the five CV sets are:  %s\" % (str(scores)))\nprint( \"the mean score is %s\" % np.mean(scores))\nprint( \"the SD of the scores in %s\" % np.std(scores))\n", "intent": "<h2>NOW FOR CROSS-VALIDATION</h2>\n"}
{"snippet": "predictions = estimator.best_estimator_.predict(X_test)\nacc = float(sum(np.equal(predictions , y_test)))/len(predictions)\nprint('accuracy of best SVM = %s' % acc)\n", "intent": "<h2>MAKE PREDICTIONS ON TEST SET</h2>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for layer, Al, weight in zip(style_layers, style_targets, style_weights):\n        Gl = gram_matrix(feats[layer])\n        loss += weight * torch.pow(Gl-Al, 2).sum()\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for layer, Al, weight in zip(style_layers, style_targets, style_weights):\n        Gl = gram_matrix(feats[layer])\n        loss += weight * tf.reduce_sum(tf.pow(Gl-Al, 2))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, layer in enumerate(style_layers):\n        gram_mat = gram_matrix(feats[layer])\n        style_loss += style_weights[idx]*tf.reduce_sum((gram_mat - style_targets[idx])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "scores = model_selection.cross_val_score(text_clf_final, X_train, y_train, cv = 5)\nprint(scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Let's see if we can get better results through cross validation:\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred)\n", "intent": "Finally we can compute the accuracy of our classifier:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n", "intent": "true positives, False Positives, True Negatives, False Negatives\nAccurary  = (True Positive + True Negative) / Total\n"}
{"snippet": "test_acc = (net.predict(best_session, X_test) == y_test).mean()\nprint('Test accuracy: ', test_acc)\n", "intent": "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.\n"}
{"snippet": "predicted_valid_SVM = clf.predict(X_valid)\nprint(\"SVM part, metrics on validation set:\")\nprint(metrics.classification_report(y_valid, predicted_valid_SVM))\n", "intent": "To be sure that I don't tune in too much the model, I predicted some data that the model has never see.\n"}
{"snippet": "vgg.predict(imgs, True)\n", "intent": "We can predict the images above using raw VGG model, this will result in category used in ImageNet\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npred_value=train['PredEveryoneDies']\ntrue_value=train['Survived']\nAccEveryoneDies=accuracy_score(true_value,pred_value,normalize=True)*100\nprint(AccEveryoneDies)\nZ=train['PredGender']\nAccGender=accuracy_score(true_value,Z,normalize=True)*100\nprint(AccGender)\n", "intent": "(7) Create variables `AccEveryoneDies` and `AccGender` using a calculation of accuracy of predictions for the `training` dataset.  \n"}
{"snippet": "import numpy as np\navg_score = 0\nfor t, gbrp, lp in zip(y_test, prediction, lasso_predict):\n    mean_pred = (np.expm1(lp) + gbrp)/2.0\n    score = sqrt(mean_squared_log_error([t], [mean_pred]))\n    avg_score += score\navg_score = avg_score/float(len(lasso_predict))\nprint(avg_score)\n", "intent": "We see that for some of the houses for which the GBR produced a bad score the prediction is improved by Lasso. Let's try combining Lasso and GBR.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(cluster,kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypredict, ytest))\n", "intent": "We can take a look at the classification report for this classifier:\n"}
{"snippet": "clf.predict([[1.5, 1.5]])\n", "intent": "After being fitted, the model can then be used to predict the class of samples:\n"}
{"snippet": "clf.predict_proba([[0, 0]])\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "yts_pred = regr.predict(Xts)\nRSS_test = np.mean((yts_pred-yts)**2)/(np.std(yts)**2)\nprint(\"RSS per sample = {0:f}\".format(RSS_test))\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "y1hat = logreg.predict(Xs)\nacc1 = np.mean(y1hat == y1)\nprint(\"Accuracy on training data = %f\" % acc1)\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "print('--- LGB')\nprint('mse on log(Price): ',mean_squared_error(y_true=test['logprice'],y_pred=lgb_prediction))\nprint('rmse on log(Price): ',np.sqrt(mean_squared_error(y_true=test['logprice'],y_pred=lgb_prediction)))\nprint('avg price error: ',np.mean(abs(np.expm1(test['logprice'])-np.expm1(lgb_prediction))))\nprint('\\n--- Ridge Regression')\nprint('mse on log(Price): ',mean_squared_error(y_true=test['logprice'],y_pred=ridge_pred))\nprint('rmse on log(Price): ',np.sqrt(mean_squared_error(y_true=test['logprice'],y_pred=ridge_pred)))\nprint('avg price error: ',np.mean(abs(np.expm1(test['logprice'])-np.expm1(ridge_pred))))\n", "intent": "Light Gradient Boosting performs better than Ridge Regression for predicting price. The comparison is shown below. \n"}
{"snippet": "print(\"Predicted chirping rate at 95 degrees: %.2f chirps/second\" % regr.predict(95)[0][0])\n", "intent": "4) Extrapolate data:  If the ground temperature reached 95&deg; F, then at what approximate rate would you expect the crickets to be chirping?\n"}
{"snippet": "def gender_swap(gender):\n    if gender == 1:\n        return 0\n    else:\n        return 1\nsal_df['Gender Swap'] = sal_df['Sex'].map(gender_swap)\nsal_df['Gender Swap Predicted Salary'] = sal_regr.predict(sal_df[['Gender Swap'] + names[1:5]])\nsal_df['Gender Swap Increase Salary'] = sal_df['Gender Swap Predicted Salary'] - sal_df['Salary']\nsal_df.head()\nsal_df.groupby(\"Sex\").sum()\n", "intent": "3) Report whether sex is a factor in salary. Support your argument with graph(s) if appropriate.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: {:.4f}'.format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "activations = activation_model.predict(img_tensor)\nfirst_layer_activation = activations[0]\n", "intent": "Running the model in predict mode\n"}
{"snippet": "dev_X = data.dev.X\ndev_y = data.dev.y\ndev_preds = model.predict(simpsons_preprocess.transform(dev_X))\n", "intent": "runs the selected model on the dev set and sets thresholds according to a specific precision, specific recall, maximize accuracy or maximize fscore.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.00)\n    for i, idx_layer in enumerate(style_layers):\n        F = feats[idx_layer]\n        G = gram_matrix(F)\n        l = tf.subtract(G,style_targets[i])\n        l = tf.reduce_sum(tf.multiply(l,l))\n        l = tf.multiply(tf.to_float(tf.constant(style_weights[i])),l)\n        loss = tf.add(loss,l)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    print (student_output)    \n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred\n", "intent": "Our tree is now trained and ready to roll! We can test it out on the X_test set and see what we get!\n"}
{"snippet": "y_predicted = est.predict(X)\nfrom sklearn.metrics import accuracy_score\nscores = accuracy_score(y, y_predicted)\nprint(\"Overall classification accuracy: {:.0%}\".format(scores))\n", "intent": "* Our model is fitted! Let's see how accurately we classified outcomes\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(ytr, m.predict(Xtr)))\n", "intent": "Several \"metrics\" exist to see how well our model is...\n"}
{"snippet": "predictions_train = model.predict(X_train_1_processed, batch_size=1)\n", "intent": "Predictions (probability maps + coordinates of keypoints):\n"}
{"snippet": "predictions = model.predict(X, batch_size=1)\nkeypoint_predictions = np.array(predictions[1])\nkeypoint_predictions *= 512 \n", "intent": "Compute predictions for keypoints:\n"}
{"snippet": "predictions = np.array([np.argmax(model.predict(np.expand_dims(feature,axis=0))) for feature in test_inception])\naccuracy = np.sum(predictions==np.argmax(test_targets,axis=1))/len(test_inception)\naccuracy*=100\nprint(\"\\nTest Accuracy: %.4f%%\" % accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf.predict([[1., 0.], [0, 0], [3, 0]])\n", "intent": "After being fitted, the model can then be used to predict the class of samples:\n"}
{"snippet": "y_pred = clf.predict(X_test)\nassert(y_pred.shape[0] == y_test.shape[0])\nprecision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\nprint('precision = {:.3f}, recall = {:.3f}, f1 = {:.3f}'.format(precision, recall, f1)) \n", "intent": "Here we follow the same evaluation from the original example and compute the $F_1$ score across the entire test set. \n"}
{"snippet": "print(classification_report(y_test, y_pred, target_names=target_classes))\n", "intent": "Overall the performance is really good at $F_1 = 0.94$. \nNow, let's find out how the performance differs between males and females.\n"}
{"snippet": "val_pred_rf = rf_model.predict(X_val_for_rf)\nval_pred_rf_text = [val.index2label[pred] for pred in val_pred_rf]\nval_pred_rf_text[:20]\n", "intent": "Predict the validation set.\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2))/bos.PRICE.count())\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "result = kmeans_predictor.predict(valid_set[0][0:100])\nclusters = [r.label['closest_cluster'].float32_tensor.values[0] for r in result]\n", "intent": "OK, a single prediction works.\nLet's do a whole batch and see how well the clustering works.\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "The classifier can then predict classes for the test data.\n"}
{"snippet": "accuracy = metrics.accuracy_score(y_test,y_pred)\n", "intent": "The accuracy of predictions is calculated with metrics.accuracy_score() from sklearn.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint classification_report(messages['label'], all_predictions)\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\">\n"}
{"snippet": "def evaluate_randomly():\n    pair = random.choice(pairs)\n    output_words, decoder_attn = evaluate(pair[0])\n    output_sentence = ' '.join(output_words)\n    print('>', pair[0])\n    print('=', pair[1])\n    print('<', output_sentence)\n    print('')\n", "intent": "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements\n"}
{"snippet": "sqrt(mean_squared_error(y_test, y_test_pred)), sqrt(mean_squared_error(y_train, y_train_pred))\n", "intent": "Compare the root mean squared error (RMSE) of test dataset against the training.\n"}
{"snippet": "r2_score(y_test, y_test_pred), r2_score(y_train, y_train_pred)\n", "intent": "r2 score can have a max value 1, negative values of R2 means suboptimal model \n"}
{"snippet": "row = X_valid_sample.values; row\nprediction, bias, contributions = ti.predict(m, row)\n", "intent": "Inspired by the SHAP Value, which we can add up indivual rows contribution to get a feature importance, I attempt to do the same for treeinterpreter\n"}
{"snippet": "print(lr.predict(X)[0:6])\n", "intent": "Now we have found all the main features which We are going to find the prices using `lr.predict`.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(expected, predicted))\n", "intent": "One of the most useful metrics is the classification_report, which combines several measures and prints a table with the results:\n"}
{"snippet": "clf.predict_proba([[2., 2.]])\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "dist = 11\nprint ('The service charge should be: Rs %.2f' % model.predict([[dist]]))\n", "intent": "lets predict the service charge \n"}
{"snippet": "print(lr.predict(X)[0:10])\n", "intent": "Now we have found all the main features which We are going to find the prices using `lr.predict`.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)\n", "intent": "The confusion_matrix() function will calculate a confusion matrix and return the result as an array.\n"}
{"snippet": "from sklearn import metrics\nlabels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 1, 1, 2]\nmetrics.adjusted_rand_score(labels_true, labels_pred)  \n", "intent": "where ${\\displaystyle n_{ij},a_{i},b_{j}} n_{ij}$, $a_i$, $b_j$ are values from the contingency table.\n"}
{"snippet": "labels_pred = [1, 1, 2, 0, 2, 0]\nmetrics.adjusted_rand_score(labels_true, labels_pred)  \n", "intent": "Furthermore, adjusted_rand_score is symmetric: swapping the argument does not change the score. It can thus be used as a consensus measure:\n"}
{"snippet": "labels_pred = labels_true[:]\nprint(id(labels_pred), id(labels_true))\nmetrics.adjusted_rand_score(labels_true, labels_pred)\nprint(labels_true, labels_pred)\n", "intent": "Perfect labeling is scored 1.0:\n"}
{"snippet": "labels_true = [0, 0, 0, 1, 1, 1]\nlabels_pred = [0, 0, 1, 0, 1, 1]\nmetrics.adjusted_rand_score(labels_true, labels_pred)  \n", "intent": "Bad (e.g. independent labelings) have negative or close to 0.0 scores:\n"}
{"snippet": "labels_pred = [1, 1, 0, 0, 3, 3]\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n", "intent": "One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:\n"}
{"snippet": "labels_pred = labels_true[:]\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred)\nmetrics.normalized_mutual_info_score(labels_true, labels_pred)\n", "intent": "Perfect labeling is scored 1.0:\n"}
{"snippet": "metrics.mutual_info_score(labels_true, labels_pred)  \n", "intent": "This is not true for mutual_info_score, which is therefore harder to judge:\n"}
{"snippet": "metrics.v_measure_score(labels_true, labels_pred)    \n", "intent": "Their harmonic mean called V-measure is computed by v_measure_score:\n"}
{"snippet": "labels_pred = [1, 1, 0, 0, 3, 3]\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)  \n", "intent": "One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score\n"}
{"snippet": "labels_pred = labels_true[:]\nmetrics.fowlkes_mallows_score(labels_true, labels_pred) \n", "intent": "Perfect labeling is scored 1.0:\n"}
{"snippet": "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\nlabels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)  \n", "intent": "Bad (e.g. independent labelings) have zero scores:\n"}
{"snippet": "print (metrics.classification_report(bank_target_test,knnpred_test))\n", "intent": "__Classifaction Report for KNN Classifier__\n"}
{"snippet": "print(metrics.classification_report(bank_target_test,treepreds_test))\n", "intent": "__Classifaction Report for Decision Tree__\n"}
{"snippet": "print(metrics.classification_report(bank_target_test,nbpreds_test))\n", "intent": "__Classification Report for naive Bayes__\n"}
{"snippet": "scores = model.evaluate(X_test, Y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "First let's see how this model performs on the test set - which hasn't been used during the training phase.\n"}
{"snippet": "yelp_vars = yelp[features]\nyelp_predictions = bernoulli.predict(yelp_vars)\nyelp['predictions'] = yelp_predictions\nyelp['accurate'] =  yelp['predictions'] == yelp['positive']\nwrong_predictions = yelp['accurate'].value_counts(dropna=False).loc[False]\nobservations = len(yelp['accurate'])\nprint('Of {} observations, there were {} incorrect predictions from our model.'.format(observations, wrong_predictions))\nprint('Or in other words, our model was wrong {:.1f} percent of the time.'.format((wrong_predictions/observations) * 100))\n", "intent": "Better! Now let's see how well it did with Yelp.\n"}
{"snippet": "ytrue = modeldf['lateness']\nypred = linreg.predict(modeldf.drop('lateness',axis=1))\nfrom sklearn.metrics import mean_squared_error\nprint(np.sqrt(mean_squared_error(ytrue, ypred)))\n", "intent": "And predicting on the entire dataset and seeing if the error is consistent.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscore = cross_val_score(regr, X, Y, cv=5, scoring='r2')\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\nscore = cross_val_score(regr, X, Y, cv=5, scoring='neg_mean_squared_error')\nprint(\"RMSE: %0.2f (+/- %0.2f)\" % (np.sqrt(np.abs(score.mean())), score.std() * 2))\n", "intent": "Not so great. Let's do the same with a partial least squared model to see if that does any better.\n"}
{"snippet": "score = cross_val_score(knn, X, Y, cv=5, scoring='r2')\nprint(\"R^2: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\nscore = cross_val_score(knn, X, Y, cv=5, scoring='neg_mean_squared_error')\nprint(\"RMSE: %0.2f (+/- %0.2f)\" % (np.sqrt(np.abs(score)).mean(), np.std(np.sqrt(np.abs(score))) * 2))\n", "intent": "And some cross validation.\n"}
{"snippet": "score = cross_val_score(knn_w, X, Y, cv=5, scoring='r2')\nprint(\"R^2: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\nscore = cross_val_score(knn_w, X, Y, cv=5, scoring='neg_mean_squared_error')\nprint(\"RMSE: %0.2f (+/- %0.2f)\" % (np.sqrt(np.abs(score)).mean(), np.std(np.sqrt(np.abs(score))) * 2))\n", "intent": "Woah, much more accurate! May be overfitting, though...\n"}
{"snippet": "cv = cross_val_score(lr, X, Y, cv=5)\nprint(cv)\nprint(cv.mean())\n", "intent": "**cross_val_score** will run cross_validation on given model, and return an array of scores on the validation set.\n"}
{"snippet": "from sklearn import metrics\nprint 'Mean Absolute Error: ', metrics.mean_absolute_error(y_test, predictions)\nprint 'Mean Squared Error: ', metrics.mean_squared_error(y_test, predictions)\nprint 'Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test, predictions))\n", "intent": "This plot shows the distribution of the difference of the actual y values and the predicted values. \n"}
{"snippet": "tt0=time()\nprint (\"cross result========\")\nscores = cross_validation.cross_val_score(dt, X, y, cv=3)\nprint (scores)\nprint (scores.mean())\ntt1=time()\nprint (\"time elapsed: \", tt1-tt0)\n", "intent": "As you can see, we achieved an accuracy of about 75% on the test set.\n"}
{"snippet": "r1 = k_means.predict(np.array([0.31, 10.35]).reshape(1,-1))\nr1\n", "intent": "Three new elements:\n* [0.31, 10.35]\n* [0.41, 10.25]\n* [0.6, 10.05]\n"}
{"snippet": "r2_score(y_val_true, y_val_pred)\n", "intent": "Not horrible. How about the R^2 score?\n"}
{"snippet": "def predict_modulo(a,n):\n    print(\"Predicted:\", model.predict([[a,n]])[0])\n    print(\"True:\", a%n)\n", "intent": "Again, not bad at all. Here is a function to test some predictions.\n"}
{"snippet": "y_test_pred = list2num(model.predict(X_test))\n", "intent": "As expected, the neural network was able to fit the training data very well. But what about the real test?\n"}
{"snippet": "predictions = model.predict(X)\nrounded = [round(x[0]) for x in predictions]\nprint(rounded)\n", "intent": "make prediciton with trained model\n"}
{"snippet": "my_first_nn.evaluate(X_validation, Y_validation, verbose=0)\n", "intent": "And if you want to assess the performance of your model on the test dataset, use the [evaluate method](https://keras.io/models/model/\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint 'Random Forests',cross_val_score(rfr,estimates,medVariance_projections)\nprint 'Decision Trees',cross_val_score(dtr,estimates,medVariance_projections)\nprint 'Nearest Neighbors',cross_val_score(knr,estimates,medVariance_projections)\n", "intent": "These predictions are for the Medium Variance Projections. The arrays below whow the accuracy of each model using cross-validation\n"}
{"snippet": "estimator = KerasClassifier(build_fn=get_model, epochs=200, batch_size=10, verbose=0)\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\nresults = cross_val_score(estimator, inputMatrix, target, cv=kfold)\nprint('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n", "intent": "K-Fold Cross Validation\n"}
{"snippet": "flag_adv = np.sum(np.argmax(detector.predict(x_test_adv), axis=1) == 1)\nprint(\"Adversarial test data (first 100 images):\")\nprint(\"Flagged: {}\".format(flag_adv))\nprint(\"Not flagged: {}\".format(100 - flag_adv))\n", "intent": "Apply the detector to the adversarial test data:\n"}
{"snippet": "flag_original = np.sum(np.argmax(detector.predict(x_test[:100]), axis=1) == 1)\nprint(\"Original test data (first 100 images):\")\nprint(\"Flagged: {}\".format(flag_original))\nprint(\"Not flagged: {}\".format(100 - flag_original))\n", "intent": "Apply the detector to the first 100 original test images:\n"}
{"snippet": "eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nnb_flag_adv = []\nnb_missclass = []\nfor eps in eps_range:\n    x_test_adv = attacker.generate(x_test[:100], eps=eps)\n    nb_flag_adv += [np.sum(np.argmax(detector.predict(x_test_adv), axis=1) == 1)]\n    nb_missclass += [np.sum(np.argmax(classifier.predict(x_test_adv), axis=1) != np.argmax(y_test[:100], axis=1))]\neps_range = [0] + eps_range\nnb_flag_adv = [flag_original] + nb_flag_adv\nnb_missclass = [2] + nb_missclass\n", "intent": "Evaluate the detector for different attack strengths `eps`\n(**Note**: for the training of detector, `eps=0.05` was used)\n"}
{"snippet": "len(m.estimators_[0].predict(X_valid))\n", "intent": "test for each tree.\n"}
{"snippet": "def rmse(x,y):return math.sqrt((x-y)**2).mean()\ndef print_score(m):\n    res = [rmse(m.predict(x), y), rmse(m.predict(val), y_val),\n                m.score(x, y), m.score(val, y_val)] \n    if hasattr(m, 'obb_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "I think this is a good way to see whether your model gets improved, instead of submitting a csv file to kaggle.\n"}
{"snippet": "pca_lpl = LemurSOAR(neutral_training_pca, smiling_training_pca, LpL(), override=True)\npca_lpl_predictions = pca_lpl.predict(neutral_test_pca).dot(V.T)\nlemur_util.plot_predictions(neutral_test, pca_lpl_predictions, smiling_test)\n", "intent": "As before, we apply PCA to make it faster.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i, idx in enumerate(style_layers):\n        gram = gram_matrix(feats[idx])\n        loss += style_weights[i] * tf.reduce_sum(tf.square(gram - style_targets[i]))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "fit = boston_smslr.predict(exog=dict(lstat=[5,10,15]))\nprint(fit)\n", "intent": "We can see the confidence interval computed by StatsModels is slightly difference than R.\n"}
{"snippet": "house2_price = regressor_multiple_features.predict(house2[my_features])\nprint(\"Predicted price of house 2 is:{0}\".format(house2_price))\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "jump_point = list(testX[-1])\nval = model.predict(np.asarray([jump_point]))\nval = testY[-1] * val\nprint(\"Based on our model, the price of %s stock one day after %s is somewhere around: %s\"%(STOCK_TO_USE, dates_of_data[-1], val[0][0]))\n", "intent": "So, with our model trained and evaluated, let's try to decide what will happen tomorrow. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "**Answer**: The accuracy is worse for both the training data and test data.\n"}
{"snippet": "prediction = classifier.predict(x)\n", "intent": "**Predict emotion on the image**\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nacc = accuracy_score(y_pred=pred_knn, y_true=te_target)\npre = precision_score(y_pred=pred_knn, y_true=te_target)\nrec = recall_score(y_pred=pred_knn, y_true=te_target)\nf1s = f1_score(y_pred=pred_knn, y_true=te_target)\nprint('Accuracy : {}'.format(acc))\nprint('Precision : {}'.format(pre))\nprint('Recall : {}'.format(rec))\nprint('F-score : {}'.format(f1s))\n", "intent": "<p>Now we can compute the different scores (don't forget that these quantities are evaluated on the test set only):</p>\n"}
{"snippet": "from sklearn.metrics import classification_report\nreport = classification_report(y_true=te_target, y_pred=pred_knn, target_names=iris.target_names)\nprint(report)\n", "intent": "<p>... and if you want ``scikit`` to give a full report (text report) you can also do :</p>\n"}
{"snippet": "labels_test =[1]; predicted = [1]\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(labels_test,predicted)\nprint(accuracy)\n", "intent": "Calculate the accuracy between the test data and known datapoints\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        A = style_targets[i]\n        G = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum((G - A) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        G = style_targets[i]\n        A = gram_matrix(model.extract_features()[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum((G - A) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print (\"The accuracy score for Gradient Boosting is, \",accuracy_score(y_test,pred_gb))\nprint (\"The accuracy score for Logistic regression is, \",accuracy_score(y_test,pred_lr))\nprint (\"The accuracy score for Random Forest is, \",accuracy_score(y_test,pred_rf))\nprint (\"The accuracy score for Decision Tree is, \",accuracy_score(y_test,pred_dt))\n", "intent": "** Printing Accuracy SCore**\n"}
{"snippet": "logisticreg_report(ua)\nlogisticreg_report(un)\nlogisticreg_report(dt)\n", "intent": "US Airways, United Airline, Delta\n"}
{"snippet": "naivebayes_report(dt)\n", "intent": "Delta, Virgin American, US Airways, United Airline, Southwest\n"}
{"snippet": "print(\"Accuracy is :\" , accuracy_score(y_valid, predicted_sentiment))\ntarget_name = ['neutral','positive','negative']\nprint(classification_report(y_valid,predicted_sentiment,target_names=target_name))\n", "intent": "print out the result from the model for all companys\n"}
{"snippet": "print(\"Accuracy is :\", accuracy_score(y_valid,predict_sentiment))\ntarget_name = ['neutral','positive','negative']\nprint(classification_report(y_valid,predict_sentiment,target_names = target_name))\n", "intent": "print out the result report from the Naive Bayes Multinomial Model\n"}
{"snippet": "print(\"RMSE: %.2f\"\n      % math.sqrt(np.mean((regr.predict(X_test) - y_test) ** 2)))\n", "intent": "- Prediction score is about 70 which is not really optimal\n"}
{"snippet": "predictions = xgb.predict(X_test)\nprint(explained_variance_score(predictions,y_test))\n", "intent": "you can check [this page](http://www.cnblogs.com/nolonely/p/7009001.html) for more details of explained_variance_score()\n"}
{"snippet": "print('\\nCalculating accuracy for 3 trained classifier...')\naccuracy1 = metrics.accuracy_score(y_test, prediction1)\naccuracy2 = metrics.accuracy_score(y_test, prediction2)\naccuracy3 = metrics.accuracy_score(y_test, prediction3)\nprint(accuracy1)\nprint(accuracy2)\nprint(accuracy3)\n", "intent": "- The accuracy is calculated by comparing both of the labels of testing datasets and the predicted results from the classifier. \n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.5f}'.format(error))\n    if error<0.001:\n        print(\"Congras! It pass\")\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss,i = 0,0\n    for layer in style_layers:\n            gram_cur = gram_matrix(feats[layer], normalize=True)\n            style_loss += style_weights[i]*(torch.sum((gram_cur - style_targets[i])**2))\n            i +=1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        idx = style_layers[i]\n        curr_gram = gram_matrix(feats[idx])\n        curr_loss = style_weights[i] * tf.reduce_sum((curr_gram - style_targets[i]) ** 2)\n        loss += curr_loss\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\nprint(scores)\nprint(np.mean(scores), \" +/- \", np.std(scores))\n", "intent": "Perform a cross validation experiment\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Getting the predictions from the trained Model\n"}
{"snippet": "X_train = train_df[features]\ny_train = train_df[target]\nX_test = test_df[features]\ny_test = test_df[target]\nprint(\"Predictions on training data: \", clf.predict(X_train))\nprint(\"Accuracy on training data: \", clf.score(X_train,y_train))\nprint(\"Predictions on test data: \",clf.predict(X_test))\nprint(\"Accuracy on test data: \",clf.score(X_test,y_test))\n", "intent": "6\\. Using the classifier built in 2.2, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?\n"}
{"snippet": "def top_k_error(top_k, total):\n    return 100.0 - top_k / total * 100.0\n", "intent": "Show the percentage of error in the top-k most probable classes.\n"}
{"snippet": "predictions=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predicted=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(Y_test,predicted))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "mod.predict(pd.Series(data={'gdp':[179926.6]}))\n", "intent": "From data we got before, we know that China GDP in Jun 2016 is 17,992,660 million yuan. We can use the data to predict RMB exchange rate.\n"}
{"snippet": "y_hat = mod.predict(x)\n", "intent": "Then we create the predicted values:\n"}
{"snippet": "mod.predict(179926.6)\n", "intent": "From data we got before, we know that China GDP in Jun 2016 is 17,992,660 million yuan. We can use the data to predict RMB exchange rate.\n"}
{"snippet": "embedding_model.predict(numpy.array([[wordvec.vocab['emotion'].index]]))[0][0] == wordvec['emotion']\n", "intent": "Looks good, right? But let's not waste our time when the computer could tell us definitively and quickly:\n"}
{"snippet": "conf = confusion_matrix(y_test, y_pred)\nprec = precision_score(y_test, y_pred, average='weighted')\nprint(prec)\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(recall)\n", "intent": "the confusion matrix tells me all about accuracy, precision and recall \n"}
{"snippet": "models = [model1, model2, model3, model4, model5]\nensemble_pred = np.zeros((5, Xtest.shape[0], 9))\nfor i, m in enumerate(models):\n  pred = m.predict(Xtest)\n  ensemble_pred[i] = pred\npred = ensemble_pred.mean(axis=0)\npred[pred < .5] = 0\npred[pred > .5] = 1\nprint pred.shape\n", "intent": "KAGGLE LB 0.81371!!! 14th place atm\n"}
{"snippet": "pred_train = model.predict_proba(train_train.iloc[:, 2:])\nscore = log_loss(train_train[\"fault_severity\"], pred_train)\nprint 'Logloss for the training set: ' + str(score)\npred_train = model.predict_proba(train_test.iloc[:, 2:])\nscore = log_loss(train_test[\"fault_severity\"], pred_train)\nprint 'Logloss for the validation set: ' + str(score)\n", "intent": "Determine the log loss on the training and validation sets.\n"}
{"snippet": "x = model.predict_proba(test_f.iloc[:,1:])\n", "intent": "Now use the fitted model to predict on the test data.\n"}
{"snippet": "if toEnsemble:\n    pred_test_l = model_l.predict_proba(test_f.iloc[:, 1:])\n    pred_test_r = model_r.predict_proba(test_f.iloc[:, 1:])\n    pred_test_x = model_x.predict_proba(test_f.iloc[:, 1:])\n    x = 0.0*pred_test_l + 0.5*pred_test_r + 0.5*pred_test_x\n", "intent": "Do below if you want to ensemble.\n"}
{"snippet": "t=time.time()\nn_predict = 10\nprint('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\nprint('For these',n_predict, 'labels: ', y_test[0:n_predict])\nt2 = time.time()\nprint(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n", "intent": "__Predict the result with the test dataset__\n"}
{"snippet": "t=time.time()\nn_predict = 10\nprint('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\nprint('For these',n_predict, 'labels: ', y_test[0:n_predict])\nt2 = time.time()\nprint(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n", "intent": "__Evaluate the accuracy of SVM__\n"}
{"snippet": "print accuracy_score(outcomes, predictions)\n", "intent": "Question 1\nUsing the RMS Titanic data, how accurate would a prediction be that none of the passengers survived?\n"}
{"snippet": "print len(predictions)\nprint accuracy_score(outcomes, predictions)\n", "intent": "*How accurate would a prediction be that all female passengers survived and the remaining passengers did not survive?*  \n"}
{"snippet": "print len(predictions)\nprint accuracy_score(outcomes, predictions)\n", "intent": "*How accurate would a prediction be that all female passengers and all male passengers younger than 10 survived?* \n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "95% accuracy looks too good to be true, so let's define a classifier that classifies images in the not-5 class\n"}
{"snippet": "dtc_pred = dtc.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,dtc_pred))\n", "intent": "And a look at the results:\n"}
{"snippet": "dtc_pred = dtc.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,dtc_pred))\n", "intent": "The graphical Decision Tree would be too complex to be presented visually so we go with numerical results only.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = boosted_classifier.predict(dtrain)\naccuracy_score(y_train_encoded, predictions)\n", "intent": "Using arbitrarily-chosen hyperparemeters, the model achieves nearly perfect accuracy on the training set.\n"}
{"snippet": "test_predictions = boosted_classifier.predict(dtest)\n", "intent": "Generate predictions on the test set using fitted classifier.\n"}
{"snippet": "i = 1\nscore_list = []\nfor model in model_list:\n    predict = to_one_hot(model.predict(x_test, batch_size = 50))\n    score = f1_score(to_labels(y_test), to_labels(predict), average = 'micro')\n    score_list.append(score)\n    print(f'F1-score for model {i} is {score}')\n    i += 1\n", "intent": "Check individual model performance on test set\n"}
{"snippet": "overall_predictions = []\nfor model in model_list:\n    overall_predictions.append(model.predict(x_test, batch_size = 50))\naggregate_predictions = sum(overall_predictions)\n", "intent": "Aggregate models for overall ensemble prediction by summing predicted probabilities for each model and taking the argmax\n"}
{"snippet": "r2Score=r2_score(y_test, clf.predict(X_test))\nmse=mean_squared_error(y_test, clf.predict(X_test))\nprint(\"LinearRegression R2 Score is: \", r2Score)\nprint(\"LinearRegression MSE is: \", mse)\n", "intent": "+ Learn how to use sklearn.metrics.r2_score and sklearn.metrics.mean_squared_error\n"}
{"snippet": "for name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))\n", "intent": "Finally, lets make sure that the models have been fitted correctly.\n"}
{"snippet": "for name, model in fitted_models.items():\n    pred = model.predict(X_test)\n    print( name )\n    print( '--------' )\n    print( 'R^2:', r2_score(y_test, pred ))\n    print( 'MAE:', mean_absolute_error(y_test, pred))\n    print('\\n')\n", "intent": "Finally, let's see how the fitted models perform on the test set!\n"}
{"snippet": "y_pred = lr.predict(X_test)\n", "intent": "Predict using the model\n"}
{"snippet": "import numpy as np\nfrom sklearn import metrics\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Calculate the root mean squared error of the testing data to evaluate the model.\n"}
{"snippet": "y_pred_train = lr.predict(X_train)\n", "intent": "Also check the root mean squared error of training data\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nprint (precision_score(y_test, y_test_pred))\nprint (recall_score(y_test, y_test_pred) )\nprint (accuracy_score(y_test, y_test_pred))\n", "intent": "Print precision, recall and accuracy\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))\n", "intent": "try it out on a few instances from the training set\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "compute the same scores for the Linear Regression model \n"}
{"snippet": "X_new = np.array([[ 5 , 2.9 , 1 , 0.2 ]])\nX_new.shape\nprediction = knn.predict( X_new )\nprediction \niris_dataset [ 'target_names' ][ prediction ]\n", "intent": "now try to make a prediction.\n"}
{"snippet": "generator_params = init_neural_net([latent_d, hidden_d, input_d])\ngenerator_out = neural_net(Z, generator_params)\nloss = tf.reduce_sum(2.*sinkhorn_loss(X, generator_out) - sinkhorn_loss(X, X) - sinkhorn_loss(generator_out, generator_out))\n", "intent": "Define the generator model...\n"}
{"snippet": "docs_new = ['God is love', 'Heart attacks are common', 'Disbelief in a proposition', 'Disbelief in a proposition means that one does not believe it to be true', 'OpenGL on the GPU is fast']\nX_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredicted = clf.predict(X_new_tfidf)\nfor doc, category in zip(docs_new, predicted):\n     print('%r => %s' % (doc, twenty_train.target_names[category]))\n", "intent": "Here tfidf_transformer is used to classify\n"}
{"snippet": "from sklearn import metrics\ny_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\", \"bird\"]\ny_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\", \"bird\"]\nprint(metrics.classification_report(y_true, y_pred,\n    target_names=[\"ant\", \"bird\", \"cat\"]))\n", "intent": "Here we will use a simple example to show classification reports and confusion matrices.\n- y_true is the test data\n- y_pred is the prediction\n"}
{"snippet": "print(metrics.classification_report(twenty_test.target, predicted_svm,\n    target_names=twenty_test.target_names))\n", "intent": "**Back to '20 newsgroups dataset'**\n"}
{"snippet": "print(np.mean(predicted_svm == twenty_test.target))\nprint(metrics.accuracy_score(twenty_test.target, predicted_svm, normalize=True, sample_weight=None))\n", "intent": "We can see where the 91% score came from.\n"}
{"snippet": "print(metrics.classification_report(twenty_test.target, predicted_svm,\n    target_names=twenty_test.target_names))\nmetrics.confusion_matrix(twenty_test.target, predicted_svm)\n", "intent": "Moving on from that lets see where the improvements where made.\n"}
{"snippet": "print(metrics.classification_report(y_true, y_pred,\n    target_names=[\"ant\", \"bird\", \"cat\"]))\n", "intent": "5 correct predictions out of 7 values. 71% accuracy\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * torch.sum(torch.pow(gram-style_targets[i], 2))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L = []\n    for i, l in enumerate(style_layers):\n        A = style_targets[i]\n        G = gram_matrix(feats[l], normalize=True)\n        L.append(style_weights[i] * tf.square(tf.norm(A - G)))\n    loss = tf.add_n(L)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "pred = svc_mod.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print(classification_report(y,\n                            y_pred,\n                            target_names=['neither', 'corn', 'soybean']))\n", "intent": "Quantify the performance on the train features, which represents the upper limit for classification accuracy when a prediction is run on a new image.\n"}
{"snippet": "y_pred = clf.predict(X)\nprint(np.unique(y_pred))\n", "intent": "Check to see what classes are predicted\n"}
{"snippet": "print(classification_report(to_y(gold_class_band),\n                            y_pred_test,\n                            target_names=['neither', 'corn', 'soybean']))\n", "intent": "Let's calculate the classification accuracy metrics (precision, recall, f1-score)\n"}
{"snippet": "target_names = ['Non-Spam', 'Spam']\nprint (classification_report(Y_test, pred_array_values, target_names=target_names))\n", "intent": "Bad performance here too by B-LSTM. \n:(\n"}
{"snippet": "print(classification_report(Y_test,Y_pred))\n", "intent": "The training loss function converges after about 150 iterations(with the hyperparameters above)\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\ndog_breed_predictions2 = [np.argmax(model2.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy2 = 100*np.sum(np.array(dog_breed_predictions2)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions2)\nprint('Test accuracy-2: %.4f%%' % test_accuracy2)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "sf['m1_label'] = model_1.predict(sf)\nsf['m2_label'] = model_2.predict(sf)\nsf['m3_label'] = model_3.predict(sf)\n", "intent": "**Step 6:** We can now store our predictions from each model in columns in the original SFrame\n"}
{"snippet": "sf_competition['prediction'] = model_1.predict(sf_competition)\n", "intent": "Now let's apply our existing model to the competition data set\n"}
{"snippet": "yhat = model_final.predict(xgb.DMatrix(X_test[predictors]))\nerror = rmspe(X_test.Sales.values, np.exp(yhat))\nprint('First validation yelds RMSPE: {:.6f}'.format(error))\n", "intent": "We dealt with overfitting, but due to the decrease of learning rate (`eta`) we got a reduced overall score on the test set (~0.11 to ~0.14).\n"}
{"snippet": "y_pred = gbm.predict(x_test)\n", "intent": "Predicting the result over small testing data (This is not the actual testing data).\n"}
{"snippet": "from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report,matthews_corrcoef\ngradient_accy = round(accuracy_score(y_test,y_pred), 3)\nprint(gradient_accy)\nprint(precision_score(y_test,y_pred,average='micro'))\nprint(matthews_corrcoef(y_test,y_pred))\n", "intent": "Checking the metrics to check the performance of model.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint metrics.flat_classification_report(y_test, y_pred, labels=sorted(labels), digits=3 )\n", "intent": "The best score is the PA without the slack variables. Let's evaluate it on the test dataset.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint metrics.flat_classification_report(y_test, y_pred, labels=sorted(labels), digits=3 )\n", "intent": "As we can observe the best number of iterations is 80. A bigger number and the \nLet's evaluate the model against the test data.\n"}
{"snippet": "predictions = lr.predict(X_test)\nprint 'Predictions completed'\n", "intent": "Then, we use this model to predict the values of the target variable 'Attrition'.\n"}
{"snippet": "predictions1 = rf.predict(X_test)\nprint \"Predictions completed\"\n", "intent": "Then, we use this model to predict the values of the target variable 'Attrition'.\n"}
{"snippet": "def predict(img_array):\n    img_array = img_array.reshape(-1,299,299,3)\n    predict_values, logit_values = sess.run([end_points['Predictions'], logits], feed_dict={inputs: img_array})\n    return {'predict_values': predict_values, 'logit_values': logit_values}\n", "intent": "Prediction function:\n"}
{"snippet": "predictions = DNN_model.predict(pred_input_fun)\n", "intent": "Drawing out the predictions of test set from the trained model\n"}
{"snippet": "svm_clf.predict([[5.5, 1.7]])\n", "intent": "Note that unliness a regression classifer this does not ouptut posterior probablites \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nwine_predictions = lin_reg.predict(wine_data_scaled)\nlin_mse = mean_squared_error(train_label_wine, wine_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Check the RMSE and MAE:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nlin_mae = mean_absolute_error(train_label_wine, wine_predictions)\nlin_mae\n", "intent": "We differ from + or - 0.65 for each predictions in terms of quality. Pretty good actually...\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, wine_data_scaled, train_label_wine,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Let compare the results with the linear regression:\n"}
{"snippet": "from sklearn import metrics\ny_pred=kmeans.predict(X_test)\nprint(metrics.confusion_matrix(y_test, y_pred))\n", "intent": "In this step, you need to provide the evaluation of your clustering model. Print out a confusion matrix.\n"}
{"snippet": "n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "Validating and choosing the best model\n"}
{"snippet": "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\nprint(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n", "intent": "We check the performance on the whole test data using the evaluate() method.\n"}
{"snippet": "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n  x={\"x\": eval_data},\n  y=eval_labels,\n  num_epochs=1,\n  shuffle=False)\neval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\nprint(eval_results)\n", "intent": "<center><img src=\"images/watiing.jpg\" width=\"650\"/></center>\n"}
{"snippet": "import warnings; warnings.simplefilter('ignore')\nscores = cross_val_score(linregscaled, X_btc_scaled, y_btc, cv = StratifiedKFold(10, shuffle = True, random_state = 10))\nprint(\"Cross validation scores: {}\".format(scores))\nprint(\"Average CV scores for linear regression: {:.7f}\".format(scores.mean()))\nscores = cross_val_score(linlasso, X_btc_scaled, y_btc, cv = StratifiedKFold(10, shuffle = True, random_state = 10))\nprint(\"Cross validation scores: {}\".format(scores))\nprint(\"Average CV scores for Lasso Regression: {:.7f}\".format(scores.mean()))\nscores = cross_val_score(linRidge, X_btc_scaled, y_btc, cv = StratifiedKFold(10, shuffle = True, random_state = 10))\nprint(\"Cross validation scores: {}\".format(scores))\nprint(\"Average CV scores for Ridge Regression: {:.7f}\".format(scores.mean()))\n", "intent": "Let us use StratifiedKFold to do a better sampling for the trainset and try cross validation for Linear, Ridge and Lasso Regression.\n"}
{"snippet": "h = .02  \nx_min, x_max = 0, 4.5\ny_min, y_max = 1.5, 5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nz_svc = svc.predict(np.c_[xx.ravel(), yy.ravel()])\nz_svc = z_svc.reshape(xx.shape)\n", "intent": "- Perform prediction over grid:\n"}
{"snippet": "svc.predict(email1_features.reshape(1, -1))[0]\n", "intent": "- If we predict on a single sample we have to reshape `X`; see warning when you don't\n"}
{"snippet": "knn.predict([15,500])\n", "intent": "**Step 4:** Predict the response for a new observation\nLet's say a writer wrote 500 words for 15 days.\n"}
{"snippet": "y_pred_prob = lr_pca.predict_proba(X_test_pca)\n", "intent": "We will now try to select the optimum probability threshold for our model\n"}
{"snippet": "ENSEMBLE_MODEL_pred = list(map(lambda x: 1 if (sum(x)>=2) else 0, zip(LR.predict(X_test),\n                                                          dtree.predict(X_test),            \n                                                          RF.predict(X_test),                                                          \n                                                          SVM.predict(X_test_pca))))\nprintMetrics(y_test, ENSEMBLE_MODEL_pred)\n", "intent": "Finally we will build ensemble mode below and take the majority vote!!\n"}
{"snippet": "results = model.evaluate(x_test, y_test)\n", "intent": "The keras model.evaluvate returns 2 value [Losses, Accuracy]\n"}
{"snippet": "cross_val_score(regr, data2,target2, cv=10)\n", "intent": "Trying the model with the improved features. It looks like it is much better, because the holdout % is much closer to the value on the test sample.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(regr, data, target, cv=10)\n", "intent": "A first attempt at the OLS model shows a very mixed r squared value with 20% holdout. \n"}
{"snippet": "scores = cross_val_score(rfc, X, Y, cv=10)\nprint('Mean {} and Standard deviation {}'.format(np.mean(scores),np.std(scores)))\n", "intent": "Scores of 10 holdout groups with the Random Forest model\n"}
{"snippet": "X = df.drop('Class',1)\nY = df['Class']\nX = pd.get_dummies(X)\nX = X.dropna(axis=1)\nscores_big = cross_val_score(rfc, X, Y, cv=10)\n", "intent": "A confusion matrix for the subsample with the Random Forest model\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print(\">\", pair[0])\n        print(\"=\", pair[1])\n        output_words, attentions = evaluate(encoder, attn_decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print(\"<\", output_sentence)\n        print('\\n')\n", "intent": "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n"}
{"snippet": "lasso_pred = lasso.predict(data_test) \n", "intent": "Predict the test data using the Lasso model\n"}
{"snippet": "XGBR_pred = regr.predict(data_test)\nprint(\"XGBoost score on test set in log format: \", mean_squared_error(pplb_test, XGBR_pred))\n", "intent": "Run prediction on training set\n"}
{"snippet": "logreg.predict(X_new)[0:10]\n", "intent": "**evaluate classification**\n"}
{"snippet": "differenced_preds = model.predict(X, batch_size=batch_size)\nlast_obs = df.total_passengers.ix[look_back:differencing_order+look_back].values\ninverted_preds = inverse_differencing(differenced_preds, last_obs, differencing_order)\ndf['fc'] = np.nan\ndf['fc'].ix[differencing_order+look_back:] = inverted_preds[differencing_order:]\ndifferenced_preds.shape, last_obs.shape, inverted_preds.shape\n", "intent": "Generate predictions, convert it from differencing scale to original scale and save it to dataframe\n"}
{"snippet": "inverted_preds = inverse_differencing(svr.predict(X), last_obs, differencing_order)\ndf['svr'] = np.nan\ndf['svr'].ix[differencing_order+look_back2:] = inverted_preds[differencing_order:].reshape(-1,)\ndifferenced_preds.shape, last_obs.shape, inverted_preds.shape\n", "intent": "Generate predictions, convert it from differencing scale to original scale and save it to dataframe\n"}
{"snippet": "inverted_preds = inverse_differencing(grid.predict(X), last_obs, differencing_order)\ndf['knn'] = np.nan\ndf['knn'].ix[differencing_order+look_back:] = inverted_preds[differencing_order:].reshape(-1,)\n", "intent": "Generate predictions, convert it from differencing scale to original scale and save it to dataframe\n"}
{"snippet": "inverted_preds = inverse_differencing(grid.predict(X), last_obs, differencing_order)\ndf['tree'] = np.nan\ndf['tree'].ix[differencing_order+look_back2:] = inverted_preds[differencing_order:].reshape(-1,)\ndifferenced_preds.shape, last_obs.shape, inverted_preds.shape\n", "intent": "Generate predictions, convert it from differencing scale to original scale and save it to dataframe\n"}
{"snippet": "out_scores,out_boxes,out_classes=predict(sess,\"0029.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "pathDir = os.listdir(\"images\")\nfor allDir in pathDir:\n    predict(sess,allDir)\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "imdb_lstm_scores = imdb_lstm_model.evaluate(imdb_x_test_padded, imdb_y_test)\nprint('loss: {} accuracy: {}'.format(*imdb_lstm_scores))\n", "intent": "Assess the model. __This takes awhile. You might not want to re-run it.__\n"}
{"snippet": "one_unit_SRNN.predict(numpy.array([ [[3], [3], [7]] ]))\n", "intent": "This passes in a single sample that has three time steps.\n"}
{"snippet": "RSS=np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint(RSS)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "test_batches = vgg.get_batches(path + 'test', batch_size=batch_size)\nimgs, labels = next(test_batches)\nprobability, idx, classes = vgg.predict(imgs, True)\nprint(prediction)\nplots(imgs, titles=classes)\n", "intent": "Here the prediction 5 and 6 are wrong. I'll try a prediction with a test set.\n"}
{"snippet": "imgs, labels = next(test_batches)\nprobability, idx, classes = vgg.predict(imgs, True)\nprint(prediction)\nplots(imgs, titles=classes)\n", "intent": "Here the fifth element was predicted as 'catalina' and should be 'ignacio'\n"}
{"snippet": "final_model = rnd_search.best_estimator_\nscores = cross_val_score(final_model, X_prepared_tr, y_tr,\n                         scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\ndef print_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\nprint_scores(rmse_scores)\n", "intent": "giving an mean score of\n"}
{"snippet": "some_data = X_te.iloc[:5]\nsome_labels = y_te.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", final_model.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))\n", "intent": "Now that the model has been trained, let us make predictions on the hold-out test set\n"}
{"snippet": "y_train_pred = final_model.predict(X_prepared_tr)\nX_prepared_te = full_pipeline.transform(X_te)\ny_test_pred = final_model.predict(X_prepared_te)\nprint('RMSE train: %.3f, test: %.3f' % (\n         np.sqrt(mean_squared_error(y_tr, y_train_pred)),\n         np.sqrt(mean_squared_error(y_te, y_test_pred))) )\nprint('R^2 train: %.3f, test: %.3f' % (\n         r2_score(y_tr, y_train_pred),\n         r2_score(y_te, y_test_pred)))\n", "intent": "Seems like the model is generalizing reasonably to new data! Let us compare the train and test errors. \n"}
{"snippet": "np.random.seed(1)\nx = np.random.randn(10, 10, 3)\ny = np.random.randn(10, 10, 3)\na = K.constant(x)\nb = K.constant(y)\ntest = style_reconstruction_loss(a, b)\nprint('Result:  ', K.eval(test))\nprint('Expected:', 0.09799164)\n", "intent": "Test your implementation:\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Now predict values for the testing data.\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "Create a classification report for the model.\n"}
{"snippet": "from sklearn import metrics\nrtest = metrics.r2_score(y_test, clf.predict(X_test))\nrtrain = metrics.r2_score(y_train, clf.predict(X_train))\nprint \"rtest:\",rtest\nprint \"rtrain:\", rtrain\n", "intent": "I'll start by implementing the r2 method\n"}
{"snippet": "MSE = metrics.mean_squared_error(y_test, clf.predict(X_test))\nprint \"Mean Squared Error:\", MSE\n", "intent": "Now I'll implement the mse method\n"}
{"snippet": "data_flattened = data.reshape(data.count(),20)\nlabels = bestModel.predict(data_flattened)\nimgLabels = labels.reshape(2,76,87)\ndraw_image(imgLabels[0,:,:])\n", "intent": "<div class=\"alert alert-info\">\nComplete the source code below to visualize the result of clustering.\n</div>\n"}
{"snippet": "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\nD_train_step = D_solver.minimize(D_loss, var_list=D_vars)\nG_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n", "intent": "Create new training steps so we instead minimize the LSGAN loss: just changing graph little bit\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        style_loss += style_weights[i]*tf.reduce_sum(tf.square(gram_matrix(feats[layer]) - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = model_5.predict(validation_data[features].to_numpy())\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "data['true_id'] = data[id_value].apply(lambda x: (x - min_id)*scale_value + min_id)\ndata['true_qid_min_rev'] = data.true_id.apply(lambda true_id: 1/(true_id+regul))\ndata['DR_seen0'] = lr.predict(data[['true_qid_min_rev', 'true_id']])\n", "intent": "Adding dueprates for seen0 group into submission\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy is: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_score_valid1=clf1.predict(X_valid)\naccuracy_valid1=(sum(y_score_valid1==y_valid)/len(y_valid))\nprint(accuracy_valid1)\ny_score_valid2=clf2.predict(X_valid)\naccuracy_valid2=(sum(y_score_valid2==y_valid)/len(y_valid))\nprint(accuracy_valid2)\n", "intent": "Validation Set - Model Selection\n"}
{"snippet": "y_score_test=clf1.predict(X_test)\naccuracy_test=(sum(y_score_test==y_test)/len(y_test))\nprint(accuracy_test)\n", "intent": "Model 1 is selected. Evaluate the model performance on test set\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test_X, y_pred_X))\n", "intent": "----\nComputing Precision, Recall, F-Score and Support\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Now predicting values for the testing data.**\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\nXception_test_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % Xception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint predicted\n", "intent": "Now we predict the class labels for the test set.\n"}
{"snippet": "probs = model2.predict_proba(X_test)\nprint probs\n", "intent": "We will also generate the class probabilities.\n"}
{"snippet": "print metrics.accuracy_score(y_test, predicted)\nprint metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "Now we generate some evaluation metrics.\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test, predicted)\n", "intent": "We can also see the confusion matrix and a classification report with other metrics.\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test, predicted)\n", "intent": "Similar accuracy to what we experienced when training and predicting on the same data.\n"}
{"snippet": "print metrics.accuracy_score(y_test, predicted)\nprint metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "Something less accurate then the second model.\n"}
{"snippet": "pred = clf.predict(Xtest)\nfrom sklearn import metrics\nconf_mat = metrics.confusion_matrix(ytest,pred)\nprint(\"Accuracy: %0.4f\" % metrics.accuracy_score(ytest,pred))\nprint \"{0}\".format(metrics.classification_report(ytest,pred))\nprint(\"Confusion matrix:\\n{0}\" .format(conf_mat))\n", "intent": "Testing on test images\n"}
{"snippet": "from __future__ import division\ndef calculate_training_error(pred, true):\n    num_wrong=0\n    for i in zip(pred,true):\n        if i[0]!=i[1]: \n            num_wrong+=1\n    print \"Training error: \"+str(num_wrong/len(pred))\n", "intent": "It's useful to define this function early on, so we can recycle it throughout the rest of the problems.\n"}
{"snippet": "from __future__ import division\ndef calculate_training_error(pred, true):\n    num_wrong=0\n    for i in zip(pred,true):\n        if i[0]!=i[1]:\n            num_wrong+=1\n    print \"Training error: \"+str(num_wrong/len(pred))\n", "intent": "It's useful to define this function early on, so we can recycle it throughout the rest of the problems.\n"}
{"snippet": "from __future__ import division\ndef calculate_training_error(pred, true):\n    num_wrong=0\n    for i in zip(pred,true):\n        if i[0]!=i[1]: \n            num_wrong+=1\n    print \"Training error: \"+str(num_wrong/len(pred))\n", "intent": "It's convenient to define a simple function that we can use to calculate the training error for our predictions.  \n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\nscorePerc = accuracy_score(y_true,y_pred)\nprint scorePerc\naccuracy_score(y_true,y_pred,normalize=False)\n", "intent": "In classification metrics checked are \nnumber of collectly identfied instance over all instance\n"}
{"snippet": "tensors = test_tensors if augment else test_Xception\nXception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(tensor, axis=0))) for tensor in tensors]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(metrics.accuracy_score(y_test, predicted))\nprint(metrics.roc_auc_score(y_test,probs[:,1]))\n", "intent": "Now let's generate some evaluation metrics.\n"}
{"snippet": "print(metrics.confusion_matrix(y_test, predicted))\nprint(metrics.classification_report(y_test, predicted))\n", "intent": "We can also see the confusion matrix and a classification report with other metrics.\n"}
{"snippet": "def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 10))\n    return rmse\n", "intent": "LASSO is a type of regression used for penalizing models with two many variables. Let's how it works on our data. \n"}
{"snippet": "log_loss(y_test, y_pred)\n", "intent": "The top scoring entry in the Kaggle competition had a log-loss value of 0.31490. How do we compare?\n"}
{"snippet": "log_loss(y_test, y_pred)\n", "intent": "Be that as it may, it does appear to provide better predictive power than logistic regression.\n"}
{"snippet": "model.load_weights('saved_models/weights.best.Resnet50.hdf5')\npredictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nmodel_simple.load_weights('saved_models/weights.best.Resnet50S.hdf5')\npredictions = [np.argmax(model_simple.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy simple case: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict_full_floor = np.floor(predict_full)\nerror_rf_full_floor = calc_error(y_test, predict_full_floor)\nrmse_rf_full_floor = calc_rmse(y_test, predict_full_floor)\nprint(\"Error = \", error_rf_full_floor)\nprint(\"RMSE = \", rmse_rf_full_floor)\n", "intent": "<B>Observation: </B>Can we simply fix lot of cases by taking floor of our prediction?\n"}
{"snippet": "kFolds = 10\ncorss_val_rf_full_g_prev = cross_val_score(full_pipeline_with_predictor, X_train, \\\n                             y_train,\n                             scoring=\"neg_mean_squared_error\", cv=kFolds)\ncorss_val_rf_full_g_prev\n", "intent": "<B>Significance Test</B>\n"}
{"snippet": "dog_breed_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "import numpy as np\ndef nearby(words, model, sess, dictionary, index_dictionary, num=20):\n  analogy = np.array([dictionary.get(w, 0) for w in line])[np.newaxis,:]\n  idx = model.predict(sess, analogy)\n  print(line)\n  for i in idx[0]:\n    print(index_dictionary[i])\n", "intent": "To see the results, we can define a function that finds the nearest words.\n"}
{"snippet": "some_data = titanic.iloc[:5]\nsome_labels = titanic_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", list(sgd_clf.predict(some_data_prepared)))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "Ad-hoc demo of predictions\n"}
{"snippet": "answer = dt_clf.predict_proba(clean_X)[:,1]  \naccuracy = accuracy_score(clean_clf_y, dt_clf.predict(clean_X))  \nprint(\"the accuracy is \",accuracy)\nprint(classification_report(clean_clf_y, answer, target_names = ['negative', 'positive']))\n", "intent": " i. the accuracy, precision and recall are as follows:\n"}
{"snippet": "LR_MSE = mean_squared_error(clean_reg_y, LR.predict(clean_X))\nprint(\"the MSE for linear regression is:\")\nprint(LR_MSE)\n", "intent": "ii.What is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "RG_MSE = mean_squared_error(clean_reg_y, RG.predict(clean_X))\nprint(\"the MSE for ridge regression is:\")\nprint(RG_MSE)\n", "intent": "ii.What is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "poly_LR_MSE = mean_squared_error(clean_reg_y, poly_LR.predict(poly_clean_X))\nprint(\"the poly regression MSE is:\")\nprint(poly_LR_MSE)\n", "intent": "ii.What is the MSE on the training set (train on all the data then test on it all)?\n"}
{"snippet": "full_answer = full_dt_clf.predict_proba(full_X)[:,1]  \nfull_accuracy = accuracy_score(full_clf_y, full_dt_clf.predict(full_X))  \nprint(\"the accuracy is \",full_accuracy)\nprint(classification_report(full_clf_y,full_answer, target_names = ['negative', 'positive']))\n", "intent": "i. the accuracy, precision and recall are as follows:\n"}
{"snippet": "full_ten_fold_accuracy = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='accuracy')\nfull_ten_fold_precision = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='precision')\nfull_ten_fold_recall = cross_val_score(full_dt_clf, full_X, full_clf_y, cv=10,scoring='recall')\nprint(\"the accuracy for 10-fold cv is:\")\nprint(full_ten_fold_accuracy)\nprint(\"the precision for 10-fold cv is:\")\nprint(full_ten_fold_precision)\nprint(\"the recall for 10-fold cv is:\")\nprint(full_ten_fold_recall)\n", "intent": "i. What are the 10-fold cross-validation accuracy, precision, and recall?\n"}
{"snippet": "predictions = rgr.predict(y_train)\n", "intent": "Now, the time to make predictions. Here we go!\n"}
{"snippet": "TSS = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint(TSS)\n", "intent": "Residual Sum of Squares:\n"}
{"snippet": "ESS = np.sum(lm.predict(X) - np.mean(bos.PRICE)) ** 2\nprint(ESS)\n", "intent": "Explained Sum-of-Squares\n"}
{"snippet": "sample_features = np.reshape(ts_sample_smooth[0:10], (1,10))\nsample_pred, sample_pred_std = clf.predict(sample_features, return_std=True)\nplot_truth_median_std(sample_features[0], ts_sample_smooth[10], sample_pred[0], sample_pred_std[0])\n", "intent": "Now we've got normalized data, sampled to even intervals, we can go ahead and try predictions.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test Accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict([\"menace\", \"prowess\"], [\"Creature\"], \"5\", \"5\", 0, 0, 6)\n", "intent": "(1) What is the most appropriate color for this card?\nScrappy Survivor\n[converted mana cost (CMC): 6]\nCreature --\n5/5\nMenace, prowess\n"}
{"snippet": "y_pred = logistic_regression.predict_proba(X_test)\nax = sns.kdeplot(y_pred[:,0],shade=True)\nax.set_title(\"Probablity distribution of train and car\")\n", "intent": "Since logstic regression did so well lets, have a closer look at the probablity distributions of it's predictions\n"}
{"snippet": "history_evaluate = model.evaluate(dek_happiness,deks_1_y);\nhistory_evaluate\n", "intent": "First picture fails\n"}
{"snippet": "num_folds = 3\naccuracy_values = cross_validation.cross_val_score(classifier, X, y, scoring='accuracy', cv=num_folds)\nprint(\"Accuracy: \" + str(round(100*accuracy_values.mean(),2)) + \"%\")\nprecision_values = cross_validation.cross_val_score(classifier, X, y, scoring='precision_weighted', cv=num_folds)\nprint(\"Precision: \" + str(round(100*precision_values.mean(),2)) + \"%\")\nrecall_values = cross_validation.cross_val_score(classifier, X, y, scoring='recall_weighted', cv=num_folds)\nprint(\"Recall: \" + str(round(100*recall_values.mean(), 2)) + \"%\")\nf1_values = cross_validation.cross_val_score(classifier, X, y, scoring='f1_weighted', cv=num_folds)\nprint(\"F1: \" + str(round(100*f1_values.mean(), 2)) + \"%\")\n", "intent": "Let's use the inbuilt functions to calculate the accuracy, precision, and recall values based on threeshold cross validation:\n"}
{"snippet": "N, H, W = x_test.shape\nx = x_test.reshape(N, H*W).astype('float') / 255\ny = to_categorical(y_test, num_classes=10)\ntest_loss, test_acc = model.evaluate(x, y, batch_size=50)\nprint 'Accuracy on test data %f' % test_acc\n", "intent": "Now training is done, we are ready to evaluate the model.\n"}
{"snippet": "def print_measurements(predictions, labels_test):\n    predictions = np.round(predictions)\n    accuracy = metrics.accuracy_score(labels_test, predictions)\n    recall = metrics.recall_score(labels_test, predictions)\n    precision = metrics.precision_score(labels_test, predictions)\n    f1 = metrics.f1_score(labels_test, predictions)\n    print('Accuracy:  %f %%' % (accuracy * 100))\n    print('Recall:    %f %%' % (recall * 100))\n    print('Precision: %f %%' % (precision * 100))\n    print('F1:        %f %%' % (f1 * 100))\n", "intent": "Since the data is highly unbalanced, Accuracy alone is not suitable to validate the model\n"}
{"snippet": "n_samples = rows*cols\nflat_pixels = bands_data.reshape((n_samples, n_bands))\nresult = classifier.predict(flat_pixels)\nclassification = result.reshape((rows, cols))\n", "intent": "Let's apply the model to the night lights image for classification. \n"}
{"snippet": "predicted = classifier.predict(objects)\n", "intent": "Now we have to transform all segments into segment models in order to classify them. \n"}
{"snippet": "print(\"Final validation..\")\n_, acc = model.evaluate(x_valid, y_valid, verbose=2)\nprint(\"Accuracy on validation set: {:.3f}\".format(acc))\n", "intent": "Finally, lets evaluate our trained model once again on validatation split, to confirm the final accuracy of the solution:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for i in range(len(style_layers)):\n        k = style_layers[i]\n        loss = style_weights[i] * torch.sum((gram_matrix(feats[k]) - style_targets[i]) ** 2)\n        style_loss += loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    layers = len(style_layers)\n    for i in range(layers):\n        k = style_layers[i]\n        gram_generate = gram_matrix(feats[k])\n        style_loss += 2 * style_weights[i] * tf.nn.l2_loss(gram_generate - style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "x_test = np.array(['i am broke unemployed and single'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "preds = model.predict_classes(val_features, batch_size=batch_size)\nprobs = model.predict_proba(val_features, batch_size=batch_size)[:,0]\nprobs[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "X_train = data_pre.drop('SalePrice',1).copy()\nY_train= data_pre['SalePrice'].copy()\nT_train_xgb = xgb.DMatrix(X_train, Y_train)\nparams = {\"objective\": \"reg:linear\"}\ngbm = xgb.train(dtrain=T_train_xgb,params=params)\nY_pred = gbm.predict(xgb.DMatrix(test_kaggle_pre))\n", "intent": "Not so bad to be the first attemp. Lets see wath happend if we make the submission for kaggle.\n"}
{"snippet": "accuracy_score(testing_labels, predictions)\n", "intent": "<h4> Accuracy </h4>\nThe prediction accuracy for the self-built network is shown below:\n"}
{"snippet": "accuracy_score(testing_labels, model_predictions)\n", "intent": "<h4> Prediction Accuracy</h4>\n"}
{"snippet": "print(metrics.roc_auc_score(y1_test, y1_pred_class))\n", "intent": "Now which is the best set of features selected by AUC\n"}
{"snippet": "predi = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predr = fores.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = logrm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "nbpred = nb.predict(xtest)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "pippred = pipln.predict(xtest)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "model.predict(x_test)\n", "intent": "In order to check the prediction for new data points, we can run:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(preiid, labels_test)\nacc\n", "intent": "**Quiz:**SVM Author ID Accuracy\n"}
{"snippet": "from sklearn.cross_validation import LeaveOneOut\nGscores = cross_val_score(Gmodel, X, y, cv=LeaveOneOut(len(X)))\nGscores\n", "intent": "Next, we will use a cross-validation scheme in which we train on all points but one in each trial known as \"leave-one-out\" cross-validation.\n"}
{"snippet": "from sklearn.cross_validation import LeaveOneOut\nRFscores = cross_val_score(RFmodel, X, y, cv=LeaveOneOut(len(X)))\nRFscores\n", "intent": "Nice, this is better than the array we got using Naive-Bayes model. Let's also try the leave-one-out method again.\n"}
{"snippet": "from sklearn.cross_validation import LeaveOneOut\nLSVCscores = cross_val_score(LSVCmodel, X, y, cv=LeaveOneOut(len(X)))\nLSVCscores\n", "intent": "This array looks slightly better than the one from the Random Forest Classification. Nice! Let's try the leave-one-out method one last time.\n"}
{"snippet": "model = initSimpleConvNN(input_shape=x_train[0].shape)\nH = train_and_report(model, x_train, x_test, y_train, y_test, lr=0.001)\n", "intent": "* Let's start from the simple convolutional model:\n"}
{"snippet": "model = initConvNN(input_shape=x_train[0].shape)\nH = train_and_report(model, x_train, x_test, y_train, y_test, lr=0.001)\n", "intent": "* Now I will train the \"Double\" convolutional model:\n"}
{"snippet": "def predict(x, t, x_test, K=1):\n    X_test = get_X(x_test, K)\n    w_hat = get_w_hat(x, t, K)\n    return  X_test * w_hat\n", "intent": "<div class=\"alert alert-info\">\nWrite a function that, when given `x`, `t` and `x_test`, computes `w_hat` and makes predictions at `x_test`.</div>\n"}
{"snippet": "print('...')\ndef evaluate(X_train, y_data,keep_prob):\n    num_examples = len(X_train)\n    total_accuracy = 0\n    sess = tf.get_default_session()\n    for offset in range(0, num_examples, BATCH_SIZE):\n        batch_x, batch_y = X_train[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:1.0})\n        total_accuracy += (accuracy * len(batch_x))\n    return total_accuracy / num_examples\n", "intent": "This function averages the accuracy of each batch to calculate the total accuracy of the model.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_breed.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def error_metrics(y_test, predictions, model):    \n    print(\"AUC: \", roc_auc_score(y_test, predictions))\n    print(\"Precision: \",precision_score(y_test, predictions, average=\"macro\"))\n    print(\"Recall: \",recall_score(y_test, predictions, average=\"macro\")) \n    print(\"F1 Score: \",f1_score(y_test, predictions, average=\"macro\"))\n    print(\"Accuracy: \", model.score(X_test, y_test))\n", "intent": "Next let's define our error metrics. We'll look at AUC (ROC), precision, recall, F1 score, and just for fun, accuracy.\n"}
{"snippet": "mean_squared_error(d['ViolentCrimesPerPop'], prediction)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n--->The MSE on the training set is 0.016516774880307179.\n"}
{"snippet": "mse1=mean_squared_error(d['ViolentCrimesPerPop'],y_pred)\nprint(\"Mean squared error for the training set is: \")\nprint(mse1)\n", "intent": "ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?\n--->The MSE on the training set is 1.69580219538e-28.\n"}
{"snippet": "def predict(model, x):\n    c = forward_prop(model,x)\n    y_hat = c['a2']\n    y_hat[y_hat > 0.5] = 1\n    y_hat[y_hat < 0.5] = 0\n    return y_hat\n", "intent": "These two moons are to be seperated. To do so we first need to define some more helper functions to complete our model\n"}
{"snippet": "def train(model,X_,y_,learning_rate, num_passes=20000, print_loss=False):\n    for i in range(0, num_passes):\n        cache = forward_prop(model,X_)\n        grads = backward_prop(model,cache,y)\n        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n        if print_loss and i % 100 == 0:\n            y_hat = cache['a2']\n            print('Loss after iteration',i,':',log_loss(y,y_hat))\n            print('Accuracy after iteration',i,':',calc_accuracy(model,X_,y_),'%')\n    return model\n", "intent": "After we have predefined all functions, the training routine of a larger network looks the same as the routine of a smaller network.\n"}
{"snippet": "model.evaluate(x=X_dev,y=y_dev)\n", "intent": "From these graphs it looks like the model learned a lot better than logistic regression, let's evaluate it on the dev set:\n"}
{"snippet": "model.evaluate(x=X_dev,y=y_dev)\n", "intent": "As you can see, with momentum, our model learned much better than before! Let's evaluate it on the dev set:\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Accuracy and loss look good. There seems to be little difference to the outcomes of our dev set.\n"}
{"snippet": "def make_predictions(X):\n    treepred = tree_classifier.predict(X).reshape(X.shape[0],1)\n    baggerpred = bagger.predict(X).reshape(X.shape[0],1)\n    forestpred = randomforest.predict(X).reshape(X.shape[0],1)\n    xgpred = xgclassifier.predict(X).reshape(X.shape[0],1)\n    nnpred = neural_net.predict(X).reshape(X.shape[0],1)\n    meta_features = np.stack((treepred,baggerpred,forestpred,xgpred,nnpred),axis=1).reshape(X.shape[0],5)\n    meta_pred = meta.predict(meta_features)\n    return meta_pred\n", "intent": "To make predictions we will define a new method:\n"}
{"snippet": "model.evaluate(x=X_test,y=y_test)\n", "intent": "To see how good our model actually is or weather it overfits the training set, let's evaluate it on the test set:\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean()) \ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "We need to define our model scores:\n"}
{"snippet": "print_score(RF)\n", "intent": "Show how our model is performing:\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in RF.estimators_])\npreds.shape\n", "intent": "**`n_estimators`:**  \nNumber of different trees we want to build and average over.\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean()) \ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "**Defining Model Score:**\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]\n", "intent": "Now look at the prediction for y from each individual tree, and see how averaging over multiple trees improves our model:\n"}
{"snippet": "def get_preds(t): return t.predict(X_valid)\nnp.mean(preds[:,0]), np.std(preds[:,0])\n", "intent": "We can parallelize this code to run faster:\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean()) \ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Defining a loss function:\n"}
{"snippet": "y_test_prediction = model.predict(X_test)\ny_test_prediction\n", "intent": "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:\n"}
{"snippet": "XCeption_predictions = [np.argmax(XCeption_model.predict(np.expand_dims(feature, axis=0))) for feature in test_XCeption]\ntest_accuracy = 100*np.sum(np.array(XCeption_predictions)==np.argmax(test_targets, axis=1))/len(XCeption_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = myNaiveBayes(X_train,y_train).predict(X_test)\naccuracy = confusion_matrix(y_test,y_pred)\nprint(accuracy)\n", "intent": "Now let's try with all the classes!\n"}
{"snippet": "SGD.predict([SomeNum])\n", "intent": "Let's see what the model says about our number from earlier.\n"}
{"snippet": "class Never7(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1),dtype = bool)\n", "intent": "Looks like our model worked pretty well!\nNot too fast there! What if we hust made a model that predicted none of the numbers are 7?\n"}
{"snippet": "y_scores = cross_val_predict(SGD, X_train, y_train_7, cv = 5, method = \"decision_function\")\n", "intent": "We've got the basic idea using one number, SomeNum, now we can get predictions for all the numbers using `cross_val_predict`.\n"}
{"snippet": "y_preds = model.predict(X_test)\n", "intent": "Predict using model on test set.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_preds)\n", "intent": "Get accuracy score.\n"}
{"snippet": "scores = cross_val_score(model, X, y)\nscores.mean()\n", "intent": "By default it does 3-fold CV.\n"}
{"snippet": "simple_eval = model_selection.cross_val_score(simple_logistic, x, y, cv=10)\nprint(\"Simple Logistic Regression\\t%3.2f\\t%3.2f\" % (np.average(simple_eval), np.std(simple_eval)))\n", "intent": "We evaluate the model using cross validation.\n"}
{"snippet": "scores = {}\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model_name in models:\n        X = iris.data[:, pair]\n        y = iris.target\n        clf = models[model_name];\n        score = cross_val_score(clf,X,y,cv=StratifiedKFold(n_splits=10,shuffle=True,random_state=random_seed))\n        scores[(model_name,str(pair))]=(np.average(score),np.std(score))\n", "intent": "For each model, we apply 10-fold stratified crossvalidation and compute the average accuracy and the corresponding standard deviation\n"}
{"snippet": "scores = {}\nfor model_name in models:\n    clf = models[model_name];\n    score = cross_val_score(clf, X, y, cv=StratifiedKFold(n_splits=10,shuffle=True,random_state=random_seed))\n    scores[model_name]=(np.average(score),np.std(score))\n    print('%26s %3.1f %3.1f'%(model_name,100.0*np.average(score),100.0*np.std(score)))\n", "intent": "For each model, we apply 10-fold stratified crossvalidation and compute the average accuracy and the corresponding standard deviation\n"}
{"snippet": "dog_breed_Resnet50_predictions = [np.argmax(dog_breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(dog_breed_Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "vel_pred_ts = regr.predict(Xts)\nn = len(time[0]) \nvar_ts = np.var(yts)\nRSS_test = np.mean((vel_pred_ts-yts)**2)/var_ts\nprint('Normalized RSS on the test data= '+str(RSS_test))\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = None\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        A = style_targets[i]\n        sub = tf.subtract(G, A)\n        square = tf.square(sub)\n        reduce_sum = tf.reduce_sum(square)\n        loss += style_weights[i] * reduce_sum\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "dtype = 'float32'\ntvm_out = executor.evaluate(func)(tvm.nd.array(data.astype(dtype)), **params)\ntop1_tvm = np.argmax(tvm_out.asnumpy()[0])\n", "intent": "Execute on TVM\n---------------\n"}
{"snippet": "dtype = 'float32'\ntvm_output = intrp.evaluate(sym)(tvm.nd.array(x.astype(dtype)), **params).asnumpy()\n", "intent": "Execute on TVM\n---------------------------------------------\n"}
{"snippet": "import numpy as np\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_predicted)))\n", "intent": "Find Root Mean Square Error value\n"}
{"snippet": "np.around(predictions,decimals=0)\npredictions[predictions>5] = 5\npredictions[predictions<1] = 1\nmetrics.mean_squared_error(y_test, predictions)\n", "intent": "There has been no significant increase in the R-Squared value, which means that the user-related metrics would have lead to overfitting.\n"}
{"snippet": "from sklearn.metrics import roc_curve\ny_pred_prob = logreg.predict_proba(X_test)[:,1] \nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) \n", "intent": "<img src = roccurve.png style=\"width: 50%; height: 50%\">\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score \ny_pred_prob = logreg.predict_proba(X_test)[:,1]\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc') \nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n", "intent": "<img src = roccurve3.png style=\"width: 50%; height: 50%\">\n"}
{"snippet": "train_predictions = best_clf.predict(X_train)\nprint \"Final accuracy score on the training data: {:.4f}\".format(accuracy_score(y_train, train_predictions))\nprint \"Final F-score on the training data: {:.4f}\".format(fbeta_score(y_train, train_predictions, beta = 0.5))\n", "intent": "**1. Checking Training Scores**\n"}
{"snippet": "def rmse_cv(model,train_x,y):\n    rmse = -model_selection.cross_val_score(model, train_x, y, \n                                    scoring=\"neg_mean_squared_error\",\n                                   cv = 5)\n    return rmse\nrf_mse = rmse_cv(rf,data_x,data_y)\nprint(rf_mse.mean(),rf_mse.std())\n", "intent": "To evaluate the model, we perform cross validation based on negative mean squared error.\n"}
{"snippet": "tracker.train_pred = tracker.model.predict(tracker.train_input)\n", "intent": "Visualize the prediction on a couple of events in the format that the model sees.\n"}
{"snippet": "model = LightFM(learning_rate=0.05, loss='warp')\nmodel.fit_partial(train, epochs=10)\ntrain_precision = precision_at_k(model, train, k=10).mean()\ntest_precision = precision_at_k(model, test, k=10).mean()\ntrain_auc = auc_score(model, train).mean()\ntest_auc = auc_score(model, test).mean()\nprint('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\nprint('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))\n", "intent": "The WARP model, on the other hand, optimises for precision@k---we should expect its performance to be better on precision.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test)\nprint(\"%s on test data: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "<h3>Evaluate model</h3>\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std() )\n    print(msg)\n", "intent": "We will display the mean and standard deviation of accuracy for each algorithm as we calculate it and collect the results for use later\n"}
{"snippet": "for i in range(10):\n    arr = np.array(inp_text)[np.newaxis,:]\n    p = model_sequence.predict(arr)[0]\n    output = [words[np.argmax(o)] for o in p]\n    for word in output:\n        print(word, end=' ')\n    inp_text = [word_indices[word] for word in output]\n", "intent": "Predicted 10 Sentences\n"}
{"snippet": "x_test = np.array(['red pandas'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "xx, yy = np.mgrid[-5:5:.01, -5:5:.01]\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n", "intent": "Next, make a continuous grid of values and evaluate the probability of each (x, y) point in the grid:\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test, predicted)\n", "intent": "The accuracy is 73%, which is the same as we experienced when training and predicting on the same data.\n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "Use pipeline again to predict and evaluate\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, preds)\n", "intent": "We can verify this using the `accuracy_score` function, which returns a high value.\n"}
{"snippet": "next_25 = fit.predict(n_periods=25)\nnext_25\n", "intent": "After your model is fit, you can forecast future values using the `predict` function, just like in sci-kit learn:\n"}
{"snippet": "predicted = classifier.predict(objects)\n", "intent": "Now we have to transform all segments into a _segment models_ in order to classify them\n"}
{"snippet": "model.predict(np.array([[2, 1]]))\n", "intent": "To make our model predict new data points, just call model.predict()\n"}
{"snippet": "from sklearn.metrics import classification_report\nyhat = lr.predict(x_test)\nresult = classification_report(y_test, yhat)\nprint(result)\n", "intent": "You can create result report using the code below.\nlr = your model\nx_test = your data test set \ny_test = your target test set\n"}
{"snippet": "from sklearn.metrics import accuracy_score, confusion_matrix\nacc_test = accuracy_score(y_test, yhat)\nprint (\"accuracy score for test dataset: \" + \"\".join('%0.2f'%(acc_test*100)),'%')\nprint (confusion_matrix(y_test,yhat))\n", "intent": "You can print out accuracy score and confusion matrix using the code below.\n"}
{"snippet": "classifier.predict(X[-1:])\n", "intent": "Once the classifier has been fitted it can be used to predict with the last element as input\n"}
{"snippet": "CS109OLSModel.predict() \n", "intent": "The above values are the same values the OLSModel returned for slope and intercept.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(svc, X, Y, cv=5)\n", "intent": "    stratified sampling\n"}
{"snippet": "def get_predicted_intent(predictions):\n    return classes[np.argmax(predictions)]\nprint(get_predicted_intent(model.predict([p])))\n", "intent": "Intent that is closest to our sentence:\n"}
{"snippet": "prediction = np.around(model.predict(np.expand_dims(inputs_test[15], axis=0))).astype(np.int)[0]\nprint(\"Actual: %s\\tEstimated: %s\" % (outputs_test[15].astype(np.int), prediction))\nprint(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\n", "intent": "Now that the model is trained and tested I can perform my own prediction using some random data. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprediction = knn_classifier.predict(X_test)\nprint(confusion_matrix(y_test, prediction))\n", "intent": "**Confusion Matrix**\n"}
{"snippet": "metrics.mean_absolute_error(target, predictions)\n", "intent": "**Computing the Mean Absoloute Error**\n"}
{"snippet": "metrics.mean_squared_error(target, predictions)\n", "intent": "**Computing the Mean Squared Error**\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(target, predictions))\n", "intent": "**Computing the Root Mean Squared Error**\n"}
{"snippet": "X_new =[[3,5,4,2],[5,4,3,2]]\nknn.predict(X_new)\n", "intent": "* Returns a NumPy array\n* Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "Classification accuracy:\n   * **Proportion** of correct predictions\n   * Common **evaluation metric** for classificaiton problems\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat==y)\nprint(\"Accuracy = %f\" %acc)\n", "intent": "Measure the accuracy of the classifier. We will see that the accuracy is bad, so we will use cross validation on training and test data later on.\n"}
{"snippet": "yhat = svc.predict(Xts)\naccuracy = np.mean(yhat==yts)\nprint(\"Accuracy = %f\" %accuracy)\n", "intent": "Measure the accuracy on the test samples. \n"}
{"snippet": "df_test_v3 = create_data_frame_v2(size = 20000, amplitude=10.0)\ninput_data_test_v3 = create_input_data(df=df_test_v3, select_feats=['x', 'y'], cross_feats=[{'dow': 7, 'hod': 24}])    \npreds = regressor_v3.predict(sess=sess, x_data=input_data_test_v3)\nerrors = preds[0] - df_test_v3['f']\ndf_test_v3['preds'] = preds[0]\ndf_test_v3['err'] = errors\ndf_test_v3['err'].plot.hist(bins=100);\n", "intent": "---\nWe can see that the loss function has indeed gone down dramatically. Now let's examine the error statistics on some fresh data.\n"}
{"snippet": "f = tf.matmul(M, x) + b\nd = tf.losses.mean_squared_error(lbls, f)\n", "intent": "The hypothesis and the loss\n"}
{"snippet": "print('MSE:',metrics.mean_squared_error(y_test,predictions))\nprint('MAE:',metrics.mean_absolute_error(y_test,predictions))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,predictions)))\n", "intent": "Now let's check our errors. We'll look at our MSE, or Mean Squared Error, MAE, or Mean Absolute Error, and RMSE, or Root Mean Squared Error.\n"}
{"snippet": "predictions_2=polynomial_model.predict(X_test_2)\n", "intent": "We've just fitted our polynomial_model to our training data. Now let's use it to make predictions using our testing data.\n"}
{"snippet": "R_Squared_2=metrics.explained_variance_score(y_test_2,predictions_2)\n", "intent": "Recall that in order to calculate our R-Squared we use the explained variance score in sklearn.\n"}
{"snippet": "MSE_2=metrics.mean_squared_error(y_test_2,predictions_2)\nMAE_2=metrics.mean_absolute_error(y_test_2,predictions_2)\nRMSE_2=np.sqrt(MSE_2)\nprint('Polynomial MSE:',MSE_2)\nprint('Polynomial MAE:', MAE_2)\nprint('Polynomial RMSE:',RMSE_2)\n", "intent": "Now let's calculate our errors.\n"}
{"snippet": "predicted = gs_model.predict(twenty_test.data)\nprint(classification_report(twenty_test.target,\n                            predicted, \n                            target_names=twenty_test.target_names))\nprint(\"Accuracy: \", accuracy_score(predicted,twenty_test.target))\n", "intent": "We can also see how our model performs on the test set,\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "My model used on the test dataset of dog images.  Minimum accuracy required was 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Use the CNN to test how well it identifies breed within our test dataset of dog images.  Print the test accuracy below.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy_i = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_i)\n", "intent": "I tried out my model on the test dataset of dog images and ensured that the test accuracy was greater than 60%.\n"}
{"snippet": "fs = ['TotLivSF', 'TotLivSF_2', 'TotLivSF_3']\ntest_predictions_RidgeCV = m_RidgeCV[2].predict(train[fs])\nget_RMS(train[['SalePrice']], test_predictions_RidgeCV)\n", "intent": "Test to make sure I do this right... should be the same as above's best RMS \n"}
{"snippet": "Y_train_pred, Y_train_pred_var = m.predict(X_train)\nY_test_pred, Y_test_pred_var = m.predict(X_test)\nplot(X_train,Y_train_pred,'b',label='pred-train')\nplot(X_test,Y_test_pred,'g',label='pred-test')\nplot(X_train,Y_train,'rx',label='ground truth')\nplot(X_test,Y_test,'rx')\nlegend(loc='best')\n", "intent": "Now we plot hor the model looks for the training and testing data.\n"}
{"snippet": "Y_train_pred, Y_train_pred_var = m.predict(X_train)\nY_test_pred, Y_test_pred_var = m.predict(X_test)\nplot(X_train,Y_train_pred,'b',label='pred-train')\nplot(X_test,Y_test_pred,'g',label='pred-test')\nplot(X_train,Y_train,'rx',label='ground truth')\nplot(X_test,Y_test,'rx')\nlegend(loc='best')\n", "intent": "And we print the model for the training and testing datasets.\n"}
{"snippet": "y_pred = mn_clf.predict(X_test)\n", "intent": "<b>Predict</b> On the X_test set, that if you remember, we vectorized early on.\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_test, y_pred)\n", "intent": "<img src = \"https://i.stack.imgur.com/z5WJHm.jpg\">\n"}
{"snippet": "print(predict([\"The product Bai Strwberry Lemonade is fine. I ordered two boxes and the outer box was too heavy to lift. So I took out the inner container which was really a box bottom with 12 bottles and a lastic cover. Very flimsy the bottles had fallen out and kept falling out. Very hard to carry.\"], \n              w2v_model_saved, \n              ensemble))\n", "intent": "And this prediction was way off.\n"}
{"snippet": "print(predict([\"Love the coconut, but the red one tastes exactly like DayQuil.\"], \n              w2v_model_saved, \n              ensemble))\n", "intent": "Looks like our model doesn't know that DayQuil isn't good!\n"}
{"snippet": "print(predict([\"Just not for me. It tastes like watered down juice.\"], \n              w2v_model_saved, \n              ensemble))\n", "intent": "Two-star ratings continue to be challenging for the model.\n"}
{"snippet": "y_pred = model_lstm.predict(x_valid)\n", "intent": "<b>Generate the Confusion Matrix</b>\nWill have to do a little data engineering to get it in the right shape for comparison.\n"}
{"snippet": "print(sklearn.metrics.classification_report((y_valid.argmax(axis=-1)), y_classes, \n                                            target_names = ['1', '2', '3', '4', '5']))\n", "intent": "<b>Classification Report</b>\n"}
{"snippet": "start = time.time()\ntrain_features = conv_base.predict(train_img)\nstop = time.time()\nprint(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")\n", "intent": "    Preprocess dataset with VGG model\n"}
{"snippet": "gnb_pred = gnb.predict(X_test_std)\n", "intent": "The learning curve indicates high bias and high variance. \n"}
{"snippet": "rf_pred = best_rf.predict(X_test_std)\n", "intent": "The learning curve suggests high bias. Maybe the Random Forest model isn't sophisticated enough.\n"}
{"snippet": "y_train_pred = svm.predict(X_train)\nprint 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\ny_val_pred = svm.predict(X_val)\nprint 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )\n", "intent": "Compute $\\theta^T  x$ for a new example $x$ and pick the class with the highest score.\n"}
{"snippet": "print(metrics.classification_report(test_y, predicted))\n", "intent": "<p>Now we can use the testing outcomes that we witheld do see how well our model performed.</p>\n"}
{"snippet": "test_X = reviews_to_matrix(test_reviews, our_vocab)\ntest_predictions = classifier.predict(test_X)\n", "intent": "The test set has not been seen yet by the classifier\n"}
{"snippet": "print '%30s: %s' % ('1 - AUC', roc_auc_score(y_train, predicted_probabilities[:, 0]))\n", "intent": "Passing the wrong column (negative class) result in 1-AUC.\n"}
{"snippet": "rater1 = ['yes'] * 10\nrater2 = ['no'] * 10\ncohen_kappa_score(rater1, rater2)\n", "intent": "This case reliably produces a `kappa` of 0\n"}
{"snippet": "prediction = np.round(nn.predict(X_test) - (threshold-0.5))\ntestAccuracy = accuracy_score(y_test, prediction)\ntestPrecision = precision_score(y_test, prediction)\ntestRecall = recall_score(y_test, prediction)\nprint(\"Accuracy: %.2f, Precision: %.2f, Recall: %.2f\"\n      % (testAccuracy, testPrecision, testRecall))\n", "intent": "Print total accuracy, precision, and recall on the test set.\n"}
{"snippet": "def rmse_crossval(model):\n    return np.sqrt(-cross_val_score(model,\n                                    train_data,\n                                    train_vals,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv = 5))\n", "intent": "Define a function which does cross validation on the training data by splitting it into 5 segments\n"}
{"snippet": "ex = enc.transform([[1, 0, 3, 0]])\nclf.predict_proba(ex)\n", "intent": "Black man with no education\n"}
{"snippet": "ex = enc.transform([[6, 1, 1, 1]])\nclf.predict_proba(ex)\n", "intent": "White woman married, with high education\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "My goal is to get accuracy greater than 1%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "My goal is to get test accuracy greater than 60%.\n"}
{"snippet": "print('Test Accuracy of SVC = ', svc.score(X_test, y_test))\nt=time.time()\nn_predict = 10\nprint('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\nprint('For these',n_predict, 'labels: ', y_test[0:n_predict])\nt2 = time.time()\nprint(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n", "intent": "Accuracy & Predictions:\n"}
{"snippet": "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n", "intent": "Finally lets evaluate the model on some previously unseen data.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_true, y_pred)\n", "intent": "It is the case.  Let's see what scikit-learn code does here.\n"}
{"snippet": "accuracy_score(true_labels, pred_labels)\n", "intent": "We can run our metrics on these via:\n"}
{"snippet": "y_log_pred = log_clf.predict(x_validation)\ny_log_pred\n", "intent": "If we only want the predicted class labels, we use predict:\n"}
{"snippet": "val_proba = log_clf.predict_proba(x_validation)\n", "intent": "There is also a method to give us the class probabilities:\n"}
{"snippet": "print(\"Accuracy score is: %f\" % accuracy_score(y_validation, y_knn_pred))\nprint(\"Unweighted precision score is: %f\" % precision_score(y_validation, y_knn_pred, average='macro'))\nprint(\"Unweighted recall score is: %f\" % recall_score(y_validation, y_knn_pred, average='macro'))\nprint(\"Unweighted F1 score is: %f\" % f1_score(y_validation,y_knn_pred, average='macro'))\n", "intent": "We can easily compute our metrics on the validation set:\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy score: ', format(accuracy_score(y_test, predictions)))\nprint('Precision score: ', format(precision_score(y_test, predictions,average=\"weighted\")))\nprint('Recall score: ', format(recall_score(y_test, predictions,average=\"weighted\")))\nprint('F1 score: ', format(f1_score(y_test, predictions,average=\"weighted\")))\n", "intent": "The performance our model can be known by computing the accuracy, precision, recall and the f1 score of our model.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(model.predict(X_test), y_test)\n", "intent": "With the model trained, we can check it's accuracy:\n"}
{"snippet": "y_pred = logistic_classifier.predict(X_train)\ncalificacion1=accuracy_score(Y_train,y_pred)\ncalificacion2=logistic_classifier.score(X_train, Y_train)\nprint(calificacion1)\nprint(calificacion2)\n", "intent": "** Importante: ** Tener instalado el paquete graphviz\n"}
{"snippet": "print(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "final_predictions = [np.argmax(final_model.predict(np.expand_dims(feature, axis=0))) for feature in test_final]\ntest_accuracy = 100*np.sum(np.array(final_predictions)==np.argmax(test_targets, axis=1))/len(final_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"car.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "Y_preds=model_visual.predict(X_test)\n", "intent": "Make the prediction.\n"}
{"snippet": "scores = model.evaluate(x_test, y_test_cat, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Let's predict the labels of the test dataset, and compare to the correct labels.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(testy, p4(testx))\nprint r2\n", "intent": "The r-squared score on the test data is kind of horrible! This tells us that our model isn't all that great...\n"}
{"snippet": "Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\nresnet50_bottleneck = ResNet50(weights='imagenet', include_top=False, pooling=POOLING)\ntrain_resnet50_bf = resnet50_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_resnet50_bf = resnet50_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('Resnet50 train bottleneck features shape: {} size: {:,}'.format(train_resnet50_bf.shape, train_resnet50_bf.size))\nprint('Resnet50 valid bottleneck features shape: {} size: {:,}'.format(valid_resnet50_bf.shape, valid_resnet50_bf.size))\n", "intent": "Extract Resnet50 bottleneck features\n"}
{"snippet": "Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\nvgg_bottleneck = VGG16(weights='imagenet', include_top=False, pooling=POOLING)\ntrain_vgg_bf = vgg_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_vgg_bf = vgg_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('VGG train bottleneck features shape: {} size: {:,}'.format(train_vgg_bf.shape, train_vgg_bf.size))\nprint('VGG valid bottleneck features shape: {} size: {:,}'.format(valid_vgg_bf.shape, valid_vgg_bf.size))\n", "intent": "Extract VGG16 bottleneck features\n"}
{"snippet": "Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\nvgg19_bottleneck = VGG19(weights='imagenet', include_top=False, pooling=POOLING)\ntrain_vgg19_bf = vgg19_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_vgg19_bf = vgg19_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('VGG19 train bottleneck features shape: {} size: {:,}'.format(train_vgg19_bf.shape, train_vgg19_bf.size))\nprint('VGG19 valid bottleneck features shape: {} size: {:,}'.format(valid_vgg19_bf.shape, valid_vgg19_bf.size))\n", "intent": "Extract VGG16 bottleneck features\n"}
{"snippet": "Xtr = x_trainX[train_idx]\nXv = x_trainX[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\nxception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING)\ntrain_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('Xception train bottleneck features shape: {} size: {:,}'.format(train_x_bf.shape, train_x_bf.size))\nprint('Xception valid bottleneck features shape: {} size: {:,}'.format(valid_x_bf.shape, valid_x_bf.size))\n", "intent": "Extract Xception bottleneck features\n"}
{"snippet": "def extract_Resnet50(tensor):\n    from keras.applications.resnet50 import ResNet50, preprocess_input\n    return ResNet50(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n", "intent": "Now, we'll write an algorithm to accept an image path as input, obtain the bottleneck features, and classify the dog breed\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    l = [content_loss(style_weights[i], gram_matrix(feats[style_layers[i]]), style_targets[i]) for i in range(len(style_layers))]\n    style_loss = tf.reduce_sum(l)\n    return style_loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is less accurate, but we are sure it is not overfit. \n"}
{"snippet": "print (np.mean((bos.PRICE - lm.predict(X))**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "history = eval_hyperparam(0.01, 50)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "**Case 1**: learning rate = 0.01\n"}
{"snippet": "history = eval_hyperparam(0.1, 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "**Case 2**: learning rate = 0.1\n"}
{"snippet": "history = eval_hyperparam(0.3, 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "**Case 3**: learning rate = 0.3\n"}
{"snippet": "history = eval_hyperparam(0.5, 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "**Case 4**: learning rate = 0.5\n"}
{"snippet": "history = eval_hyperparam(0.7, 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "**Case 5**: learning rate = 0.7 and 1 (examples of very high values)\n"}
{"snippet": "history = cyclical('triangular', 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "Let's begin with the differents modes.\n**Case 1**: Triangular CLR\n"}
{"snippet": "history = cyclical('triangular2', 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "**Case 2**: Triangular2\n"}
{"snippet": "history = cyclical('exp_range', 25)\nplot_accuracy(history)\nplot_loss(history)\n", "intent": "The results are similar to the mode triangular. The value of the errors are near of 0.20%.\n**Case 3**: exp_range\n"}
{"snippet": "Inc_predictions = [np.argmax(bn_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inc_predictions)==np.argmax(test_targets, axis=1))/len(Inc_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "L2_lambda = 0.05\nmean_sq_err = tf.reduce_mean(tf.square(y - y_))\nL2_norm = cross_entropy + \\\n          L2_lambda * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))\n", "intent": "+ Mean Squared Error\n+ L2 normalization (which needs an regularization coefficient)\n"}
{"snippet": "VGG19_Predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_Predictions)==np.argmax(test_targets, axis=1))/len(VGG19_Predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_start = timer()\npredictions = model.predict(test)\ntest_end = timer()\ntest_time = test_end - test_start\nmape = 100 * np.mean( abs(predictions - test_targets) / test_targets)\n", "intent": "Making predictions in Scikit-Learn using the `predict` method of a model. It takes in only the testing features.\n"}
{"snippet": "print((np.mean((bos.PRICE - lm.predict(X)) ** 2)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy  = {}'.format(accuracy_score(y_test,predictions)))\nprint('Precision = {}'.format(precision_score(y_test,predictions)))\nprint('Recall    = {}'.format(recall_score(y_test,predictions)))\nprint('F1 Score  = {}'.format(f1_score(y_test,predictions)))\n", "intent": "* Accuracy  : Correct/total\n* Precision : TP/(TP+FP)\n* Recall    : TP/(TP+FP)\n* F1 score\n"}
{"snippet": "mse = (np.sum((bos.PRICE - lm.predict(X)) ** 2))\nmse = mean(mse)\nmse\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"RMSE on the test set equals to: {0}\".format(mean_squared_error(strat_test_set[\"median_house_value\"].values,best_pipe_random_forest.predict(strat_test_set))**(1/2)))\nprint(\"NRMSE on the test set equals to: {0:.2f} %\".format(100*mean_squared_error(strat_test_set[\"median_house_value\"].values,best_pipe_random_forest.predict(strat_test_set))**(1/2)/(np.max(strat_test_set[\"median_house_value\"].values)-np.min(strat_test_set[\"median_house_value\"].values))))\n", "intent": "Final error estimation on test set\n"}
{"snippet": "y_pred_rfdtr= rfr_best.predict(X_test)\ny_pred_rfdtr\n", "intent": "Random forest has recommonded the same selections. Again, all the features were included in the OLS and regulization models.\n"}
{"snippet": "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\n", "intent": "Evaluate the model using the test data set.\n"}
{"snippet": "print 'Predictions and scores for SVC classifier-------------------------------------\\n'\npred_y = svc.predict(X_evaluation)\nprint str(pred_y) + '\\n'\nprint 'F1 score for SVC classifier:\\n '+  str(classification_report(y_evaluation, pred_y)) + '\\n'\nprint 'Confusion Matrix for SVC classifier:\\n '+  str(confusion_matrix(y_evaluation, pred_y)) + '\\n\\n'\nprint 'Predictions and scores for Dummy classifier-------------------------------------\\n'\ndummy_pred_y = dummy_clf.predict(X_evaluation)\nprint str(dummy_pred_y) + '\\n'\nprint 'Accuracy for Dummy Classifier:\\n ' + str(classification_report(y_evaluation, dummy_pred_y)) + '\\n'\nprint 'Confusion Matrix for  Dummy Classifier:\\n ' + str(confusion_matrix(y_evaluation, dummy_pred_y)) + '\\n'\n", "intent": "Now that we scalled our data lets predict it.\n"}
{"snippet": "rate = 0.001\nlogits = LeNet(x)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\ntrainable_variables   = tf.trainable_variables() \nl2_loss = tf.add_n([tf.nn.l2_loss(v) for v in trainable_variables])*0.001\nloss_operation = tf.reduce_mean(cross_entropy+l2_loss)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)\n", "intent": "Create a training pipeline that uses the model to classify MNIST data.\nYou do not need to modify this section.\n"}
{"snippet": "def run_classifier(clf, X, y, penalty=None, random=1):\n    start_time = time()\n    kf = KFold(X.shape[0], random_state=random, shuffle=False)\n    predictions = cross_val_predict(clf, X, y, cv=kf) \n    metrics = compute_metrics(clf.__class__.__name__, predictions)\n    duration = int(round((time() - start_time) * 1000))\n    return metrics, duration\n", "intent": "Create a Generic Classifier Function\n"}
{"snippet": "def run_classifier_proba(clf, X, y, penalty=None, random=1):\n    start_time = time()\n    kf = KFold(X.shape[0], random_state=random)\n    proba = cross_val_predict(clf, X, y, cv=kf, method='predict_proba')\n    duration = int(round((time() - start_time) * 1000))\n    return proba, duration\n", "intent": "Run Classifier Pipeline with predict proba\n"}
{"snippet": "from sklearn.metrics import classification_report\nfor clf in [clf_A, clf_B, clf_C]:\n    print '\\nReport for {}:\\n'.format(clf.__class__.__name__)\n    print classification_report(y_test, clf.predict(X_test))\n    print '-'*52\n", "intent": "Classification Report:\n"}
{"snippet": "print ('predicted:', spam_detector.predict(tfidf4)[0])\nprint ('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "all_predictions = spam_detector.predict(messages_tfidf)\nprint (all_predictions)\n", "intent": "Hooray! You can try it with your own texts, too.\nA natural question is to ask, how many messages do we classify correctly overall?\n"}
{"snippet": "print (classification_report(messages['label'], all_predictions))\n", "intent": "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:\n"}
{"snippet": "print ('before:', svm_detector.predict([message4])[0])\nprint ('after:', svm_detector_reloaded.predict([message4])[0])\n", "intent": "The loaded result is an object that behaves identically to the original:\n"}
{"snippet": "from sklearn import cross_validation\nfrom sklearn.metrics import roc_auc_score\ndef calculate_ensemble_score(model1, model2, Xcols, ycol, dataset, cv=10):\n", "intent": "Define a function for cross-validating and plotting ensemble of my two models:\n"}
{"snippet": "train_error = mean_squared_error(tar_train, model.predict(pred_train))\ntest_error = mean_squared_error(tar_test, model.predict(pred_test))\nprint('The train error is : ', train_error, '\\nThe test error is  : ', test_error)\n", "intent": "<i>Mean Squared Error from the training and test data sets</i>\n"}
{"snippet": "predicted_value = my_model.predict(test_data)\n", "intent": "Step 6: Apply model built in above steps to predict House price for test data\n"}
{"snippet": "group_ = 'ad_group_12'\nddf = df_prediction.ix[df_prediction.ad == group_, {'shown'}].copy(deep = True)\nddf = ddf.resample('1D').first() \nddf = ddf.interpolate(axis = 0, limit = 2).dropna()\ntry:\n    ARMA_predict = fitted_model[group_].predict('2015-11-15', '2015-12-15',\n                                                       dynamic = False, typ = 'levels')\nexcept TypeError:\n    ARMA_predict = fitted_model[group_].predict('2015-11-15', '2015-12-15', dynamic = False)\n", "intent": "<b> Sample plot with forecast:\n"}
{"snippet": "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=300)\neval_spec = tf.estimator.EvalSpec(input_fn=validation_input_fn, steps=40, throttle_secs=60)\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n", "intent": "<b> 4e. Train and Evaluate</b>\n"}
{"snippet": "knn.predict([3,5,4,2]) \n", "intent": "**step 4: Predict the response for a new observation**\n"}
{"snippet": "y_pred = logreg.predict(X)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y,y_pred) \n", "intent": "How do we know which model to use?\n*This method is not useful for testing models*\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**classification accrucy**: percentage of correct predictions\n"}
{"snippet": "print (FP+FN)/float(TP+TN+FP+FN)\nprint 1 - metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification Error**: Overall, how often is the classifier incorrect? \"Misclassification Rate\"\n"}
{"snippet": "print TP / float(TP+FN)\nprint metrics.recall_score(y_test, y_pred_class)\n", "intent": "**Sensitivity**: When the actual value is positive, how often is the prediction correct?\n- Also known as \"True Positive Rate\" or \"**Recall**\"\n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve:**\nhighest AUC value indicates good classifier model\n"}
{"snippet": "y_predict = model.predict(X_test)\n", "intent": "Now let's predict using the trained model. Call `predict` on `X_test`, and print the `confusion_matrix` and the `classification_report`.\n"}
{"snippet": "score = accuracy_score(y_test_2, forest_predict)\nprint(np.round(score, decimals=2))\n", "intent": "Now, call `accuracy_score` on your test data and your predictions. Print these scores using `np.round` and 2 decimal places.\n"}
{"snippet": "score = accuracy_score(test_labels, lr_doc2vec.predict(test_arrays))\nscore\n", "intent": "Call `score` on your classifier, passing in your `test_arrays` and `test_labels`.\n"}
{"snippet": "logReg.predict([X_test[20], X_test[21]]) \n", "intent": "Let's look at the song at index 20 and see what the prediction probabilities were!\n"}
{"snippet": "logReg.predict_proba(X_test[20].reshape(1, -1))\n", "intent": "You can see the classifier predicted the song at index 20 as `dance`. How confident was it? We use the `predict_proba` method to find out!\n"}
{"snippet": "vote_class.predict(X_scaled[20].reshape((1, -1)))\n", "intent": "Let's look at the prediction for song at index 20 !\n"}
{"snippet": "for k,v in industry_models.items():\n    print (k + \":\\t\" + str(v.predict([sample2])[0]))\n", "intent": "All but MNB call it 'Sea Transport'\n"}
{"snippet": "list(zip(industry_models['LOG'].classes_, industry_models['LOG'].predict_proba([sample2])[0]))\n", "intent": "But only gets a 0.0909 for 'Sea Transport' for SVM, despite that being the top choice.\n"}
{"snippet": "y_hat = loaded_model.predict(x_test)\ny_hat_grid = np.argmax(y_hat, axis=1).reshape((n_linspace, n_linspace))\n", "intent": "Make predictions using the loaded model.\n"}
{"snippet": "model_NB = joblib.load(\"twitter_sentiment.pkl\" )\np = model_NB.predict([sample_str])\ndef sentiment_str(x):\n    if x==0:\n        return 'Negative'\n    else:\n        return 'Positive'\nprint(\"the sentence: \\n\\n'{}' \\n\\nhas a {} sentiment\".format(sample_str,sentiment_str(p[0])))\n", "intent": "Below I tested the model on some random text from the internet to get a better feel of how it would perform on twitter and in a real word scenario.\n"}
{"snippet": "accuracy = accuracy_score(outputs, predicted_outputs)\nprecision = precision_score(outputs, predicted_outputs, average='samples')\nrecall = recall_score(outputs, predicted_outputs, average='samples')\n(accuracy, precision, recall)\n", "intent": "We compute the accuracy, precision and recall. All of these need to be 1.\n"}
{"snippet": "probs = model.predict(X_test)\nprint(probs[:3])\ny_predicted = [seq.argmax() for seq in probs]\nprint(y_predicted)\nprint(accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))\nprint(y_test)\n", "intent": "So we got an accuracy of 60%.\nTo get the predictions back, we need to do a bit of coding, to transform the softmax probabilities back to labels:\n"}
{"snippet": "from collections import Counter\nmost_freq_class = Counter(y_dev).most_common()[0][0]\ny_maj = [most_freq_class for _ in y_dev]\naccuracy_score(y_dev, y_maj)\n", "intent": "We get an accuracy of 77.9%. How good is this? We need to compare it to a baseline. \n"}
{"snippet": "pred = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "classifier = caffe.Classifier('/opt/caffe/examples/mnist/lenet.prototxt', \n                              '/opt/caffe/examples/mnist/lenet_iter_10000.caffemodel')\nfor i in xrange(5):\n    digit = next(test_set)\n    label = digit[0]; image = digit[1]\n    prediction = classifier.predict([image], oversample=False)\n    predicted_label=np.argmax(prediction)    \n    plot_mnist_digit(image, \"LABEL: \" + str(label) + \" PREDICTED LABEL: \"+ str(predicted_label))\n", "intent": "Now we can do some predictions using our trained LeNet model\n"}
{"snippet": "from sklearn.metrics import zero_one_loss\nyFitLinear = modelLinear.predict(X)\nyFitRbf = modelRbf.predict(X)\nprint(\"0/1 loss -- Linear: {:.3f}      Rbf: {:.3f}\".format(zero_one_loss(y, yFitLinear),zero_one_loss(y, yFitRbf)))\n", "intent": "Let us compare the linear and rbf training error using the zero one loss (the proportion of misclassified examples).\n"}
{"snippet": "targetPred = model.predict(XTest)\nprint(\"Predicted    True\")\nfor idx,val in enumerate(targetPred):\n    print(\"{:4.1f}          {:.0f}\".format(val,float(yTest.iloc[idx])))\n", "intent": "Let us take a look at our predictions.\n"}
{"snippet": "forest_load = load(filename)\npredictions = forest_load.predict(hmeq[all_inputs])\npredictions\n", "intent": "Score entire HMEQ.csv dataset\n"}
{"snippet": "upper_latent_mean, upper_latent_log_var = encoder.predict(upper_points)\nlower_latent_mean, lower_latent_log_var = encoder.predict(lower_points)\nprint('Average absolute latent mean (upper points)', \n      np.mean(np.absolute(upper_latent_mean), axis=0))\nprint('Average latent variance (upper points)', \n      np.mean(np.exp(upper_latent_log_var), axis=0))\nprint('Average absolute latent mean (lower points)', \n      np.mean(np.absolute(lower_latent_mean), axis=0))\nprint('Average latent variance (lower points)', \n      np.mean(np.exp(lower_latent_log_var), axis=0))\n", "intent": "Calculate the average mean and variance output by the encoder for a point from the upper / lower class.\n"}
{"snippet": "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=10)\nprint(loss_and_metrics)\n", "intent": "Evaluate your performance in one line:\n"}
{"snippet": "classes = model.predict(x_test, batch_size=10)\nprint(classes.shape)\n", "intent": "Or generate predictions on new data:\n"}
{"snippet": "import h5py\nwith h5py.File('input/file.hdf5', 'r') as f:\n    x_data = f['x_data']\n    model.predict(x_data)\n", "intent": "You can use the ```HDF5Matrix``` class from ```keras.utils.io_utils```. \nYou can also directly use a HDF5 dataset:\n"}
{"snippet": "def epicycle_error(c):\n    a = c[0::3]\n    w = c[1::3]\n    p = c[2::3]\n    phi_epi = [f_phi(a, w, p, i) for i in range(200)]\n    return np.sum((np.unwrap(phis - phi_epi))**2)\n", "intent": "Now, let's fit the parameters to describe the function better:\n"}
{"snippet": "p1 = best.predict(integers1)[0]\np2 = best.predict(integers2)[0]\np3 = best.predict(integers3)[0]\np4 = best.predict(integers4)[0]\nfind(p1)\nfind(p2)\nfind(p3)\nfind(p4)\n", "intent": "Predicting the sample text using the best classifier.\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\nfpr, tpr, thresholds_roc = roc_curve(y_true, y_score)\nroc_auc = auc(fpr, tpr)\nprecision, recall, thresholds_pr = precision_recall_curve(y_true, y_score)\naverage_precision = average_precision_score(y_true, y_score) \n", "intent": "[scikit-learn precision-recall curve plotting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score\nprint('Accuracy score: {}'.format(accuracy_score(predict(X_test), y_test)))\nprint('F1 score: {}'.format(f1_score(predict(X_test), y_test)))\n", "intent": "Now that the model is build, the next step is to examine the accuracy of the classifier.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy for Resnet50: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score, acc = model_cnn.evaluate(valid_feats_lstm, features_to_cat(np.asarray(valid_labels)), batch_size=3*batch_size)\nprint('\\n\\nTest loss:', score)\nprint('Test accuracy:', acc)\n", "intent": "As previously, the curve reveals there is little chance of overfitting. This is mainly due to the use of a high ratio of **dropout**.\n"}
{"snippet": "prob_results = clf.predict_proba([[0, 0], [0, 1], \n                                  [1, 0], [0, 1], \n                                  [1, 1], [2., 2.], \n                                  [1.3, 1.3], [2, 4.8]])\nprint(prob_results)\n", "intent": "Instead of just looking at the class results, we can also use the predict_proba method to get the probability estimates.\n"}
{"snippet": "score = Xception_model.evaluate(test_Xception, test_targets, verbose=0)\naccuracy = 100*score[1]\nprint('Test accuracy: %.4f%%' % accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = model1.predict(x=images)\n", "intent": "We then use the restored model to predict the class-numbers for those images.\n"}
{"snippet": "df['forest'] = forest.predict(X_featu)\n", "intent": "plot the original data vs predicted: \n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Y_pred = clf_chosen.predict(X_val).reshape(-1)\nY_pred_proba = clf_chosen.predict_proba(X_val)\ndf_confusion = pd.crosstab(pd.Series(Y_val, name='Actual'), pd.Series(Y_pred,name='Predicted'))\ndf_confusion\n", "intent": "<a id=\"ch5\"></a>\n<a id=\"ch5.1\"></a>\nFirst let's take a look at the confusion table\n"}
{"snippet": "DogX_predictions = [np.argmax(DogX_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(DogX_predictions)==np.argmax(test_targets, axis=1))/len(DogX_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "accuracies = list()\nfor model in models:\n    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n    accuracies.append(accuracy)\naccuracies\n", "intent": "We compute and show the accuracies of the individual models on the test data.\n"}
{"snippet": "model.evaluate(x_val, y_val)\n", "intent": "model.evaluate(x_train, y_train\n"}
{"snippet": "dropout_model.evaluate(x_train, y_train)\n", "intent": "Again, let's compare the performance of the model on the training, validation and test set.\n"}
{"snippet": "model.evaluate(x_test, y_test)\n", "intent": "Evaluate the CNN's performance on the test data set.\n"}
{"snippet": "cross_val_score(nb, prepared_data, data_2015['Western Europe'],\n                scoring='accuracy', cv=15)\n", "intent": "Let's try cross validation to gauge performance.\n"}
{"snippet": "confusion_matrix(data_2015['Western Europe'], nb.predict(prepared_data))\n", "intent": "Compute the confusion matrix.\n"}
{"snippet": "rr_scores = cross_val_score(rr, X, Y, scoring='neg_mean_squared_error', cv=10)\n", "intent": "Compute the error using 10-fold cross validation.\n"}
{"snippet": "Y_rr = rr.predict(X)\n", "intent": "Visualize the predictions of the model versus the actual happiness socres for the training data.\n"}
{"snippet": "rf_scores = cross_val_score(rf, X, Y, scoring='neg_mean_squared_error', cv=10)\n", "intent": "Cross validation confirms that this model is inferior to ridge regression.\n"}
{"snippet": "Y_test_rr = rr.predict(X_test)\n", "intent": "Compute the predictions by the ridge regression model.\n"}
{"snippet": "Y_test_rf = rf.predict(X_test)\n", "intent": "Compute the random forrest predictions.\n"}
{"snippet": "model_xgboost = xgb.XGBRegressor(colsample_bytree=0.4603,gamma = 0.0468, learning_rate=0.05,max_depth=3,min_child_weight=1.7817,n_estimators=2200,reg_alpha=0.4640,reg_lambda=0.8571,subsample=0.5213,silent=True,nthread=-1)\ncv_score = np.sqrt( -cross_val_score(model_xgboost, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))\n", "intent": "XGBoost is an implementation of gradient boosted decision trees **designed for speed and performance**.\n"}
{"snippet": "measurements = [71, 31, 88, 17.5]\nscaled = scaler.transform(measurements)\nmy_pbf = ridge.predict(scaled)+data['pbf_b'].mean()\nprint(\"My PBF = {} +/- {}\".format(my_pbf, -scores.mean()))\n", "intent": "Finally, I can predict my own body fat percentage. I'll use the CV error previously calculated as an estimate of the uncertainty.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(model_InceptionV3.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    print(gram)\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "print \"For Random Forest classifier:\\nAccuracy Score is: \"\naccuracy_score(test_data[\"label\"], random_forest.predict(test_data.iloc[:,:-1])) * 100\n", "intent": "We will test the models on our test data set and determine the most suited model for our problem.\n"}
{"snippet": "my_ResNet50_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(my_ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(my_ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": " __Classification Accuracy:__ Percentage of correct predictions\n"}
{"snippet": "logreg.predict(X_test)[0:10]\n", "intent": "<a id=\"AdjClassThershold\"></a>\n"}
{"snippet": "y = model.predict(x)\nprint(np.argmax(y,axis=1))\n", "intent": "In this notebook, it's not really trained to predict\n"}
{"snippet": "cross_val_score(lr,X,y, scoring=\"precision\")\n", "intent": "TP + TN / ( TP + TN + FP + FN)\n"}
{"snippet": "y_pred_rfc = rfc_model.predict(X_test)\n", "intent": "The RFC model produces an accuracy score of about 80.1% for both the train and test sets, so it does seem to generalize well.\n"}
{"snippet": "y_pred_knn = knn_model.predict(X_test)\n", "intent": "The accuracy is again similar between the train and test sets, so the model does appear to generalize well.\n"}
{"snippet": "print('recall: ', recall_score(test_y,y_preds))   \nprint('precision: ', precision_score(test_y,y_preds))\n", "intent": "Since we are essentially dealing with a binary classification, we can use recall and precision scores as well. As we see we get fairly high values\n"}
{"snippet": "list(clf.predict(iris.data[:3]))\n", "intent": "regressor has a continous range, classifier has a discrete one\n"}
{"snippet": "crossvalidation = KFold(n_splits=10, shuffle=False, random_state=1)\nscores = cross_val_score(lr, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=1)\nrmse_scores = [np.sqrt(abs(s)) for s in scores]\nr2_scores = cross_val_score(lr, X, y, scoring='r2', cv=crossvalidation, n_jobs=1)\nprint('Cross-validation results:')\nprint('Folds: %i, mean R2: %.3f' % (len(scores), np.mean(np.abs(r2_scores))))\nprint('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))\n", "intent": "<b> So the above results show that standardizing makes it perform even worse! </b>\n"}
{"snippet": "r2_scores = cross_val_score(rf, X, y, scoring='r2', cv=crossvalidation, n_jobs=-1)\nscores = cross_val_score(rf, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=-1)\nrmse_scores = [np.sqrt(abs(s)) for s in scores]\nprint('Cross-validation results:')\nprint('Folds: %i, mean R2: %.3f' % (len(scores), np.mean(np.abs(r2_scores))))\nprint('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))\n", "intent": "<b> So standardizing the data set does not make a difference in terms in the Random Forest prediction model. </b>\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    diff = tf.abs(labels - predictions)\n    condition = tf.less(diff, delta)\n    case_a = tf.square(diff) / 2\n    case_b = delta * diff - tf.square(delta) / 2\n    return tf.cond(condition, lambda: case_a, lambda: case_b)\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "def NN_predict(x):\n    dists = [dist(x, xi) for xi in X]\n    index = np.argmin(dists)\n    return y[index]\n", "intent": "<p>\n    Here's a function that implements 1NN:\n</p>\n"}
{"snippet": "print(\"[Test Classification Report with C=10:]\")\nprint(classification_report(ytest, ypred))\n", "intent": "Run the Classification Report with the \"best\" C value (C=10).\n"}
{"snippet": "print(\"[Test Classification Report with C=10:]\")\nprint(classification_report(ytest, ypredLROR))\n", "intent": "Classification Report with the \"best\" value of C\n"}
{"snippet": "print(\"[Test Classification Report with C=100:]\")\nprint(classification_report(ytest, ypredLRUR))\n", "intent": "Classification Report with the \"best\" value of C\n"}
{"snippet": "print r2_score(df.price, predictions.price)\n", "intent": "The R-Squared model also shows how well the model scored compared to having guessed the mean at every point.\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "This is the report using Pclass **WITHOUT** the get_dummies method and spliting the trainning data with test_split\n"}
{"snippet": "print(classification_report(y_test_pclass, predictions_pclass))\n", "intent": "Separating Pclass values in columns (using get_dummies method)\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "Pclass with its original value\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "**Logistic Regression**\n"}
{"snippet": "y_prediction = model.predict(X_train)\ny_prediction.shape\n", "intent": "We can use even untrained NN to predict something, which will be mostly random\n"}
{"snippet": "model.evaluate(X_train, y_train, batch_size=2000)\n", "intent": "You can actually compute accuracy right away using Keras\n"}
{"snippet": "keras_output = model.predict(X_batch)\nkeras_output[0]\n", "intent": "We can check whether it is looks like the output from the keras directly (**you can see that they are identical**)\n"}
{"snippet": "'Keras Accuracy', model.evaluate(X_batch, y_batch)[1]\n", "intent": "Which is consistent with keras evaluation accuracy on this batch\n"}
{"snippet": "result = model.evaluate(x=np.array(X),\n                        y=np.array(Y))\n", "intent": "Now that the model has been trained we can test its performance on the test-set. This also uses numpy-arrays as input.\n"}
{"snippet": "y_pred = model.predict(x=np.array(X))\n", "intent": "coba disini bikin prediksi\n"}
{"snippet": "y_pred = model.predict(x=np.array(X))\n", "intent": "mencoba prediksi dan test model\n"}
{"snippet": "result = model3.evaluate(x=np.array(xx),\n                        y=np.array(yy))\n", "intent": "Now that the model has been trained we can test its performance on the test-set. This also uses numpy-arrays as input.\n"}
{"snippet": "y_pred = model3.predict(x=np.array(xx)[9].reshape(1,5,1025))\n", "intent": "coba disini bikin prediksi\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['non-science', 'science']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n", "intent": "__Logistic regression__\n"}
{"snippet": "target_names = ['non-science', 'science']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n", "intent": "__Random forest classifier__\n"}
{"snippet": "predicted = rfc.predict([[3,4,5,6]])\nprint(iris.target_names[predicted])\n", "intent": "<center><img src=\"./images/graduate-01.jpg\" width=500></center>\n"}
{"snippet": "y_predict = clf.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_predict) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_predict))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_predict, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance on the validation dataset\n"}
{"snippet": "y_predict = clf.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_predict) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_predict))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_predict, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance on the test dataset\n"}
{"snippet": "clf = SuperLearnerClassifier()\nscores = cross_val_score(clf, X_train_plus_valid, y_train_plus_valid, cv=cv_folds)\nprint(scores)\nprint(\"Mean of the scores is:%f\" %scores.mean())\n", "intent": "We can see that the overall mean performance has decreased from the tests above. \n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print_report(predicts, [1])\n", "intent": "**Results on validation set with basic features**\n"}
{"snippet": "print_report(predicts2, [1])\n", "intent": "**Results on validation set with improved features**\n"}
{"snippet": "clf_result = predicts2[1]\nclf = clf_result.get_clf()\ny_pred_sp = clf.predict(x_test_sp)\nprint(\"Accuracy\", accuracy(y_test_sp, y_pred_sp, [\"O\"]))\nprint(metrics.classification_report(y_test_sp, y_pred_sp, digits=3, labels=LABELS[:-1]))\n", "intent": "**Results on test set with improved features**\n"}
{"snippet": "for classifier in classifiers:\n    last = classifier.layers[len(classifier.layers)-1]\n    print(last, last.name)\n    for key, dataset in varieties_datasets.items():\n        if key == last.name:\n            x_test, y_test = dataset['test']\n            scores = classifier.evaluate(x_test, y_test, verbose=1)\n            print(\"Test\", scores)\n", "intent": "    1. Evaluate the Model for each finer classifier.\n"}
{"snippet": "local_results = pipeline.predict(test_features)\nlocal = pd.Series(local_results, name='local')\n", "intent": "Test the model with the entire test set and print out some of the results.\n"}
{"snippet": "model.predict(processed_instances)\n", "intent": "Then we'll pass the processed data to the model for classification:\n"}
{"snippet": "pdfs_bf, gofs = train_BF.fit_predict(phot_test, err_test, mask_test,\n                                     redshifts[train_sel], \n                                     np.ones_like(train_sel) * rsmooth,\n                                     label_dict=rdict, save_fits=False,\n                                     return_gof=True)\nlmap_bf, levid_bf = gofs\n", "intent": "Now we'll generate predictions using our training (labeled) data.\n"}
{"snippet": "for i, j in zip(xrange(10), model.predict(X_test[50:51])[0]):\n    print 'Digit: {}\\t Probability: {}'.format(i, j)\n", "intent": "Plugging this sample into our model, it will give us confidence scores for how likely it thinks each number is\n"}
{"snippet": "info = [list(model.predict(X_test[i:i+1])[0]) for i in xrange(len(X_test))]\ndf = pd.DataFrame.from_records(info)\n", "intent": "**Which digit is the most difficult to identify?**\n"}
{"snippet": "plot_and_predict('data/8.png')\n", "intent": "Damn, guess I draw 7s in a way that is different from all participants in the training set. What about another number?\n"}
{"snippet": "print(\"Test rmse: \", sqrt(mean_squared_error(y_test, y_test_pred)), \n      \"\\nTraining rmse:\", sqrt(mean_squared_error(y_train, y_train_pred)))\n", "intent": "Compare the root mean squared error (RMSE) of test dataset against the training.\n"}
{"snippet": "y_pred = kmeans.predict(X_std)\n", "intent": "Predict cluster for each point based on the KMeans model.\n"}
{"snippet": "pd.crosstab(y_test, gbc.predict(X_test))\n", "intent": "The score using a Gradient Boosting Classifier is substantially higher\n"}
{"snippet": "del x_test\ngc.collect()\nx_test, x_test_filename_additional = data_helper.preprocess_test_data(test_jpeg_additional, img_resize)\nnew_predictions = classifier.predict(x_test)\ndel x_test\ngc.collect()\npredictions = np.vstack((predictions, new_predictions))\nx_test_filename = np.hstack((x_test_filename, x_test_filename_additional))\nprint(\"Predictions shape: {}\\nFile name shape: {}\\nlst predictions entry:\\n{}\".format(predictions.shape, x_test_filename.shape, predictions[0]))\n", "intent": "Now lets launch the predictions on the additionnal dataset (updated on 05/05/2017 on Kaggle)\n"}
{"snippet": "Ypred = clfo.predict(trainX)\ntrainerr_svm = mean(Ypred != trainY)\nYpred = clfo.predict(testX)\ntesterr_svm = mean(Ypred != testY)\nprint \"SVM errors:\", trainerr_svm, testerr_svm\n", "intent": "Here are the training and test errors.\n"}
{"snippet": "docs_new = ['God is love', 'OpenGL on the GPU is fast']\nX_new_tfidf = tfidf_vect.transform(docs_new)\nprint(X_new_tfidf.shape)\npredicted = clf.predict(X_new_tfidf)\nfor idx, doc in enumerate(docs_new):\n    print('%r => %s' % (doc, predicted[idx]))\n", "intent": "When alpha = 0, performace improved significantly.\n"}
{"snippet": "y_predicted = forest_reg.predict(train_Features_scaled)\ntest_Features_scaled = scaler_Features.transform(test_Features)\ntest_Label_scaled = scaler_Label.transform(test_Label)\ny_predicted_test = forest_reg.predict(test_Features_scaled)\nprint('\\nPerformance (RMSE) on the test dataset:')\nprint(np.sqrt(mean_squared_error(test_Label_scaled,y_predicted_test)))\n", "intent": "Random forest regressor ensures the lowest RMSE and therefore is the selected method for predicting $t$.\n"}
{"snippet": "regr1.predict(95)\n", "intent": "- 4. Extrapolate data:  If the ground temperature reached 95&deg; F, then at what approximate rate would you expect the crickets to be chirping?\n"}
{"snippet": "final_precision = precision_score(y_test, y_proba[:, 1] > optimal_threshold)\nfinal_recall = recall_score(y_test, y_proba[:, 1] > optimal_threshold)\naccuracy = np.mean(y_test == (y_proba[:, 1] > optimal_threshold))\nprint 'Decision threshold: %.2f' % optimal_threshold\nprint 'Precision: %.2f' % final_precision\nprint 'Recall: %.2f' % final_recall\nprint 'Accuracy: %.2f' % accuracy\n", "intent": "And the requested metrics for the selected decision rule are:\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint(\"Precision score: {}\".format(precision_score(y_true,y_pred)))\n", "intent": "TP - True Positives<br>\nFP - False Positives<br>\nPrecision - Accuracy of positive predictions.<br>\nPrecision = TP/(TP + FP)\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))\n", "intent": "Report which includes Precision, Recall and F1-Score.\n"}
{"snippet": "y_error = y_true-y_pred\nnumerator = np.sum((y_error-np.mean(y_error))**2)\nexplained_var = 1-numerator/total_sum_of_squares\nexplained_var_sklearn=explained_variance_score(y_true,y_pred)\nif explained_var == explained_var_sklearn:\n    print(\"Explained variance score: {}\".format(explained_var))\n", "intent": "Similar to R^2 Score.\n"}
{"snippet": "print(classification_report(labels_test,rfc_predictions))\n", "intent": "As seen in the classification report below, our model has 97% precision, recall, and accuracy.\n"}
{"snippet": "print(classification_report(y_test, rfc_pred))\n", "intent": "The most important features in predicting race using the Random Forest algorithm are age and racial demographics.\n"}
{"snippet": "print('Root Mean Squared Error: {}'.format(sqrt(mean_squared_error(y, y_all_pred))))\nprint('Mean Absolute Error: {}'.format(mean_absolute_error(y, y_all_pred)))\nprint('Correlation Coefficient: {}'.format(corrcoef(y, y_all_pred)[0,1]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "example = np.array([2500, 1700, 900, 11, 100, 50])\nexample_sca = sca_all.transform(example.reshape(1,-1))\nrf.predict(example_sca.reshape(1,-1))\n", "intent": "Predicting the coverage with an example predicted to be of high risk with our fire risk model.\n"}
{"snippet": "example = np.array([2500, 750,0, 11, 800, 50])\nexample_sca = sca_all.transform(example.reshape(1,-1))\nrf.predict(example_sca.reshape(1,-1))\n", "intent": "It will naturally grow Lodgepole Pine\n"}
{"snippet": "x_test = test[feature_cols]\ny_test = test.price\ny_pred = treereg.predict(x_test)\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each observation?\n"}
{"snippet": "softmax_reg.predict([[5,2]])\n", "intent": "Estimating the type of iris with petal of 5 cm long and 2 cm wide:\n"}
{"snippet": "softmax_reg.predict_proba([[5, 2]])\n", "intent": "According to our model, such an iris is an Iris-Verginica)\n"}
{"snippet": "svm_clf.predict([[5.5,1.7]])\n", "intent": "Now we can use the model to make predictions:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Accuracy:** percentage of correct predictions\n"}
{"snippet": "print((TP + TN)/ (TP + TN + FP +FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "Classification Accuracy: how often is the classifier correct\n"}
{"snippet": "print((FP + FN)/(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "Classification Error/Misclassification Rate: how often is the classifier incorrect\n"}
{"snippet": "print(TP / (TP + FP))\nprint(metrics.precision_score(y_test, y_pred_class))\n", "intent": "Precision: when  a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "print(metrics.roc_auc_score(y_test,y_pred_prob))\n", "intent": "**higher AUC value is indicative of a better overall classifier**  \nalternative to classification accuracy\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in bottleneck_features['test']]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = clf2.predict(validation_X)\nf1_score(validation_Y, y_pred, average='macro')\n", "intent": "**Macro F1 score on validation set**\n"}
{"snippet": "f1_score(validation_Y, y_pred, average='micro')\n", "intent": "**Micro F1 score on validation set**\n"}
{"snippet": "print(classification_report(validation_Y, y_pred, target_names=le.classes_))\n", "intent": "**Class-wise Precision Recall on validation set**\n"}
{"snippet": "yhatTest = svc.predict(xTest)\nacc = np.mean(yhatTest == yTest)\nprint('Accuracy = {0:f}'.format(acc))\n", "intent": "Test the accuracy of the classifier\n"}
{"snippet": "dec_predictions = dtree.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,dec_predictions))\nprint(confusion_matrix(y_test,dec_predictions))\n", "intent": "Prediction and Evaluation through Decision Trees\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncmat = confusion_matrix(y_true=test['sentiment'].values,\n                        y_pred=model1.predict(test_matrix),\n                        labels=model1.classes_) \nprint ' target_label | predicted_label | count '\nprint '--------------+-----------------+-------'\nfor i, target_label in enumerate(model1.classes_):\n    for j, predicted_label in enumerate(model1.classes_):\n        print '{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j])\n", "intent": "1. Confusion matrix\n2. Accuracy\n3. Precision\n4. Recall\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_weights)):\n        G = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(G - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_prob = gnb.predict_proba(X_clf)\nprint(y_prob.shape)\n", "intent": "Building an ROC curve requires to use the probability estimates for the test data points *before* they are thresholded.\n"}
{"snippet": "class LeastSquaresRegr():\n    def __init__(self,):\n        self.coef_ = None\n    def fit(self, X, y):\n    def predict(self, X):\n", "intent": "__Question:__ Fill in the LeastSquareRegr class below.\n"}
{"snippet": "regr = LeastSquaresRegr()\npred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\nprint(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr,pred))\n", "intent": "Let us now evaluate your least squares regression on the data:\n"}
{"snippet": "regr = seq_LeastSquaresRegr()\npred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\nprint(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr, pred))\n", "intent": "Let us now evaluate your sequential least squares regression on the data:\n"}
{"snippet": "clf = LogisticRegr()\npred = cross_validate_clf(X_clf, y_clf, clf, folds_clf)\nprint(\"Accuracy: %.3f\" % metrics.accuracy_score(y_clf, pred > 0.5))\n", "intent": "Let us now evaluate your logistic regression on the data:\n"}
{"snippet": "from sklearn import preprocessing\nscaler = \nX_clf_scaled = \nclf = \nypred_logreg_scaled = \nprint(\"Accuracy: %.3f\" % metrics.accuracy_score(ypred_logreg_scaled > 0.5, 1, 0))\n", "intent": "**Question** Scale the data, and compute the cross-validated predictions of the logistic regression on the scaled data.\n"}
{"snippet": "clf = BaggingTreesClassifier()\npred = cross_validate_clf(design_matrix=X_iris, classifier=clf, cv_folds=folds_iris, labels=y_iris)\nprint(metrics.accuracy_score(y_iris, np.where(pred > 0.5, 1, 0)))\n", "intent": "**Question** Compute the predicted labels by a bagging trees classifier of 5 decision trees on the test data. Display the accuracy.\n"}
{"snippet": "cifar_score = cifar_model.evaluate(x_test, y_test, batch_size=16)\nprint(\"test loss: \" + str(cifar_score[0]) + \", test acc: \" + str(cifar_score[1]))\n", "intent": "Lastly, we use the unseen test data to get a true evaluation of how good (or bad) our trained model is.\n"}
{"snippet": "av_score = av_model.evaluate(x_test, y_test_av, batch_size=16)\nprint(\"test loss: \" + str(av_score[0]) + \", test acc: \" + str(av_score[1]))\n", "intent": "We use our updated test labels, `y_test_av`, as the ground truth labels.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==\n                          np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('\\nTest accuracy: %.4f%%' %test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print np.average((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "outliers = outliers_modified_z_score(train.shares.values)[0]\ntrain.drop(outliers, inplace=True)\n", "intent": "Detecting the outliers of the training set and removing it from the data.\n"}
{"snippet": "resid_sum = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint resid_sum\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf2.predict(xtest)))\n", "intent": "Answer: this result is less accurate on both the training and test data, but avoids overfit.\n"}
{"snippet": "plot_digit(some_digit)\npred = knn_pipe.predict([some_digit])\nprint('Prediction that digit is >= 7 :',pred[0][0])\nprint('Prediction that digit is odd: ', pred[0][1])\n", "intent": "Let's test it out on our test digit:\n"}
{"snippet": "def classifier_boundary_2d(classifier,X):\n    step_size = 0.05\n    maxs  = X.max(axis=0)\n    mins  = X.min(axis=0)\n    x1,x2 = np.meshgrid(np.arange(mins[0]-step_size,maxs[0]+2*step_size,step_size), np.arange(mins[1]-step_size,maxs[1]+2*step_size,step_size))\n    output = classifier.predict(np.c_[x1.ravel(),x2.ravel()])\n    output = output.reshape(x1.shape)\n    return x1,x2,output\n", "intent": "We can look at decision boundaries for both pedal length and width:\n"}
{"snippet": "pred_logistic = logistic.predict(x_test)\npred_logistic\n", "intent": "After our model has been trained, we then start testing on the testing set:\n"}
{"snippet": "log_prob_lr = logistic.predict_proba(x_test)\nprint(log_prob_lr)\n", "intent": "We also tested the likelihood of each label:\n"}
{"snippet": "accuracy_list = cross_val_score(logistic, x, y, cv=10, scoring='accuracy')\nprint(accuracy_list)\n", "intent": "Cross validation of logistic regression:\n"}
{"snippet": "pred_decision = decision.predict(x_test)\nprint(pred_decision)\n", "intent": "After training, we then use it to predict the testing data:\n"}
{"snippet": "for hours in [3.1, 7.0]:\n    [prob_fail, prob_pass] = lr.predict_proba([[1, hours]])[0]\n    print(\"Probability of student passing if they spend %0.1f hours studying: %0.02f%%\" % (hours, prob_pass*100))\n", "intent": "Our model's mean accuracy is `0.75`.  Lets use it to predict if a student will pass the exam for the given data points.\n"}
{"snippet": "np.mean(sk_model.predict(X_train)==labels_train)\n", "intent": "<div class=\"alert alert-block alert-info\"> Problem 2.2 </div>\nCompute model accuracy on the training set\n"}
{"snippet": "X_val=images_val.reshape((images_val.shape[0],-1))\nnp.mean(sk_model.predict(X_val)==labels_val)\n", "intent": "<div class=\"alert alert-block alert-info\"> Problem 2.3 </div>\nCompute accuracy of the model on the validation set\n"}
{"snippet": "np.mean(log_model.predict(X_train)==labels_train)\n", "intent": "<div class=\"alert alert-block alert-info\"> Problem 3.2 </div>\nCompute model accuracy in the training data set\n"}
{"snippet": "np.mean(log_model.predict(X_val)==labels_val)\n", "intent": "<div class=\"alert alert-block alert-info\"> Problem 3.3 </div>\nCompute model accuracy in the valuation data set\n"}
{"snippet": "custom_predictions = [np.argmax(custom_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(custom_predictions)==np.argmax(test_targets, axis=1))/len(custom_predictions)\nprint('Accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = predict(net, md.val_dl)\n", "intent": "For each of the images in the validation set, we are making 10 predictions (probability for each of the numbers)\n"}
{"snippet": "def get_preds(t): return t.predict(X_valid)\nnp.mean(preds[:,0]), np.std(preds[:,0])\n", "intent": "```python\n```\n```python\nparallel_trees(m, fn, njobs=8)\nm = model\nfn = function\nnjobs = threads\nWill run all the trees\n```\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(classification_report(y_test, yhat))\n", "intent": "So, the accuracy of the model is 92.3%\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Accuracy obtained with the following CNN:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Accuracy obtained with the following network:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Accuracy obtained with the following chose network:\n"}
{"snippet": "def sentence_score(text,sorted_scores):\n    sentence_score = []\n    for each in text:\n        score = 0.0\n        for word in each.split(\" \"):\n            word = word.lower()\n            if word in sorted_scores:\n                score += sorted_scores[word]\n        sentence_score.append([each,score])    \n    return sentence_score\n", "intent": "<h3> Sentence Score </h3>\n"}
{"snippet": "from scipy.spatial.distance import cdist\ndef model_error_1(data, model):\n    errors = data.map(lambda x: cdist([x], [model.centers[model.predict(x)]], metric='sqeuclidean'))\n    return errors.sum()[0][0]\n", "intent": "```python\nfrom scipy.spatial.distance import cdist\ndef model_error_1(data, model):\n    ...\n    return data....\n```\n"}
{"snippet": "interp = 'spline36'\nk_index = 1\nlabels = data.map(lambda x: bestModel.predict(x))\nimgLabels = labels.toarray().reshape(2, 76, 87)\n", "intent": "```python\nlabels = bestModel.predict(...)\nimgLabels = labels...\ndraw_image(imgLabels[:,:,0])\n```\n"}
{"snippet": "t0 = time()\nclusterLabelCount = labelsAndData.map(lambda row: ((clusters.predict(row[1]), \\\n                                    row[0]), 1)).reduceByKey(operator.add).collect()\nt1 = time()\nlabel = 0\nfor item in clusterLabelCount:\n    label += 1\n    print(\"Label \n    print(item)\nprint(\"Time: \", (t1-t0))\n", "intent": "```python\nclusterLabelCount = ...\nfor item in clusterLabelCount:\n    print(item)\n```\n"}
{"snippet": "ks = [5, 10, 50, 150, 200]\nfor k in ks:\n    print('K = ' + str(k))\n    dataHotNormWithClusterNumber, clusters = train_and_predict(normalizedDataHot, k)\n    print('Total not-empty clusters = ' + str(clusters.k))\n    s_avg, s_RDD = compute_approx_silhouette(clusters.centers, dataHotNormWithClusterNumber)\n    print('Silhouette = ' + str(s_avg))\n    plot_silhouette(s_RDD, s_avg, clusters.k)\n", "intent": "<div class=\"alert alert-success\">\nLet's try using this metric for model selection\n</div>\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, layer in enumerate(style_layers):\n        loss = gram_matrix(feats[layer]) - style_targets[idx]\n        loss = loss.pow(2).sum()\n        style_loss += loss * style_weights[idx]\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, layer in enumerate(style_layers):\n        loss = gram_matrix(feats[layer]) - style_targets[idx]\n        loss = tf.pow(loss, 2)\n        loss = tf.reduce_sum(loss)\n        style_loss += style_weights[idx] * loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Xception test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(Y_test, gscv_ypred)\n", "intent": "The kNN model has more False Positives and False Negatives.\n"}
{"snippet": "Y_hat = lm.predict(Z)\n", "intent": "First lets make a prediction \n"}
{"snippet": "Yhat=lm.predict(X)\nprint('The output of the first four predicted value is: ', Yhat[0:4])\n", "intent": "We can predict the output i.e., \"yhat\" using the predict method, where X is the input variable:\n"}
{"snippet": "mse = mean_squared_error(df['price'], Yhat)\nprint('The mean square error of price and predicted value is: ', mse)\n", "intent": "we compare the predicted results with the actual results \n"}
{"snippet": "print('The mean square error of price and predicted value using multifit is: ', \\\n      mean_squared_error(df['price'], Y_predict_multifit))\n", "intent": " we compare the predicted results with the actual results \n"}
{"snippet": "r_squared = r2_score(y, p(x))\nprint('The R-square value is: ', r_squared)\n", "intent": "We apply the function to get the value of r^2\n"}
{"snippet": "mean_squared_error(df['price'], p(x))\n", "intent": "We can also calculate the MSE:  \n"}
{"snippet": "yhat=lm.predict(new_input)\nyhat[0:5]\n", "intent": "Produce a prediction\n"}
{"snippet": "-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n", "intent": "We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. \n"}
{"snippet": "yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]\n", "intent": "Prediction using training data:\n"}
{"snippet": "yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]\n", "intent": "Prediction using test data: \n"}
{"snippet": "yhat = poly.predict(x_test_pr)\nyhat[0:5]\n", "intent": "We can see the output of our model using the method  \"predict.\" then assign the values to \"yhat\".\n"}
{"snippet": "yhat = RigeModel.predict(x_test_pr)\n", "intent": " Similarly, you can obtain a prediction: \n"}
{"snippet": "print('Predicting...')\npredicted = text_clf.predict(X_test)\n", "intent": "Now we can procede to apply the classifier to the test set and calculate the predicted values\n"}
{"snippet": "print(metrics.classification_report(y_test, predicted, target_names=labels))\n", "intent": "Let's build a text report showing the main classification metrics with the Precision/Recall/F1-score measures for each element in the test data.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.7f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "labels = clf_pipeline.predict(features)\ncandidates = points[labels == 2, :]\n", "intent": "Predict lables for these sample points, note that overfitting may appeare.\n"}
{"snippet": "tree_clf.predict_proba([[5, 1.5]])\n", "intent": "* Decision Tree can also estimate probability that an instance belongs to a particular class\n"}
{"snippet": "transfer_predictions = [np.argmax(model_tl.predict(np.expand_dims(feature, axis=0))) for feature in test_transfer]\ntest_accuracy = 100*np.sum(np.array(transfer_predictions)==np.argmax(test_targets, axis=1))/len(transfer_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscore = cross_val_score(model,train,label,cv=10,n_jobs=1)\nprint(\"accuracy: %.3f +/- %.3f\"%(np.mean(score),np.std(score)))\n", "intent": "we alson can use cross_validation to check the model\n"}
{"snippet": "predictions = svmfull.predict(distort_data)\ncorrect = test_labels - predictions \ntest_accuracy = (correct == 0).sum() / len(correct)\ntest_accuracy * 100\n", "intent": "Some accuracy loss- by a few percentage points, but still better than random (10%)\n"}
{"snippet": "y_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred, axis = 1)\nsave_results(y_pred)\n", "intent": "Now we will make predictions for out test set, and save results with use of function save_results\n"}
{"snippet": "def predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n    return pred\n", "intent": "Wth our similiarity matrices we can now predict ratings. \n"}
{"snippet": "y_pred = model.predict([test.user_id, test.movie_id])\nprint(mean_absolute_error(y_pred, test.rating))\n", "intent": "Because of Early Stopping technique, training session stoppped just after 4 epoch. to evaluate model we will use 'Mean Absolute Error'\n"}
{"snippet": "score, acc = model.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint('test score:', score, ' test accuracy:', acc)\n", "intent": "Finally lets check the performance of our model\n"}
{"snippet": "prediction = knnmodel.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "prediction = lm.predict(X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "prediction = logmodel.predict(X_test)\n", "intent": "** Using the generated model to predict the test data.**\n"}
{"snippet": "predic_GBC_krk = clf_GBC.predict(X_test_krk)\n", "intent": "Prediction after training on the X and Y training values and using it on the X testing values\n"}
{"snippet": "y_pred_train = clf.predict(train_x)\nconfusion_matrix(train_y, y_pred_train)\n", "intent": "As expectedly, training set is fit exactly.\n"}
{"snippet": "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag) \ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\n", "intent": "Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n"}
{"snippet": "client_data = [[5, 17, 15], \n               [4, 32, 22], \n               [8, 3, 12]]  \nfor i, price in enumerate(reg.predict(client_data)):\n    print(\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))\n", "intent": "* What price would you recommend each client sell his/her home at? \n* Do these prices seem reasonable given the values for the respective features? \n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0,dtype=tf.float32)\n    for l in range(len(style_layers)):\n        current_gram = gram_matrix(feats[style_layers[l]])\n        loss_layer   = style_weights[l]*tf.reduce_sum((current_gram-style_targets[l])**2)\n        style_loss   = tf.add(style_loss,loss_layer)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "sgdc_scores =  cross_val_score(sgdc_clf, train_data, train_labels, cv=5)\nprint(\"scores: %s  mean: %f  std: %f\" % (str(sgdc_scores), np.mean(sgdc_scores), np.std(sgdc_scores)))\nwith open(\"sentiment/sgdc_clf.pickle\", \"wb\") as sgdc_file:\n    pickle.dump(sgdc_clf, sgdc_file)\nsgdc_clf.score(test_data, test_labels)\n", "intent": "Let's save it for later.\n"}
{"snippet": "precision, recall, fscore, support = precision_recall_fscore_support(test_labels, sgdc_clf.predict(test_data))\nprecision, recall, fscore\n", "intent": "Let's take a look to the precision, recall and F-score of the classifier.\n"}
{"snippet": "preds = clf_rs.predict(test)\ncreate_submission_file(preds, \"knn_tuned_predictions\")\n", "intent": "Finally, predict on the text set and save the \n"}
{"snippet": "ftraining = r\"C:\\P\\Machine Learning\\kocicky 1\\test1000.csv\"\ntesting = np.loadtxt(ftraining,delimiter=',')\nX_testing = scaler.transform(testing)\ny_testing = clf.predict(X_testing)\n", "intent": "Predict using testing data. Don't forget to scale the input too using StandardScaler\n"}
{"snippet": "clf.predict_proba(X_testing)\n", "intent": "Probabilities of P(y|x)\n"}
{"snippet": "kaggle_predictions = knn.predict(test)\n", "intent": "To enter the Kaggle Competition:\n"}
{"snippet": "print(bnb.predict(tf.transform([\n    \"I totally agree with you.\",\n    \"You are so stupid.\",\n    \"I love you.\"\n    ])))\n", "intent": "10. Finally, let's test our estimator on a few test sentences.\n"}
{"snippet": "predict_train = model.predict(X_train)\nprint predict_test[:10]\n", "intent": "- predicting labels for train data\n"}
{"snippet": "predict_test = model.predict(X_test)\nprint predict_test[:10]\n", "intent": "- predicting class labels & scores for test data\n"}
{"snippet": "print metrics.classification_report(y_test, predict_test)\n", "intent": "- Classification report\n"}
{"snippet": "feature_vec = [val for key, val in situation.items() if key in features]\nfeature_vec = data['scaler'].transform(feature_vec)\nprobs['pre_play_wp'] = model.predict_proba(feature_vec)[0][1]\nfor scenario, outcome in scenarios.items():\n    feature_vec = [val for key, val in outcome.items() if key in features]\n    feature_vec = data['scaler'].transform(feature_vec)\n    pred_prob = model.predict_proba(feature_vec)[0][1]\n    if scenario in ('fg', 'fail', 'punt', 'missed_fg', 'touchdown'):\n        pred_prob = 1 - pred_prob\n    probs[str(scenario + '_wp')] = pred_prob\n", "intent": " Note there is more information in situation than just model features.\n"}
{"snippet": "dev_predicted = pipeline.predict(['I saw that my order was cancelled. Can you reorder it please'])\n", "intent": "We test a sample sentence\n"}
{"snippet": "results = cross_val_score(estimator, X, categorical_Y, cv=kfold)\nprint('Accuracy: {}'.format(results.mean()))\n", "intent": "Now to the accuracy. We obtain our results using the cross_val_score provided through Keras:\n"}
{"snippet": "rate = 0.00097\nlogits,weights = LeNet(x)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nregularizers=0.00\nfor w in weights:\n    regularizers+=tf.nn.l2_loss(w)\nloss_operation = tf.reduce_mean(cross_entropy)+1e-5*regularizers\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)\n", "intent": "Create a training pipeline that uses the model to classify MNIST data.\nYou do not need to modify this section.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Mean squared error: %.2f\"\n      % np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(diabetes_X_test, diabetes_y_test))\n", "intent": "Now we can print out the information about the model.\n"}
{"snippet": "lr_valid_pred = clflr.predict(df_validX)\nlr_valid_prob = clflr.predict_proba(df_validX)[:,1]\nprint(classification_report(df_valid['Cardiology'],lr_valid_pred))\nprint(\"Logistic regression validation accuracy = \" + str(accuracy_score(df_valid['Cardiology'],lr_valid_pred)))\n", "intent": "Display the logistic regression classification report and accuracy score for the **validation data**. \n"}
{"snippet": "brf_test_pred = best_clfrf.predict(df_testX)\nbrf_test_prob = best_clfrf.predict_proba(df_testX)[:,1]\nbrf_test_pred_spec = np.where(brf_test_prob >= brf_spec_threshold, 1, 0)\nprint(classification_report(df_test['Cardiology'],brf_test_pred))\nprint(\"Random forest CV test accuracy = \" + str(accuracy_score(df_test['Cardiology'],brf_test_pred)))\n", "intent": "Now it is time to use the **`test`** data to verify the tuned model.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        feature = feats[style_layers[i]]\n        loss = tf.reduce_sum((gram_matrix(feature) - style_targets[i])**2)\n        style_loss += style_weights[i] * loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions=nb.predict(text_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions=pipeline.predict(text_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "fscore = f1_score(y_test, y_pred)\nprint(fscore)\n", "intent": "How Accurate and Precise is our model?\n"}
{"snippet": "print(skmetrics.classification_report(y_true=df['class'], y_pred=df['prediction']))\nprint(np.count_nonzero(np.where(df['class'] == df['prediction'], 1, 0)) / len(df))\n", "intent": "<p style=\"background-color:\n"}
{"snippet": "clf = xgb.XGBClassifier()\ncv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\nprint(cv_scores.mean())\n", "intent": "Sadly, I have some problems with *sklearns GBClf* and *LightGBM clf*, later I'll fix it, but now let`s try **XGBoost**:\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(\"\\nFinal Model\\n\",classification_report(actual,final))\nprint(\"\\nMultiple Linear Regression\\n\",classification_report(actual,y_pred_lm))\nprint(\"\\nLogistic Regression\\n\",classification_report(actual,y_pred_logit))\nprint(\"\\nAdaboost Decision Tree\\n\",classification_report(actual,y_pred_ada))\nprint(\"\\nNeural Networks\\n\",classification_report(actual,y_pred_nn))\nprint(\"\\nSupport Vector Machine\\n\",classification_report(actual,y_pred_svm))\n", "intent": "The following is a detailed breakdown on the performance of each model. \n"}
{"snippet": "predictions = tree_classifier.predict(X_test)\n", "intent": "Predicting new data:\n"}
{"snippet": "probabilities = random_forest.predict_proba(X_test)[:,1]\n", "intent": "Predicting probabilities\n"}
{"snippet": "predict_sample = data['test_data'][0][0]\nresponse = predictor.predict(data)\nprint('Raw prediction result:')\nprint(response)\n", "intent": "We can now use this endpoint to classify hand-written digits.\n"}
{"snippet": "predicted_sentiment_train_sentiment = sentiment_model.predict(train_matrix)\ntrain_data['predicted_sentiment_ts'] = predicted_sentiment_train_sentiment\nacc_ts = round(float(sum(train_data['predicted_sentiment_ts'] == train_data['sentiment']))/len(train_data.index),2)\n", "intent": "the accuracy of the sentiment_model and the simple mode\n"}
{"snippet": "predicted_sentiment_test_sentiment = sentiment_model.predict(test_matrix)\ntest_data['predicted_sentiment_ts'] = predicted_sentiment_test_sentiment\nacc_ts_test = round(float(sum(test_data['predicted_sentiment_ts'] == test_data['sentiment']))/len(test_data.index),2)\nacc_ts_test\n", "intent": "compare the accuracy of the sentiment_model and the simple model\n"}
{"snippet": "decision_tree_model.predict(sample_validation_data.drop('safe_loans',1))\n", "intent": "Quiz Question: What percentage of the predictions on sample_validation_data did decision_tree_model get correct?\n"}
{"snippet": "predict_safeloans = sample_model.predict(validation_data.drop('safe_loans',1))\n", "intent": "Calculate the number of false positives made by the model on the validation_data.\n"}
{"snippet": "validation_data['predictions'] = sample_model.predict_proba(validation_data.drop('safe_loans',1))[:,1]\n", "intent": "In this section, we will find the loans that are most likely to be predicted safe.\n"}
{"snippet": "scores = cross_val_score(classifier, X_train, y_train, cv=5)\nscores\n", "intent": "Now apply this to the classification model built earlier\n"}
{"snippet": "train_score = rnn_24.evaluate(x_train, y_train, verbose=0)\nprint('Train Score: %.2f MSE (%.2f RMSE)' % (train_score, math.sqrt(train_score)))\ntest_score = rnn_24.evaluate(x_test, y_test, verbose=0)\nprint('Test Score: %.2f MSE (%.2f RMSE)' % (test_score, math.sqrt(test_score)))\nplot_forecast(rnn_24, x_train, y_train, x_test, y_test)\n", "intent": "<p>I stopped training the above RNN after the loss function did not significantly decrease after 400 epochs.</p>\n"}
{"snippet": "base_df['cluster'] = kmeans.predict(base_df)\nbase_df.head(5)\n", "intent": "**Let's save the clusters to our dataframe.**\n"}
{"snippet": "threshold_df['cluster'] = kmeans.predict(threshold_df)\nthreshold_df.head(5)\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">threshold_df</code>.**\n"}
{"snippet": "pca_df['cluster'] = kmeans.predict(pca_df)\npca_df.head(5)\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">pca_df</code>.**\n"}
{"snippet": "mat = loadmat('spamTest.mat')\nXtest = mat['Xtest']\nytest = mat['ytest']\np = model.predict(Xtest)\n", "intent": "After training the classifier, we can evaluate it on a test set. We\nhave included a test set in spamTest.mat\n"}
{"snippet": "x_test_new_attack = count_vect.transform(ls_test.data)[:, top_feature]\nx_test_new_attack[ind_spam,0] = 1\nx_test_new_attack[ind_spam,3] = 1\nx_test_new_attack[ind_spam,4] = 1\ny_predict_attack = mNomBF.predict(x_test_new_attack)\nfn = np.count_nonzero((ls_test.target==1) & (y_predict_M_BF==0))/num_spam\nfn_attack = np.count_nonzero((ls_test.target==1) & (y_predict_attack==0))/num_spam\nprint('False Negative Rate Before Modification = {0:f}'.format(fn))\nprint('False Negative Rate After  Modification = {0:f}'.format(fn_attack))\nprint('Minimum unit cost to fool the spam filter is '+ str(cost_edit/num_spam))\n", "intent": "Modified the test set and predict it with original classifier.\n"}
{"snippet": "def evaluate(logits, labels):\n    pass\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "y_pred = fit.predict(X_test)\ny_pred = pd.Series(y_pred)\ny_pred_vals_df = y_class_dist_df(y_pred)\ny_pred_vals_df\n", "intent": "So, currently, our most common class is ~36% of data, so a dumb model could achieve 36% accuracy predicting only this class. Did the model do that?\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprob_y_2 = clf.predict_proba(X_test)\n", "intent": "Checking class balance, and Classification Report\n"}
{"snippet": "X_predictions = [np.argmax(X_model.predict(np.expand_dims(feature, axis=0))) for feature in test_X]\ntest_accuracy = 100*np.sum(np.array(X_predictions)==np.argmax(test_targets, axis=1))/len(X_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def absolute_error(actual, reported):\n    return np.sum(np.abs(actual - reported)) / len(actual)\n", "intent": "A simple such measure is the Manhattan distance between the actual and the reported distribution, divided by the number of points.\n"}
{"snippet": "def relative_error(actual, reported):\n    return np.sum(np.abs(reported - actual)) / actual.sum()\n", "intent": "An alternative is computing the count of wrong estimates (by how much we overshoot / undershoot) divided by the real counts:\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "And validate it against the test data (test isn't validation)!\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding_LR[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding_LR[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_namesLR(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding_LR[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding_LR[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names_LR(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>\n"}
{"snippet": "print 'Coefficients: \\n', regr.coef_\nprint \"Residual sum of squares: %.2f\" % np.mean((regr.predict(X_test) - y_test) ** 2)\nprint 'Variance score: %.2f' % regr.score(X_test, y_test)\n", "intent": "e) Evalute the $R^2$ on the testing data. Is this good? Bad? Why?\n"}
{"snippet": "model.predict_proba(data)\n", "intent": "We can apply the model against this data to label points where we had danger nearby.\n"}
{"snippet": "np.mean(np.round(naive_pred) == truth), np.mean(model.predict(temperatures) == truth)\n", "intent": "Suddenly, when we compare the difference, it becomes better.\n"}
{"snippet": "cv_model_1 = cross_val_score(model_1, \n                             dataframe[features], \n                             dataframe[output], \n                             cv=kf, \n                             scoring='accuracy' \n                            )\n", "intent": "Hint: Use `cross_val_score` function\nhttp://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n"}
{"snippet": "probabilities = model.predict(x_test_new, batch_size=128)\nclasses = probabilities.argmax(axis=-1)\nclasses\n", "intent": "**You can see that test accuracy of CNN model is close to 99%**\n"}
{"snippet": "preds_prob = clf.predict(dtest)\nlabels = dtest.get_label()\npreds = preds_prob > 0.5\ncorrect = 0\nfor i in range(len(preds)):\n    if (labels[i] == preds[i]):\n        correct += 1\nprint('Predicted correctly: {0}/{1}'.format(correct, len(preds)))\nprint('Error: {0:.4f}'.format(1 - correct / len(preds)))\n", "intent": "Although it should be obvious how the classifier will perform, let's test it out anyway.\n"}
{"snippet": "estimator = KerasRegressor(build_fn = baseline_model, nb_epoch = 100, batch_size = 5, verbose = 0)\nkf = KFold(n_splits = 10, random_state = seed)\nresults = cross_val_score(estimator, X, Y, cv = kf)\none_mean, one_std = print_results(results)\n", "intent": "Evaluate the data using a simple topology, and unstandardized data. Our baseline performance.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(\"/n\")\nprint(confusion_matrix(y_test,predictions))\n", "intent": "Evaluation.\nWe can check precision,recall,f1-score using classification report and confiusion matrix!\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "Prediction and Evaluation.\nLet's evaluate our decision tree.\n"}
{"snippet": "best_classifier = grid_search.best_estimator_\nbest_params = grid_search.best_params_\ny_test_hat = best_classifier.predict(test_data['review'].values)\nprint(\"Best parameters set found on development set:\\n\\n\", best_params)\nprint(\"\\nCV score: %.2f%%\" % (grid_search.best_score_ * 100))\nprint(\"Test score: %.2f%%\" % (best_classifier.score(test_data['review'].values, test_data['sentiment'].values) * 100))\nprint(\"\\nTest classification report:\\n\\n\")\nprint(classification_report(test_data['sentiment'].values, y_test_hat))\nplot_confusion_matrix(test_data['sentiment'], y_test_hat, title='Test confusion matrix')\n", "intent": "<a id='validation'></a>\n[back to index](\n"}
{"snippet": "cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\nprint(\"\\n\")\nreport_results(cv_scores)\n", "intent": "<a id='cross_validation'></a>\n[back to index](\n"}
{"snippet": "predictions = classifier.predict(x_test)\nprint(list(predictions[:20]))\nprint(y_test[:20])\n", "intent": "Our model is now complete. \nWe can try applying the model to some of the examples in our test set to see how well it classifies them. \n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0120.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_losses = 0\n    for i in range(len(style_layers)):\n        cur_index = style_layers[i]\n        cur_feat = feats[cur_index]\n        cur_weight = style_weights[i]\n        cur_style_target = style_targets[i] \n        grammatrix = gram_matrix(cur_feat) \n        style_losses += cur_weight * tf.reduce_sum(tf.squared_difference(grammatrix,cur_style_target))\n    return style_losses\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "for age in age_cat_dict.keys():\n  age_input = [ [ age_cat_dict[age] ] ];\n  pred_children = reg.predict(age_input)[0];\n  print(\"Women in Age (%s) will likely have %.3f children\" % (age, pred_children));\n", "intent": "After the model is trained and we are content with the evaluation metrics it can be put to use for predicting values \n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnetmodel.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions_Xception = [np.argmax(model_Xception.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_Xception = 100*np.sum(np.array(predictions_Xception)==np.argmax(test_targets, axis=1))/len(predictions_Xception)\nprint('Test accuracy: %.4f%%' % test_accuracy_Xception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "(test_loss, test_acc) = network.evaluate(test_image, test_labels)\nprint('acc: %f'%test_acc)\n", "intent": "The above training shows an accuracy of **98%**. Now we will testing our network to be sure it works well. We do this using the `network.evaluate()`.\n"}
{"snippet": "print('predicted y value: ', sgd_clf2.predict([X_test2[111]]))\nprint('actual y value: ', y_test2[111])\n", "intent": "With the trained model, the data scientist can predit the result of the test data.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_breed_classificator.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "bench_f_score = f1_score(bench_candidate_2, test_y[:test_points], labels=[0,1],average='macro')\nprint(bench_f_score)\n", "intent": "We see that the benchmark candidate containing all 1's has the better accuracy score, which is .5377.\n"}
{"snippet": "print show_error(data.index)\n", "intent": "After replacing missing values, let's examine outliers through the IQR rule.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, model.predict(X_test))\n", "intent": "check the accuracy score of trained naive bayes model with sklearn.metrics module\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "see classification_report of trained naive bayes model with classification_report\n"}
{"snippet": "print(classification_report(y_test, pred_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(college[\"Cluster\"], kmn.labels_))\nprint(classification_report(college[\"Cluster\"], kmn.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred = clf.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "np.mean(cross_val_score(lr_pipe, X_train, y_train, cv=5))\n", "intent": "**Significant increase in model train score**\n"}
{"snippet": "prob1 = clf1.predict_proba(test[features])[:,1]\nprob2 = clf2.predict_proba(test[features])[:,1]\ntwo_model_preds = np.round((prob1+prob2) / 2)\nauc_two_model = roc_auc_score(test['high_income'],two_model_preds)\nprint('Auc score for the combined model is {}'.format(auc_two_model))\n", "intent": "`The 2 classifiers got auc score 69.6% and 67.8 respectivly`\n"}
{"snippet": "print(classification_report(y_test,y_pred))\n", "intent": "We have 48,131 correct predicitons and 26,944 incorrect predictions\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Lastly, we predict the new labels, based on the words in the test set.\n"}
{"snippet": "x_test = np.array(['legend wait for it dary'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "print(svm.predict(X_train))\nprint(y_train)\n", "intent": "4) Apply / evaluate\n"}
{"snippet": "svc_predictions = svc_model.predict(X_test)\n", "intent": "Evaluating the model:\n"}
{"snippet": "rfc_predictions = rfc_model.predict(X_test)\n", "intent": "Evaluating the RFC model:\n"}
{"snippet": "knn_predictions = knn_model.predict(X_test)\n", "intent": "Evaluating the model:\n"}
{"snippet": "print('Confusion matrix:\\n', pd.crosstab(y_test,predictions, colnames=['Actual'], rownames=['Predicted'], margins=True))\nprint('\\nAccuracy: ', accuracy_score(y_test,predictions))\n", "intent": "Checking model fit quality\n"}
{"snippet": "train_error = mean_squared_error(y_train, model.predict(X_train))\ntest_error = mean_squared_error(y_test, model.predict(X_test))\nprint('training data MSE', train_error)\nprint('test data MSE', test_error)\n", "intent": "MSE from training and test data\n"}
{"snippet": "prediction = pd.Series(clf.predict(X_test_reduced), index = y_test.index, name='prediction')\ndf_prediction = pd.concat((df, prediction), axis=1, join='inner')\ndf_prediction_correct = df_prediction.loc[lambda df:df.card_type==df.prediction, :]\ndf_prediction_wrong = df_prediction.loc[lambda df:df.card_type!=df.prediction, :]\ndf_prediction_wrong\n", "intent": "Let see which image cannot be predicted correctly to investigate the weakness of our model.\n"}
{"snippet": "preds = net1.predict(X_test)\n", "intent": "Prediction:\n-----------\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, j in enumerate(style_layers):\n        style_loss += style_weights[i] *torch.sum((gram_matrix(feats[j]) - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def eval_results(model, X, Y):\n    predict_probs = model.predict(X, batch_size=batch_size)\n    Y_prob_score = predict_probs[:,1]\n    return roc_auc_score(Y, Y_prob_score)\n", "intent": "Evaluation function\n"}
{"snippet": "myVGG19_predictions = [np.argmax(myVGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(myVGG19_predictions)==np.argmax(test_targets, axis=1))/len(myVGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "svc_pred = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "note_predictions = list(classifier.predict(X_test));\n", "intent": "** Using predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "def evaluation(y_test,predict):\n    from sklearn import metrics\n    print('Model Evaluation Results')\n    print('variance score',metrics.explained_variance_score(y_test,predict))\n    print('MAE',metrics.mean_absolute_error(y_test,predict))\n    print('MSE',metrics.mean_squared_error(y_test,predict))\n    print('RMSE',np.sqrt(metrics.mean_squared_error(y_test,predict)))\n", "intent": "Lets run all these regression models in order to predict the data accuratly and also which model suits well\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a classification report for the model**\n"}
{"snippet": "predictions = dtree.predict(x_test)\n", "intent": "Let's evaluate the decision tree.\n"}
{"snippet": "predictions = svc_model.predict(x_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))\n", "intent": "** Now take that grid model and create some predictions using the test set and create classification reports and confusion matrices for them.**\n"}
{"snippet": "pred = knn.predict(x_test)\n", "intent": "**Use the predict method to predict values using your KNN model and x_test.**\n"}
{"snippet": "predictions = rfc.predict(x_test)\n", "intent": "** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "**Now create a classification report from the results.**\n"}
{"snippet": "grid_predictions = grid.predict(x_test)\n", "intent": "Predictions are re-run on this grid object just like you would with a normal model.\n"}
{"snippet": "dist2_1 = calc_chi_from_thre(0.07576159, 0.48443631, arg[0], 1, arg[2], 1) \nchi2_1 = calc_chi_from_thre(0.07576159, 0.48443631, arg[0], arg[1], arg[2], arg[3]) \nrocDist2_1 = roc_auc_score(true, dist2_1) \nrocChi2_1 = roc_auc_score(true, chi2_1) \nprint 'ROC AUC score using distance is %1.4f' %rocDist2_1\nprint 'ROC AUC score using weighted distance is %1.4f' %rocChi2_1\nfprDist2_1, tprDist2_1, threshDist2_1 = roc_curve(arg[4], dist2_1)\nfprChi2_1, tprChi2_1, threshChi2_1 = roc_curve(arg[4], chi2_1)\nprint 'The informedness using distance is %1.4f' %np.max(tprDist2_1-fprDist2_1)\nprint 'The informedness using weighted distance is %1.4f' %np.max(tprChi2_1-fprChi2_1)\n", "intent": "Threshold = a$\\times$rKronMag + b\n"}
{"snippet": "y_pred2 = KMean2.predict(X_test)\nprint('Comparing k-means clusters against the data:')\nprint('Comparing k-means clusters against the data:')\nprint(pd.crosstab(y_pred2, y_test))\n", "intent": "Applying pca does not seem to improve the result therefore I left it out. These result are not that good.\n"}
{"snippet": "out = model.predict(batch)\n", "intent": "Run the model on the input image\n"}
{"snippet": "y_pred = clf_rf.predict(X_blind)\nblind['Prediction'] = y_pred\n", "intent": "Now it's a simple matter of making a prediction and storing it back in the dataframe:\n"}
{"snippet": "y_unknown = clf_rf.predict(X_unknown)\nwell_data['Facies'] = y_unknown\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "y_hat_svm = clf.predict(X_Train)\nacc1, score1, conf1 = Calc_Accuracy(Y_Train, y_hat_svm)\nprint(\"Accuracy = \", acc1)\nprint(\"F1 Score = \", score1)\nprint(\"\")\nprint(\"Confusion Matrix\")\nconf1[[\"Labels\", \"Actual True\", \"Actual False\"]]\n", "intent": "Now we have our SVM (with a linear kernel) trained lets see how it performs\n"}
{"snippet": "y_hat_svm_CV = clf.predict(X_CV)\nacc2, score2, conf2 = Calc_Accuracy(Y_CV, y_hat_svm_CV)\nprint(\"Accuracy = \", acc2)\nprint(\"F1 Score = \", score2)\nprint(\"\")\nprint(\"Confusion Matrix\")\nconf2[[\"Labels\", \"Actual True\", \"Actual False\"]]\n", "intent": "And the performance on Cross Validation Data is \n"}
{"snippet": "preds = first_model.evaluate(x = X_Train, y = Y_Train)\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Accuracy = \" + str(preds[1]))\n", "intent": "Now to evaluate how our model does\n"}
{"snippet": "train_pred = best_model.predict(x = X_Train)\ncv_pred = best_model.predict(x = X_CV)\n", "intent": "Seems good so far let's have a look at the confusion matrix.\nHowever first we need to normalize the output predictions with a simple function.\n"}
{"snippet": "test_pred = test_model.predict(x = X_Test)\ntest_hat = normalize_predictions(test_pred)\n", "intent": "Below cells just contain the code to output a prediction set for upload.\n"}
{"snippet": "predictions = clf.predict_proba(test[columns])[:,1]\npredictions2 = clf2.predict_proba(test[columns])[:,1]\ncombined = (predictions + predictions2) / 2\nrounded = numpy.round(combined)\nprint(roc_auc_score(test[\"high_income\"], rounded))\n", "intent": "Add predictions and predictions2, then divide by 2 to get the mean.\n"}
{"snippet": "from random import randint\ndigit = randint(0, 1000)\ndisplay_digit(testX[digit])\noutput = model.predict((testX[digit], ))\nprint([\"%0.3f\" % i for i in output[0]])\n", "intent": "Lets see how the model did on an example from the test set\n"}
{"snippet": "predictions = np.array(model.predict(testX)).argmax(axis=1)\nactual = testY.argmax(axis=1)\ntest_accuracy = np.mean(predictions == actual, axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "Now, see how we do on the whole test set\n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set f1 score = \", acc)\n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "R_SS = np.sum((bos.PRICE - lm.predict(X)) ** 2) \nprint R_SS\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "print np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint R_SS/(len(X))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "ResNet50_predictions = [np.argmax( model_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint('expected:', messages.label[3])\n", "intent": "classifying a single random message and checking\n"}
{"snippet": "Performance_report(tuned_clf, X_train, y_train, X_test, y_test, printout=True)\n", "intent": "Let's take a closer look at the performance of the gradient boosting classifier\n"}
{"snippet": "print('R^2: ', r2_score(y_test, pred))\nprint('MAE: ', mean_absolute_error(y_test, pred))\n", "intent": "For comparison, calculate and print $R^2$ and MAE - our scoring functions.\n"}
{"snippet": "pred = model.predict_proba(X_test)\npred = [p[1] for p in pred]\nscore = roc_auc_score(y_test, pred)\nprint('AUROC', score)\n", "intent": "**Prediction of our model, and printing the <code style=\"color:steelblue\">roc_auc_score</code>.**\n"}
{"snippet": "logistic.predict(X)\n", "intent": "<code style=\"color:steelblue\">.predict()</code> function for classification model's is 0's and 1's.\n"}
{"snippet": "l1_pred = fitted_models['l1'].predict(X_test)\nl1_pred[:10]\n", "intent": "Firstly lets take a look at our **confusion matrix** \n"}
{"snippet": "base_df['clusters'] = k_means.predict(base_df)\nbase_df.head()\n", "intent": "**Saving the clusters to our dataframe.**\n"}
{"snippet": "threshold_df['clusters'] = k_means2.predict(threshold_df)\nthreshold_df.head()\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">threshold_df</code>.**\n"}
{"snippet": "pca_df['clusters'] = k_means3.predict(pca_df)\npca_df.head()\n", "intent": "**Save the clusters to <code style=\"color:steelblue\">pca_df</code>.**\n"}
{"snippet": "y_pred = xgboost.predict_proba(X_test)\ny_pred[:5]\n", "intent": "**By default, the predict() made by XGBoost are binary, we'll need to use predict_proba() for probabilities**\n"}
{"snippet": "split = 0.67\nX, XT, Z, ZT = loadDataset(split)\nprint('Train set: ' + repr(len(X)))\nprint('Test set: ' + repr(len(XT)))\nk = 3\nYT = predict(X, Z, XT, k)\naccuracy = getAccuracy(YT, ZT)\nprint('Accuracy: ' + repr(accuracy) + '%')\n", "intent": "Should output an accuracy of 0.95999999999999996%.\n"}
{"snippet": "sample = test_dataset[:30]\nlabels = test_labels[:30]\npred = logit.predict(sample.reshape(len(sample), 28*28))\nresult = labels - pred\nprint 'predicted:', pred\nprint 'actual   :', labels\nprint 'result   : ', ' '.join(['-' if r == 0 else 'x' for r in result])\n", "intent": "--- \nSample Results\n---\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(\"Root Mean Squared Error: \", lin_rmse)\n", "intent": "Let's measure the RMSE on the training set:\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_pred)\n", "intent": "A confusion matrix would count the number of times instances of class A have been classified as class B.\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n", "intent": "1. First, we will look at the confusion matrix of the predictions\n"}
{"snippet": "for m in models:\n    labels = data.map(lambda x: m.predict(x))\n    imgLabels = labels.toarray().reshape(2, 76, 87)\n    draw_image(imgLabels[0,:,:])\n", "intent": "<div class=\"alert alert-info\">\nComplete the source code below to visualize the result of clustering.\n</div>\n"}
{"snippet": "test_acc = (best_net.predict(X_test) == y_test).mean()\nprint('Test accuracy: ', test_acc)\n", "intent": "When you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100 * np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: {:.4f}'.format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ev = classifier.evaluate(input_fn=lambda: input_fn(X_test,y_test),steps=1)\n", "intent": "Evaluate method returns some statistics like accuracy, auc after being called on the test data\n"}
{"snippet": "pipe_pred = yelp_pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_predict = knn.predict(x_test)\nscore = accuracy_score(y_predict, y_test)\nprint(score) \n", "intent": "Calculate the accuracy of your prediction as you learned in Lab3\n"}
{"snippet": "sklearn.metrics.accuracy_score(tar_test, predictions)\n", "intent": "Evaluate accuracy and confusion matrix:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ntrain_error = mean_squared_error(tar_train, model.predict(pred_train))\ntest_error = mean_squared_error(tar_test, model.predict(pred_test))\nprint ('training data MSE')\nprint(train_error)\nprint ('test data MSE')\nprint(test_error)\n", "intent": "MSE from training and test data:\n"}
{"snippet": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel = LinearDiscriminantAnalysis()\nresults = cross_validation.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n", "intent": "+ Assumes a Gaussian distribution for the numerical input variables.\n+ Can model binary and multi-class classification.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_mean = np.mean(y_train)\ny_pred = np.full(y_train.shape, y_mean)\nbaseline = mean_squared_error(y_train, y_pred)\nbaseline\n", "intent": "As a regression problem we are going to predict always the mean score of the wine and calculate the MSE:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))\n", "intent": "We also make predictions and evaluate the model.\n"}
{"snippet": "from collections import Counter\ndef kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n    train(X_train, y_train)\n    for i in range(len(X_test)):\n        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n", "intent": "***Performing a majority vote using counter***\n"}
{"snippet": "predictions = []\nkNearestNeighbor(X_train, y_train, X_test, predictions, 5)\npredictions = np.asarray(predictions)\naccuracy = accuracy_score(y_test, predictions)\nprint(accuracy*100)\n", "intent": "***Finding out the accuracy of the model as follows:***\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nprint('Variance Score:', explained_variance_score(y_test,Y_pred))\n", "intent": "Calculating variance score\n"}
{"snippet": "from sklearn import metrics\nprint('Mean squared Error MSE:', metrics.mean_squared_error(y_test, Y_pred))\n", "intent": "Calculating mean squared error\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test,Y_pred)\n", "intent": "Calculating R2 score\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Predicting test results\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Predicting test value\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Predicting test values\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nprint('Variance score::',explained_variance_score(Y_test,Y_pred))\n", "intent": "***Calculating variance score of model fit***\n"}
{"snippet": "from sklearn import metrics\nprint('Mean squared Error MSE:', metrics.mean_squared_error(Y_test, Y_pred))\n", "intent": "***Calculating the mean squared error***\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nprint('Variance Score:',explained_variance_score(Y_test,Y_pred))\n", "intent": "***Calculating variance score of model fit***\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nprint('Variance Score:', explained_variance_score(Y_test,Y_pred))\n", "intent": "***Calculating variance score of model fit***\n"}
{"snippet": "y_pred1 = classifier1.predict(X_test)\ny_pred1\n", "intent": "** Predicting the Test set results**\n"}
{"snippet": "accuracy_score(target_test, target_pred, normalize = True) \n", "intent": "***Calculating Accuracy Score for Naive bayes classification***\n"}
{"snippet": "((bos.PRICE - lm.predict(X)) ** 2).mean()\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predicted = model.predict(X_test)\npredicted.mean()\n", "intent": "**Below is how the model from the training set can be used to predict winning games for the testing set. **\n"}
{"snippet": "silhouette_score = metrics.silhouette_score(processed_current_observations, current_labels, sample_size=7000)\n", "intent": "https://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "predictions = [pred['predictions'] for pred in estimator.predict(input_fn=predict_input_func)]\n", "intent": "__Calling the `predict` method on the estimator api will return a generator, which we will extract as a list.__\n"}
{"snippet": "rfc_prediction = rfc.predict(X_test)\nrfc_score=accuracy_score(y_test, rfc_prediction)\nprint(rfc_score)\n", "intent": "<a id=\"741\"></a> <br>\n"}
{"snippet": "xgb_prediction = xgboost.predict(X_test)\nxgb_score=accuracy_score(y_test, xgb_prediction)\nprint(xgb_score)\n", "intent": "<a id=\"751\"></a> <br>\n"}
{"snippet": "def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n", "intent": "<a id=\"73\"></a> <br>\n"}
{"snippet": "print(clf.predict(X[:5])) \n", "intent": "<a id=\"15\"></a> <br>\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n", "intent": "<a id=\"43\"></a> <br>\nNext, compare how the model performs on the test dataset:\n"}
{"snippet": "predictions = model.predict(test_images)\n", "intent": "<a id=\"44\"></a> <br>\nWith the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "$$precision = \\frac{tp}{tp+fp}$$\n$$recall = \\frac{tp}{tp+fn}$$\n"}
{"snippet": "def RegressionPerf(y,yhat):\n    (Person, P_value) = pearsonr(y,yhat)\n    print('=================================')\n    print(f' MSE  = {mean_squared_error(y,yhat)}')\n    print(f' R-Squared = {r2_score(y,yhat)}')\n    print(f' Pearson = {Person}')\n    print(f' P-value = {P_value}')\n    print('=================================')\n", "intent": "- R2-score\n- MSE\n- Pearson\n- P-Value\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(X, X_preimage)\n", "intent": "Now we can compute the reconstruction pre-image error:\n"}
{"snippet": "for i in range(ensembleSize):\n    auc = roc_auc_score(devData[\"signal\"].values.astype('int'),\n                        predict(X_dev, ensemble, weights, i+1))\n    if not i:\n        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n    else:\n        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))\n", "intent": "What benefit does ensembling bring? Let's take a look!\n"}
{"snippet": "for i in range(ensembleSize):\n    auc = roc_auc_score(valData[\"signal\"].values.astype('int'),\n                              predict(X_val, ensemble, weights, i+1))\n    if not i:\n        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n    else:\n        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))\n", "intent": "Again we can confirm that ensembling helps. **N.B.** Do not use the validation data for any tuning!\n"}
{"snippet": "for i in range(ensembleSize):\n    auc = roc_auc_score(devdataTrain[\"signal\"].values.astype('int'),\n                        predict(X_dev, ensemble, weights, i+1))\n    if not i:\n        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n    else:\n        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))\n", "intent": "What benefit does ensembling bring? Let's take a look!\n"}
{"snippet": "for i in range(ensembleSize):\n    auc = roc_auc_score(valdataTrain[\"signal\"].values.astype('int'),\n                              predict(X_val, ensemble, weights, i+1))\n    if not i:\n        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n    else:\n        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))\n", "intent": "Again we can confirm that ensembling helps. **N.B.** Do not use the validation dataTrain for any tuning!\n"}
{"snippet": "f1_score(y_train,predictions)\n", "intent": "* Not a good way to figure the goodness of the model because of the imbalance. Just wrote it so that it can be used!\n"}
{"snippet": "from sklearn import metrics\nprint('Score test:', metrics.accuracy_score(y, y_pred))\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "scoring = 'accuracy'\nfolds = 10\nprint(cross_val_score(CRnd_Frst_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\nprint(cross_val_score(CExt_Trees_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\nprint(cross_val_score(CSVM_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\nprint(cross_val_score(CGNB_w2v_tfidf, X, y, cv=folds, scoring=scoring).mean())\n", "intent": "Finally the algorithms can execute on the test partition of the dataset. A 10-fold cross valdation was performed.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(scores)\nmodel.save(\"sequential.h5\")\n", "intent": "Evaluate the final model and save.\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\ndc = DummyClassifier(strategy='most_frequent')\nscores = cross_val_score(dc, counts, fixed_target, cv=10)\nscores.mean()\n", "intent": "Compared with a baseline model with [DummyClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n"}
{"snippet": "print(classification_report(q_test,pred))\n", "intent": "c.Classification Report\n"}
{"snippet": "predictions = clf.predict(test_data)\n", "intent": "Predict labels for the test data using the newly created Decision Tree\n"}
{"snippet": "df_test.loc[gnb.predict(x_test) != y_test, ['label']]\n", "intent": "This is deceiving since both article are clearly part of `Sport` categrory (both are about tennis).\n"}
{"snippet": "df_test.loc[2*kmeans.predict(x_test)-1 != y_test, ['label']].head()\n", "intent": "We are not diplaying all misclassified article (too many). Here are just the first 5.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracyResnet50 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracyResnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        feat = feats[layer]\n        gram_current = gram_matrix(feat,normalize=True)\n        style_loss += style_weights[i] * torch.sum((gram_current - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(torch.from_numpy(score_real), torch.from_numpy(score_fake))\n    g_loss = ls_generator_loss(torch.from_numpy(score_fake))\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, layer in enumerate(style_layers):\n        feat = feats[layer]\n        gram_current = gram_matrix(feat,normalize=True)\n        style_loss += style_weights[layer] * torch.sum((gram_current - style_targets[layer]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(0, len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        A = style_targets[i]\n        style_loss += style_weights[i] * tf.reduce_sum((G - A)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(0,len(style_layers)):\n        idx = style_layers[i]\n        gram_current = gram_matrix(feats[idx])\n        distance = tf.norm(gram_current - style_targets[i])  \n        style_loss += (style_weights[i] * distance)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "clf.predict(feature_test[:5]), label_test[:5]\n", "intent": "> Test the classfier Now\n"}
{"snippet": "def ErrorRate(data, labels):\n    numOfErrors = 0\n    predicted_values = clf.predict(data)\n    for i in range(data.shape[0]):\n        if(labels[i] != predicted_values[i]):\n            numOfErrors+=1\n    return numOfErrors/data.shape[0]*100\nprint(\"Training Error Rate \\t:: \\t{}%\".format(ErrorRate(feature_train,label_train)))\nprint(\"Testing Error Rate \\t:: \\t{}%\".format(ErrorRate(feature_test,label_test)))\n", "intent": "Above we can see that our classifier performs quite well\n> Lets calculate its accuracy\n"}
{"snippet": "mlp.predict(X_train_scaled)\n", "intent": "- Why `stratify` in train_test_split\n- What are the solvers?\n"}
{"snippet": "r2_score(y_test, clf.predict(X_test))\n", "intent": "<li> r2: Coefficient of determination\n<li>mse : Mean squared error regression loss\n"}
{"snippet": "X_new = [[17.99, 10.38, 122.80, 1001.00, 0.12, 0.28, 0.30, 0.15, 0.24, 0.08,\n              1.10, 0.91, 8.59, 153.40, 0.01, 0.05, 0.05, 0.02, 0.03, 0.01,\n              25.38, 17.33, 184.60, 2019.00, 0.16, 0.67, 0.71, 0.27, 0.46, 0.12], \n         [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02, 8.690e-02,\n          7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01, 3.398e+00, 7.408e+01,\n          5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02, 1.389e-02, 3.532e-03, 2.499e+01,\n          2.341e+01, 1.588e+02, 1.956e+03, 1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01,\n          2.750e-01, 8.902e-02]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "cv = cross_validation.LeaveOneLabelOut(labels=regions)\nprediction = cross_validation.cross_val_predict(predictor, ranked, temperature, cv=cv)\nprint(\"R2: {}\".format(metrics.r2_score(temperature, prediction)))\n", "intent": "- Cross-validation can be a very powerful scientific tool\nhttp://luispedro.org/files/Coelho2013_Bioinformatics_extra/crossvalidation.html\n"}
{"snippet": "knn1_preds = knn1.predict(X_test)\nprint(knn1_preds)\n", "intent": "Our predictivity is worse than if we just guessed Red every time!\n"}
{"snippet": "knn3_preds = knn3.predict(X_test)\nprint(knn3_preds)\n", "intent": "If we increase the number of neighbors used, skill increases, but only because all of our guesses converge on all Red.\n"}
{"snippet": "forest_preds = forest.predict(X_test)\nfsc = forest.score(X_test,y_test)\nprint(fsc)\nprint(forest_preds)\n", "intent": "Not only that, but we still can't beat the level of skill achieved just by picking Red every time!\n"}
{"snippet": "DXD_predictions = [np.argmax(DXD_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(DXD_predictions)==np.argmax(test_targets, axis=1))/len(DXD_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = rf.predict_proba(oe_test)[:, 1]\n", "intent": "Submitting to kaggle\n"}
{"snippet": "print np.sum(np.sqrt((bos.PRICE - lm.predict(X)) ** 2))/506\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "from sklearn.metrics import log_loss\nprint(log_loss(yTest, predictions))\n", "intent": "NumerAI uses Log Loss as their error metric, so I'll do the same.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Testing time, lets hope its accuracy is greater than 3.349% (which we obtain earlier)\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Testing time for ResNet50, ReseNet50 has outperformed the VGGNET on the ImageNet challenge, lets hope we get same result here.\n"}
{"snippet": "print('R^2:', r2_score(y_pred=pred, y_true=y_test))\nprint('MAE', mean_absolute_error(y_pred=pred, y_true = y_test))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "pred = model.predict_proba(X)\nprint(pred[:10])\n", "intent": "Call <code style=\"color:steelblue\">.predict_proba()</code> on the first 10 observations and display the results.\n"}
{"snippet": "_, pred1 = retention_model.predict_proba(raw_data, clean=True, augment=True)\n_, pred2 = retention_model.predict_proba(cleaned_new_data, clean=False, augment=True)\n_, pred3 = retention_model.predict_proba(augmented_data, clean=False, augment=False)\n", "intent": "If implemented correctly, these next three statements should all work.\n"}
{"snippet": "mt.mean_absolute_error(y_test,y_pred)\n", "intent": "mean absolute error\n"}
{"snippet": "mt.classification_report(y_test,y_pred)\n", "intent": "Classification report\n"}
{"snippet": "pred=log.predict(x_test)\nprint mt.confusion_matrix(y_test,pred)\n", "intent": "Confusion matrix for model\n"}
{"snippet": "print mt.classification_report(y_test,pred) \n", "intent": "Classification report for model\n"}
{"snippet": "y_pred_prob_gaunb = gaunb.predict_proba(X_test_dtm)[:, 1]\n", "intent": "On line item 154, Bill tweets about climate frequently. I wonder why the computer thinks this tweet actually belong to Bill instead of Neil. \n"}
{"snippet": "y_pred_ggboost = grid_search_gboost.predict(X_test)\n", "intent": "Gradient Boosting with Gridsearch suggests using 2000 trees with 0.03 learning rate. \n"}
{"snippet": "MSE = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint('{:.2f}'.format(MSE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "test_data20=skin_numeric[~skin_numeric.isin(skin_random)].dropna()\ntest_data=test_data20['skin']\ntest_data_aux=np.array(test_data20['skin'],dtype=\"|S6\")\ntest_data20['skin']=test_data20.skin.astype('float')\nsb.countplot(x='skin', data=test_data20, color=plot_color[2])\nprint(test_data20.skin.value_counts())\ntest_data20=test_data20.drop(['skin'],axis=1)\npredictions = forest.predict(test_data20)\n", "intent": "As Michelle suggested, we split the data into two dataframes (80-20)%.\nSo we can predict 20% of data untested.\nAnd show our 20% prediction.\n"}
{"snippet": "print('Coefficients: \\n',regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n     % np.mean((regr.predict(X_test)-y_test)**2))\nprint('R^2 Score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lr, X_train, y_train, cv=5)\nprint(scores)\nprint(\"Judgeing from cross valdiation score, my model is robust.\")\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant([0.0], dtype=tf.float32)\n    for list_index, layer_index in enumerate(style_layers):\n        style_loss += style_weights[list_index] * tf.reduce_sum((gram_matrix(\n            feats[layer_index])-style_targets[list_index])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(\"Accuracy Score of Decision tree with Gini is {0}\".format(accuracy_score(y_test,y_pred)));\nAccuracy_score_train_test_Gini=accuracy_score(y_test,y_pred)\nprint(\"\\nClassification report of Decision tree with Gini is\\n {0}\".format(classification_report(y_test,y_pred)))\n", "intent": "Let' check the accuracy of the decision tree\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmodel = CatBoostClassifier(iterations=400, loss_function='MultiClass', custom_loss='Accuracy')\nscores = cross_val_score(model, x_tr, y_tr, scoring='accuracy', n_jobs=-1, fit_params=None)\n", "intent": "Each of the arrays contains loss in each iteration (like `staged_predict`):\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n", "intent": "Lets see confusion matrix and the reports:\n"}
{"snippet": "df_2018['Prediction'] = clf.predict(x)\n", "intent": "We can assign the predictions of our model back to the dataframe. \n"}
{"snippet": "f1_score(df_2018['Round'], df_2018['Prediction'], average='weighted')\n", "intent": "And, how did we do?!\n"}
{"snippet": "print(\"Mean squared error: %.2f\"  % np.mean((regr.predict(income_X_test) - income_y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(income_X_test, income_y_test))\nprint(\"Mean squared error: %.2f\"% np.mean((regr.predict(income_X_test) - income_y_test) ** 2))\n", "intent": "The highest co relation is found in the number of years of education\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, clf.predict(X_test))\n", "intent": "Error Rate is 0.095745\n"}
{"snippet": "confusion_matrix(y_test, clf2.predict(X_test))\n", "intent": "Error Rate is 0.031915\n"}
{"snippet": "confusion_matrix(y_test, clf3.predict(X_test))\n", "intent": "Error Rate is 0.021277\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(rfst, X_train_transformed, y_train,\n               scoring='accuracy', cv=5)\nprint('accuracy :', np.mean(scores), '% +/-', np.std(scores), '%')\n", "intent": "The next cell may take some time.\n"}
{"snippet": "y_score = rf.predict_proba(Xtest)[:,-1]\ny_true = ytest\nfpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\nroc_auc = auc(fpr, tpr)\n", "intent": "This leads us to the ROC Curve, where we can see how cutoff values will impact false positives and false negatives (as well as tp and tn).\n"}
{"snippet": "print('RMSE: ', np.sqrt(mean_squared_error(y_test, pred)))\n", "intent": "RMSE = sum((y_test - pred)^2)/n\n"}
{"snippet": "y_pred_val = pd.Series(np.round(rand_forest_model.predict(X_val)).astype(np.int64))\ny_pred_train = pd.Series(np.round(rand_forest_model.predict(X_train)).astype(np.int64))\n", "intent": "Now that we have the best hyperparameters, we can train the model and obtain the prediction on the test set.\n"}
{"snippet": "val_mae = np.round(mean_absolute_error(y_true=y_val, y_pred=y_pred_val),1)\nval_rmse = np.round(np.sqrt(mean_squared_error(y_true=y_val, y_pred=y_pred_val)),1)\ntrain_mae = np.round(mean_absolute_error(y_true=y_train, y_pred=y_pred_train),1)\ntrain_rmse = np.round(np.sqrt(mean_squared_error(y_true=y_train, y_pred=y_pred_train)),1)\nprint('Training MAE is  ', train_mae)\nprint('Training RMSE is ', train_rmse)\nprint('Validation MAE is  ', val_mae)\nprint('Validation RMSE is ', val_rmse)\n", "intent": "We can observe very good results and no overfitting between the training and the validation set\n"}
{"snippet": "y_pred_test = pd.Series(np.round(rand_forest_model.predict(X_test)).astype(np.int64))\ny_pred_test.head()\n", "intent": "Here, the unknown sales price will be predicted\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "To test, we can try the model out on the digit we checked before.\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "We can test the error.\n"}
{"snippet": "knn_clf.predict([some_digit])\n", "intent": "To test this out, we can give it our ol' sample digit.\n"}
{"snippet": "some_index = 723\ndirty_digit = X_test_noise[some_index]\nclean_digit = knn_clf.predict([dirty_digit])\nplot_digit(dirty_digit)\nplot_digit(clean_digit)\n", "intent": "Now we test it out!\n"}
{"snippet": "roc_auc_lgt = sklearn.metrics.roc_auc_score(y_test,y_pred_lgt)\nroc_auc_lgt\n", "intent": "Reporting AUC score for ROC curve, instead of accuracy:\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint('Actual & Predicted y values')\nprint(np.vstack([y_test, y_pred]).T)\nprint('\\n')\nknn_accuracy_score = accuracy_score(y_test, y_pred)\nprint('Accuracy score: {}'.format(knn_accuracy_score))\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred))\n", "intent": "Then we'll evaluate the model with the test data. I'll calculate the accuracy score and confusion matrix.\n"}
{"snippet": "print('Accuracy score: {score}'.format(score=accuracy_score(y_test, y_pred)))\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test, y_pred))\n", "intent": "Evaluate the performance\n"}
{"snippet": "mseNOX = np.mean((bos.PRICE - lm.predict(X[['NOX']])) ** 2)\nprint mseNOX\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy (ResNet-50): %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "validation_iter_1 = evaluate(y_pred, target)\nfor item in validation_iter_1:\n    print('{}: {}'.format(item, validation_iter_1[item]))\n", "intent": "Sensitivity was generally pretty good, but specificity remained abysmal no matter what I did.\n"}
{"snippet": "pscore = clf.predict_proba(X_test)[:,1]\nClients = X_test[:,-1]\n", "intent": "The accuracy obtained is high enough, which indicates a good generalization of the function.\nNow let's test the propensity function for this case!.\n"}
{"snippet": "map_score(data_credit,Clients,pscore)\n", "intent": "Mapping the propensity to every dataframe's row\n"}
{"snippet": "pscore = clf.predict_proba(X_test)[:,1]\n", "intent": "A good accuracy in the model; let's study the propensity\n"}
{"snippet": "train_predictions = clf.predict(X_train)\nresult_train = [predict == true for predict, true in zip(train_predictions, y_train)]\n", "intent": "predict on training data and calculate training accuracy\n"}
{"snippet": "test_predictions = clf.predict(X_test)\nresult_test = [predict == true for predict, true in zip(test_predictions, y_test)]\n", "intent": "predict on testing data and calculate testing accuracy\n"}
{"snippet": "train_predictions1 = clf1.predict(X_train1)\nresult_train1 = [predict == true for predict, true in zip(train_predictions1,\\\n                                                        y_train1)]\n", "intent": "predict on training data and calculate training accuracy\n"}
{"snippet": "test_predictions1 = clf1.predict(X_test1)\nresult_test1 = [predict == true for predict, true in zip(test_predictions1, y_test1)]\n", "intent": "predict on testing data and calculate testing accuracy\n"}
{"snippet": "np.mean(clf.predict(X_test) == y_test)\n", "intent": "To evaluate its quality we can compute the average number\nof correct classifications on the test set:\n"}
{"snippet": "from sklearn import metrics\nprint(\"precision:\") \nprint(metrics.precision_score(y_test, y_pred))\nprint(\"recall:   \") \nprint(metrics.recall_score(y_test, y_pred))\n", "intent": "For convenience, these can be computed using the tools in the metrics sub-package of scikit-learn:\n"}
{"snippet": "print(metrics.classification_report(y_test, y_pred, target_names=['Stars', 'QSOs']))\n", "intent": "For convenience, sklearn.metrics provides a function that computes all\nof these scores, and returns a nicely formatted string. For example:\n"}
{"snippet": "y_pred = forest.predict(X_test)\n", "intent": "Now we'll evaluate the performance of our classifier\n"}
{"snippet": "print(metrics.classification_report(y, y_pred_sent, target_names=['negative review', 'positive review']))\n", "intent": "And we'll compare our results with the entire target vector (because we are not doing training at this point)\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0 \n    for i in range(len(style_layers)):\n        loss += ((style_targets[i] - gram_matrix(feats[style_layers[i]]))**2).sum() * style_weights[i]\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "cnt_predictions = reg_predictions + cas_predictions\nprint 'MAE (Overall): ', mean_absolute_error(target_test_overall, cnt_predictions)\n", "intent": "Finally, our overall prediction is the sum of both models\n"}
{"snippet": "test = df[df['Survived'].isnull()]\ntest_X = test[useful]\ntest_y = clf.predict(test_X)\n", "intent": "Finally, let's use the trained classifier to get predictions for the test dataset.\n"}
{"snippet": "scores = cross_val_score(\n    estimator=xgb_final_550, X=X_prepped, y=y,\n    fit_params=dict(eval_metric='mlogloss'),\n    cv=3, scoring=scoring, n_jobs=N_JOBS,\n)\nprint('Mean CV score: {:>10.4f} (on final model training data)'\n      .format(np.mean(scores)))\n", "intent": "Output:\n```\nHold-out set score:    -0.6205 (550 estimators)\n```\n"}
{"snippet": "prop_table['propensity_scores'] = pd.Series(logistic.predict_proba(X)[:,1])\nprop_table.head()\n", "intent": "We add the propensity score as a new column. We can interpret this scores as the \"probability of being a treated subject\".\n"}
{"snippet": "print('F1 score for basic parameters : ', metrics.f1_score(y_valid, pred, average='macro'))\n", "intent": "We find immediately that the results are much better according to both metrics.\n"}
{"snippet": "class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n", "intent": "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit.\n"}
{"snippet": "def detect_trigger_word_spectrum(x):\n    x = x.swapaxes(0, 1)\n    x = np.expand_dims(x, axis=0)\n    predictions = model.predict(x)\n    return predictions.reshape(-1)\n", "intent": "Checking for Trigger Word\n"}
{"snippet": "examples = [\n    \"Hi, let me tell you about the project I was working on. It is ready now.\",\n    \"The world most effecctiive male enhaancement pi1l\"\n]\nexample_counts = count_vectorizer.transform(examples)\npredictions = classifier.predict(example_counts)\nprint(predictions)\n", "intent": "Well, that might not seem like much but our classifier is ready and trained.\nLike in the fruits example, we can try to classify some new emails.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(Resnet50_model_tf.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\naccuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %f' % accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def get_error(predicted, actual):\n    diff = predicted - actual\n    return numpy.mean(numpy.square(diff))\ndef get_error_from_params(x, y, params):\n    predicted = numpy.polyval(params, x)\n    return get_error(predicted, y)\n", "intent": "Cost Function with respect to given parameters ($\\Theta$): $$J(\\Theta) = \\frac{1}{m} \\sum\\limits_{i=1}^n (h_\\Theta(x^i) - y^i)^2$$\n"}
{"snippet": "def my_super_scoring(est, X, y):\n    return np.mean(est.predict(X) == y) - np.mean(est.coef_ != 0)\n", "intent": "We trained a linear model and want to penalize having non-zero coefficient in our model selection :\n"}
{"snippet": "xg = xgboost()\ny_hat_xg = xg.predict(numeric_encoded_train)\nprint(roc_auc_score(y_train, y_hat_xg)) \nunique, counts = np.unique(y_hat_xg, return_counts=True)\nprint(unique, counts)\n", "intent": "Better results on these columns than we got on the feature hashed columns previously.\n"}
{"snippet": "clf.predict(X[1000].reshape(1, -1))\n", "intent": "As seem above, X[1000] is a zero\n"}
{"snippet": "recall_score(y_train0, y_train_pred) \n", "intent": "True positive rate; rate of positive correclty detected by the classifier (2nd row)\ntrue positive / (true positive + false negative)\n"}
{"snippet": "recall_score(y_train0, y_train_pred_90)\n", "intent": "90% precision score\n"}
{"snippet": "roc_auc_score(y_train0, y_scores_forest)\n", "intent": "Performance of Random Forest is better\n"}
{"snippet": "preds = model.predict(tst_seq)\n", "intent": "Predict the sentiment for each review in the test dataset\n"}
{"snippet": "scaled_test = scaler.transform(data_test)\nreduced_test = pca.transform(scaled_test)\npredicted = clf.predict(reduced_test)\nprint('SVM classification accuracy = {:3.2f}'.format(clf.score(reduced_test,target_test)))\n", "intent": "Crucially, we need to apply the same transformations to our test data set.\n"}
{"snippet": "clf.predict_proba(reduced_test)[:10]\n", "intent": "We can also predict the probabilities of any given pixel belonging to a brownfield.\n"}
{"snippet": "grid_searchXGB.refit\ny_pred=grid_searchXGB.predict(X_test)\nbest_accuracyXGB = grid_searchXGB.best_score_ \nbest_parametersXGB = grid_searchXGB.best_params_ \n", "intent": "Refit the model if the parameters are changed\n"}
{"snippet": "accuracy=accuracy_score(y_true=y_test,y_pred=y_pred)\nprecision=precision_score(y_true=y_test,y_pred=y_pred,average='binary')\n", "intent": "Calculate the accuracy and the precision score\n"}
{"snippet": "new_pred=grid_searchXGB.predict(X_new)\n", "intent": "Predict on the new data using the trained classifier\n"}
{"snippet": "def predict(rating, similarity):\n    weighted_sum = similarity.dot(rating)\n    return weighted_sum / np.array([np.abs(similarity).sum(axis=1)]).T\n", "intent": "Now with our similarity matrix, we can predict ratings.\n"}
{"snippet": "x_test = np.array(['angry like hell'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "review='This movie is not remarkable, touching, or superb in any way'\nrotten,fresh=clf.predict_proba(vectorizer.transform([review]))[0]\nprint('rotten probability: %0.2f\\n'%(rotten),'fresh probability: %0.2f' %fresh)\n", "intent": "Why is the test accuracy still so low???\n"}
{"snippet": "prediction = clf.predict(X)\nprediction_from_proba = (clf.predict_proba(X)[:,1]>0.5).astype(int)\n", "intent": "I will build a prediction based on the predicted probabilities.\nIf the predicted probability is > 0.5, label wine as class 1\n"}
{"snippet": "predictions = forest.predict(x_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(college_data['Cluster'],kmeans.labels_))\nprint(classification_report(college_data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print (classification_report(y_pred_class_senders, y_h_emails_test))\n", "intent": "This shows a slightly better distribution for prediction and that's why see a better accuracy score for predicting senders instead of recipients.\n"}
{"snippet": "train_pred1 = model.predict(X_train1)\n", "intent": "Calculate Training and Test RSS\n"}
{"snippet": "from sklearn import metrics\ny_pred_class = knn.predict(X_test)\nprint((metrics.accuracy_score(y_test, y_pred_class)))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "Y_test_pred=regre.predict(test.values)\n", "intent": "**Then we can predict all the sale prices of testing samples:**\n"}
{"snippet": "labels = [label[1] for label in classification_results]\npredictions = [label[2] for label in classification_results]\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(labels, predictions)\nprint 'Testing: Accuracy is %f' % acc\n", "intent": "we are going to select the best scores for true positives and false positives in the test set\n"}
{"snippet": "print(\"Detecting faces on the test set using Eigenface with SVM\")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(classification_report(y_test, y_pred, target_names=(\"nonface\", \"face\")))\n", "intent": "Quantitative evaluation of the model quality on the test set\n"}
{"snippet": "y_pred = fitted_model.predict(X_test)\ny_pred\n", "intent": "Predict on training and test set, and calculate residual values.\n"}
{"snippet": "import pickle\nfrom sklearn.externals import joblib\nclf = joblib.load( os.path.join(os.getcwd(), 'sklearn_mnist_model.pkl'))\ny_hat = clf.predict(X_test)\n", "intent": "Feed the test dataset to the model to get predictions.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test)\nprint \"\\n=======confusion matrix==========\"\nprint confusion_matrix(y_test, y_pred)\n", "intent": "Print the confusion matrix\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n", "intent": "* cross_val_predict() - returns actual predictions instead of scores\n"}
{"snippet": "precision_score(y_train_5, y_train_pred)\n", "intent": "* Precision - accuracy of the positive predictions\n    * Formula: TP / (TP + FP)\n"}
{"snippet": "recall_score(y_train_5, y_train_pred)\n", "intent": "* Recall (aka sensitivty or True Posititve Rate) - ratio of positive instances that are correctly detected\n    * Formula: TP / (TP + FN)\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                            method=\"decision_function\")\n", "intent": "* Best to plot the precision-recall curve to help see which threshold to use\n"}
{"snippet": "roc_auc_score(y_train_5, y_scores_forest)\n", "intent": "* As you can see above, RandomForestClassifier curve is better as it is closer to the top-left corner\n"}
{"snippet": "from sklearn.cross_validation import ShuffleSplit\nn_samples = iris.data.shape[0]\ncv = ShuffleSplit(n_samples, test_size=0.3, random_state=0)\ncross_val_score(clf, iris.data, iris.target, cv=cv)\n", "intent": "It is also possible to use other cross validation strategies by passing a cross validation **iterable** instead, for instance:\n"}
{"snippet": "test_preds = X_test_level2.dot(np.array([best_alpha, 1-best_alpha]).reshape(2, 1))\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ndef scoring(estimator, X, y):\n    y_pred = estimator.predict_proba(X)\n    return roc_auc_score(y, y_pred[:, 1])\n", "intent": "The metric used for the competition is ROC AUC score which can be calculated as follows:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(pl, train_total.Comment, train_total.Insult, scoring=scoring)\n", "intent": "Using this scoring metric we can calculate the performance of our pipeline using cross-validation:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, target_names=target_names))\n", "intent": "Similar to the previous sections, we obtain the performance measures for the new classifier:\n"}
{"snippet": "conv_test_feat = load_array(path + 'results/conv_computed/conv_test_feat.dat')\nval_pseudo_test = bn_model_bigger.predict(conv_test_feat, batch_size = 2)\n", "intent": "That was a great improvement. Lets expand further using the test set.\n"}
{"snippet": "test_pseudo = bn_model_bigger.predict(conv_test_feat, batch_size = 2)\n", "intent": "So we do see a pretty nice improvement. Enough to warrent us trying with the entire test set.\n"}
{"snippet": "valid_batches_pred = get_batches(path + 'valid', batch_size = 50, target_size = (224, 224), shuffle = False, class_mode = None)\nconv_val_feat_pred = conv_model.predict_generator(valid_batches_pred, np.int(valid_batches_pred.samples / valid_batches_pred.batch_size))\nval_predictions = bn_model_bigger.predict(conv_val_feat_pred, batch_size = 50)\n", "intent": "We start by finding the optimal level of clipping.\n"}
{"snippet": "test_predictions = bn_model_bigger.predict(conv_test_feat, batch_size = 2)\n", "intent": "Here it is said, that no clipping is best. Weird. Lets submit two. One clipped and one not clipped. First we compute the predictions.\n"}
{"snippet": "conv_feat = vgg_conv.predict(train, batch_size = 17)\nconv_valid_feat = vgg_conv.predict(valid, batch_size = 31)\nconv_test_stg1_feat = vgg_conv.predict(test_stg1, batch_size = 20)\nconv_test_stg2_feat = vgg_conv.predict(test_stg2, batch_size = 31)\n", "intent": "Next pre-compute the convolutional, validation and test features.\n"}
{"snippet": "fc_model_bb.evaluate(conv_valid_feat, [valid_bbox, valid_labels])\n", "intent": "And evaluate the model\n"}
{"snippet": "conv_valid_feat = vgg_conv_640.predict(valid, batch_size = 1, verbose = 1)\nconv_train_feat = vgg_conv_640.predict(train, batch_size = 1, verbose = 1)\nconv_test_feat_stg1 = vgg_conv_640.predict(test_stg1, batch_size = 1, verbose = 1)\nconv_test_feat_stg2 = vgg_conv_640.predict(test_stg2, batch_size = 1, verbose = 1)\n", "intent": "Precompute the features\n"}
{"snippet": "inp = np.expand_dims(conv_valid_feat[53], 0)\nnp.round(f_conv_model.predict(inp)[0], 2)\n", "intent": "We have to add an extra dimension to our input since the CNN expects a 'batch' (even if it's just a batch of one).\n"}
{"snippet": "img_num = 3\ninp = np.expand_dims(conv_valid_feat[img_num], 0)\nnp.round(f_conv_model_large.predict(inp)[0], 2)\n", "intent": "Get the input again\n"}
{"snippet": "preds_test = inception_model.predict(conv_test_feat,\n                                     batch_size = 7,\n                                     verbose = 1)\n", "intent": "Then predict the labels.\n"}
{"snippet": "inception_model.evaluate(conv_valid_feat, valid_labels, batch_size * 2)\n", "intent": "Submit the final results to Kaggle. Evaluate first.\n"}
{"snippet": "preds = inception_model.predict(conv_test_feat, batch_size = 7, verbose = 1)\n", "intent": "Predict labels for test set.\n"}
{"snippet": "score = cv_score(clf, x_train, y_train)\nprint 'cv_score: ', score\n", "intent": "*Let's try 5-Fold Cross-Validation on the training set*\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "Evaluate the precictions.\n"}
{"snippet": "grid_predict = grid.predict(X_test)\n", "intent": "Predict the test data.\n"}
{"snippet": "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\ndef cv_rmse(model, x, y):\n    r = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return r\nr = range(2003, 2017)\nkm_year = 10000\n", "intent": "Trying with some model from scikit learn: LinearRegression, LR with L2 regularization and others.\n"}
{"snippet": "display_size = 400, 400\nfor img in ['car_1.jpg', 'car_2.jpg', 'car_3.jpg', 'car_4.jpg', 'car_5.jpg', 'car_6.jpg', 'car_7.jpg']:\n    im = Image.open(\"extra images\\\\\" + img)\n    im_copy = im.copy()\n    im_copy.thumbnail(display_size)\n    display(im_copy)\n    print(\"Output: \", translate_prediction(car_classifier.predict(im)))\n", "intent": "Finaly, several examples of predictions for the images out of the initial dataset.\n"}
{"snippet": "predicted_labels_valid = clf_lin.predict(valid_cars)\n", "intent": "The results are too good to be true. Let's check for the validation dataset.\n"}
{"snippet": "def least_squares_error(x, y, w, c):\n    squared_error = np.sum(np.power((y-w*x-c),2))\n    return squared_error\nprint('Squared error for w = 1.5, c = 0.5 is ', \n      least_squares_error(x_train, y_train, w=1.5, c=0.5))\n", "intent": "Write a function that calculates the least squared error on the training data for a particular value of the parameters $w$ and $c$.\n"}
{"snippet": "class LinearRegression:\n    def __init__(self):\n        self.weights_ = None\n    def fit(self, X, t):\n        num_samples, num_features = X.shape\n        self.weights_ = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, t))\n        return self\n    def predict(self, X):\n        num_samples, num_features = X.shape\n        return np.dot(X, self.weights_)\n", "intent": "Roughly following the API of sklearn, linear regression can be implemented in Python as follows:\n"}
{"snippet": "scores = cross_validation.cross_val_score(pipeline, reviews_df['reviewText'], reviews_df['helpfulness_range'], cv=5)\nprint(scores.mean())\n", "intent": "(You should run this after each pipline)\n"}
{"snippet": "y_tr_predicted = np.squeeze(model.predict(Xstrain))\nRSStrain = np.mean(((y_tr_predicted-ytrain)**2)/(np.std(ytrain)**2))\nprint(\"Normalized RSS training = {0:f}\".format(RSStrain))\ny_ts_predicted = np.squeeze(model.predict(Xstest))\nRSStest = np.mean(((y_ts_predicted-ytest)**2)/(np.std(ytest)**2))\nprint(\"Normalized RSS test = {0:f}\".format(RSStest))\n", "intent": "Get the predicted result of both train and test data\n"}
{"snippet": "needsCentroids = kmmodel.cluster_centers_\nshelterCentroids = kmSHmodel.cluster_centers_\nkmlabels = kmmodel.labels_\nkmslabels = kmSHmodel.labels_\nkmpredlabels = kmmodel.predict(s2)\naclabels = acmodel.labels_\nacslabels = acSHmodel.labels_\n", "intent": "<h3><i>We can now create a few useful variables to help evauate the efficiency of the shelters</i></h3>\n"}
{"snippet": "kmAC = metrics.fowlkes_mallows_score(aclabels, kmlabels)\nprint(\"kMeans model matches AC by\", kmAC*100, \"%\")\n", "intent": "<h3><i>So AC model is pretty good as well. Now compare kMeans to AC</i></h3>\n"}
{"snippet": "acsheltSil = metrics.silhouette_score(s2, acslabels)\nprint(\"Agglomerative Clustering Silhouette Score: \", acsheltSil)\n", "intent": "<h2>3.3 Evaluate Agglomerative Hierarchical model for shelters data</h2>\n"}
{"snippet": "print(\"Fowlkes Mallows: \", metrics.fowlkes_mallows_score(kmslabels, kmpredlabels))\n", "intent": "<h3>4.2.1 Using Fowlkes Mallows score gives idea of how many needs have a nearest shelter within the cluster diameter</h3>\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(model, X, y, cv=5)  \n", "intent": "* if we want to use more than 2 groups, splitting becomes tedious\n* but scikit-learn knows how to do that already\n"}
{"snippet": "from sklearn.cross_validation import LeaveOneOut\nscores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))\nscores\n", "intent": "* now use the same number of folds as we have points\n* train on all points but one in each run\n"}
{"snippet": "x_test = np.array(['you are no fun anymore'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "hide_code\nprint(accuracy_score(outcomes, predictions))\n", "intent": "*Using the RMS Titanic data, how accurate would a prediction be that none of the passengers survived?*\n"}
{"snippet": "hide_code\nprint(accuracy_score(outcomes, predictions))\n", "intent": "*How accurate would a prediction be that all female passengers survived and the remaining passengers did not survive* (`prediction_1()`)?\n"}
{"snippet": "hide_code\nprint(accuracy_score(outcomes, predictions))\n", "intent": "*How accurate would a prediction be that all female passengers and all male passengers younger than 10 survived* (`prediction_2()`)?\n"}
{"snippet": "hide_code\nfrom sklearn.metrics import recall_score, accuracy_score, precision_score\nprint (\"Predictions have an accuracy of {:.2f}%.\".format(accuracy_score(outcomes, predictions)*100))\nprint (\"Predictions have an recall score equal to {:.2f}%.\".format(recall_score(outcomes, predictions)*100))\nprint (\"Predictions have an precision score equal to {:.2f}%.\".format(precision_score(outcomes,predictions)*100))\n", "intent": "There is a set of metrics in the **scikit-learn** package for evaluation the quality of the prediction.\n"}
{"snippet": "hide_code\nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "hide_code\nInceptionV3_predictions = \\\n[np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\nInceptionV3_test_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==\\\n                                       np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % InceptionV3_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = mlp_mc_model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "y_pred = model2.predict(X_test)\na = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\nprint cm\nprint cr\nprint a\n", "intent": "We end up having a model with a score and precision of 80%\n"}
{"snippet": "img = load_img(\"joey.jpg\")\npred = model.predict(img)\nvol = pred[0] * 255\nvol = np.swapaxes(vol, 0, 2)\nverts, faces, normals, values = measure.marching_cubes_lewiner(vol, 1)\nmesh = np.array((verts, faces, normals, values))\nnp.save('joey_mesh.npy', mesh)\n", "intent": "I'll save the mesh that I will use later in the face swap script.\n"}
{"snippet": "y_pred_prob = forest_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresh = roc_curve(y_test, y_pred_prob)\n", "intent": "Random forest performed better than logistic regression.\n"}
{"snippet": "y_plot = model.predict([[40],[230]])\ny_plot\n", "intent": "The model would look like $mpg = 39.9 - 0.15 * horsepower$.  \nLet's display the linear regression line in the previous chart:\n"}
{"snippet": "y_test_prediction = model.predict(X_test)\n", "intent": "Use the model to predict the 'label' of X_test.  \nSciKit-Learn will use float64 to generate the predictions so let's take those values back to int16\n"}
{"snippet": "seasonalM = model.predict(monthsOHE)\n", "intent": "Finally, we get the seasonality by predicting the value month by month:\n"}
{"snippet": "WinsModel.predict([[1, 135]])\n", "intent": "135 Runs are necessary for 95 wins.  \nWe can verify it by predicting the wins for 135 Runs:\n"}
{"snippet": "RSmodel2.predict([[1, 0.339, 0.43]])\n", "intent": "Or we can use the handy function predict():\n"}
{"snippet": "RAmodel.predict([[1, 0.307, 0.373]])\n", "intent": "Let's predict also RA even if the model is very weak, just as example.  \nWe know that Team OOBP in 2002 is 0.307 and Team OSLG is 0.373\n"}
{"snippet": "WinsModel.predict([[1, (805-622)]])\n", "intent": "And finally the number of wins:\n"}
{"snippet": "import numpy as np\ndef get_loss(yhat, target):\n    loss = np.dot((target - yhat), (target - yhat))\n    return(loss)\n", "intent": "There are several way of implementing the loss, I use the squared error here.  \n$L = [y - f(X)]^2$\n"}
{"snippet": "baselineCost = get_loss(get_baseline_predictions(), y)\nprint (\"Training Error for baseline RSS: {:.0f}\".format(baselineCost))\nprint (\"Average Training Error for baseline RMSE: {:.0f}\".format(np.sqrt(baselineCost/m)))\n", "intent": "To better see the value of the cost function we use also the RMSE, the Root Mean Square Deviation.  \nBasically the average of the losses, rooted.\n"}
{"snippet": "predictions = simple_model.predict(sales[['sqft_living']])\nsimpleCost = get_loss(predictions, y) \nprint (\"Training Error for baseline RSS: {:.0f}\".format(simpleCost))\nprint (\"Average Training Error for baseline RMSE: {:.0f}\".format(np.sqrt(simpleCost/m)))\n", "intent": "Now that we can make predictions given the model, let's again compute the RSS and the RMSE.\n"}
{"snippet": "better_predictions = better_model.predict(more_features)\nprint (better_predictions[0]) \n", "intent": "Again we can use the .predict() function to find the predicted values for data we pass. For the model above:\n"}
{"snippet": "predictions = better_model.predict(more_features)\nbetterCost = get_loss(predictions, y) \nprint (\"Training Error for baseline RSS: {:.0f}\".format(betterCost))\nprint (\"Average Training Error for baseline RMSE: {:.0f}\".format(np.sqrt(betterCost/m)))\n", "intent": "Now that we can make predictions given the model, let's write a function to compute the RSS of the model. \n"}
{"snippet": "print (get_loss(model_1.predict(sales[model_1_features]), y))\nprint (get_loss(model_2.predict(sales[model_2_features]), y))\nprint (get_loss(model_3.predict(sales[model_3_features]), y))\nprint (get_loss(model_4.predict(sales[model_4_features]), y))\nprint (get_loss(model_5.predict(sales[model_5_features]), y))\n", "intent": "We can use the loss function from earlier to compute the RSS on training data for each of the models.\n"}
{"snippet": "print (get_loss(model_1.predict(test_data[model_1_features]), test_y))\nprint (get_loss(model_2.predict(test_data[model_2_features]), test_y))\nprint (get_loss(model_3.predict(test_data[model_3_features]), test_y))\nprint (get_loss(model_4.predict(test_data[model_4_features]), test_y))\nprint (get_loss(model_5.predict(test_data[model_5_features]), test_y))\n", "intent": "Now compute the RSS on TEST data for each of the models.\n"}
{"snippet": "Y_prediction = lr.predict(test_set_x.T)\nY_prediction.shape\n", "intent": "We can measure the model accuracy using the test set:\n"}
{"snippet": "print (\"On the training set:\")\npredictions_train = predict(train_set_x, train_set_y, fit_params_reg)\nprint (\"On the test set:\")\npredictions_test = predict(X_test.T, y_test.T, fit_params_reg)\n", "intent": "Let's check the new accuracy values:\n"}
{"snippet": "predictions = [model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in test_tensors]\n", "intent": "The model is then implemented on the test dataset of Statefarm driver images.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_declined_pred = lr.predict(X_declined_onehot)\nprint Y_declined_pred.sum()/len(Y_declined_pred)\n", "intent": "How about the people who were not granted loans? We can use my model to predict what would happen.\n"}
{"snippet": "print \"Prediction (probability): \", round(lr.predict_proba(X_test_onehot)[2, 1], 3)\n", "intent": "We again see that low ``savings``, low ``yearly_salary``, high ``limit_used``  are the most dominant factors that result in the rejection.\n"}
{"snippet": "Y_declined_pred = rf.predict(X_declined_label)\nprint Y_declined_pred.sum()/len(Y_declined_pred)\n", "intent": "How about the people who were not granted loans?\n"}
{"snippet": "loss = model.evaluate(\n    x=test_data,\n    y=test_labels\n)\nprint('loss: ' + str(np.sum(loss)))\n", "intent": "Now, let's see what the net says on our test set\n"}
{"snippet": "pred_X = gcf.predict(X_te)\nprint(pred_X)\n", "intent": "Now checking the prediction for the test set:\n"}
{"snippet": "pred_X = gcf.predict(X_te)\nprint(pred_X)\n", "intent": "... and predicting classes ..\n"}
{"snippet": "predictions = model.predict(test['size_kw'.split()])\npredictions\n", "intent": "Higher result for CA 0.97 vs 0.96 for the full data_date2 dataset\n"}
{"snippet": "predictions = model.predict(test['size_kw'.split()])\npredictions\n", "intent": "Wonder why that would be \nWould expect less. Insolation should be less.\nNeed to clean those data and review.\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "Now we can feed `X_test` to our model and get the predictions!\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test[net]]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error,mean_absolute_error\nimport math\nprint \"Score for test data : \",model.score(X_test,y_test)\nprint \"Score for training data : \",model.score(X_train,y_train)\nprint \"Mean absolute error of prediction : \",mean_absolute_error(y_test,model.predict(X_test))\nprint \"Mean square error of prediction : \",mean_squared_error(y_test,model.predict(X_test))\nprint \"Mean square deviation of prediction : \",math.sqrt(mean_squared_error(y_test,model.predict(X_test)))\n", "intent": "The results look similar to the polynomial regression. Lets look at the scores.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error,mean_absolute_error\nimport math\nprint \"Score for test data : \",model.score(X_test,y_test)\nprint \"Score for training data : \",model.score(X_train,y_train)\nprint \"Mean absolute error of prediction : \",mean_absolute_error(y_test,model.predict(X_test))\nprint \"Mean square error of prediction : \",mean_squared_error(y_test,model.predict(X_test))\nprint \"Mean square deviation of prediction : \",math.sqrt(mean_squared_error(y_test,model.predict(X_test)))\n", "intent": "At first look the error seems to be lesser than the Kernel Ridge, lets check by scoring\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Lets test it on the test data set to see what is the accuracy.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "And test its accuraccy\n"}
{"snippet": "print(\"Test Set: \", list(map(int, y_test)))\nprint(\"Predictions: \", list(map(int, logreg.predict(x_test))))\n", "intent": "Now evaluate performance on test points\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_pred, y_test))\n", "intent": "We can take a look at the classification report for this classifier:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "Accuracy Score Prediction\n"}
{"snippet": "plot_average_loss(results=third_network_results, \n                  layer_names=layer_names, \n                  layer_sizes=layer_sizes,\n                  loss_name=\"earth mover's distance\")\n", "intent": "Let's plos some graphs!\n"}
{"snippet": "log_predictions = logmodel.predict(X_test)\nlog_predictions\n", "intent": "**getting predictions**\n"}
{"snippet": "print(classification_report(y_test, log_predictions))\n", "intent": "** Getting classification report **\n"}
{"snippet": "tree_predictions = dtree.predict(X_test)\n", "intent": "** getting predictions **\n"}
{"snippet": "print(classification_report(y_test, tree_predictions))\nconfusion_matrix(y_test, tree_predictions)\n", "intent": "** classification report & confusion matrix **\n"}
{"snippet": "rndmf_predictions = rndmf.predict(X_test)\n", "intent": "** getting predictions **\n"}
{"snippet": "print(classification_report(y_test, rndmf_predictions))\nconfusion_matrix(y_test, rndmf_predictions)\n", "intent": "** Getting classification report & Get confusion matrix **\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n", "intent": "Epochs = 3\nBatch = 32\nLucas : 1 dense layer, 2 dense layers, sigmoid vs relu\nRobert : Bidirectional stuff\nNick : layer dimensions / output size\n"}
{"snippet": "print('score after removing tackles_pg: {:0.4f}'.format(\n    pseudo_cross_val_lasso_score(Points_Statistics_df.drop('tackles_pg', axis = 1), 5, optimized_alpha)))\nprint('score after removing interceptions_pg: {:0.4f}'.format(\n    pseudo_cross_val_lasso_score(Points_Statistics_df.drop('interceptions_pg', axis = 1), 5, optimized_alpha)))\nprint('score after removing fouled_pg: {:0.4f}'.format(\n    pseudo_cross_val_lasso_score(Points_Statistics_df.drop('fouled_pg', axis = 1), 5, optimized_alpha)))\nprint('score after removing aerials_won: {:0.4f}'.format(\n    pseudo_cross_val_lasso_score(Points_Statistics_df.drop('aerials_won', axis = 1), 5, optimized_alpha)))\nprint('score after removing discipline_red: {:0.4f}'.format(\n    pseudo_cross_val_lasso_score(Points_Statistics_df.drop('discipline_red', axis = 1), 5, optimized_alpha)))\n", "intent": "Try to remove the five features with lowest correlation with points: 'tackles_pg', 'interceptions_pg', 'fouled_pg' 'aerials_won', 'discipline_red'.\n"}
{"snippet": "Points_Statistics_df_important = Points_Statistics_df.drop(['fouled_pg', 'interceptions_pg'], axis =  1)\nprint('score after removing less correlated features: {:0.4f}'.format( \n    pseudo_cross_val_lasso_score(Points_Statistics_df_important, 5, optimized_alpha)))\n", "intent": "The removal of fouled_pg and interceptions_pg increased my prediction accuracy. I'll remove the two features.\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    d_const = tf.constant(delta, name='delta_const')\n    residuals = tf.abs(labels - predictions, name='residuals')\n    quadratic = 0.5 * tf.square(residuals)\n    linear = (d_const * residuals) - 0.5 * d_const * d_const\n    return tf.where(residuals < d_const, quadratic, linear)\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "test_accuracy, test_prediction = evaluate(X_test, y_test, BATCH_SIZE)\nt = PrettyTable(['Model accuracy against the test set'])\nt.add_row(['{:.3f}'.format(test_accuracy)])\nprint(t)\n", "intent": "Last but definitely not least, the model accuracy against the test dataset.\n"}
{"snippet": "numeric_features = data.drop(['sales','salary'], axis=1)\nevaluate(df=numeric_features,iterations=5,model=rf_model,target_col='left',test_size=0.3)\n", "intent": "First we try random forests just dropping the categorical features\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print(scores)\n    print((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores)))\n", "intent": "This function will serve to perform and evaluate a cross validation:\n"}
{"snippet": "results = model.evaluate(x_test, y_test)\nprint(results)\n", "intent": "Now, let's test the model:\n"}
{"snippet": "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\ntest_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\nprint (\"Training set accuracy: {accuracy}\".format(**train_eval_result))\nprint (\"Test set accuracy: {accuracy}\".format(**test_eval_result))\n", "intent": "Run predictions for both training and test set.\n"}
{"snippet": "labelPred = logit.predict(np.c_[test_set.x1[:], test_set.x2[:]])\nlabelActual = test_set.y[:]\n", "intent": "- Predicting the test set\n"}
{"snippet": "print('Accuracy score of the model: ' + str(accuracy_score(logreg_cv.predict(Xtestlr), ytestlr)))\n", "intent": "This model does not give me the same C values as the calculation I did previously.\n"}
{"snippet": "siegfried_taxonomy = ['.png', '.jpeg', '.java', '.xml', '.txt', '.html', '.js', '.json', '.css', '.svg', '.gif', '.avi']\nget_classification_report(directory_dataframe, siegfried_taxonomy, predictor_file_type=\"siegfried_filetype\")\n", "intent": "> Using the file extension as ground truth. Did the prediction match the file extension?\n"}
{"snippet": "fido_taxonomy = ['.png', '.jpeg', '.java', '.xml', '.txt', '.py', '.html', '.tex', '.js', '.json']\nget_classification_report(directory_dataframe, fido_taxonomy, predictor_file_type=\"fido_filetype\")\n", "intent": "> Using the file extension as ground truth. Did the prediction match the file extension?\n"}
{"snippet": "guessland_taxonomy = ['.java', '.cpp', '.c', '.py', '.js', '.html', '.sh', '.sql']\nget_classification_report(directory_dataframe, guessland_taxonomy, predictor_file_type=\"guessland_filetype\")\n", "intent": "> Using the file extension as ground truth. Did the prediction match the file extension?\n"}
{"snippet": "fpr, tpr, thresholds = roc_curve(b_test, logmodel.predict_proba(A_test)[:,1])\n", "intent": "**Plotting ROC curve**\n"}
{"snippet": "fpr, tpr, thresholds = roc_curve(y_test, log_reg.predict_proba(X_test)[:,1])\nfpr100, tpr100, thresholds100 = roc_curve(y_test, log_reg100.predict_proba(X_test)[:,1])\nfpr001, tpr001, thresholds001 = roc_curve(y_test, log_reg001.predict_proba(X_test)[:,1])\n", "intent": "**Plotting ROC curve**\n"}
{"snippet": "test_X = test_df[['Sex','Pclass_2','Pclass_3','had_Parch','had_SibSp','Age_(20, 40]',\\\n        'Age_(40, 60]','Age_(60, 80]','Fare_bin_(-1, 15]','Fare_bin_(15, 513]']]\npredictions = gs.best_estimator_.predict(test_X)\ntest_df['Survived'] = predictions\nprint 'Training bias =',df.Survived.mean()\nprint 'Test bias =',test_df.Survived.mean()\n", "intent": "Slightly better score - let's see if the model generalises better to the test data.\n"}
{"snippet": "testArr = X_test.values\ntestRes = y_test.values\nresults = rf.predict(testArr)\n", "intent": "Now our classifier is ready, and all we need to do it predict on our test data set (for which we have to convert it into a numpy array)\n"}
{"snippet": "Y = clf.predict(X_test)\nerrors = (Y_test != Y).sum()\ntotal = X_test.shape[0]\nprint(\"Success rate:\\t %d/%d = %f\" % ((total-errors,total,((total-errors)/float(total)))))\nprint(\"Error rate:\\t %d/%d = %f\" % ((errors,total,(errors/float(total)))))\n", "intent": "We will test all the digits in our test dataset and check the error rate\n"}
{"snippet": "mean_sq_err_lasso_test = mean_squared_error(lasso.predict(X_test), y_test)\nprint(\"Estimating prediction risk on test data\")\nprint(\"OutOfTheBox Lasso: {0: 5f}\".format(mean_sq_err_lasso_test))\n", "intent": "Even mean squared error is better with lasso!\n"}
{"snippet": "X_test_preds = grid.predict(X_test)\nscore = mean_squared_error(y_test, X_test_preds)\nprint(score)\n", "intent": "<b>Model Evaluation</b>\nMean Squared Error is taken as the metrics for calculating the accuracy of our model\n"}
{"snippet": "y_pred = logreg.predict(X_test)\nacc = logreg.score(X_test,y_test)\nprint acc\n", "intent": "**Before:** Our model was 80% accurate on training data.\n**After adding in order-ad_unit_1 interactive feature:** Our model is now at 83% accuracy.\n"}
{"snippet": "recall = metrics.recall_score(\n    y_true=y_test,\n    y_pred=y_pred\n)\nrecall\n", "intent": "**Recall** is better, more balanced, but precision is less balanced. We care about fewer false negatives, so recall is more relevant, anyway.\n"}
{"snippet": "y_pred = logreg_cv.predict(X_test)\nacc = logreg_cv.score(X_test,y_test)\nacc\n", "intent": "Best performance against test data.\n"}
{"snippet": "recall = metrics.recall_score(\n    y_true=y_test,\n    y_pred=y_pred\n)\nrecall\n", "intent": "Recall is more balanced, precision still not so much.\n"}
{"snippet": "y_pred = multilogreg.predict(X_test)\nacc = multilogreg.score(X_test,y_test)\nacc\n", "intent": "Ooh, this model is a *bit* better than the binomial linear regression against the training data.\n"}
{"snippet": "recall = metrics.recall_score(\n    y_true=y_test,\n    y_pred=y_pred\n)\nrecall\n", "intent": "No real difference to precision and recall.\n"}
{"snippet": "y_pred = multilogreg_cv.predict(X_test)\nacc = multilogreg_cv.score(X_test,y_test)\nacc\n", "intent": "Oof, this model's performance went way down!\n"}
{"snippet": "recall = metrics.recall_score(\n    y_true=y_test,\n    y_pred=y_pred\n)\nrecall\n", "intent": "Haha this is so bad.\n"}
{"snippet": "auroc = metrics.roc_auc_score(\n    y_true=y_test,\n    y_score=y_pred,\n    average=None\n)\nauroc\n", "intent": "Nice. This is the best performing model! Although I lose interpretability.\n"}
{"snippet": "recall = metrics.recall_score(\n    y_true=y_test,\n    y_pred=y_pred\n)\nrecall\n", "intent": "Precision more balanced, but recall is less.\n"}
{"snippet": "auroc = metrics.roc_auc_score(\n    y_true=y_test,\n    y_score=y_pred,\n    average=None\n)\nauroc\n", "intent": "Precision decreased, but recall increased.\n"}
{"snippet": "print(\"Importing the model from model.pkl\")\nf2 = open('data/model.pkl', 'rb')\nclf2 = pickle.load(f2)\nX_new = [[2,180,74,24,21,23.9091702,1.488172308,22]]\nprint ('New sample: {}'.format(X_new))\npred = clf2.predict(X_new)\nprint('Predicted class is {}'.format(pred))\n", "intent": "Then when we have some new observations for which the lable is unknown, we can load the model and use it to predict values for the unknown label:\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\npredictions = clf.predict(X_test)\nprint('Accuracy: ', accuracy_score(Y_test, predictions))\n", "intent": "With our model trained, we'll use it to predict labels for the test data and evaluate its accuracy using the known labels.\n"}
{"snippet": "predictions = clf.predict(X_test)\nprint('Predicted labels: ', predictions)\nprint('Actual labels: ' ,Y_test)\n", "intent": "Let's start by predicting the labels for the test features, and comparing the predicted labels to the actual labels:\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy: ', accuracy_score(Y_test, predictions))\n", "intent": "Looks pretty good. What's the overall *accuracy* of the model when used with the test dataset?\n"}
{"snippet": "from sklearn. metrics import classification_report\nprint(classification_report(Y_test, predictions))\n", "intent": "OK, how about some other metrics? We can calculate the *precision*, *recall*, and *f1-score* for each class:\n"}
{"snippet": "X_new = [[6.6,3.2,5.8,2.4]]\nprint ('New sample: {}'.format(X_new))\npred = clf.predict(X_new)\nprint('Predicted class is', iris.target_names[pred])\n", "intent": "OK, so now we have a trained model. Let's use it to predict the class of a new iris observation:\n"}
{"snippet": "model_Resnet50_predictions = [np.argmax(model_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(model_Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(model_Resnet50_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "with tf.name_scope('loss') as scope:\n    loss = tf.losses.mean_squared_error(predictions=predict_op, labels=Y)\n", "intent": "Mean square error loss is used as this is a regression problem not classification.\n"}
{"snippet": "recall_score(y_test, y_pred_lr)\n", "intent": "Wrongly predicting a diabetic patient as not having diabetes is more costly than the converse. So we will calculate the recall score for this model\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score\nscore = accuracy_score(test_lbl, y_pred)\nprint('accuracy score: ', score)\nf1score = f1_score(test_lbl, y_pred, average='micro')\nprint('f1-score: ',f1score)\n", "intent": "**Calculate the accuracy**\n"}
{"snippet": "C1 = evaluate(\"exp(1e-20 * (1e10 * A) ** 2) * B * 2\")\n", "intent": "Evaluate accepts a string of the RHS of what you want to calculate\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Now you can use it to detect images of the number 5:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "A good way to evaluate a model is to use cross-validation\n"}
{"snippet": "from sklearn.metrics import f1_score\ny_pred = [1] * len(y_true)\nprint(\"F1 Score: {:.4f}\".format(f1_score(y_true, y_pred, pos_label=None, average=\"weighted\")))\n", "intent": "**Note**: 54.0% is the chance we'll guess correctly if we were to randomly pick a game and guess that the home team won\n"}
{"snippet": "y_pred = grid.predict(X_all_2015)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "**We are attempting to outperform an F1 score of 0.38.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(labels_test, classifier.predict(features_test))\n", "intent": "We can see that some of the test points drop into the opposite area and the next step is to calculate the accuracy of the prediction.\n"}
{"snippet": "result = clf.predict(X_test)\ntest['Status'] = result\nData = test.sort_values(['Score', 'Age'], ascending=[False, True])[:10]\nsns.barplot( 'Score', 'Name', data= Data, palette = sns.color_palette(\"Blues_d\"))\n", "intent": "Plotting Best Young Attacking Talent in the game:\n-------------------------------------------------\n"}
{"snippet": "result = clf.predict(X_test)\ntest['Status'] = result\nData = test[test['Status'] != '0'].sort_values(['Age', 'Score'], ascending=[True, False])[:10]\nsns.barplot( 'Score', 'Name', data= Data, palette = sns.color_palette(\"Set1\", n_colors=10, desat=.5))\n", "intent": "Plotting Best Young Defensive Talent in the game:\n-------------------------------------------------\n"}
{"snippet": "result = clf.predict(X_test)\ntest['Status'] = result\nData = test[test['Status'] != '0'].sort_values(['Age', 'Score'], ascending=[True, False])[:10]\nsns.barplot( 'Score', 'Name', data= Data, palette = sns.color_palette(\"Set1\", n_colors=10, desat=.5))\n", "intent": "Plotting Best Young MidfieldTalent in the game:\n-------------------------------------------------\n"}
{"snippet": "xToPredict = [3.5, 5, 5.5, 7];\npredictions = model.predict(xToPredict)\nindex = 0;\nfor val in xToPredict:\n    print ('The predicted value of {0}\\tis\\t{1}'.format(val, predictions[index][0]));\n    index = index + 1;\n", "intent": "Here, we are going to predict the output value of the following input values {3.5, 5, 5.5, 7}\n"}
{"snippet": "xToPredict = [3.5, 5, 5.5, 7];\nfor val in xToPredict:\n    print ('The predicted value of {0}\\tis\\t{1}'.format(val, model.predict(val)[0]));\n", "intent": "Here, we are going to predict the output value of the following input values {3.5, 5, 5.5, 7}\n"}
{"snippet": "mean_squared_error(y_train,lr.predict(X_train))\n", "intent": "Linear Model's training set MSE:\n"}
{"snippet": "mean_squared_error(y_test,lr.predict(X_test))\n", "intent": "Linear Model's test set MSE:\n"}
{"snippet": "pred_y_vgg = vgg_model.predict(test_X)\npred_y_inv3 = inv3.predict(test_X)\n", "intent": "We do the predictions\n"}
{"snippet": "print('predicted:', spam_detection.predict(tfidf9)[0])\nprint('expected:', messages.label[8])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, l in enumerate(style_layers):\n        G = gram_matrix(feats[l])\n        style_loss += style_weights[i]*torch.sum((G - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predYSVCGrid= clfGrid.predict(testX)\naccuracySVCGrid = accuracy_score(testY, predYSVCGrid)\nprecisionSVCGrid = precision_score(testY, predYSVCGrid, average='macro')\nrecallSVCGrid = recall_score(testY, predYSVCGrid, average='macro')\nF1ScoreSVCGrid = 2 / ((1 / precisionSVCGrid) + (1 / recallSVCGrid))\nprint(\"Accuracy of SVM with grid search in training set: %0.4f\" % accuracySVCGrid)\nprint(\"\\nF-measure for SVM with grid search in training set:\")\nprint(\"Precision:\", precisionSVCGrid,\"\\nRecall:\", recallSVCGrid, \"\\nF1 score:\", F1ScoreSVCGrid)\n", "intent": "Best parameters are shown above.\nThe table above shows the rank of parameters searching by grid search.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_losses = 0.0\n    for i, idx in enumerate(style_layers):\n        style_losses += np.float32(style_weights[i]) * tf.reduce_sum((gram_matrix(feats[idx]) - style_targets[i]) ** 2)\n    return style_losses\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(metrics.accuracy_score(y_test, predicted))\nprint(metrics.roc_auc_score(y_test, probs[:, 1]))\n", "intent": "Remember the first model accuaracy\n"}
{"snippet": "print(metrics.accuracy_score(y_test_selected_words, predicted_selected_words))\nprint(metrics.roc_auc_score(y_test_selected_words, probs_selected_words[:, 1]))\n", "intent": "Now for the new model (selected_words) accuaracy\n"}
{"snippet": "def mse_pixel_loss(y, t):\n    loss_per_pixel = torch.pow(t.sub(y), exponent=2)\n    return torch.mean(loss_per_pixel)\noptimizer = optim.SGD(net.parameters(), lr=0.25)\nloss_function = mse_pixel_loss\n", "intent": "Following we define the PyTorch functions for training and evaluation.\n"}
{"snippet": "_y, _t = torch.randn(1, 3), torch.randn(1, 3)\nx = mse_pixel_loss(_y, _t)\nprint(type(x))\nprint(x)\n", "intent": "This function can be called with `torch.Tensor`'s and will return a float, but we will use `Variable` as inputs to compute and use gradients.\n"}
{"snippet": "y_train_pred=lm.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_train,y_train_pred))\n", "intent": "Linear Regression on Training dataset\n"}
{"snippet": "y_test_pred=lm.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_test,y_test_pred))\n", "intent": "Linear Regression on Testing dataset\n"}
{"snippet": "y_train_pred=rf.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_train,y_train_pred))\n", "intent": "Random Forest on Training dataset\n"}
{"snippet": "y_test_pred=rf.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_test,y_test_pred))\n", "intent": "Random Forest on Testing dataset\n"}
{"snippet": "y_train_pred=lm.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_train,y_train_pred))\n", "intent": "Linear Regression on training dataset\n"}
{"snippet": "y_train_pred=mlp.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_train,y_train_pred))\n", "intent": "Neural Network on Training Dataset\n"}
{"snippet": "y_test_pred=mlp.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_test,y_test_pred))\n", "intent": "Neural Network on Testing Dataset\n"}
{"snippet": "y_train_pred=rf.predict(x_train_sc)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_train,y_train_pred))\n", "intent": "Checking the accuracy of data for training set.\n"}
{"snippet": "y_test_pred=rf.predict(x_test_sc)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_test,y_test_pred))\n", "intent": "Checking the accuracy of data for testing set.\n"}
{"snippet": "y_train_pred=pipe.predict(x_train)\nprint(\"R2   :\",r2_score(y_train,y_train_pred))\nprint(\"MAE  :\",mean_absolute_error(y_train,y_train_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_train,y_train_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_train,y_train_pred))\n", "intent": "Finding R2, MAE , RMSE and MAPE on training data\n"}
{"snippet": "y_test_pred=pipe.predict(x_test)\nprint(\"R2   :\",r2_score(y_test,y_test_pred))\nprint(\"MAE  :\",mean_absolute_error(y_test,y_test_pred))\nprint(\"RMSE :\",np.sqrt(mean_squared_error(y_test,y_test_pred)))\nprint(\"MAPE :\",mean_absolute_percentage_error(y_test,y_test_pred))\n", "intent": "Finding R2, MAE , RMSE and MAPE on testing data\n"}
{"snippet": "class GMMClassifier():\n    def __init__(self, models):\n        self.models = models\n    def predict(self, data):\n        result = []\n        for cls in self.models:\n            llk = self.models[cls].score_samples(data)[0]\n            llk = np.sum(llk)\n            result.append((cls, llk)) \n        return sorted(result, key=lambda f: - f[1])\n", "intent": "We define our multi-class classifier that uses [**sklearn's GMM**](http://scikit-learn.org/stable/modules/mixture.html\n"}
{"snippet": "prob = model.predict_proba(predict_x)\nprob_admitted = [row[1] for row in prob]\nprob_admitted[:10]\npd.Series(prob_admitted).plot()\n", "intent": "Plot the probability of being admitted into graduate school, stratified by GPA and GRE score.\n"}
{"snippet": "from sklearn.metrics import average_precision_score\nlogregAuc= roc_auc_score(testYBinary,logPredprob[:,1])\naverage_precision = average_precision_score(testYBinary,logPredprob[:,1])\n", "intent": "Finally, let's calculate the area under the curve for each.\n"}
{"snippet": "score = model.evaluate(xTest2, yTest2, verbose= 0)\n", "intent": "Now let's evaluate the results!\n"}
{"snippet": "models= [('lin_reg', lin_reg), ('random forest', forest), ('decision tree', tree)]\nfrom sklearn.metrics import mean_squared_error\nfor i, model in models:    \n    predictions = model.predict(x_train)\n    MSE = mean_squared_error(y_train, predictions)\n    RMSE = np.sqrt(MSE)\n    msg = \"%s = %.2f\" % (i, round(RMSE, 2))\n    print('RMSE of', msg)\n", "intent": "... and then test them on train data, in terms of MSE:\n"}
{"snippet": "for i, model in models:\n    predictions = model.predict(x_train)\n    errors = abs(predictions - y_train)\n    mape = np.mean(100 * (errors / y_train))\n    accuracy = 100 - mape    \n    msg = \"%s= %.2f\"% (i, round(accuracy, 2))\n    print('Accuracy of', msg,'%')\n", "intent": "We can use the Mean absolute percentage error (MAPE) to compute a measure of accuracy that is more immediate to understand.\n"}
{"snippet": "random_best= rf_random.best_estimator_.predict(x_train)\nerrors = abs(random_best - y_train)\nmape = np.mean(100 * (errors / y_train))\naccuracy = 100 - mape    \nprint('The best model from the randomized search has an accuracy of', round(accuracy, 2),'%')\n", "intent": "Now, let's find out about the performance metrics of our new model: MAPE and RMSE. \n"}
{"snippet": "train_two_layer_preds_reduced = lr_meta_reduced.predict(X_train_level2_reduced) \nrmsle_train_stacking_reduced =  rmsle(y_train_level2, train_two_layer_preds_reduced) \nprint(rmsle_train_stacking_reduced)\nval_two_level_preds_reduced = lr_meta_reduced.predict(first_level_pred_reduced(X_val)) \nrmsle_valid_stacking_reduced = rmsle(y_val, val_two_level_preds_reduced)\nprint(rmsle_valid_stacking_reduced)\n", "intent": "Remove knn and lr from stack. Leave only rf and xgb\n"}
{"snippet": "print('Prediction of Price of the house of size 2500 sq feet and 3 bedrooms')\nour_prediction = minimised_theta[0, 0] + (minimised_theta[0, 1] * 2500) + (minimised_theta[0, 2] * 3)\nprint('Our implementation: $', our_prediction)\ntest_data = np.array([2500,3]).reshape(1,-1)\nsci_prediction = regr.predict(test_data)\nprint('      Scikit-learn: $', sci_prediction[0][0])\n", "intent": "**Comparing the results of our implementation with the scikit-learn **\n"}
{"snippet": "print('Prediction of profit for a city with population of 47000')\nour_prediction = minimised_theta[0, 0] + (minimised_theta[0, 1] * 4.7)\nprint('Our implementation: $', our_prediction*10000)\nsci_prediction = regr.predict(4.7)\nprint('      Scikit-learn: $', sci_prediction[0][0]*10000)\n", "intent": "**Comparing the results of our implementation with the scikit-learn **\n"}
{"snippet": "predicted = EQ_default_params_fit.predict(X_test)\nprobs = EQ_default_params_fit.predict_proba(X_test)\nConfMatrix  = metrics.confusion_matrix(y_test,predicted)\nScoreMetric =  metrics.accuracy_score(y_test, predicted)\nprint ScoreMetric,\"\\n\",ConfMatrix\n", "intent": "Now let's use the classifier to predict the classes in the test set:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "model1.evaluate(X_val, Y_val)\n", "intent": "- Category1 (435 -> 3045)\n"}
{"snippet": "model2.evaluate(X_val, Y_val)\n", "intent": "- All Categories to 7196\n"}
{"snippet": "list(zip (y_test, lr.predict(X_test)))\n", "intent": "<li> Making a Prediction\n"}
{"snippet": "r2Value=r2_score(y_test,y_lrPred)\nr2Value\n", "intent": "<li> Co-Efficient of Determination - $R^2$\n"}
{"snippet": "mseValue=mean_squared_error(y_test,y_lrPred)\nmseValue\n", "intent": "<li> Mean Squared Error - MSE\n"}
{"snippet": "list(zip (y_test,rlr.predict(X_test)))\n", "intent": "<li> Making a Prediction with Ridge\n"}
{"snippet": "r2rlrValue=r2_score(y_test,y_rlrPred)\nr2rlrValue\n", "intent": "<li> $R^2$ of Ridge\n"}
{"snippet": "list(zip (y_test,llr.predict(X_test)))\n", "intent": "<li> Making a Prediction with Lasso\n"}
{"snippet": "r2llrValue=r2_score(y_test,y_llrPred)\nr2llrValue\n", "intent": "<li> $R^2$ of Lasso\n"}
{"snippet": "xgtest = xgb.DMatrix(test[predictors])\npreds = xgb5.predict_proba(test[predictors])[:,1]\n", "intent": "Increasing the learning to 0.15 and cutting down the number of boosting trees to 100 increased the predictive power highly.\n"}
{"snippet": "result = pca_predictor.predict(train_set[0][0])\n", "intent": "Now let's try getting a prediction for a single record using the `predict` function of the estimator.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "fvdev = extractFeatures(X_dev, words_dev, windowsize, word_to_num)\nyp = lr.predict(fvdev)\nnerbase.full_report(y_dev, yp, tagnames) \n", "intent": "Testing using dummy dataset\n"}
{"snippet": "y_p_lr=lr.predict(X)\ny_p_knn=knn.predict(X)\ny_p_rf=rf.predict(X)\ny_p_gb=gb.predict(X)\nX_stem=[y_p_lr,y_p_knn,y_p_rf,y_p_gb]\nX_stem=np.asarray(X_stem)\n", "intent": "<img src='Stemming.png'>\n"}
{"snippet": "l2_loss_noise = noise_l2_lambda * tf.nn.l2_loss(x_noise)\n", "intent": "Define the l2-loss function.\n"}
{"snippet": "def predict(X):\n    y_pred = np.zeros(len(X))\n    for i in progressbar(range(len(y_pred)), desc=\"Predict\"):\n        email_vector = X[i]\n        ham_posterior = ham_log_phi[email_vector].sum() + ham_log_prior\n        spam_posterior = spam_log_phi[email_vector].sum() + spam_log_prior\n        y_pred[i] = 0 if ham_posterior > spam_posterior else 1\n    return y_pred  \n", "intent": "The denominator $P(words)$ is the same for both classes, so it will be ignored in predictions.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The training accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "def portfolio_return_score(actual, predictions, get_portfolio = False):\n    signals = strategy_with_forecast(actual, predictions)\n    return backtest_portfolio(actual, signals, get_portfolio = get_portfolio)  \n", "intent": "To evaluate, we also use the portfolio return obtained through the strategy using the predictions from the model.\n"}
{"snippet": "print(clasifire.predict(dataset_training_test[0:1]))\nprint(clasifire.predict(dataset_training_test[0:16]))\n", "intent": "Now lets do some prediction with our model\n"}
{"snippet": "forest100_pred = randForest100.predict(random_forest_feature_matrix_test_set.toarray())\nnp.save('forest100pred', forest100_pred)\n", "intent": "Making predictions with random forest set at 100 learners\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test Loss Value:', score[0])\nprint('Test Accuracy Value:', score[1])\n", "intent": "Now let's the test the DNN Model built above with mnist test dataset(X_test,Y_test). \n"}
{"snippet": "score = rmodel.evaluate(X_test, Y_test, verbose=0)\nprint('Test Loss Value:', score[0])\nprint('Test Accuracy Value:', score[1])\n", "intent": "Now let's the test the DNN Model built above with mnist test dataset(X_test,Y_test). \n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0072.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "model = SEResNeXt(size=size).model\nmodel.load_weights(\"trained_model/SEResNeXt.h5\", by_name=True)\npredictions = model.predict(x_test)\nprediction_idx = map( np.argmax, predictions )\ntest_idx = map(np.argmax, y_test)\ncorrespondence = list( map( lambda x,y: x == y, list(prediction_idx), list(test_idx) ) )\nprint( sum(correspondence) / len(correspondence) )\n", "intent": "Evaluation: simple accuracy.\n"}
{"snippet": "print(classification_report(y_test, y_hat))\n", "intent": "> *Our model has a slightly easier time (for Recall) classifying Republicans , but it is within the margin of error*\n"}
{"snippet": "all_predictions = model.predict(x_test).argmax(axis=1)\nprint(all_predictions)\n", "intent": "Now let's use the model to predict the test set images.\n"}
{"snippet": "manually_calculated = all_true.target.sum() / all_true.shape[0]\nfrom_scikit = precision_score(df.target, preds)\nprint('manually calculated precision: {}\\nPrecision from scikit:         {}'.format(\n    manually_calculated, from_scikit))\n", "intent": "One more time, just to make sure it's clear:\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall_score(df.target, preds)\n", "intent": "And of course, this one is in scikit as well\n"}
{"snippet": "(recall_score(df.target, preds) + recall_score(~df.target, ~preds)) / 2\n", "intent": "Maybe try the same for recall?\n"}
{"snippet": "reordered_pred_train['f1'] = reordered_pred_train.apply(\n    lambda row: f1_score(row['product_id_x'], row['product_id_y']), \n    axis=1\n)\nreordered_pred_train['f1'].mean()\n", "intent": "Let's calculate the F1 scores for our predictions.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "solution = model.predict(df.loc[test_start:test_end])\nprint(solution)\n", "intent": "Generate the solution through 'predict'\n"}
{"snippet": "mf2.conf_m(confusion_matrix(isPass[np.invert(mask)], clf.predict(X[np.invert(mask)])))\n", "intent": "Linear regression just barely outperforms this!\n"}
{"snippet": "mf2.conf_m(confusion_matrix(nisPass[np.invert(mask)], gbt.predict(X[np.invert(mask)])))\n", "intent": "Dropping out season doesn't change predictive accuracy, showing that we aren't fitting with info that you wouldn't have at the time of play.\n"}
{"snippet": "from sklearn.cross_validation import ShuffleSplit\ncv = ShuffleSplit(n=len(X), n_iter=5)\ncross_val_score(clf, X, y, cv=cv)\n", "intent": "We can use different splitting strategies, such as random splitting\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=1)\nprint(\"Resultado do teste final de acerto da rede\")\nprint(score)\nY_pred = model.predict(x_test, batch_size=batch_size, verbose=1)\n", "intent": "<h3>Getting Results</h3>\n"}
{"snippet": "print(model.predict(100))\nprint(model.predict(350))\n", "intent": "The accuracy score of our model is 70%. Therefore, we can now make prediction using random linear regressor predict function. \n"}
{"snippet": "kfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint('Baseline: %.2f (%.2f) MSE' % (results.mean(), results.std()))\n", "intent": "Now we're ready to evaluate the model. We'll use 10-fold cross-validation. Note that the process will take several minutes on a desktop CPU. \n"}
{"snippet": "print(\"Test set predictions: \\n{}\".format((reg.predict(X_test).round(3))))\n", "intent": "Now we can make predictions of the test set:\n"}
{"snippet": "print(\"Thresholded decision function: \\n{}\".format(\n    gbrt.decision_function(X_test) > 0))\nprint(\"Predictions: \\n{}\".format(gbrt.predict(X_test)))\n", "intent": "We can recover the prediction by looking only at the sign of the decision function:\n"}
{"snippet": "print(\"Argmax of predicted probabilities: \\n{}\".format(\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\nprint(\"Predictions: \\n{}\".format(gbrt.predict(X_test)))\n", "intent": "We can again recover the predictions by computing the `argmax` of `predict_proba`:\n"}
{"snippet": "from tensorflow.python.keras.applications import ResNet50\nmy_model = ResNet50(weights='inputs/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\ntest_data = read_and_prep_images(img_paths)\npreds = my_model.predict(test_data)\n", "intent": "**Create a model with the pre-trained weights file and make predictions:**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nscores\n", "intent": "On to the cross-validation scores.\n"}
{"snippet": "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\nX2 = X.drop(potential_leaks, axis=1)\ncv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\ncv_scores.mean()\n", "intent": "Now that that pitfall has presented itself, it's time to build a model that is more data-leakage resistant:\n"}
{"snippet": "predictions = my_model.predict(test_X)\npredictions[:5]\n", "intent": "And now on to evaluating the model and making predictions, also like in scikit-learn.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "We can use the results to calculate the mean absolute error:\n"}
{"snippet": "MeanSquaredError = np.mean((np.sum((bos.PRICE - lm.predict(X)) ** 2)))\nprint(MeanSquaredError)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        A = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum((A - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "final_results[\"renewal\"] = ensemble_predict(test_data, final_models, prediction_shape=test_data.shape[0])\n", "intent": "Predict test probabilities\n"}
{"snippet": "reg_loss = tf.nn.l2_loss(W)\nreg_loss += tf.nn.l2_loss(b)\n", "intent": "$$\nR(W) = \\|W\\|_2^2\\\\\nR(b) = \\|b\\|_2^2\n$$\nHint: You may use the function *tf.nn.l2_loss()*\n"}
{"snippet": "x_raw = [\"Hillary just admitted Trump should be elected president.\", \n         \"Trump to resign from the race to the White House.\",\n         \"ALERT: Police Pull Over Islamic Refugee, Horrified To See What Was In The Car!!! The media has swept this under the rug, but it is a severe national security threat.\",\n         \"If Trump becomes President, this will not end well.\",]\npredictions = model.predict(x_raw)\nfor pred, message in zip(predictions, x_raw):\n    if pred == 0:\n        print(\"TRUE: \", message)\n    else:\n        print(\"FAKE: \", message)\n", "intent": "Predict the truthfulness of new messages.\n"}
{"snippet": "with open(\"./models/rf_classifier.pkl\", \"rb\") as fid:\n    rf_classifier_loaded = pickle.load(fid)\nprint(f\"Length of test data {len(X_all)}\")\nprint(f\"Number of masses in the test data {len(x_test_all)}\")\npredictions_loaded = rf_classifier_loaded.predict(x_test_all)\nprint(f\"Detected percentage of masses {accuracy_score(y_test_all, predictions_loaded)}\")\n", "intent": "Now we will load classifier and test it again\n"}
{"snippet": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(labelCol='classIndexed', metricName='areaUnderROC')\nmetric = evaluator.evaluate(train)\nmetric\n", "intent": "Since we have class imbalance problem, that's why we will use area under ROC curve as metric.\n"}
{"snippet": "ypred = rf.predict(Xte)\nprint(confusion_matrix(yte,ypred))\nprint(classification_report(yte,ypred))\n", "intent": "Real performance is found using held out set that is 20% of total number of records. This was never used in training or comparing across the models.\n"}
{"snippet": "Resnet50_predictions= [np.argmax(chi_pheo.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "network_predictions = [np.argmax(model_name.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(network_predictions)==np.argmax(test_targets, axis=1))/len(network_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\ntest_accuracy = np.mean(predictions == testY[:,0], axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "Run the network on the test set to measure it's performance. Remember, *only do this after finalizing the hyperparameters*.\n"}
{"snippet": "def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content))\nlayer_features = layers['block2_conv2']\ncontent_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(content_image_features, combination_features)\n", "intent": "The content loss is the (scaled, squared) Euclidean distance between feature representations of the content and combination images.\n"}
{"snippet": "def total_variation_loss(x):\n    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return backend.sum(backend.pow(a + b, 1.25))\nloss += total_variation_weight * total_variation_loss(combination_image)\n", "intent": "Add [total variation loss](http://arxiv.org/abs/1412.0035) (a regularisation term) for spatial smoothness.\n"}
{"snippet": "def Accuracy_Predict(model, test): \n    predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test]\n    test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)        \n    return test_accuracy\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "for  i in range(len(preTrained)):\n    print('Test accuracy for %s is %.4f%%' % (preTrained[i],Accuracy_Predict(models[i],test[i])))\nprint \"I pick up Xception for best accuracy above 80%\"\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.exceptions import NotFittedError\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(A_test)\n        print(name, 'has been successfully fitted.')\n    except NotFittedError as e:\n        print(repr(e))\n", "intent": "**Now, we'll perform a final check on the fitted models**\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)\n", "intent": "**Confusion Matrix:**\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(\"Precision:\", precision_score(Y_train, predictions))\n", "intent": "**Precision and Recall:**\n"}
{"snippet": "def get_cv_score(model, x, y, fold=5, scoring='accuracy', seed=SEED):\n", "intent": "> **Get Cross Validation Accuracy**\n"}
{"snippet": "predictions_sklearn = nn_2.predict_proba(X_test) \n", "intent": "plot out the image and the result to get a better sense of it\n"}
{"snippet": "y_pred = model.predict(X_test)\npd.Series(y_pred).value_counts()\n", "intent": "**Make Prediction & Evaluation**\n"}
{"snippet": "y_pred = model_wine.predict(X_test)\npd.Series(y_pred).value_counts()\n", "intent": "**Make Prediction & Evaluation**\n"}
{"snippet": "gbm.predict(X_test)[1:10]\n", "intent": "**Predict the final result**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Now that we have found better parameters from the GridSearch, we can now use these optimized parameters and input them into our model.\n"}
{"snippet": "preds = preds_frame.get_predict().take(1)\nfor pre in preds:\n    path, pre_score = pre\n    P = imagenet_utils.decode_predictions(np.expand_dims(pre_score, axis=0))\n    print(\"Prediction for {}\".format(path))\n    for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n        print(\"    {}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n", "intent": "* Show result of the first image\n"}
{"snippet": "classes = model.predict(x_test_flat, batch_size=128)\n", "intent": "95% accuracy on test set. Not state of the art but not bad at all considering how simple it was to set it up. \n"}
{"snippet": "acc_logreg_bl = cross_val_score(logreg_bl, X_train_baseline, y_train_baseline, cv=5, scoring='accuracy')\nacc_logreg = cross_val_score(logreg, X_train, y_train, cv=5, scoring='accuracy')\nprint ('Accuracy of baseline model = {}, vals = {}'.format(np.mean(acc_logreg_bl), acc_logreg_bl))\nprint ('Accuracy of optimal model = {}, vals = {}'.format(np.mean(acc_logreg), acc_logreg))\n", "intent": "Accuracy measures a fraction of the classifier's predictions that are correct.\n"}
{"snippet": "acc_logreg_bl = cross_val_score(logreg_bl, X_train_baseline, y_train_baseline, cv=5, scoring='accuracy')\nacc_logreg = cross_val_score(logreg, X_train, y_train, cv=5, scoring='accuracy')\nprint 'Accuracy of baseline model = {}, vals = {}'.format(np.mean(acc_logreg_bl), acc_logreg_bl)\nprint 'Accuracy of optimal model = {}, vals = {}'.format(np.mean(acc_logreg), acc_logreg)\n", "intent": "Accuracy measures a fraction of the classifier's predictions that are correct.\n"}
{"snippet": "result = model.evaluate(x=x_test_pad,y=y_test)\n", "intent": "Now that the model has been trained we can calculate its classification accuracy on the test-set.\n"}
{"snippet": "y_pred = model.predict(x=x_test_pad[0:1000])\n", "intent": "In order to show an example of mis-classified text, we first calculate the predicted sentiment for the first 1000 texts in the test-set.\n"}
{"snippet": "predictions = mnist_predictor.predict(test_image)\n", "intent": "Now, let's run our predictor on it:\n"}
{"snippet": "predicted_train = clf.predict(tfidf_model.transform(legislation_train))\nfor i in range(0,len(predicted_train)):\n    print(\"Actual centrality: \" + legislation_category_train[i] + \",\\tPredicted centrality: \" + predicted_train[i])\n", "intent": "Let's check our accuracy on our training set!\n"}
{"snippet": "predicted_train = clf.predict(tfidf_model.transform(party_train))\nfor i in range(0,len(predicted_train)):\n    print(\"Actual party: \" + party_category_train[i] + \",\\tPredicted party: \" + predicted_train[i])\n", "intent": "What's our success rate on our training data?\n"}
{"snippet": "predicted_test = clf.predict(tfidf_model.transform(party_test))\nfor i in range(0,len(predicted_test)):\n    print(\"Actual party: \" + party_category_test[i] + \",\\tPredicted party: \" + predicted_test[i])\n", "intent": "Again, 90% accuracy on training -- but how does it hold up in test?\n"}
{"snippet": "prelim_truth = prelim_test_set.iloc[:,-1:].values.ravel()\nprelim_predictors = prelim_test_set.iloc[:,:-1]\nprelim_knn_prediction = knn.predict(prelim_predictors)\ncustom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_knn_prediction))\n", "intent": "Not too shabby, but we remember that we're certainly overfitting a bit, so this accuracy will go down in test.  What happens in prelim_test?\n"}
{"snippet": "prelim_logistic_prediction = logistic.predict(prelim_predictors)\ncustom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_logistic_prediction))\n", "intent": "Much higher accuracy!  Again, let's do some prelim testing to see if this improved accuracy holds up in testing.\n"}
{"snippet": "prelim_mlp_prediction = mlp.predict(prelim_predictors)\ncustom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_mlp_prediction))\n", "intent": "This one's pretty good.  Does it hold up to preliminary testing?\n"}
{"snippet": "prelim_tree_prediction = tree.predict(prelim_predictors)\ncustom_accuracy(precision_recall_fscore_support(prelim_truth, prelim_tree_prediction))\n", "intent": "Holy mackerel!  This is very accurate.  Again, we have to be careful about the danger of overfitting, so let's not get too psyched.\n"}
{"snippet": "x_test = np.array(['you are great'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "Zgauss1 = remodel.predict(sig)\nZgauss2 = Zgauss1 == 0\n", "intent": "Use the fitted HMM to predict the hidden states found:\n"}
{"snippet": "Z1 = remodel.predict(X1)\nZ2 = Z1 == 0\n", "intent": "Use the fitted model generate predictions for the two hidden states and plot the results.\n"}
{"snippet": "pred = clf.predict(scaler.transform([[104.01, 1.12], [104.03, 1.11], [ 104.029, 1.135], [104.04, 1.13 ], [104.042,1.155]]))\npred\n", "intent": "Try some test coordinates to see whether they are classified as 'home' (0) or 'other' (1):\n"}
{"snippet": "X1 = np.delete(X, 0, axis=1)\nprint(accuracy_score(y, clf.fit_predict(X1)))\n", "intent": "We will remove one feature at a time to see how our accuracy changes.\n"}
{"snippet": "print(accuracy_score(y, clf.fit_predict(x_scaled)))\n", "intent": "Non-normalized was 89%\n"}
{"snippet": "x1_scaled = np.delete(x_scaled, 0, axis=1)\nprint(accuracy_score(y, clf.fit_predict(x1_scaled)))\n", "intent": "Non-normalized was 95%\n"}
{"snippet": "survival_prediction = result.predict(test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(survial,survival_prediction)\n", "intent": "Predict and evaluate accuracy against test data set\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        current_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum((current_gram - style_targets[i])**2)\n    return style_loss \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print(confusion_matrix(y_train, model.predict(X_train)))\nprint(confusion_matrix(y_test, model.predict(X_test)))\n", "intent": "Here with 'screen' again\n"}
{"snippet": "def log_likelihood(clf, x, y):\n    prob = clf.predict_log_proba(x)\n    neg = y == 0\n    pos = ~neg\n    return prob[neg, 0].sum() + prob[pos, 1].sum()\n", "intent": "We write a `log_likelihood` function which predicts the log likelihood of the dataset according to a Naive Bayes classifier. \n"}
{"snippet": "xception_predictions = [np.argmax(dog_xception.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet50_predictions = [np.argmax(model_Resnet50.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==\n                           np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "mnb_predict = mnb.predict(xtest) \nmnb_acc = metrics.accuracy_score(Y_test, mnb_predict)\nprint('We obtained ', round(mnb_acc, 6), '% accuracy for the model')\n", "intent": "**Model Predictions:** \n"}
{"snippet": "print(metrics.classification_report(Y_test, mnb_predict))\n", "intent": "**Classification Report**\n"}
{"snippet": "scores = cross_val_score(mnb, xtest, Y_test, cv=5) \nprint(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "**Cross Validation of Accuracy:**\n"}
{"snippet": "print(metrics.classification_report(Y_test, lr_predict))\n", "intent": "**Classification Report:**\n"}
{"snippet": "scores = cross_val_score(lr, xtest, Y_test, cv=5) \nprint(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "**Cross Validation:**\n"}
{"snippet": "print(metrics.classification_report(Y_test, mnb_predict))\n", "intent": "**Classification Report:**\n"}
{"snippet": "scores = cross_val_score(svm, xtest, Y_test, cv=5) \nprint(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "**Cross Validation:**\n"}
{"snippet": "scores = cross_val_score(knn, xtest, Y_test, cv=5) \nprint(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "**Cross Validation:**\n"}
{"snippet": "scores = cross_val_score(ranfor, xtest, Y_test, cv=5) \nprint(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "**Cross Validation:**\n"}
{"snippet": "sum(((lm1.predict()-bblnrgdata_cut_2['log_energy_total'])**2)/e)\n", "intent": "chisq = $\\sum_i \\frac{(model(x_i) - data(x_i))^2 }{ error_i^2}$?\n"}
{"snippet": "reconstruction_error = mean_squared_error(x_sample, x_reconstruct) \nprint \"reconstruction error = {}\".format(reconstruction_error) \n", "intent": "Calculate the reconstruction probability. See e.g., algorithm 4 of http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf \n"}
{"snippet": "sentence2= ['This movie is not ugly and a waste of time']\nclassifier2 = vectorizer.transform(sentence2)\nif clf.predict(classifier2)[0]==1:\n    print('Probability Review is Fresh:  %2.2f' %(100*clf.predict_proba(classifier2)[0,1]))\nelse:\n    print('Probability Review is Fresh:  %2.2f' %(100*clf.predict_proba(classifier2)[0,0]))\n", "intent": "Is the result stated true? We can test this on a different review, which outputs is bad due to usage of negative words.\n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nmse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print(fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response))\n", "intent": "**b.** Now that you've added the categorical data, let's see how it works with a linear model!\n"}
{"snippet": "scores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n", "intent": "To evaluate the performance, we only need to run the following code snippet.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\nprint('R^2 score: %.2f' % r2_score(y_test, y_pred))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "y_hat2 = model_2.predict(X2)\npredict_plot(y2, y_hat2);\n", "intent": "Also add plots of model prediction against your feature variable and residuals against feature variable.\n"}
{"snippet": "model = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\nscore = cross_val_score(model, train_X[:100000], train_y[:100000], scoring='neg_mean_squared_error', cv=2)\nprint(\"average: {} std: {}\".format(score.mean(), score.std()*2))\n", "intent": "**baseline model** is neural network with one deep layer containing 12 units.\n"}
{"snippet": "model = KerasRegressor(build_fn=deep_model, epochs=10, batch_size=5, verbose=0)\nscore = cross_val_score(model, train_X[:100000], train_y[:100000], scoring='neg_mean_squared_error', cv=2)\nprint(\"average: {} std: {}\".format(score.mean(), score.std()*2))\n", "intent": "**deep model** is neural network with two deep layers, first with twelve units and second with six\n"}
{"snippet": "model = KerasRegressor(build_fn=wide_model, epochs=10, batch_size=5, verbose=0)\nscore = cross_val_score(model, train_X[:100000], train_y[:100000], scoring='neg_mean_squared_error', cv=2)\nprint(\"average: {} std: {}\".format(score.mean(), score.std()*2))\n", "intent": "**wide model** is neural network with one deep layer containing 20 cells\n"}
{"snippet": "def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "We first define a rmse evaluation function \n"}
{"snippet": "def mse_loss(a,b,x,y):\n    return mse(lin(a,b,x), y)\n", "intent": "Because we really hate typing, we'll create a function to calculate the predictions for us based on our predicted a and b\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        idx = style_layers[i]\n        src_gram = style_targets[i]\n        current_gram = gram_matrix(feats[idx])\n        style_loss += style_weights[i]*tf.reduce_sum((current_gram-src_gram)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "eval_summary(\"CV LR\",  model_lr_cv.predict(test_term_document_matrix_lr_cv), test_labels)\neval_summary(\"TF LR\",  model_lr_tf.predict(test_term_document_matrix_lr_tf), test_labels)\n", "intent": "Eval scores and confusion  matrix of Logistic regression\n"}
{"snippet": "eval_summary(\"CV SVC\",  clf1.predict(test_term_document_matrix_svc_cv), test_labels)\neval_summary(\"TF SVC\",  clf2.predict(test_term_document_matrix_svc_tf), test_labels)\n", "intent": "Eval scores and confusion matrix of SVClassifier\n"}
{"snippet": "eval_summary(\"LR TFIDF w/ W2V\", lrcv_w2v_model.predict(X_test), test_labels)\n", "intent": "**Eval Summary of model with trained features**\n"}
{"snippet": "y_pred = clf.predict(x)\naccuracy_score(y_pred, y)\n", "intent": "** (2) What is the training set accuaracy for 3-nearest neighbor algorithm?**  \n** Ans : 0.0% **\n"}
{"snippet": "X = X.drop('smoke', 1)\nscore = cross_val_score(rfc, X, Y, cv=10)\nprint(\"Cross Validation Scores:\\n\", score)\nprint(\"\\nCross Validation mean of: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n", "intent": "No change from the prior test, our 'smoke' variable is still least impactful, so let see what happens when we remove it.\n"}
{"snippet": "y = parsed_data.map(lambda x: x.label)\ny_pred = parsed_data.map(lambda x: model.predict(x.features))\n", "intent": "Any idea about the \"Cleaned shuffle\" messages?\nHint: narrow versus wide transformations.\n"}
{"snippet": "knn_ascore = []\nfor k,p in zip(k_value,range(3)):\n    accuracy = accuracy_score(test_labels,knn_predict_list[p]) \n    knn_ascore.append(accuracy*100) \n    print(\"For K=%d -> Accuracy = %.3f\" % (k,accuracy) ) \n", "intent": "> - Next, using the **accuracy_score()** function from *sklearn.metrics* we calculate the accuracy of the model.\n"}
{"snippet": "nb_accuracy = accuracy_score(test_labels,nb_prediction)\nprint('Naive Bayes Accuracy = %.3f' % nb_accuracy) \n", "intent": "> - Calculate Accuracy using **accuracy_score()** function from *sklearn.metrics*.\n"}
{"snippet": "print(classification_report(test_labels,nb_prediction)) \n", "intent": "> - Showing summary of the statistics.\n"}
{"snippet": "y_pred_knn = knn_estimator2.predict(X_test)\ny_prob_knn = knn_estimator2.predict_proba(X_test)[:,1]\n", "intent": "Only 3 LDA components are necessary for kNN to perform almost as well as logistic regression. \n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\nprint(dog_breed_predictions[1])\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "input_fun_test = tf.estimator.inputs.numpy_input_fn(x={\"X\": X_valid}, y=y_valid, shuffle=False)\nevalu = dnn_clf.evaluate(input_fn = input_fun_test)\n", "intent": "Evaluate and Predict on the validation data \n"}
{"snippet": "print('Predicting new values..')\ny_pred = model.predict(test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(test_labels, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n", "intent": "This reads as though its overfitting even though I am feeding it test data\n"}
{"snippet": "predictedQuality = lm.predict(wineDataInput)\npredictedQuality[0:10]\n", "intent": "Let's use these parameters to predict the outputs on our input set:\n"}
{"snippet": "predicted = lr2.predict(occupancyDataInput_test)\nprint(predicted)\nprobs = lr2.predict_proba(occupancyDataInput_test)\nprint(probs)\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "from sklearn import metrics\nprint(\"Accuracy: %f\", metrics.accuracy_score(occupancyDataOutput_test, predicted))\nprint(\"AUC: %f\", metrics.roc_auc_score(occupancyDataOutput_test, probs[:, 1]))\nprint(\"Classification confusion matrix:\")\nprint(metrics.confusion_matrix(occupancyDataOutput_test, predicted))\nprint(\"Classification report:\")\nprint(metrics.classification_report(occupancyDataOutput_test, predicted))\n", "intent": "The model is assigning a true whenever the value in the second column (probability of \"true\") is > 0.5\nLet us now see some evaluation metrics:\n"}
{"snippet": "print('Precision score = %1.3f' % precision_score(df_results['target'], predicted_outcome))\nprint('Recall score = %1.3f' % recall_score(df_results['target'], predicted_outcome))\nprint('F1 score = %1.3f' % f1_score(df_results['target'], predicted_outcome))\n", "intent": "Rather imbalanced! Approximately 83% of the labels are 0. Let's take a look at the other metrics more appropriate for this type of datasets:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\ndef rmse(true_values, predicted_values):\n    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n    return rmse\n", "intent": "I have used Root Mean Squared Error (RMSE) to measure the loss of the model and R2 Score to evaluate the performance of the model.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model2.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "y = df['tohn/hour']\nX = df.drop(['tohn/hour'],axis=1)\nX = preprocessing.scale(X)\nX, y = shuffle(X, y)\nencv = linear_model.ElasticNetCV(cv=10,max_iter=3000, n_alphas=10)\npredicted_encv = model_selection.cross_val_predict(\n    encv, X, y.ravel(), cv=20)\nscore_encv = model_selection.cross_val_score(encv,X, y.ravel(),\n                                         scoring='r2',cv=20)\n", "intent": "We can use additional features and more complex model, e.g. ElasticNet.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndef answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    y_pred = answer_seven()\n    return accuracy_score(y_test, y_pred)\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "fpr, tpr, thresholds=roc_curve(Y_test, logitreg.predict_proba(X_test[\"max_popularity\"].values.reshape(-1,1))[:,1])\nroc_auc = auc(fpr, tpr)\nroc_auc\n", "intent": "This baseline model seems pretty uninformative, given we can achive about 75% accuracy by just predicting all 1's for our response.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint('\\n classification report: \\n\\n', classification_report(yts1,yhat1,target_names=['class 0','class 1']))\nprint('\\n confusion matrix: \\n\\n', confusion_matrix(yts1,yhat1))\n", "intent": "Importing classification report and confusion matrix to check our model efficiency.\n"}
{"snippet": "print('\\n classification report: \\n\\n',classification_report(yts2,yhat2, target_names=['class 0','class 1']))\nprint('\\n confusion_matrix: \\n\\n', confusion_matrix(yts2,yhat2))\n", "intent": "Applying classification and confusion matrix to check the efficiency of our model.\n"}
{"snippet": "print('Classification report :\\n', classification_report(yts1,yhat_rf1,target_names=['Class 0','Class 1']))\nprint('Confusion matrix :\\n\\n', confusion_matrix(yts1,yhat_rf1))\n", "intent": "Analyzing our model efficiency through classification report and confusion matrix.\n"}
{"snippet": "y_pred = classifier.predict(npX_test)\nreversefactor = dict(zip(range(3),['poor', 'average', 'popular']))\ny_test = np.vectorize(reversefactor.get)(y_test)\ny_pred = np.vectorize(reversefactor.get)(y_pred)\ncnf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Species'], colnames=['Predicted Species'])\n", "intent": "Once the classifier is trained, we predict our test inputs and checkout the confusion matrix!\n"}
{"snippet": "xx,yy = np.meshgrid(np.linspace(0.0,10.0,201),np.linspace(0.0,10.0,201))\npred_bc = bc.predict(np.c_[xx.ravel(),yy.ravel()])\nZ_bc = pred_bc.reshape(np.shape(xx))\npred_rfc = rfc.predict(np.c_[xx.ravel(),yy.ravel()])\nZ_rfc = pred_rfc.reshape(np.shape(xx))\n", "intent": "<p>Mapping the parameters space and visualizing the result... and check the difference</p>\n"}
{"snippet": "xx,yy = np.meshgrid(np.linspace(0.0,10.0,201),np.linspace(0.0,10.0,201))\npred = dtr.predict(np.c_[xx.ravel(),yy.ravel()])\nZ = pred.reshape(np.shape(xx))\n", "intent": "<p>In order to visualize the classification result we are going to ask the predictino for a collectino of points mapping the features space.</p>\n"}
{"snippet": "xx,yy = np.meshgrid(np.linspace(0.0,10.0,301),np.linspace(0.0,10.0,301))\npredict = knn.predict(np.c_[xx.ravel(),yy.ravel()])\nZ = predict.reshape(np.shape(xx))\n", "intent": "<p>Next we can try to map the predictions by asking the prediction for several points in our features space :</p>\n"}
{"snippet": "xx,yy = np.meshgrid(np.linspace(-3.0,3.0,201),np.linspace(-2.0,2.0,201))\npred = lsvc.predict(np.c_[xx.ravel(),yy.ravel()])\nZ_lsvc = pred.reshape(np.shape(xx))\n", "intent": "<p>Mapping the parameters space and visualizing the result</p>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred = knn_clf.predict(X_train)\nknn_mse = mean_squared_error(y_train, y_pred)\nknn_rmse = np.sqrt(knn_mse)\nknn_rmse\n", "intent": "Predict and calculate RMSE\n"}
{"snippet": "def predict_cluster(test_data, test_np_matrix, kmeans_model):\n    value = pd.concat([test_data['ArtistID'], pd.Series(kmeans_model.predict(test_np_matrix))], axis=1)\n    value.columns = ['ArtistID', 'ClusterID']\n    return value\nkmeans_5_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_5)\nkmeans_25_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_25)\nkmeans_50_predicted = predict_cluster(user_art_mat_test, user_np_matrix_test, kmeans_50)\n", "intent": "**b.** For each artist in the test set, call **[predict](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "def f1_loss(y_true, y_pred):\n    TP = K.sum(y_true * y_pred)\n    soft_precision_positive = TP / (K.sum(y_pred) + 1e-7)\n    soft_recall_positive = TP / (K.sum(y_true) + 1e-7)\n    f1_loss_positive = 2 * soft_precision_positive * soft_recall_positive / (soft_precision_positive + soft_recall_positive + 1e-7)\n    TN = K.sum((1 - y_true) * (1 - y_pred))\n    soft_precision_negative = TN / (K.sum(1 - y_pred) + 1e-7)\n    soft_recall_negative = TN / (K.sum(1 - y_true) + 1e-7)\n    f1_loss_negative = 2 * soft_precision_negative * soft_recall_negative / (soft_precision_negative + soft_recall_negative + 1e-7)\n    return 1 - (0.99 * f1_loss_positive + 0.01 * f1_loss_negative)\n", "intent": "This loss was designed to directly maximize the f1-score.  \nUsing this loss did not give much improvement.\n"}
{"snippet": "cost = -tf.reduce_sum(Y_gt*tf.log(Y_pred))\nregularizer = (tf.nn.l2_loss(W3_FC1) + tf.nn.l2_loss(B3_FC1) + tf.nn.l2_loss(W4_FC2) + tf.nn.l2_loss(B4_FC2))\ncost += 5e-4 * regularizer\ntrain_op = tf.train.RMSPropOptimizer(LEARNING_RATE, 0.9).minimize(cost)\ncorrect_predict = tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y_gt, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_predict, 'float'))\npredict = tf.argmax(Y_pred, 1)\n", "intent": "We defined cross-entropy for the cost function with L2-regularization.\n"}
{"snippet": "cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=1)\n", "intent": "To compute the score method of an estimator, the sklearn exposes a helper function:\n"}
{"snippet": "lm.predict(60)\n", "intent": "There's a shortcut for this, which will come in handy when we add variables\n"}
{"snippet": "proba = nn.predict_proba(X_test)[:,1]\nthreshold = np.where(proba>0.2,1,0)\ncross = pd.crosstab(threshold, y_test)\nprint(cross)\nacc = (cross[0][0]+cross[1][1])/len(y_hat)\nsensitive = cross[1][1]/(cross[1][0]+cross[1][1])\nprint('Accuracy is: ', acc)\nprint('Sensitivity is: ', sensitive)\n", "intent": "The model is pretty accurate, but the missed cases of cancer should be improved as this carries a high cost.\n"}
{"snippet": "inceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(inceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import numpy as np\nsms_review_array= np.array([\"Chk in ur belovd ms dict\"])\nsms_review_vector= vectorizer.transform(sms_review_array)\nprint clf.predict(sms_review_vector)\n", "intent": "The output should be '1' which indicates that the SMS is spam (or) '0' which indicates that the SMS is not a spam.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)\n", "intent": "__Confusion Matrix__\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))\n", "intent": "__Precision and Recal__\n"}
{"snippet": "results = []\nnames = []\nfor name,model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    cv_result = model_selection.cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n    kfold = model_selection.KFold(n_splits=10)\n    names.append(name)\n    results.append(cv_result)\nfor i in range(len(names)):\n    print(names[i],results[i].mean(),\"(\",results[i].std(),\")\")\n", "intent": "Below are the accuracy of each model-\n"}
{"snippet": "x_unuse_delay = x_unuse[y_unuse_class == 1]\ny_unuse_delay = y_unuse_delay[y_unuse_class == 1]\nr2_ols_sig_predictors_unuse = r2_score(\n    y_unuse_delay, ols_model_sig_predictors.predict(sm.add_constant(x_unuse_delay[ols_all_sig_predictors.index[1:]])))\nprint(\"OLS (significant predictors) Unused Data R^2: %.5f\" % r2_ols_sig_predictors_unuse)\n", "intent": "The reduction of predictors gives a slightly worse training score but improve on the testing score slightly, which means the overfitting is reduced.\n"}
{"snippet": "x_unuse_delay_forward = x_unuse_delay[best_predictors_forward[0]]\nr2_ols_forward_unuse = r2_score(\n    y_unuse_delay, ols_model_forward.predict(sm.add_constant(x_unuse_delay_forward)))\nprint(\"OLS Unused Data R^2: %.5f (Forward Selection) vs %.5f (Method 1: Significant Predictors)\" \n      % (r2_ols_forward_unuse, r2_ols_sig_predictors_unuse))\n", "intent": "The performance of method 3 (Forward selection) is slightly better than method 1 (significant predictors using p-values)\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": "Accuracy is computed as $\\frac{TP + TN}{TP + TN + FP + FN}$ -- in other words, what proportion of the data has been correctly classified?\n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "Below is a scoring functin that will determine the accuracy of our model.\n"}
{"snippet": "model.evaluate(ds.X[ds.idx_test], ds.Y_class[ds.idx_test])\n", "intent": "Well, can't complain about those results.\n"}
{"snippet": "folds = 10\nseed = 10\nnp.random.seed(seed)\nmodel = KerasClassifier(build_fn = buildModel, epochs = 200, batch_size = 5, verbose = 0)\nkFold = KFold(n_splits = folds, random_state = seed)\nresults = cross_val_score(model, xTrain, yTrain, cv = kFold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n", "intent": "Now we can write the rest of the code:\n"}
{"snippet": "idx = 154\nexp = explainer.explain_instance(X_samples[1][idx].decode('utf-8'), c.predict_proba, num_features=6)\nprint('Document id: %d' % idx)\nprint (X_samples[1][idx])\nprint('Probability(True) =', c.predict_proba([X_samples[1][idx]])[0,1])\nprint('True class: %s' % class_names[y_samples[0][idx]])\n", "intent": "We then generate an explanation with at most 6 features for an arbitrary document in the test set.\n"}
{"snippet": "results = m.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))\n", "intent": "After the model is trained, we can evaluate how good our model is at predicting the labels of the test data:\n"}
{"snippet": "y_pred_valid = model.predict(x2)\nprint_model_performance(y_pred_valid, y_valid.values)\n", "intent": "It's important to measure your performance on data that the model hasn't seen. This is the purpose of the validation set. \n"}
{"snippet": "df_test['Prob'] = DT.predict_proba(X_test)[:,1]\nthresh = 0.08\ny_pred = df_test['Prob'].apply(lambda e: 1 if e>thresh else 0)\nprofit_calculator(confusion_matrix(df_test['Success'], y_pred))\n", "intent": "threshold value 0.08 maximizes profit in training set so we use it for test set to find the profit\n"}
{"snippet": "residual_sum= (np.sum((bos.PRICE - lm.predict(X)) ** 2))\nresidual_sum\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "prediction = cl.predict(X_test)\n", "intent": "**Creating predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "prediction1 = f.predict(X_test)\n", "intent": "** Predicting the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, prediction1))\n", "intent": "**creating a classification report from the results.**\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(model, x, y, scoring='roc_auc')\nprint('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))\n", "intent": "Try modifying the inputs to the `CountVectorizer` or `TfidfVectorizer` models and the `RandomForestClassifier` model\n"}
{"snippet": "samples = [\n    \"She will travel at the speed of light in that rocket\",\n]\ntransformed_samples = vectorizer.transform(samples)\nclf.predict(transformed_samples)\n", "intent": "Now we can predict the categories of the held out texts and assess how well our classifier did.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "Or you can do it the `scikit-learn` way.\n"}
{"snippet": "samples = [\n    \"She will travel at the speed of light in that rocket\",\n]\ntransformed_samples = vectorizer.transform(samples)\nclf.predict(transformed_samples)\n", "intent": "Now we can predit the categories of the held out texts and assess how well our classifier did.\n"}
{"snippet": "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])\n", "intent": "Again plot some sample images including the predictions.\n"}
{"snippet": "prediction=logmodel.predict(x_test) \n", "intent": "here,we see all the detail of logistic regression\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,prediction))\n", "intent": "till now we do import the logisticregression,create the model,fit the model and predict the model\n"}
{"snippet": "print(accuracy_score(y_test,prediction)*100) \n", "intent": "we got accuracy 0.78,it is good not bad\n"}
{"snippet": "predictions = booster.predict(dtest)\nfor i in range(len(predictions)):\n    if predictions[i] > 0.5:\n        predictions[i] = 1\n    else:\n        predictions[i] = 0\npredictions\n", "intent": "Unfortunately tuning the hyperparamters does not seem to improve the results (in the contrary).\nTime for the last step: Predicting the kaggle data.\n"}
{"snippet": "print(classification_report(test_target, predicted))\nprint(confusion_matrix(predicted, test_target))\n", "intent": "The results show that the model performs very well with 94% overall performance.   \n"}
{"snippet": "knn.predict_proba(X_test)\n", "intent": "Can you explain why only certain values appear?\n"}
{"snippet": "forecast_set = classifier.predict(X_lately)\nprint(forecast_set, confidence, forecast_days)\n", "intent": "Prediction of stock values fot the next 34 days.\n"}
{"snippet": "cv = KFold(n_splits=5, shuffle=False, random_state=33)\nscores = cross_val_score(model, X, y, cv=cv)\nprint(\"Scores in every iteration\", scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "This is alternative to splitting the dataset into train and test. It will run k times slower than the other method, but it will be more accurate.\n"}
{"snippet": "preds = dtc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "print(classification_report(y_test, rfc_preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(data[\"Cluster\"], kmeans.labels_))\nprint(classification_report(data[\"Cluster\"], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, preds))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$\n"}
{"snippet": "silhouette_score(X, labels, metric = 'euclidean')\n", "intent": "Compute the Silhouette Score.\n"}
{"snippet": "print 'Accuracy score in skLearn', accuracy_score(y_test, y_pred).round(4)\n", "intent": "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$\n"}
{"snippet": "print 'Precision score in skLearn', precision_score(y_test, y_pred, average = 'macro').round(4)\n", "intent": "$$Precision_{Negative} = \\frac{TN}{TN+FN}$$\n"}
{"snippet": "print 'Recall score in skLearn', recall_score(y_test, y_pred, average = 'macro').round(4)\n", "intent": "$$Recall_{Negative} = \\frac{TN}{TN+FP}$$\n"}
{"snippet": "print 'F1 score in skLearn', f1_score(y_test, y_pred, average = 'macro').round(3)\n", "intent": "$$F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\n"}
{"snippet": "folds = 3\nscores = cross_val_score(knn, X_scaled, y, cv = folds)\nprint 'Mean accuracy is', scores.mean().round(4)\n", "intent": "Cross validation is used to evaluate the accuracy of the model.\n"}
{"snippet": "print(model.predict([10,1,4,0,0,0]))\nprint(model.predict([10,0,4,0,0,0]))\n", "intent": "And then we can make our prediction:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nrf = XGBClassifier()\nnp.mean(cross_val_score(rf, dataset.data, dataset.target))\n", "intent": "Now let's fit simple XGBoost model with default parameters and see how it scores on the dataset\n"}
{"snippet": "scores = cross_val_score(clf, X_train, Y_train, cv=10)\nprint(\"Cross validation scores are {}\".format(scores))\nprint(\"\\nMean is %f and Standard deviation is %f\" % (scores.mean(), scores.std()))\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "def f_prime(s):\n    return f(s) + predict_error(s)\n", "intent": "Using the `predict_error` function $f'(s)$ can now be created,.\n"}
{"snippet": "def predict(img):\n    threshold = 5.0\n    if np.sum(green_mask(img)) > threshold:\n        return 1\n    elif np.sum(orange_mask(img)) > threshold:\n        return 2\n    else:\n        return 0\n", "intent": "If the there a certain number of pixels in the image that are within the color range, there is assumed to be a piece in that square.\n"}
{"snippet": "pred = [predict(img) for img in validation_imgs]\na = np.sum(pred == d.y_val) / float(d.y_val.shape[0])\nprint \"Accuracy: {}\".format(a)\n", "intent": "The validation data is used to test the accuracy of the prediction model.\n"}
{"snippet": "y = train[\"ViolentCrimesPerPop:\"]\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train, y, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)\n", "intent": "We will be using scikits inbuilt regression modules. We will try to tune the parameters based on RMSE scores.\n"}
{"snippet": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "Lets make our prediction off this alpha value:\n"}
{"snippet": "y = train[\"ViolentCrimesPerPop:\"]\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n", "intent": "We will be using scikits inbuilt regression modules. We will try to tune the parameters based on RMSE scores.\n"}
{"snippet": "def predict(theta_hat, X):\n    theta_hat = np.matrix(theta_hat).T\n    X = np.matrix(X)\n    z = X * theta_hat\n    y_hat = z > 0\n    return y_hat.astype(int)\n", "intent": "Again, we re-use some lines of code from ex 03.\n"}
{"snippet": "def rmse(y_true, y_pred):\n    return(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\nfrom sklearn.metrics import make_scorer\nscorer = make_scorer(rmse)\n", "intent": "Now let's build some models! But first let's define some helper functions:\n"}
{"snippet": "some_data = housing.iloc[:5] \nsome_labels = housing[\"median_house_value\"].iloc[:5] \nsome_prepared_data = full_pipeline.transform(some_data) \nlin_reg.predict(some_prepared_data)\n", "intent": "Let us evaluate the model on some of the observations in the training data itself\n"}
{"snippet": "y_test_predict=prepare_and_train.predict(X_test)\n", "intent": "Let us evaluate our model on the test data\n"}
{"snippet": "def validate(y_true, y_pred):\n    return log_loss(y_true, y_pred)\n", "intent": "Indeed, perfect, lets put it into a function.\n"}
{"snippet": "model.resampling_strategy = 'holdout'\nmodel.refit(x_train, y_train)\nvalidate(y_train, model.predict_proba(x_train))\n", "intent": "Apparently, prediction is not supported when doing CV.\n"}
{"snippet": "print(np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(X))\nprint(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))t\n", "intent": "Try some other values for yourself. Does the prediction make sense?\nOK, that works fine. Now let's see how good our classifier is on our test set.\n"}
{"snippet": "linreg.predict(X_test_scaled)\n", "intent": "Feed in the test data to make out \"out-of-sample\" predictions.\n"}
{"snippet": "logistic.predict(X_test_scaled)\n", "intent": "Observe the predictions are now True/False values\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), \n           rmse(m.predict(X_valid), y_valid),\n           m.score(X_train, y_train), \n           m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): \n        res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "def rmse(x, y):\n    return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    r = [rmse(m.predict(X_train),y_train),\n         rmse(m.predict(X_valid), y_valid),\n         m.score(X_train, y_train),\n         m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): \n        res.append(m.oob_score_)\n    print(r)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\n", "intent": "- Now let's check that the model performs well on the test set, too:\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n", "intent": "- Encode and decode some digits. Note that we take them from the test set.\n"}
{"snippet": "sum(rf.predict(X_testing)[Y_testing>=1]>=0.5)/len(Y_testing[Y_testing>=1]), sum(rf.predict(X_testing[Y_testing<1])<0.5)/len(Y_testing[Y_testing<1])\n", "intent": "** In the testing set, the prediction accuracy of users who make purchases is about 82%, and of users who do not make purchases is about 79%. **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(Y_test, rfc.predict(X_test))\n", "intent": "<b>Confusion Matrix</b>\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint \"AUC Score:\", f1_score(Y, logreg2.predict(X))\n", "intent": "<b>AUC and ROC curve</b>\n"}
{"snippet": "y_tr_pred = regr.predict(Xtrain)\nRSS_tr = np.mean((y_tr_pred-ytrain)**2)/(np.std(ytrain)**2)\nRsq_tr = 1-RSS_tr\nprint(\"RSS per sample = {0:f}\".format(RSS_tr))\nprint(\"R^2 =            {0:f}\".format(Rsq_tr))\n", "intent": "Rsq is 0.89  near to 0.90    Both nice fit \n"}
{"snippet": "y_tr_pred = regr.predict(Xtrain)\n", "intent": "Plot the predicted and actual current `I2` over time on the same plot.  Create a legend for the plot.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    cnt = 0\n    for i in style_layers:\n        gram_i = gram_matrix(feats[i],normalize = True)\n        style_loss += torch.sum((gram_i-style_targets[cnt])**2)*style_weights[cnt]\n        cnt += 1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "validation_data = create_dataset(10)\nvalidation_data['predicted'] = np.array(clf.predict(validation_data[['a', 'b', 'c']].values))\nvalidation_data['diff'] = validation_data['predicted'] - validation_data['f']\nvalidation_data\n", "intent": "We have a very *acceptable* score ($99.888\\% $ accuracy score), so let's find out how it's behaving with a small number of new samples.\n"}
{"snippet": "print('Train accuracy: ', accuracy_score(y_train, np.zeros(len(y_train))))\nprint('Trest accuracy: ', accuracy_score(y_test, np.zeros(len(y_test))))\n", "intent": "Compute the accuracy if all the predicted values where the most common class (0, 'NO')\n"}
{"snippet": "mse = mean_squared_error(y_test, clf.predict(X_test))\nprint \"MSE: \", mse\nprint \"RMSE: \", sqrt(mse)\n", "intent": "Lower values indicate a better fit.\n"}
{"snippet": "data2016 = [model.predict([[homeTeamName2016[i]], [normalizedHomeTeamStats2016[i]], [visitingTeamName2016[i]], [normalizedVisitingTeamStats2016[i]]]) for i in range(len(homeTeamStats2016))]\n", "intent": "Running the same analysis on the 2016 season.\n"}
{"snippet": "print('the f1 score using Multinomial Naive Bayes with TF features is %0.3f' % f1_score(ytf_pred,ytf_test))\nprint('the f1 score using Gaussian Naive Bayes with TFIDF features is %0.3f' % f1_score(ytfidf_pred,ytfidf_test))\nprint('the f1 score using Logistic Regression with Naive Bayes features is %0.3f' % f1_score(y_nbtfidf_pred,ytfidf_test))\n", "intent": "Let's compare the f1 scores of the three models:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n", "intent": "Let's check what the hell is going on with k-fold cross validation: (scikit learn works with utility functions, not losses. Thus, negative)\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Now we can see that the Decision Tree was badly overfitting. \n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100 * np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions) == np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "mod0_train_probas = mod0.predict_proba(X_train)\nmod0_test_probas = mod0.predict_proba(X_test)\n", "intent": "For that we need to predict probabilities...\n"}
{"snippet": "def predict(input):\n    result = feed_forward(network, input)[-1]\n    return [round(n,3) for n in result]\npredict(inputs[7])\n", "intent": "It works well on the training set, obviously:\n"}
{"snippet": "predict([0,1,1,1,0,  \n         0,0,0,1,1,  \n         0,0,1,1,0,  \n         0,0,0,1,1,  \n         0,1,1,1,0]) \n", "intent": "We can also apply our neural network to differently drawn digits, like the author's stylized 3:\n"}
{"snippet": "predict([0,1,1,1,0,  \n         1,0,0,1,1,  \n         0,1,1,1,0,  \n         1,0,0,1,1,  \n         0,1,1,1,0]) \n", "intent": "The network still thinks it looks like a 3, whereas the author's stylized 8 gets votes for being a 5, an 8, and a 9:\n"}
{"snippet": "results_train = model.evaluate(train_final, label_train_final)\n", "intent": "Now, you can use the test set to make label predictions\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))\n", "intent": "The following model is definitely an improvement:\n"}
{"snippet": "scores = cross_val_score(pipeline,  \n                         msg_train,  \n                         class_train,  \n                         cv=10,  \n                         scoring='accuracy'\n                         )\nprint scores\n", "intent": "Use cross-validation to check the stability of the training data\n"}
{"snippet": "predictions = pl.predict(X)\naccuracy = (predictions == y).sum()/len(y)\nprint(accuracy)\n", "intent": "We obtain an accuracy of 100% on the training set and a validation accuracy of 92%.  Overall, the accuracy on X is :\n"}
{"snippet": "print(metrics.classification_report(y_test, nb_predict_test,labels=[1,0]))\n", "intent": "Classification report\n"}
{"snippet": "print(test_images.shape) \ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc*100.0, '%')\n", "intent": "To see if our model is any good, we need to evaluate it on the test images. This will tell us how well our model generalizes to other MNIST data\n"}
{"snippet": "def generate_text(model, length, vocab_size, ix_to_char):\n    ix = [np.random.randint(vocab_size)]\n    y_char = [ix_to_char[ix[-1]]]\n    X = np.zeros((1, length, vocab_size))\n    for i in range(length):\n        X[0, i, :][ix[-1]] = 1\n        print ix_to_char[ix[-1]],\n        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n        y_char.append(ix_to_char[ix[-1]])\n    return ('').join(y_char)\n", "intent": "Before we go further, a function for generating text given the model\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(G - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "sorted_labels = sorted(\n    labels, \n    key=lambda name: (name[1:], name[0])\n)\nprint(sorted_labels)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "Inspect per-class results in more detail:\n"}
{"snippet": "for name, model in fitted_models.items():\n    print(name)\n    print(\"-----------\")\n    pred = model.predict(X_test)\n    print( 'R^2:', r2_score(y_test, pred ))\n    print( 'MAE:', mean_absolute_error(y_test, pred))\n    print()\n", "intent": "Each model's scores on the test set\n"}
{"snippet": "Inception_predictions = [np.argmax(inc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xgb_cv_full = cross_val_score(XGBoostClassifier(), features_all_columns, target_all_columns, cv=4,\n                              scoring=\"roc_auc\")\nprint(xgb_cv_full.mean(), xgb_cv_full.std())\n", "intent": "Have any ideas why normalization went wrong?\n"}
{"snippet": "def predict_author(text, vocab,tokenizer, model):\n    tokens = clean_text(text)\n    tokens = [w for w in tokens if w in vocab]\n    line = ' '.join(tokens)\n    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n    yhat = model.predict(encoded, verbose=0)\n    return yhat\n", "intent": "Now, we are ready to predict on test data and submit the results\n"}
{"snippet": "from sklearn import metrics\nprint \"Silhoutte Coefficient: %0.3f\" % metrics.silhouette_score(X, db.labels_)\n", "intent": "<p style=\"font-family:courier;\">5. We show the silhoutte coefficient</p>\n"}
{"snippet": "def build_loss(logits, targets, lstm_size, num_classes):\n    y_one_hot = tf.one_hot(targets,num_classes)\n    y_reshaped = tf.reshape(y_one_hot,logits.get_shape())\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y_reshaped)\n    loss=tf.reduce_mean(loss)\n    return loss\n", "intent": "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. \n"}
{"snippet": "y_pred_base = XGBoost_base.best_estimator_.predict(X_test)\n", "intent": "It achieves high roc_auc score with default parameters, let's now calculate score as stated in the description of the dataset\n"}
{"snippet": "y_pred_opt = XGBoost_opt.best_estimator_.predict(X_test)\n", "intent": "Now let's investigate test score for optimized parameters for XGBoost Classifier:\n"}
{"snippet": "sms_dl.plot_acc()\nsms_dl.plot_loss()\n", "intent": "+ Plotting accuracy curves and learning curves for training and validation stage with respect to accuracy and loss.\n"}
{"snippet": "expected = digits.target[n_samples / 2:]\npredicted = classifier.predict(data[n_samples / 2:])\n", "intent": "Predict the value of the digit on the second half\n"}
{"snippet": "print(\"Classification report for classifier %s:\\n%s\\n\"\n      % (classifier, metrics.classification_report(expected, predicted)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n", "intent": "Now lets print the results\n"}
{"snippet": "score, acc = model.evaluate(Xts, test_y)\nprint(\"Accuracy is {}%\".format(round(acc*100, 2)))\n", "intent": "We test the model using 10% of our dataset:\n"}
{"snippet": "from sklearn import metrics \nprint metrics.accuracy_score(dtc.predict(X_train), y_train)\nprint metrics.accuracy_score(dtc.predict(X_test), y_test)\n", "intent": "6\\. Using the classifier built in 2.3, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?\n"}
{"snippet": "print(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "Calculate MSE using scikit-learn Mean Squared Error (MSE) is the mean of the squared errors:\n"}
{"snippet": "x_decoded=vae.predict(test_data_doubles_plus[0:sample_size],batch_size=batch_size)\ndigit_wt = x_decoded[0].reshape(digit_size,sequence_size)\ndigit_wt = normalize(digit_wt,axis=0, norm='l1')\nmut_sample=100\ndigit_p= x_decoded[mut_sample].reshape(digit_size,sequence_size)\ndigit_p = normalize(digit_p,axis=0, norm='l1')\n", "intent": "We repeat the same analysis for double mutants. \n"}
{"snippet": "incep_predictions = [np.argmax(incep_model.predict(np.expand_dims(feature, axis=0))) for feature in test_incep]\ntest_accuracy = 100*np.sum(np.array(incep_predictions)==np.argmax(test_targets, axis=1))/len(incep_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted = model.predict(test[[\"fat_100g\"]])\n", "intent": "*Predict nutrition score using the test set*\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    Ls = tf.zeros((1,))\n    for i, sl in enumerate(style_layers):\n        Al = style_targets[i]\n        Gl = gram_matrix(feats[sl])\n        Ls += style_weights[i] * tf.reduce_sum(tf.square(Al - Gl))\n    return Ls\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step)) \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) \n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')\n", "intent": "The following function will be used to plot the decision function of a\nclassifier given some data.\n"}
{"snippet": "examples = ['Free Viagra now!!!', \"Hi Amit,do u want to play cricket tomorrow?\"]\nexample_counts = vectorizer.transform(examples)\npredictions = classifier.predict(example_counts)\npredictions\n", "intent": "Few Testing Mails which i can test\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print(\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return f1_score(target.values, y_pred, pos_label=1)\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Built from student_intervention project and modified as needed for this project\n"}
{"snippet": "predictions = clf_nb.predict(counts_test)\n", "intent": "**We have 7643 documents in our test set, each has 355860 features - exactly the same features we set while processing the training set of course.**\n"}
{"snippet": "predict(image_path='./images/parrot_cropped1.jpg')\n", "intent": "We can then use the VGG16 model on a picture of a parrot which is classified as a macaw (a parrot species) with a fairly high score of 79%.\n"}
{"snippet": "ResNet50_predictions= [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)== np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test Accuracy: %.4f%%' %test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_predicted = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y = model.predict(x)\nprint np.argmax(y, axis=1)\n", "intent": "In this notebook, it's not really trained to predict\n"}
{"snippet": "X_new = [[3, 5, 8, 2], [5, 4, 3, 3]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\"\n- Can predict for multiple observations at once\n"}
{"snippet": "clf2 = pickle.load(open(ROBOT_POSE_CLF))\nprint all_data[-1]\nclf2.predict(all_data[-1]), all_target[-1]\n", "intent": "Then, in the application we can load the trained classifier again.\n"}
{"snippet": "predictions = model.predict(test_features_df)\n", "intent": "Get predictions for test dataset\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(predictions, test_labels))\n", "intent": "Calculate model mean squared error for test dataset\n"}
{"snippet": "best_if_test = np.array([0 if x == 1 else 1 for x in best_if.predict(X_test_if)])\nif_test_score = accuracy_score(y_test[:,1], best_if_test)\nprint('Accuracy score of the best model on test set: {:f}'.format(if_test_score))\n", "intent": "Finally let's see the result of the best model on test set. The result is lower than on validation set, but the difference is not very significant.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    i = 0\n    for idx in style_layers:\n        cgram = gram_matrix(feats[idx])\n        tgram = style_targets[i]\n        wt = style_weights[i]\n        style_loss += wt * tf.reduce_sum(tf.squared_difference(cgram,tgram))\n        i +=1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    style_loss = 0\n    for i in np.arange(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(feats[style_layers[i]]) - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = linreg.predict(X_test)\n", "intent": "$$y = 2.88 + 0.0466 \\times TV + 0.179 \\times Radio + 0.00345 \\times Newspaper$$\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "** Now taking that grid model and creating some predictions using the test set and create classification reports and confusion matrices for them. **\n"}
{"snippet": "x=np.array([18])\ninc=lrsklearn.predict(x.reshape(1,1))\nprint('The predicted income for an individual with 18 years of education is: $' + str(inc))\n", "intent": "3. Predicting Income for a new individual with Years of Education x=18\n------------------------\n"}
{"snippet": "y_pred = grid.predict(X_test)\n", "intent": "Assess the performance\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"lotsofcarspar.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "model.evaluate(features, hot_labels, batch_size=1000)\n", "intent": "Just test on the training set, shouldn't make a difference.\n"}
{"snippet": "print(classification_report(QOFL_y_test, QOFL_predicted_test_y))\n", "intent": "We see in general that our training F1-score is about 75% after training on 80% of the QOFL dataframe\n"}
{"snippet": "print(classification_report(EH_y_test, EH_predicted_test_y))\n", "intent": "We see in general that our training F1-Score is about 86% after training on 80% of the QOFL dataframe\n"}
{"snippet": "print(classification_report(FP_y_test, FP_predicted_test_y))\n", "intent": "We see in general that our training F1-Score is about 87%, given we are training on 80% of the QOFL dataframe\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting n_estimators to 100\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting minimum samples per leaf to 5\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting minimum samples per leaf to 10\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting minimum samples per leaf to 3\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting minimum samples per leaf to 4\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting max depth to 10\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting max depth to 20\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting max depth to 40\n"}
{"snippet": "cv_score(random_forest_model, X_train, y_train, metric=mean_squared_error)\n", "intent": "setting max depth to 50\n"}
{"snippet": "Y_pred = regr.predict(X_test)\nrmse = sqrt(metrics.mean_squared_error(Y_test, Y_pred))\nrmse \n", "intent": "Next use our test set to see how well the model did by computing the root mean square error.\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2))/len(lm.predict(X)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss  = tf.zeros([1])\n    for i,l in enumerate(style_layers):\n        layer = feats[l]\n        style_loss = style_loss + style_weights[i] * tf.square(tf.norm(style_targets[i] - gram_matrix(layer)))\n    return style_loss[0]\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "x_test = np.array(['feeling is super happy with a new ball'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))\n", "intent": "We now have a working Linear Regression Model, Let's try it out on few instances from the training set,\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Let's compute the same cross-validation scores for the Linear Regression model,\n"}
{"snippet": "rf_proba = rf.predict_proba(CV.drop([\"id\", \"target\"], axis=1))\nrf_proba_0 = rf_proba[:, 1].reshape(rf_proba.shape[0],1)\n", "intent": "<img src= \"ps_images/feature_importance_barplot.png\">\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test,prediction)\n", "intent": "Find the R_square value\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 =r2_score(y_test,prediction)\nmodel.append(\"Multiple Linear\")\nrmse.append(r)\nr_sqr.append(r2)\n", "intent": "Find the R_square value\n"}
{"snippet": "data['mean'] = data['cost'].mean()\ny_test = data['cost'].values\nprediction_mean = data['mean'].values\nbaseline = np.sqrt(mean_squared_error(y_test, prediction_mean))\nprint(baseline)\n", "intent": "<a id='baseline'></a>\n"}
{"snippet": "y_pred = base_pipeline.predict(X_dev_str)\nprint(\"Precision of the base model is\", precision_score(y_dev, y_pred, average='macro'))\nprint(\"Recall of the base model is\", recall_score(y_dev, y_pred, average='macro'))\nprint(\"Recall of the base model is\", f1_score(y_dev, y_pred, average='macro'))\n", "intent": "Precision and recall are better measures to use here with high class imbalance.\n"}
{"snippet": "y_pred = model2_pipeline.predict(X_dev)\nprint(\"Precision of the model is\", precision_score(y_dev, y_pred, average='macro'))\nprint(\"Recall of the model is\", recall_score(y_dev, y_pred, average='macro'))\n", "intent": "Precision and recall are better measures to use here with high class imbalance.\n"}
{"snippet": "y_pred = grid_search.predict(X_dev_str)\nprint(\"Precision of the model is\", precision_score(y_dev, y_pred, average='macro'))\nprint(\"Recall of the model is\", recall_score(y_dev, y_pred, average='macro'))\nprint(\"F1 score of the model is\", f1_score(y_dev, y_pred, average='macro'))\n", "intent": "Precision and recall are better measures to use here with high class imbalance.\n"}
{"snippet": "y_pred = grid_search.predict(X_dev_str)\nprint(\"Precision of the base model is\", precision_score(y_dev, y_pred, average='macro'))\nprint(\"Recall of the base model is\", recall_score(y_dev, y_pred, average='macro'))\nprint(\"F1 of the base model is\", f1_score(y_dev, y_pred, average='macro'))\n", "intent": "Precision and recall are better measures to use here with high class imbalance.\n"}
{"snippet": "predictions = list(classifier.predict(X_test, as_iterable=True))\nscore = metrics.accuracy_score(y_test, predictions)\nprint('Accuracy: {0:f}'.format(score))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "metrics.accuracy_score(y_test, knn_uniform.predict(X_test))\n", "intent": "How well did we classify wines in our test data as being red or white?\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df_data['Cluster'],cluster_pair.labels_))\nprint(classification_report(df_data['Cluster'],cluster_pair.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "def show_accuracy(model, test_model, name=''):\n    predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_model]\n    accuracy = 100 * np.sum(np.array(predictions) == np.argmax(test_targets, axis=1))/len(predictions)\n    print(name + ' Accuracy: ', accuracy)\nshow_accuracy(model_ResNet50, test_ResNet50, 'ResNet50 SGD')\nshow_accuracy(model_Xception, test_Xception, 'Xception SGD')\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "rfpredict = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "test_set = np.loadtxt('out.dta')\nfor k in [3,4,5,6,7]:\n    phi_train = transform(train_set[:, [0,1]], k)\n    phi_suedo = calc_suedo(phi_train)\n    phi_w = lin_reg(phi_suedo, train_set[:,2])\n    phi_test = transform(test_set[:,[0,1]], k)\n    err_test = calc_error(phi_w, phi_test, test_set[:,2])\n    print 'Test classification error for k = %i: '%k , err_test\n", "intent": "Answer Choice: **E**, $k=7$<br>\nReasoning: see code.\n"}
{"snippet": "data = np.loadtxt('in.dta')\ntrain_set = data[25:]\nvalid_set = data[0:25]\nfor k in [3,4,5,6,7]:\n    phi_train = transform(train_set[:, [0,1]], k)\n    phi_suedo = calc_suedo(phi_train)\n    phi_w = lin_reg(phi_suedo, train_set[:,2])\n    phi_valid = transform(valid_set[:,[0,1]], k)\n    err_valid = calc_error(phi_w, phi_valid, valid_set[:,2])\n    print 'Validation classification error for k = %i: '%k , err_valid \n", "intent": "Answer Choice: **D**, $k=6$<br>\nReasoning: see code.\n"}
{"snippet": "test_set = np.loadtxt('out.dta')\nfor k in [3,4,5,6,7]:\n    phi_train = transform(train_set[:, [0,1]], k)\n    phi_suedo = calc_suedo(phi_train)\n    phi_w = lin_reg(phi_suedo, train_set[:,2])\n    phi_test = transform(test_set[:,[0,1]], k)\n    err_test = calc_error(phi_w, phi_test, test_set[:,2])\n    print 'Test classification error for k = %i: '%k , err_test\n", "intent": "Answer Choice: **D**, $k=6$<br>\nReasoning: see code.\n"}
{"snippet": "arch_predictions = [np.argmax(arch_model.predict(np.expand_dims(feature, axis=0))) for feature in test_bottleneck]\ntest_accuracy = 100*np.sum(np.array(arch_predictions)==np.argmax(test_targets, axis=1))/len(arch_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"maxresdefault.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "accuracy = accuracy_score(Y_test, bestrfc.predict(X_test))\nprint accuracy\n", "intent": "Calculating the accuracy of the model\n"}
{"snippet": "print classification_report(Y_test, bestrfc.predict(X_test))\n", "intent": "Printing the Classification Report\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(bestrfc, df, Y, cv=10)\nprint scores\n", "intent": "Implementing K-Fold Cross Validation\n"}
{"snippet": "def mean_absolute_err(y, yhat):\n    return 100*np.mean(np.abs((y[1:]-yhat)/yhat))\ndef mean_forecast_err(y,yhat):\n    return np.mean(y[1:]-yhat)\nall_predictions = ar_results.predict(dynamic=False)\nprint('Forecast Error is:', mean_forecast_err(assaults, all_predictions))\nprint('Mean Abs % Error is:' ,mean_absolute_err(assaults, all_predictions))\n", "intent": "Now that we know errors occur at the extreme values or rapid changes in assaults we can see how good of a fit we are doing over-all.\n"}
{"snippet": "predicted_m2_3 = m2_3.predict(X).ravel()\nactual = df_m2_3['pty_crime_log']\nresidual_m2_3 = actual - predicted\n", "intent": "Re-test for heteroscedascity.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "*** Using the optimized alpha we were able to remove overfitting issue in training sample. So this is much better classifier. ***\n"}
{"snippet": "with tf.name_scope('loss'):\n    loss = tf.reduce_sum(tf.square(predictions - Y)) + 0.01 * tf.nn.l2_loss(w)\n", "intent": "The cost function is the sum of squares of differences between predictions and true outputs. A regularization term is added to this value.\n"}
{"snippet": "probas = pd.Series(lr.predict_proba(encodings_)[:,1])\nargs = probas[probas < 0.25].sort_values().index\nargs = [arg for arg in args]\ndisplay(montagify([np.array(arrays_rescaled_[arg]) for arg in args],\n    (128,128), \n    tiles=(9,9))\n)\ndisplay(probas.ix[args].sort_values())\n", "intent": "Again, roundish jolly old fat women is what I don't like but most poeple wouldn't either so I'll fix general attraction on the next run\n"}
{"snippet": "\"Rsquare on training dataset: {0}; Rsquare on testing dataset: {1}.\".format(\n    skl.metrics.r2_score(Y_train.values, Y_train.mean() * np.ones_like(Y_train)),\n    skl.metrics.r2_score(Y_test, Y_test.mean() * np.ones_like(Y_test))\n    )\n", "intent": "Let's see its performance on training and testing set:\n"}
{"snippet": "def RocCurve(predprobs, actual_y):\n    pos_probs = []\n    for i in predprobs:\n        pos_probs.append(i[1])\n    fpr, tpr, threshold = roc_curve(actual_y, pos_probs)\n    score = roc_auc_score(actual_y, pos_probs, average = 'weighted')\n    return fpr, tpr, threshold, score\n", "intent": "define function to return Roc parameters and AUC score\n"}
{"snippet": "def cifar_loss(logits, targets):\n    targets = tf.squeeze(tf.cast(targets, tf.int32))\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n    return cross_entropy_mean\n", "intent": "Define our loss function.  Our loss will be the average cross entropy loss (categorical loss).\n"}
{"snippet": "print('Declare Loss Function.')\nloss = cifar_loss(model_output, targets)\naccuracy = accuracy_of_batch(test_output, test_targets)\n", "intent": "Loss and accuracy functions:\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\n    print('Maximum error is {:.5f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0 \n    for i in range(len(style_layers)):    \n        G = gram_matrix(feats[style_layers[i]].clone())\n        A = style_targets[i]\n        dif = G - A\n        L = style_weights[i] * torch.sum(dif*dif)\n        style_loss += L\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "classLosses = []\nfor digitClass in range(0,10):\n    lossTensor = losses.mean_absolute_error(digitDict[digitClass],ae1.predict(digitDict[digitClass]))\n    res = sess.run(lossTensor)\n    res.sort()\n    classLosses.append(res)\n", "intent": "We will now explore the reconstruction error distributions over the digit classes. \n"}
{"snippet": "classificationThresh = 0.5\nactual = mnist.train.images\npred = ae1.predict(actual)\nres = losses.mean_absolute_error(actual,pred)\nres = sess.run(res)\nsnnRes = snn.predict(res)\n", "intent": "**3.5.5. Evaluate the classification accuracy**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nnb = NB()\nscores = cross_val_score(nb, X, y, cv=5)\nprint('Max score:', max(scores))\nprint('Min score:', min(scores))\nprint('Mean:', scores.mean())\nprint('                     ')\n", "intent": "Let's compute cross validation to test our class accuracy: \n"}
{"snippet": "accr = model.evaluate(test_sequences_matrix,Y_test)\n", "intent": "Evaluate the model on the test set.\n"}
{"snippet": "test_denoised = model.predict(X_test)\n", "intent": "Now let's take a look at the results. Top, the noisy digits fed to the network, and bottom, the digits are reconstructed by the network.\n"}
{"snippet": "test_denoised = model.predict(X_test)\n", "intent": "Now let's take a look at the results. Top, the noisy images fed to the network, and bottom, the images are reconstructed by the network.\n"}
{"snippet": "from keras.utils.np_utils import np\nnp.round(model.predict(X))\n", "intent": "You can check that this indeed learned the word:\n"}
{"snippet": "model.predict(X)\n", "intent": "It detects 'shining' flawlessly:\n"}
{"snippet": "model.predict(X)\n", "intent": "Obviously, it predicts things as well:\n"}
{"snippet": "yfinal_pred = clf2.predict_proba(pactx2)[:,1]\nprint yfinal_pred[0:40]\n", "intent": "** Create predicted output for the test file **\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i,layer in enumerate(style_layers):\n        gram = gram_matrix(feats[layer].clone())\n        style_loss += style_weights[i]*torch.sum((gram-style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i,layer in enumerate(style_layers):\n        gram = gram_matrix(feats[layer])\n        style_loss += style_weights[i]*tf.reduce_sum(tf.pow(gram-style_targets[i],2))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "for name, model in fitted_models.items():\n    pred = model.predict_proba(X_test)\n    pred = [p[1] for p in pred]\n    fpr, tpr, thresholds = roc_curve(y_test, pred)\n    print( name, auc(fpr, tpr) )\n", "intent": "<span id=\"auroc\"></span>\n**Area under ROC curve** is one of the most reliable metric for classification tasks.\n"}
{"snippet": "resnet_preds = resnet_model.predict(res_X_test)\ninception_preds = inception_model.predict(inc_X_test)\n", "intent": "Predict values for test set\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) \\\n                           for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred_prob = knn.predict_proba(X_test)[:,1]\n", "intent": "* Conclusion : Our biggest problem are 462 sessions we have predicted as converted \n* butactually its a non converted sessions\n"}
{"snippet": "metrics.accuracy_score(y_test,y_pred_class)\n", "intent": "* We are interested in FP only 1699. we want to reduce 1699 sessions.\n* False Positive Rate 1699\n"}
{"snippet": "def EvalLogLoss(predicted, actual):\n    return sklearn.metrics.log_loss(actual, predicted, eps=1e-15)\n", "intent": "This is the \"LogLoss\" function that use to eveluate how well our classifier is doing. \n"}
{"snippet": "ham_predicted_test = spampred.predict(test_emails.loc[:, words])\nprint(classification_report(test_emails.ham, ham_predicted_test))\n", "intent": "Now let's see how the classifier performed.\n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = list(map(int, clf.predict(test_x)))\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_y, \n                            predict_y, \n                            target_names=['Not Survived', 'Survived']))\n", "intent": "Display the classification report:\nPrecision=fracTPTP+FP\nRecall=fracTPTP+FN\nF1=frac2TP2TP+FP+FN\n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = map(int, clf.predict(test_x))\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "p = Resnet50_model.predict(test_Resnet50)\nResnet50_predictions = np.argmax(p,axis=-1)\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nmean_absolute_error_linearReg_split = mean_absolute_error(Y_test,predict_linearReg_split)\nmean_squared_error_linearReg_split = mean_squared_error(Y_test,predict_linearReg_split)\nR2_score_linearReg_split = r2_score(Y_test,predict_linearReg_split)\nprint mean_absolute_error_linearReg_split\nprint mean_squared_error_linearReg_split\nprint R2_score_linearReg_split\n", "intent": "With these lines of codes is possible to calculate the metrics relative to our problem.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nmean_absolute_error_linearReg_CV = mean_absolute_error(DF_target,predict_linearReg_CV)\nmean_squared_error_linearReg_CV = mean_squared_error(DF_target,predict_linearReg_CV)\nR2_score_linearReg_CV = r2_score(DF_target,predict_linearReg_CV)\nprint mean_absolute_error_linearReg_CV\nprint mean_squared_error_linearReg_CV\nprint R2_score_linearReg_CV\n", "intent": "As before is possible to evaluate the metrics also for this model.\n"}
{"snippet": "from sklearn.metrics import f1_score\ndef predict_labels(clf, features, target):\n    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n    start = time.time()\n    y_pred = clf.predict(features)\n    end = time.time()\n    print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n    return f1_score(target.values, y_pred, pos_label=1)\ntrain_f1_score = predict_labels(clf, X_train, y_train)\nprint \"F1 score for training set: {}\".format(train_f1_score)\n", "intent": "After obtaining accurecny of each model, I will use grid search to search parameter space to find the best parmeters.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, best_rfc.predict(X_test))\nprint \"Accuracy: \", accuracy\n", "intent": "finding the bset AUC for the best_rfc\n"}
{"snippet": "ridge_1 = joblib.load('ridge_1.pkl')\nridge_performance_1 = BinaryClassificationPerformance(ridge_1.predict(X), y, 'ridge_1')\nridge_performance_1.compute_measures()\nprint(ridge_performance_1.performance_measures)\n", "intent": "<b>Ridge Regression Classifier: </b> accuracy: 0.92402197802197805 <p>\nridge_1 = linear_model.RidgeClassifier(alpha=0.1)\n"}
{"snippet": "from models.svm_basic import *\nsvm = SVM(X_train, y_train, verbose=True)\nsvm.train()\ny_pred = svm.predict(X_val)\ny_pred[y_pred >=0] = 1\ny_pred[y_pred < 0] = -1\nerror_rate = sum((y_pred != y_val)) / float(y_val.size)\nprint \"Error rate on the validataion data: {}\".format(error_rate)\n", "intent": "Now we can start training the SVM model.\n"}
{"snippet": "from models.svm_advanced import *\nsvm_adv = SVMAdvanced(X_train, y_train, C=100.0, kernel_choice='Gaussian', \n                      kernel_params={'sigma':6.0}, verbose=True)\nsvm_adv.train(print_every=10000)\ny_pred = svm_adv.predict(X_train[0])\ny_pred[y_pred >=0] = 1\ny_pred[y_pred < 0] = -1\nerror_rate = sum((y_pred != y_train)) / float(y_train.size)\nprint \"Error rate on the validataion data: {}\".format(error_rate)\n", "intent": "I'm now going to apply the Gaussian kernel to the data above and seperate the two classes.\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import fbeta_score\nprint(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\nprint(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\nprint(\"F =\", fbeta_score(y_test, clf.predict(X_test), beta=1))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$$$Recall = \\frac{TP}{TP + FN}$$$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, clf.predict(X_test))\n", "intent": "this is the number of samples of class $i$ predicted as class $j$.\n"}
{"snippet": "testData, cols = getData(test=True)\ngenerate_submission(model1.predict(testData), '1')\n", "intent": "Let's generate a submission and see what our rank on Leaderboard is\n"}
{"snippet": "score = model.evaluate(test_X, test_y, batch_size=50)\n", "intent": "After the model is fit, we can forecast for the entire test dataset.\n"}
{"snippet": "score = model.evaluate(test_X, test_y, batch_size=50)\nprint(score)\n", "intent": "After the model is fit, we can forecast for the entire test dataset.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n", "intent": "Then the error metric: RMSE on the log of the sale prices:\n"}
{"snippet": "f1_score(data_norm.target[:100000], tree.predict(data_norm[:100000].drop('target', axis=1)), average='macro')\n", "intent": "Now calculate the F1 score for each model.\n"}
{"snippet": "predictions= knn.predict(X_test)\n", "intent": "**Predict on test set**\n"}
{"snippet": "predictions= dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg, features, y, cv=6)\nscores\n", "intent": "Let's make a cross validation, so we can be sure we don't have an overfitting error.\n"}
{"snippet": "def make_an_error():\n    raise RuntimeError(\"I'm an error!\")\nmake_an_error()\n", "intent": "Errors are usually exceptions in Python.\n"}
{"snippet": "log_predictions_on_test = logmodel.predict(X=test_data_tran)\n", "intent": "Now that the model is ready lets predict the classes on the test data\n"}
{"snippet": "accuracy_score(y_pred=log_predictions_on_test, y_true=test_target_data)\n", "intent": "Lets calculate the accuracy of the predictions\n"}
{"snippet": "log_predictions_on_eval = logmodel.predict(X=eval_data_tran)\n", "intent": "Precision and Recall are also 89%. \nIts time to apply the model on the evaluation data \n"}
{"snippet": "mnb_predictions = mdbmodel.predict(test_data_tran)\n", "intent": "Model creation is faster than the Logisitic Regression. Its time to predict the target values using this model\n"}
{"snippet": "print(accuracy_score(y_pred=mnb_predictions, y_true=test_target_data))\n", "intent": "Lets check the accuracy of the predictions\n"}
{"snippet": "import numpy\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nprecision = precision_score(y_pred=mnb_predictions, y_true=test_target_data, average='weighted')\nprint(precision)\nrecall = recall_score(y_pred=mnb_predictions, y_true=test_target_data, average='weighted')\nprint(recall)\n", "intent": "Got an accuracy of 77% . Check for precision and recall\n"}
{"snippet": "mnb_predictions_on_eval = mdbmodel.predict(X=eval_data_tran)\n", "intent": "Got recall of 81% and precision of 77% on the test data\nLets predict the target for the evaluation data\n"}
{"snippet": "svm_predictions = svmmodel.predict(test_data_tran)\n", "intent": "As expected the model has taken lot of time to process. \nTime to do the prediction on test data using the SVM model\n"}
{"snippet": "print('Accuracy-',accuracy_score(y_pred=svm_predictions, y_true=test_target_data))\nimport numpy\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nprecision = precision_score(y_pred=svm_predictions, y_true=test_target_data, average='weighted')\nprint('Precision - ',precision)\nrecall = recall_score(y_pred=svm_predictions, y_true=test_target_data, average='weighted')\nprint('Recall - ', recall)\n", "intent": "Calculate the accuracy, precision and recall metrics for the predictions\n"}
{"snippet": "pred_gen = model.predict(input_fn=pred_fn)\n", "intent": "** Each item in your list will look like this: **\n"}
{"snippet": "trainScore = model.evaluate(X_train, y_train, verbose=0)\nprint('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\ntestScore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n", "intent": "To terminal\ntensorboard --logdir=/Users/kachunfung/python/projects/logs\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) \n                         for feature in test_inception]\ninception_test_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % inception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "x_test = np.array(['my model awesome'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "required_data = mp.build_features(dt,sect='test')\ny_pred = clf.predict(required_data)\njson.dump(list(y_pred), open(\"y_pred.json\", \"w+\"))\n", "intent": "To conclude, the required predictions are saved into the file \"y_pred.json\". A plot of the distribution of the classes predicted is shown below.\n"}
{"snippet": "expected = ['Setosa', 'Versicolor', 'Virginica']\npredict_x = {\n    'SepalLength': [5.1, 5.9, 6.9],\n    'SepalWidth': [3.3, 3.0, 3.1],\n    'PetalLength': [1.7, 4.2, 5.4],\n    'PetalWidth': [0.5, 1.5, 2.1],\n}\npredictions = classifier.predict(\n    input_fn=lambda:eval_input_fn(predict_x, None, batch_size=100)) \npredictions\n", "intent": "give the model 4 features, no label... this example has 3 unlabeld flowers to predict.\n"}
{"snippet": "from sklearn import metrics\nMAE = metrics.mean_absolute_error(y_test,prediction)\nMSE = metrics.mean_squared_error(y_test,prediction)\nRMSE = np.sqrt(MSE)\nprint('MAE  :', MAE)\nprint('MSE  :' , MSE)\nprint('RMSE :',RMSE)\n", "intent": "We use :\n   - Mean Absolute Error     \"MAE\"\n   - Mean Squared Error      \"MSE\"\n   - Root Mean Squared Error \"RMSE\"\n"}
{"snippet": "z_predictions = [np.argmax(z_model.predict(np.expand_dims(feature, axis=0))) for feature in test_z]\ntest_accuracy = 100*np.sum(np.array(z_predictions)==np.argmax(test_targets, axis=1))/len(z_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(f\"Accuracy score: {accuracy_score(y_test, y_pred):.{2}}\")\n", "intent": "We got just 3 errors, but we can get even more performance metrics via the homonym module\n"}
{"snippet": "lr.predict_proba(X_test_std[0,:].reshape(1, -1)).round(3)\n", "intent": "We can predict class probabilities via the `predict_proba` method\n"}
{"snippet": "y_pred = model.predict_classes(X, verbose=0)\nprint(classification_report(y, y_pred))\n", "intent": "The classification report showing the precision and recall of our model.\n"}
{"snippet": "plot_decision_boundary(lambda x: model.predict(x), X, y)\n", "intent": "The decision boundary again passes from the middle of the data, but now we have much more misclassified points.\n"}
{"snippet": "plot_decision_boundary(lambda x: model.predict(x), X, y)\n", "intent": "The ANN is able to come up with a perfect separator to distinguish the classes.\n"}
{"snippet": "y_pred = model.predict_classes(X, verbose=0)\nprint(classification_report(y, y_pred))\nplot_confusion_matrix(model, X, y)\n", "intent": "100% precision, nothing misclassified.\n"}
{"snippet": "plot_decision_boundary(lambda x: model.predict(x), X, y)\n", "intent": "Similarly the decision boundary looks just like one we would draw by hand ourselves. The ANN was able to figure out an optimal separator.\n"}
{"snippet": "y_pred = model.predict_classes(X, verbose=0)\nprint(classification_report(y, y_pred))\nplot_confusion_matrix(model, X, y)\n", "intent": "Just like above we get 100% accuracy.\n"}
{"snippet": "plot_decision_boundary(lambda x: model.predict(x), X, y, figsize=(12, 9))\n", "intent": "The ANN was able to model a pretty complex set of decision boundaries.\n"}
{"snippet": "y_pred = model.predict_classes(X, verbose=0)\nprint(classification_report(y, y_pred))\nplot_confusion_matrix(model, X, y)\n", "intent": "Precision is 99%, we have 14 misclassified points out of 2400.\n"}
{"snippet": "features_predictions = [np.argmax(features_model.predict(np.expand_dims(feature, axis=0))) for feature in test_features]\ntest_accuracy = 100*np.sum(np.array(features_predictions)==np.argmax(test_targets, axis=1))/len(features_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet50_predictions = [\n    np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) \n    for feature in test_resnet50\n]\nresnet50_test_accuracy = (np.array(resnet50_predictions) == np.argmax(test_targets, axis=1)).mean()\nprint('Test accuracy: {0:.2%}'.format(resnet50_test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = cross_val_score(clf_bdt, features_train, target_train, cv = 10)\nnp.set_printoptions(formatter = {'float': '{: 0.4f}'.format}, linewidth = 45)\nprint(cv_results(scores))\n", "intent": "> Printing the default parameters tells us that the base Boosting model uses 50 trees (`n_estimators=50`)\n"}
{"snippet": "print metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\nprint  metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n", "intent": "Perfect labelings are both homogeneous and complete, hence have score 1.0:\n"}
{"snippet": "print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n", "intent": "**Question:** Labelings that **assign all classes members to the same clusters** are: ______________, but not __________:\n"}
{"snippet": "print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n", "intent": "If classes members are **completely split across different clusters**, the assignment is totally incomplete, hence the V-Measure is null:\n"}
{"snippet": "print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n", "intent": "Clusters that include samples from **totally different classes** totally destroy the ____________ of the labeling, hence:\n"}
{"snippet": "prediction = ridgecv.predict(X_train)\nprint(prediction)\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "mean_cv_lasso = cross_val_score(lassocv,X_train, y_train,cv=10).mean()\n", "intent": "Is it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "mean_cv_en = cross_val_score(encv,X_train, y_train,cv=10).mean()\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "model.predict([np.array([3]), np.array([6])]) \n", "intent": "We can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_true=test_data['sentiment'], y_pred=model.predict(test_matrix))\nprint(\"Test Accuracy: %s\" % accuracy)\n", "intent": "$$\n\\mbox{accuracy} = \\dfrac{\\mbox{\n$$\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision = precision_score(y_true=test_data['sentiment'], \n                            y_pred=model.predict(test_matrix))\nprint(\"Precision on test data: %s\" % precision)\n", "intent": "$$\n[\\mbox{precision}] = \\dfrac{[\\mbox{\n$$\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall = recall_score(y_true=test_data['sentiment'],\n                      y_pred=model.predict(test_matrix))\nprint(\"Recall on test data: %s\" % recall)\n", "intent": "$$\n[\\text{recall}] = \\dfrac{[\\text{\n$$\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Mean absolute error (MAE): %.2f\"\n      % mean_absolute_error(y_test, y_pred))\nprint(\"Mean squared error (MSE): %.2f\"\n      % mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error (RMSE): {}\".format(rmse))\n", "intent": "Let's evaluate our model performance by calculating variance score, R^2, MAR, MSE and RMSE\n"}
{"snippet": "pred = knn.predict(X_test)\n", "intent": "**Using the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "p = Poisson()\n_, a1 = p.generate(14)\n_, a2 = p.generate(3,2)\n_, a3 = p.generate(8)\n_, a4 = p.generate(2,2)\nactual = numpy.concatenate((a1,a2,a3,a4))\nsarimax = SARIMAX()\npred = sarimax.predict(actual)\n", "intent": "This turns out to be ARIMA (0, 0, 2) x (1, 1, 2, 24).  Let's try it.\n"}
{"snippet": "IncV3_predictions = [np.argmax(IncV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(IncV3_predictions)==np.argmax(test_targets, axis=1))/len(IncV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = regressor.predict(x_test)\n", "intent": "Predicting test set results\n"}
{"snippet": "import math\ndef Log_credit_score(score):\n    return math.log(score)\n", "intent": "This comes in really handy when you don't know if the data is clean:\n"}
{"snippet": "logged_credit_scores = []\nfor score in creditScores:\n    try:\n        logged_credit_scores.append(Log_credit_score(score))\n    except:\n        pass\n", "intent": "instead we can use a try-except:\n"}
{"snippet": "y_pred = classifier.predict(Xtest)\ny_pred = (y_pred > 0.5)\n", "intent": "Once the model has been trained on train data, it can be assessed by using test data.\n"}
{"snippet": "accuracy_score(y_test,pred)*100\n", "intent": "Question 6: Print the following: \na. Accuracy score\nb. Confusion Matrix\nc. Classification Report\n"}
{"snippet": "accuracy_score(y,labels)\n", "intent": "Question 4: Print the accuracy score and confusion matrix\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(test_label, model.predict(test_feat))\n", "intent": "The logistic regression classification accuracy is 0.807.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy for VGG19 model: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "for name, model in names_classifiers:\n    print('Testing machine learning model                      : ', name)\n    filename = os.path.join(train_directory, name) + '_model.sav'\n    loaded_model = pickle.load(open(filename, 'rb'))\n    predictions = loaded_model.predict(data_test)\n    cm_test = confusion_matrix(test_labels, predictions, labels = sorted(list(set(ground_truth))))\n    plot_recall(name, cm_test, test_directory)\n    plot_confusion_matrix(cm_test, test_directory, name, \n                          classes=sorted(list(set(ground_truth))), \n                          normalize=True,title='Normalized confusion matrix')\n", "intent": "------------- Inference -------------\n"}
{"snippet": "accuracy1 = accuracy_score(df_test.Occupancy.values, C02_pred)\naccuracy1*100\n", "intent": "Calculating the accuracy using only the CO2 levels.\n"}
{"snippet": "dt_base_probs = dt_base.predict_proba(features[\"test\"]) [:, 1]\nlin_probs = lin_mdl.predict_proba(features[\"test\"]) [:, 1]\nevaluation.roc_plot(test.funny,\n                    [dt_base_probs, lin_probs],\n                    [\"Untuned Tree\", \"Benchmark Linear\"],\n                    'reports/figures/ROC_dt_base.svg')\n", "intent": "The above accuracy is quite low, lower than the linear model. This is not good. Let us calculate the AUC and compare it to the benchmark lienar model\n"}
{"snippet": "y_maj = np.array([1] * len(designatedSurvivor_test_files))\ntestingAccuracy_maj=accuracy_score(y_maj, clf.predict(designatedSurvivor_X_test))\nprint ('\\nThe Testing accuracy for predicting the majority class all the time (Designated Survivor)= %.4g' %testingAccuracy_maj)\ny_maj1 = np.array([1] * len(lethalWeapon_test_files))\ntestingAccuracy_maj1=accuracy_score(y_maj1, clf.predict(lethalWeapon_X_test))\nprint ('\\nThe Testing accuracy for predicting the majority class all the time (Lethal Weapon)= %.4g' %testingAccuracy_maj1)\n", "intent": "In our project, majority class is Positive class, whose label is 1.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = model.predict(X_test)\naccuracy_score(y_test,predictions)\n", "intent": "Calculate and display the accuracy of the testing set (data_test and label_test):\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = knmodel.predict(X_test)\naccuracy_score(y_test,predictions)\n", "intent": "Calculate and display the accuracy of the testing set:\n"}
{"snippet": "prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Name of predicted target name: {}\".format(\n       iris_dataset['target_names'][prediction]))\n", "intent": "This was ran as a 2 dimensional array because that's what it expects in. (Don't forget the meaning behind X)\n"}
{"snippet": "def compute_square_loss(X, y, theta):\n    num_instances = X.shape[0]\n    predictions = np.dot(X,theta)\n    differences = predictions-y\n    loss = 0.5/(num_instances)*np.dot(differences,differences)\n    return loss\n", "intent": "5) The following shows the code for compute_squre_loss:\n"}
{"snippet": "import os\npredict = stack_predict(X_test)\nfile = \"Id,SalePrice\" + os.linesep\nstartId = 1461\nfor i in range(len(X_test)):\n    file += \"{},{}\".format(startId, (int)(predict[i])) + os.linesep\n    startId += 1\n", "intent": "* Good results without __data_gauss__\n"}
{"snippet": "import os\npredictions = gsrf.best_estimator_.predict(X_test)\npassengerId = 892\nfile = \"PassengerId,Survived\" + os.linesep\nfor i in range(len(X_test)):\n    file += \"{},{}\".format(passengerId, (int)(predictions[i]))  + os.linesep\n    passengerId += 1\n", "intent": "*_Note: Following code is not related to the calculations but how to compose the csv required by the Kaggle competition._\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\nprint(  y_test - test_predict )\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n", "intent": "Evaluate accuracy\nNext, compare how the model performs on the test dataset:\n"}
{"snippet": "predictions = model.predict(test_images)\n", "intent": "Make predictions\nWith the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "print(grid_knn.best_params_)\nprint(grid_knn.score(fscaled_train, l_train))\nprint(grid_knn.score(fscaled_val, l_val))\nlabels_knn = grid_knn.predict(fscaled_val)\n", "intent": "Let's find out how we've done:\n"}
{"snippet": "test_preds = handwriting_model.predict(test_data)\n", "intent": "We'll use the handwriting_model to make predictions for the test data\n"}
{"snippet": "probs = clf.predict_proba(vectorizer.transform([\"Your title here\"]))\nprint \"P(gawker) = {}\".format(probs[0][1])\n", "intent": "Make your own headline and see if it belongs in Gawker or WSJ!\n"}
{"snippet": "from basic_model import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\nclass supervised_training:\n    input_sequence = tf.placeholder('int32', [None, None])\n    reference_answers = tf.placeholder('int32', [None, None])\n    logprobs_seq = model.symbolic_score(input_sequence, reference_answers)\n    crossentropy = -select_values_over_last_axis(logprobs_seq, reference_answers)\n    mask = infer_mask(reference_answers, out_voc.eos_ix)\n    loss = tf.reduce_sum(crossentropy * mask) / tf.reduce_sum(mask)\n    train_step = tf.train.AdamOptimizer().minimize(loss, var_list=model.weights)\ninitialize_uninitialized(s)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "predicted_inflow = list(inflow_results.predict(start =1, end = 5040,dynamic = False))\naccuracy = mean_squared_error(true_in,predicted_inflow)\naccuracy\n", "intent": "Here casting the model to training set to check how accurate model is predicting the in_flow values through mean_squared_error\n"}
{"snippet": "predicted_outflow = list(outflow_results.predict(start =1, end = 5040,dynamic = False))  \naccuracy = mean_squared_error(true_out,predicted_outflow)\naccuracy\n", "intent": "Here casting the model to training set to check how accurate model is predicting the in_flow values through mean_squared_error\n"}
{"snippet": "def _count_list(time_A, time_B):\n    index_A = datetime_to_index(time_A)\n    index_B = datetime_to_index(time_B)\n    return inflow_results.predict(start = index_A, end = index_B,dynamic = False), outflow_results.predict(start = index_A, end = index_B,dynamic = False)\n", "intent": "This function takes two timestamps and forecast the inflow and outflow till end timestamp\n"}
{"snippet": "pred = results.predict(start = len(test)-1, end = (2*len(test)-2),dynamic = False)  \npredict_list = list(pred)\npredict_list\n", "intent": "Forcasting the client for no of sales equal to given no of sales\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\ny_pred = Reg.predict(x[:, None])\nprint('MAE:', metrics.mean_absolute_error(y, y_pred))\nprint('MSE:', metrics.mean_squared_error(y, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y, y_pred)))\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "print('MSE LINEAR REGRESSION:', metrics.mean_squared_error(y_test, y_predR))\nprint('MSE MLPRegressor:', metrics.mean_squared_error(y_test, y_predMLP))\nprint('MSE LINEAR REGRESSION Standarized:', metrics.mean_squared_error(y_test, y_predRS))\nprint('MSE MLPRegressor Standarized:', metrics.mean_squared_error(y_test, y_predMLPS))\n", "intent": "3. Which model has better performance? Why?\n"}
{"snippet": "prediction = clf.predict(X_test_imp)\nprint(prediction.reshape(y_size, x_size))\nprint(numpy.unique(prediction, return_counts=True))\nprint(numpy.unique(y, return_counts=True))\n", "intent": "This is the array that we obtain after the prediction.\n"}
{"snippet": "print('Labels Test Set:',y_test[:10].reshape(1,10))\ny_hat = model.predict_classes(X_test[:10],verbose=0)\nprint('Labels Predicted:  ',y_hat[:10])\ny_prob = model.predict_proba(X_test[:10],verbose=0)\nprint('Probabilities:\\n',y_prob.max(axis=1))\n", "intent": "Some examples from the output of the Network:\n"}
{"snippet": "def true_box(idx):\n    return Y_test[idx]\ndef predict_box(idx, model):\n    X_ = X_test[idx]\n    if X_.ndim == 1:\n        X_ = X_.reshape(1, -1)\n    return model.predict(X_).ravel().astype(int)\n", "intent": "Let's see how it works for some random image; first functions to predict and get the truth.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\npredicted = cross_val_predict(regr, X, y, cv=10)\nmetrics.accuracy_score(y, predicted) \n", "intent": "http://scikit-learn.org/stable/modules/cross_validation.html\n"}
{"snippet": "yp = simple_logistic.predict(x);\ndef PrintConfusionMatrix(model, true_y, predicted_y, positive=1, negative=-1):\n    cm = confusion_matrix(true_y,predicted_y)\n    print(\"\\t\"+str(model.classes_[0])+\"\\t\"+str(model.classes_[1]))\n    print(str(model.classes_[0]) + \"\\t\",cm[0][0],\"\\t\",cm[0][1])\n    print(str(model.classes_[1]) + \"\\t\",cm[1][0],\"\\t\",cm[1][1])\nprint(\"Confusion Matrix - Simple Logistic\")\nPrintConfusionMatrix(simple_logistic, y, yp)\n", "intent": "$accuracy = \\frac{number of correclty classified examples}{number of examples}$\n"}
{"snippet": "print(\"Precision %3.2f\" % precision_score(y,yp))\nprint(\"Recall    %3.2f\" % recall_score(y,yp))\n", "intent": "$precision = \\frac{TP}{TP+FP}$\n$recall = \\frac{TP}{TP+FN}$\n"}
{"snippet": "scores = model.evaluate(X_traindata,Y_traindata)\n", "intent": " Evaluate the performance of the network on train data\n"}
{"snippet": "def total_variation_loss(x):\n    assert K.ndim(x) == 4\n    if K.image_data_format() == 'channels_first':\n        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n    else:\n        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n", "intent": "Imposes local spatial continuity between the pixels of the combination image, giving it visual coherence.\n"}
{"snippet": "y_pred = regression_model.predict(x_test)\n", "intent": "Apply the model on the test set and compare the Actual and Predicted values.\n"}
{"snippet": "y_pred = lm.predict(x)\nrmse = np.sqrt(np.mean((y_pred-y)**2))\nrmse\n", "intent": "*Calculate error using **Root Mean Squares** method.*\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\nresnet_test_accuracy = 100*np.sum(np.array(resnet_predictions) == np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % resnet_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import math\nfrom sklearn.metrics import mean_squared_error\ndef calc_rmse(a, b):\n    return math.sqrt(mean_squared_error(a, b))\n", "intent": "Real values $x_t$ and predicition $\\hat{x}_t$\n$RMSE = \\sqrt{\\dfrac{1}{n}\\sum\\limits_{t=1}^n (\\hat{x}_t - x_t)^2}$\n"}
{"snippet": "y_train_predicted_labels_mybag = classifier_mybag.predict(X_train_mybag)\ny_train_predicted_labels_tfidf = classifier_tfidf.predict(X_train_tfidf)\n", "intent": "Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n"}
{"snippet": "score_data = x[int(0.9*samples_count): ]\npredictions = loadedModelArtifact.model_instance().predict(score_data)\n", "intent": "Make local prediction\n"}
{"snippet": "evaluation = model.evaluate(X_test, y_test)\nprint('Loss in Test set:      %.02f' % (evaluation[0]))\nprint('Accuracy in Test set:  %.02f' % (evaluation[1] * 100))\n", "intent": "Wow, much better! At least on the training data. What about the test data?\n"}
{"snippet": "y_pred = model.predict(X_test)\ny_pred[:10,:]\n", "intent": "What are the predicted values of the test set?\n"}
{"snippet": "if run_3D_convnet:\n    evaluation = model.evaluate(X_test, y_test)\n    print('Loss in Test set:      %.02f' % (evaluation[0]))\n    print('Accuracy in Test set:  %.02f' % (evaluation[1] * 100))\n", "intent": "How about the performance on the test data?\n"}
{"snippet": "y_pred = decoder.predict(X_test)\n", "intent": "Now that the `SpaceNet` is fitted to the training data. Let's see how well it does in predicting the test data.\n"}
{"snippet": "replace = df[missing][df['height'].isnull()][['race', 'age', 'sex', 'body_weight']].values\nnew_values = regr.predict(replace)\nfor idx, val in enumerate(df[missing][df['height'].isnull()].index):\n    df.loc[val, 'height'] = new_values[idx]\n", "intent": "Much better than trying to predict weight\n"}
{"snippet": "from  sklearn.metrics import r2_score\nprint(r2_score(y, predictions))\n", "intent": "The same with built in r2_score function:\n"}
{"snippet": "predictions_scikit = regr.predict(boston_dataframe)\n", "intent": "With .predict() method you can make the predictions from the input dataset\n"}
{"snippet": "print(regr.score(boston_dataframe, y))\nprint(r2_score(y, predictions_scikit))\n", "intent": "What's the $R^2$ score for such model?\n"}
{"snippet": "score = network.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Question 5: Evaluate your model** \n"}
{"snippet": "df_train_scores['pred_xgb'] = xgb4.predict_proba(df_train[predictors])[:,1]\ndf_test_scores['pred_xgb'] = xgb4.predict_proba(df_test[predictors])[:,1]\n", "intent": "Reducing the `learning_rate` did not work as expected and the score reduced slightly.\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\nmodel.save('trained_sign_model.h5')\nmodel.save_weights('trained_sign_model_weights.h5')\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "y_predict = rf_clf.predict(X_test)\naccuracy_score(y_test, y_predict)\n", "intent": "The model appears to be performing okay right now - not too high evidence of overfitting\n"}
{"snippet": "pred=model.predict(X_test)\nprint(np.unique(pred, return_counts=True))\n", "intent": "With the priors we can see that with the bitcoin data in the first 9 months 61% of the days we value went up and 39% the value went down.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print scores\n    print (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores))\n", "intent": "If we evaluate our algorithm with a three-fold cross-validation, we obtain a mean score of around 0.81.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint np.sqrt(-cross_val_score(multi_linear_model,X,y,cv=10, scoring = 'mean_squared_error')).mean()\n", "intent": "And the RMSE of our model is...\n"}
{"snippet": "predictions = clf.predict(train[columns])\nerror = roc_auc_score(predictions, train['high_income'])\nprint(error)\n", "intent": "We can see that the error is 0.7 on the test set. Next, we check the AUC score for the fit on the training set to see if we are overfitting the data.\n"}
{"snippet": "predictions = lr.predict(filtered_cars[['horsepower']])\nprint(predictions[0:5])\nprint(filtered_cars['mpg'][0:5])\n", "intent": "Fitting a Linear Model on Horsepower and MPG columns.\n"}
{"snippet": "mse = mean_squared_error(filtered_cars['mpg'],predictions)\nprint(mse)\nrmse = mse ** (1/2)\nprint(rmse)\n", "intent": "The next step we move onto is calculating the error metric i.e both MSE and RMSE.\n"}
{"snippet": "from sklearn.metrics import accuracy_score, confusion_matrix\nprint(\"kNN CV acc = {:.4f}\".format(accuracy_score(y_train, \n                                                  y_train_preds)))\n", "intent": "The super useful [`metrics`](http://scikit-learn.org/stable/modules/classes.html\n"}
{"snippet": "def calc_accuracy(mod, test_data):\n    predictions = [np.argmax(mod.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\n    return (100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred=clf.predict(x_test)\n", "intent": "Predict the labels of new data (new images)\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy_Resnet = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Resnet)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\ndef get_report(a,b):\n    target_names = ['class 0', 'class 1']\n    return classification_report(a, b, target_names=target_names)\nprint(get_report(Y_train_original,Y_train_original))    \n", "intent": "Understand the data\n"}
{"snippet": "model = rfc_weighted_balanced\ntrans = pca\ndef predict_classify(X):\n        return model.predict(trans.transform(X))    \n", "intent": "predict the X_test from the objective.\n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, y_pred)\nprint(acc)\n", "intent": "The confusion matrix based on test dummy dataset shows True Positive Rate as 77.28 which conforms with accuracy score below.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscore = cross_val_score(knn,X,Y,cv=5)\nprint(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\nscore_w = cross_val_score(knn_w,X,Y,cv=5)\nprint(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))\n", "intent": "Validating KNN is very similar to validating other classifiers or regression. Cross val, holdouts, r-squared all still apply.\n"}
{"snippet": "print(\"For our clusters:\")\nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(targetDF['targetSenator'], exampleKM.labels_)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(targetDF['targetSenator'], exampleKM.labels_)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(targetDF['targetSenator'], exampleKM.labels_)))\nprint(\"Adjusted Rand Score: {:0.3f}\".format(sklearn.metrics.adjusted_rand_score(targetDF['targetSenator'], exampleKM.labels_)))\n", "intent": "Now for the metrics:\n"}
{"snippet": "print(\"For our complete clusters:\")\nprint(\"Homogeneity: {:0.3f}\".format(sklearn.metrics.homogeneity_score(targetDF['targetSenator'], example_hierarchicalClusters_full)))\nprint(\"Completeness: {:0.3f}\".format(sklearn.metrics.completeness_score(targetDF['targetSenator'], example_hierarchicalClusters_full)))\nprint(\"V-measure: {:0.3f}\".format(sklearn.metrics.v_measure_score(targetDF['targetSenator'], example_hierarchicalClusters_full)))\nprint(\"Adjusted Rand Score: {:0.3f}\".format(sklearn.metrics.adjusted_rand_score(targetDF['targetSenator'], example_hierarchicalClusters_full)))\n", "intent": "And consider the metrics:\n"}
{"snippet": "train_ng_df['nb_predict'] = MultinomialNB_ng.predict(np.stack(train_ng_df['vect'], axis=0))\nprint(\"Training score:\")\nprint(MultinomialNB_ng.score(np.stack(train_ng_df['vect'], axis=0), train_ng_df['category']))\n", "intent": "And save the predictions to the dataframe\n"}
{"snippet": "biv_outliers_mask = ee.predict(params_data) == -1\nbiv_outliers_data = params_data[biv_outliers_mask]\nbiv_outliers_name = state_code[biv_outliers_mask]\n", "intent": "Get the names and positions of outliers.\n"}
{"snippet": "state_code[ocsvm.predict(data) == -1]\n", "intent": "List the names of the outlying states based on the one-class SVM.\n"}
{"snippet": "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\n", "intent": "4 - Prediction and Evaluation\n"}
{"snippet": "y2_pred_class = knn.predict(x2_test)\nprint((metrics.accuracy_score(y2_test, y2_pred_class)))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "print(np.mean((lm.predict(X)-bos.PRICE)**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\ntest_preds = model.predict([user_id_test, item_id_test])\nprint(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\nprint(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))\n", "intent": "- MAE is better in terms of interpretation because it belongs to the same magnitude of ratings\n"}
{"snippet": "accuracies = [accuracy_score(y_test, tree.predict(X_test)) for tree in trees]\n", "intent": "Now evaluate the accuracy for each of these subtrees against the test dataset,\ncalculating the accuracy score.\n"}
{"snippet": "results = {}\nfor clf in [GNB, DecisionTree, RandomForest]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        train_predict_evaluate(clf, samples, X_train, y_train, X_test, y_test)\nvs.visualize_classification_performance(results)\n", "intent": "The models will be trained on different sizes of the datasets to explore how their performance increases with more cases to learn from.\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred, target_names=['benign', 'malignant'])\n", "intent": "<b> 3) Precision, Recall, Support, and f-measure</b>\n"}
{"snippet": "Y2 = regr.predict(X1)\n", "intent": "For every value of X the Y value is predicted\n"}
{"snippet": "DogRes_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogRes]\ntest_accuracy = 100*np.sum(np.array(DogRes_predictions)==np.argmax(test_targets, axis=1))/len(DogRes_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\nTest acc. is 82.6%\n"}
{"snippet": "reg_cv = cross_val_score(reg, X, bos.PRICE, cv=4)\nreg_cv\n", "intent": "Score is higher for 4-fold corss-validation, \nTest MSE for train-split above is : 28.541367275619013, MSE for 4-fold CV is -21.8797\n"}
{"snippet": "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\nprint(\"-\"*50)\nprint(\"Confusion Matrix: \\n{}\".format(confusion_matrix(y_test, y_pred)))\nprint(\"-\"*50)\nprint(\"Classification Report: \\n{}\".format(classification_report(y_test, y_pred)))\n", "intent": "Now, let's print **accuracy_score**, **confusion_matrix** and **classification_report** from **sklearn.metrics**\n"}
{"snippet": "scores = cross_val_score(mlp, x_train, y_train, cv=10)\nprint(scores)\n", "intent": "Task 7: Let's go one step further and talk about cross validation\n"}
{"snippet": "y_hat = model.predict(x_test)\n", "intent": "**Remember** The classes are 0 to 6 instead of 1 to 7.\n"}
{"snippet": "y2_hat = model2.predict(x_test)\n", "intent": "**Remember** The classes are 0 to 6 instead of 1 to 7.\n"}
{"snippet": "y_hat = model.predict(x_test, batch_size=64)\n", "intent": "**Remember** The classes are 0 to 6 instead of 1 to 7.\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision_score(college_target_train, college_train_predict,average= None)\n", "intent": "** 3) Precision and Recall **\n"}
{"snippet": "test_predict  = train_fit.predict(test_set)\n", "intent": "Calculate the predictions, that is, the predicted y equals to the round values of predicted probability.\n"}
{"snippet": "test_proba    = train_fit.predict_proba(test_set)\n", "intent": "Calculate the probabilities $\\hat{p}$ by using the `predict_proba` method.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(estimator= train_fit_iclevel,     \n                X= test_ind,  \n                y = test_target_iclevel,      \n                scoring = \"accuracy\",               \n                cv=3)                              \nprint(\"Accuracy per fold: \")\nprint(scores)\nprint(\"Average accuracy: \", scores.mean())\n", "intent": "The following set of codes are about target variable `ICLEVEL`.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "- proprtion of correct predictions\n- Common evaluation metric for classification problems\n"}
{"snippet": "xception_predictions = [np.argmax(xceptionModel.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_preds = (X_test_level2 * [best_alpha, 1 - best_alpha]).sum(axis=1) \nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr2.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2,train_preds)\ntest_preds = lr2.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test,test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits,random_state=skf_seed,shuffle=True)\n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF,X,Y,cv=skf)\n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "print 'Coefficients: \\n' + str(simp_reg.coef_)\nprint 'Mean squared error: ' + str(np.mean((simp_reg.predict(X_test) - y_test) ** 2))\nprint 'Variance score: ' + str(simp_reg.score(X_test, y_test))\n", "intent": "Now that we have trained our first model, let's see how well it did and test it against our validation set.\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring = 'accuracy')\n", "intent": "This array tells us with what probability the random forest classifier attempted to classify different digits. It predicted \"4\" with 90% probability\n"}
{"snippet": "ResNet50_predictions = [np.argmax(model_resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.3f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "sklearn.metrics.accuracy_score(y_test, pred)\n", "intent": "and the accuracy score.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_labels_Xception, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pscore = model.predict_proba(features)\ndata = data.assign(propensity_score = pscore[:,0])\ndata.head()\n", "intent": "First, the propensity scores are calculated and added to the dataframe\n"}
{"snippet": "test_loss = compute_loss(test_set, 1 - best_threshold)\nprint \"Per token accuracy: %f\" % test_loss.acc\nprint \"Per token f1 score: %f\" % test_loss.f1\nprint \"Fraction of right compressions %f\" % test_loss.match\n", "intent": "Compute the accuracy and auc in every phrase and report the avearag. Also compute the number of phrases that match exactly.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n", "intent": "Let's use cross_val_predict to check the prediction's:\n"}
{"snippet": "RSS= print (np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "MSE= print(np.mean((bos.PRICE-lm.predict(X))**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This is better since the training and the testing accuracy are close by.\n"}
{"snippet": "yf = reg.predict(X)\n", "intent": "A better visualization: Plot data points with stems coming out from the fit surface.\n"}
{"snippet": "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\nY_test_oh = convert_to_one_hot(Y_test, C = 15)\nloss, acc = model.evaluate(X_test_indices, Y_test_oh)\nprint()\nprint(\"Test accuracy = \", acc)\n", "intent": "Ideally model should perform close to **100% accuracy** on the training set.  Run the following cell to evaluate your model on the test set. \n"}
{"snippet": "C = 15\ny_test_oh = np.eye(C)[Y_test.reshape(-1)]\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\npred = model.predict(X_test_indices)\nfor i in range(len(X_test)):\n    x = X_test_indices\n    num = np.argmax(pred[i])\n    if(num != Y_test[i]):\n        print('Expected class:'+ label_to_class(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_class(num).strip())\n", "intent": "Accuracy between 80% and 95% should be OK for now. Run the cell below to see the mislabelled examples. \n"}
{"snippet": "x_test = np.array(['morning sir'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_class(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(expected, predicted))\n", "intent": "Quantify the performance detail\n------------------------\nPrint the classification report\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(expected, predicted))\n", "intent": "Print the classification report\n"}
{"snippet": "print(logisticRegr.predict(x_test[0].reshape(1,-1)))\nprint(logisticRegr.predict(x_test[0:10]))\npredictions = logisticRegr.predict(x_test)\n", "intent": "- - - - - - - - - - - - - - - - - - - - - - -\nNow we have a model, so we can make predictions.\n"}
{"snippet": "print(regress.predict([[6]]))\n", "intent": "So now we can make predictions with new points based off our data\n"}
{"snippet": "predictions = model.predict(X_test.reshape((-1,28*28)))\n", "intent": "Let's see what it predicts from those images we looked at before\n"}
{"snippet": "score = model.evaluate(X_test,y_test, verbose=0)\nprint('Accuracy : {}'.format(score[1]*100))\n", "intent": "Trade-off between time and accuracy has to be considered here.\n"}
{"snippet": "mean_squared_error(validation_df[\"SalePrice\"], predictions)\n", "intent": "The mean squared error essentially tells us how wrong we are with bad predictions being peanlized even more.\n"}
{"snippet": "y_pred = knn.predict(X_train)\nX_new = X_test[[0]]\nnew_prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(new_prediction))\n", "intent": "Having fit a k-NN classifier, you can now use it to predict the label of a new data point.\n"}
{"snippet": "final_cnn_predictions = [np.argmax(final_cnn.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(final_cnn_predictions)==np.argmax(test_targets, axis=1))/len(final_cnn_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "grid_prediction = grid.predict(X_test)\n", "intent": "**Lets predict with the new found optimised parameters**\n"}
{"snippet": "predictions = lm1.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "predictions = lm2.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))\n", "intent": "F1 score equally combines precision and recall into a single numner\nF1= 2*[(precision*recall)/(precision+recall)]= (2*TP)/(2*TP+FN+FP)\n"}
{"snippet": "cv_prediction = cross_val_predict(model, data[feature_labels], data['delta_e'], cv=10, n_jobs=-1)\n", "intent": "Quantify the performance of this model using 10-fold cross-validation\n"}
{"snippet": "resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "examples = ['Free Viagra now!!!', \"Hi Bob, how about a game of golf tomorrow?\"]\nexample_counts=vectorizer.transform(examples)\npredictions=classifier.predict(example_counts)\npredictions\n", "intent": "Giving couple of example emails for predicting their category\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report((df['True label'] == 'A').astype(int),\n                            (df['AF prob'].astype(float) >= 0.5).astype(int)))\n", "intent": "Print some classification metrics to quantify the quality of predictions:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report((df['True label'] == 'A').astype(int),\n                            (df['AF prob'].astype(float) >= 0.5).astype(int),\n                            target_names = ['NO', 'A']))\n", "intent": "Print some classification metrics to quantify the quality of predictions:\n"}
{"snippet": "mus, sigmas = gp.predict(all_x.reshape(-1,1), return_std=True)\n", "intent": "The predict method returns both mean and std.\n"}
{"snippet": "training_data_num = 1\nneed_X_train = X_train[:training_data_num]\nneed_y_train = y_train[:training_data_num]\nC = exponential_cov(need_X_train, need_X_train, kernel_parameters)\nmus, cov = predict(all_x, X_train[:training_data_num], y_train[:training_data_num], exponential_cov, kernel_parameters, cholesky=True)\nsigmas = np.sqrt(np.diag(cov))\nplot_gp(all_x, mus, np.sqrt(sigmas), need_X_train, need_y_train, true_y)\nprint(\"rmse = {0}\".format(np.sqrt(mean_squared_error(mus, true_y))))\n", "intent": "Posterior Distribution - single points\nNow we start by feeding our GP with a single datum.\n"}
{"snippet": "training_data_num = 50\nneed_X_train = X_train[:training_data_num]\nneed_y_train = y_train[:training_data_num]\nmus, cov = predict(all_x, need_X_train, need_y_train, exponential_cov, kernel_parameters, cholesky=True)\nsigmas = np.sqrt(np.diag(cov))\nplot_gp(all_x, mus, np.sqrt(sigmas), need_X_train, need_y_train, true_y)\nprint(\"rmse = {0}\".format( np.sqrt(mean_squared_error(mus, true_y))))\n", "intent": "50 points\nThe warning is caused by some very small negative number that's so close to 0.\n"}
{"snippet": "training_data_num = 10\nn_samples = 10\nmus, cov = predict(all_x, X_train[:training_data_num], y_train[:training_data_num], exponential_cov, kernel_parameters, cholesky=False)\nsigmas = np.sqrt(np.diag(cov))\nsamples = sample_cholesky(mus, cov, len(all_x), n_samples)\nplot_gp(all_x, mus, sigmas, X_train[:training_data_num], y_train[:training_data_num], samples=samples)\n", "intent": "Sample from posterior with 10 points\n"}
{"snippet": "from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nr2=r2_score(y_test, regr.predict(X_test))\nprint r2\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()\n", "intent": "We can run a cross validation using the area of the curve as the score:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = grid.best_estimator_.predict(X_test)\nprint accuracy_score(y_pred, y_test)\n", "intent": "Store the predictions of the optimal model:\n"}
{"snippet": "from sklearn import metrics\ny_pred = knn.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)\n", "intent": "Now let's to make predictions for the test sepal and pedal lengths: \n"}
{"snippet": "obs = vectorizer.transform([' '.join(cameron), ' '.join(darwin)])\nclf.predict(obs)\n", "intent": "Let's check predictions on the training data:\n"}
{"snippet": "kubrick = scrape_and_tokenize('Stanley_Kubrick')\nkarle = scrape_and_tokenize('Jerome_Karle')\nobs = vectorizer.transform([' '.join(kubrick), ' '.join(karle)])\nclf.predict(obs)\n", "intent": "Now make prediction using data that the model has no seen:\n"}
{"snippet": "bg['*-predict'] = linreg.predict(tfidf.transform(bg['text'].values))\nbg.head()\n", "intent": "We see there are 43 reviews at a minimum.\n"}
{"snippet": "def calc_score(user, item, sim_matrix, user_ratings):\n    w = sim_matrix.loc[item_neighbors.tolist()][item].values.reshape(-1)\n    r = user_ratings[item_neighbors].values.reshape(-1)\n    return w.dot(r) / np.sum(w)\n", "intent": "Use the formula (raw format).\n"}
{"snippet": "print(lda.predict_proba(x1)[0][1])\nprint(lda.predict_proba(x2)[0][1])\n", "intent": "The probability of Balance>1500 for x1 and x2 are 0.06772607 and 0.8270789,respectively. And the balace is subject to 0 for x1, and 1 for x2.  \n"}
{"snippet": "Xception_predictions = [np.argmax(model_Xception.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifierXg, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\n", "intent": "- k-Fold Cross Validation\n- Grid Search\n    - Fit and Predict with Best Model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is better, becauses values for training and test sets are close to each other.\n"}
{"snippet": "print \"Class predictions according to GraphLab Create:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from sklearn.\n"}
{"snippet": "print \"Class predictions according to sklearn:\" \nprint  sentiment_model.predict_proba(sample_test_matrix)[:,1]\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from sklearn.\n"}
{"snippet": "print \"Training data, classification error (model 1):\", evaluate_classification_error(model_1, train_data, target)\nprint \"Training data, classification error (model 2):\", evaluate_classification_error(model_2, train_data, target)\nprint \"Training data, classification error (model 3):\", evaluate_classification_error(model_3, train_data, target)\n", "intent": "Let us evaluate the models on the **train** and **validation** data. Let us start by evaluating the classification error on the training data:\n"}
{"snippet": "print \"Validation data, classification error (model 1):\", evaluate_classification_error(model_1, validation_set, target)\nprint \"Validation data, classification error (model 2):\", evaluate_classification_error(model_2, validation_set, target)\nprint \"Validation data, classification error (model 3):\", evaluate_classification_error(model_3, validation_set, target)\n", "intent": "Now evaluate the classification error on the validation data.\n"}
{"snippet": "print \"Validation data, classification error (model 4):\", evaluate_classification_error(model_4, validation_set, target)\nprint \"Validation data, classification error (model 5):\", evaluate_classification_error(model_5, validation_set, target)\nprint \"Validation data, classification error (model 6):\", evaluate_classification_error(model_6, validation_set, target)\n", "intent": "Calculate the accuracy of each model (**model_4**, **model_5**, or **model_6**) on the validation set. \n"}
{"snippet": "print \"Validation data, classification error (model 7):\", evaluate_classification_error(model_7, validation_set, target)\nprint \"Validation data, classification error (model 8):\", evaluate_classification_error(model_8, validation_set, target)\nprint \"Validation data, classification error (model 9):\", evaluate_classification_error(model_9, validation_set, target)\n", "intent": "Now, let us evaluate the models (**model_7**, **model_8**, or **model_9**) on the **validation_set**.\n"}
{"snippet": "probabilities = model.predict_proba(test_matrix)[:,1]\npred = [+1 if p >= 0.98 else -1 for p in probabilities]\nconfusion_matrix(y_test, pred)\n", "intent": "**Quiz Question**: Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? \n"}
{"snippet": "probabilities = model.predict_proba(baby_matrix)[:,1]\nprobabilities\n", "intent": "Now, let's predict the probability of classifying these reviews as positive:\n"}
{"snippet": "test_eval = model.evaluate(X_test, y_test)\nprint()\nprint('    Test set loss:', test_eval[0])\nprint('Test set accuracy:', test_eval[1])\n", "intent": "After trained, we can evaluate our CNN with the test data.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "<div class=\"girk\">\nlet s make it predict all 5</div><i class=\"fa fa-lightbulb-o \"></i>\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n", "intent": "* area under the curve (AUC), perfectly 1, randomly 0.5\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\npredictions = clf.predict(test[columns])\nerror = roc_auc_score(test[\"income\"], predictions)\nprint(error)\n", "intent": "AUC ranges from 0 to 1, so it's ideal for binary classification. The higher the AUC, the more accurate the predictions.\n"}
{"snippet": "test_mse = mean_squared_error(test[target], test_prediction)\ntrain_mse = mean_squared_error(train[target], train_prediction)\ntest_rmse = np.sqrt(test_mse)\ntrain_rmse = np.sqrt(train_mse)\nprint([test_rmse, train_rmse])\n", "intent": "Now let's calculate the RMSE for the training and test predictions\n"}
{"snippet": "predicted_labels_2 = lr1.predict(admissions[[\"gpa\",\"gre\"]])\nadmissions[\"predicted_labels_two_features\"] = predicted_labels_2\nadmissions.head()\n", "intent": "As we can see in the scatter plots above, there's a stronger relationship between admission and gre scores than admissions and gpa scores.\n"}
{"snippet": "from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y, predict(w,X))\n", "intent": "fpr = fp/(fp+tn) = 101/(101+414) = 0.19\ntpr = tp/(tp+fn) = 167/(167+86) = 0.66\n"}
{"snippet": "pred=net.activateOnDataset(mnist_test)\npredicted=[np.argmax(i) for i in pred]\nprint accuracy_score(mnist_test_hidden_y,predicted)\n", "intent": "1) the trained neural network\n"}
{"snippet": "print classification_report(y_test,y_pred_dtc)\n", "intent": "The decision tree classifier resulted in the best score of all the previous models used.\n"}
{"snippet": "print(\"Random Forest\")\ny_pred = gs.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(\"Log Reg\")\ny_pred = clf.predict(X_test_lr)\nprint(classification_report(y_test,y_pred))\n", "intent": "Random Forest Manages to overfit a great deal it seems.\n"}
{"snippet": "yhat = model.transform(test)\nevaluator = RegressionEvaluator(labelCol=\"PRICE\")\nrmse = evaluator.evaluate(yhat)\nprint(rmse)\n", "intent": "Now validate the model on the test set, and check the Root Mean Squared Error.\n"}
{"snippet": "yhat = model.transform(test)\nevaluator = RegressionEvaluator(labelCol=\"PRICE\")\nrmse = evaluator.evaluate(yhat)\nprint(rmse)\n", "intent": "Finally, validate the model on the test set and check the Root Mean Squared Error again.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"285.JPG\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\nnever_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')\n", "intent": "* Look at a very dumb classifier that just classifies every single image in the 'not-5' class\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "** We can combine precision and recall into a single metric called the F1 score **\nEquation 3-3. F1 score\n* F1=TP/(TP+(FN+FP)/2)\n"}
{"snippet": "pred = nb_clf.predict(x_test)\nsvm_pred = svm_clf.predict(x_test)\n", "intent": "Now we can calculate the accuracy of our model from the validation set\n"}
{"snippet": "def train_one_epoch():\n    loss_total = 0\n    data = get_train_batch(x_train, y_train, batch_size = batch_size)\n    for i in range(num_steps_train):\n        x_tr, y_tr = next(data)\n        model.train_on_batch(x_tr, y_tr)\n        if i%100 == 0 :\n            loss = model.evaluate(x= x_tr,y = y_tr,verbose = 0)\n            print ('Batch({}/{}: loss{})'.format(i,num_steps_train,loss))\n", "intent": "My computer went dead when I use Model.fit_generator..I don't know why..So I wrote this function to call modle.train_on_batch to avoid that.\n"}
{"snippet": "eval_result = classifier.evaluate(\n    input_fn=lambda:eval_input_fn(test_x, test_y, 3000))\n", "intent": "The following code runs the testing evaluate function on the network.\n"}
{"snippet": "print(labels_test)\nprint(classifier.predict(data_test))\n", "intent": "We now have a model: given data, it will return its prediction for the label. We use the testing data to check the model:\n"}
{"snippet": "y_test = test_df['churndep']\nx_test = test_df.drop('churndep', 1)\npre_train = dt.predict(x_axis, check_input=True)\npre_test = dt.predict(x_test, check_input=True)\ntrain_accuracy = dt.score(x_axis, y_axis, sample_weight=None)\ntest_accuracy = dt.score(x_test, y_test, sample_weight=None)\n", "intent": "Predict `\"churndep\"` on both the train_df and test_df data sets and find out the accuracy on each.\n"}
{"snippet": "prediction = tr.predict(features)\nprint(\"Accuracy: {:.1%}\".format(np.mean(prediction == labels)))\n", "intent": "Evaluating performance on the training set (which is **not the right way to do it**: see the discussion in the book)\n"}
{"snippet": "from sklearn import model_selection\npredictions = model_selection.cross_val_predict(\n    tr,\n    features,\n    labels,\n    cv=model_selection.LeaveOneOut())\nprint(np.mean(predictions == labels))\n", "intent": "We can do the same leave-on-out cross validation with scikit-learn's `model_selection` module:\n"}
{"snippet": "predict = model_selection.cross_val_predict(rf, features, target)\nprint(\"RF accuracy: {:.1%}\".format(np.mean(predict == target)))\n", "intent": "Again, use cross-validation to evaluate:\n"}
{"snippet": "from sklearn.model_selection import KFold, cross_val_predict\nkf = KFold(n_splits=5)\np = cross_val_predict(lr, x, y, cv=kf)\nrmse_cv = np.sqrt(mean_squared_error(p, y))\nprint('RMSE on 5-fold CV: {:.2}'.format(rmse_cv))\n", "intent": "Now, we will use **cross-validation** for evaluating the regression quality:\n"}
{"snippet": "kf = KFold(n_splits=5)\npred = cross_val_predict(lr, data, target, cv=kf)\nprint('RMSE on testing (5 fold), {:.2}'.format(np.sqrt(mean_squared_error(target, pred))))\nprint('R2 on testing (5 fold), {:.2}'.format(r2_score(target, pred)))\n", "intent": "However, we do not do so well on cross-validation:\n"}
{"snippet": "pred = cross_val_predict(met, data, target, cv=kf)\nprint('[EN 0.1] RMSE on testing (5 fold): {:.2}'.format(np.sqrt(mean_squared_error(target, pred))))\nprint('[EN 0.1] R2 on testing (5 fold): {:.2}'.format(r2_score(target, pred)))\n", "intent": "Not a perfect prediction on the training data anymore, but let us check the value on cross-validation:\n"}
{"snippet": "met = ElasticNetCV(n_jobs=-1, l1_ratio=[.01, .05, .25, .5, .75, .95, .99])\npred = cross_val_predict(met, data, target, cv=kf)\nprint('[EN CV l1_ratio] RMSE on testing(5 fold), {:.2}'.format(np.sqrt(mean_squared_error(target, pred))))\nprint('[EN CV l1_ratio] R2 on testing (5 fold), {:.2}'.format(r2_score(target, pred)))\n", "intent": "This is a a pretty good general-purpose regression object:\n"}
{"snippet": "good_post = (2, 1, 100, 5, 4, 1, 0)\npoor_post = (1, 0, 10, 5, 6, 5, 4)\nproba = clf.predict_proba([good_post, poor_post])\nproba\n", "intent": "We can now use the classifier's `predict_proba()` to calculate the probabilities for the classes `poor` and `good`:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nscore = accuracy_score(Y_train > .5, predict_train > .5)\nprint(\"Score (on training data): %.2f\" % score)\nscore = accuracy_score(Y_test > .5, predict_test > .5)\nprint(\"Score (on testing data): %.2f\" % score)\n", "intent": "Now for the results for this new classifier:\n"}
{"snippet": "predicted = predict_stacked(train)\nr2 = metrics.r2_score(test[test > 0], predicted[test > 0])\nprint('R2 score (stacked prediction): {:.1%}'.format(r2))\n", "intent": "We can use the same evaluation as before:\n"}
{"snippet": "sfeatures = []\nfor d in alldescriptors:\n    c = km.predict(d)\n    sfeatures.append(np.bincount(c, minlength=256))\nsfeatures = np.array(sfeatures, dtype=float)\nscores = model_selection.cross_val_score(\n   clf, sfeatures, labels, cv=cv)\nprint('Accuracy (5 fold x-val) with Logistic Regression [SURF]: {:.1%}'.format(scores.mean()))\n", "intent": "We now use the kmean structure on **all the descriptors**:\n"}
{"snippet": "allfeatures = np.hstack([sfeatures, ifeatures])\nscore_SURF_global = model_selection.cross_val_score(\n    clf, allfeatures, labels, cv=cv).mean()\nprint('Accuracy (5 fold x-val) with Logistic Regression [All features]: {:.1%}'.format(\n    score_SURF_global.mean()))\n", "intent": "Combine the features for the final result\n"}
{"snippet": "predicted_price_non_std = slr.predict(np.array(num_rooms).reshape(len(num_rooms),1))\nprint(\"Predicted Price in $1000's Using un-standardized data: %.3f\" % predicted_price_non_std)\n", "intent": "Now let's predict using the model that uses un-standardized data\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nMSE1 = mean_squared_error(y_true= y, y_pred= slr.predict(X))\nr2_1 = r2_score(y_true= y, y_pred= slr.predict(X))\nprint('MSE for regression model using un-standrdized features: %.3f' % MSE1)\nprint('RMSE:', np.sqrt(MSE1))\nprint('r2:', r2_1)\nprint('_____________________________________________________')\n", "intent": "evaluate the performance of the regresion model on training data\n"}
{"snippet": "from sklearn.metrics import accuracy_score \naccuracy_score(y_array, y_pred_array)\n", "intent": "Then we can compute the accuracy using `accuracy_score` of scikit learn.\n"}
{"snippet": "X_test_df, y_test_array = problem.get_test_data()\nX_test_array = fe.transform(X_test_df)\ny_test_proba_array = clf.predict_proba(X_test_array)\ny_test_pred_array = [problem._prediction_label_names[y] for y in np.argmax(y_test_proba_array, axis=1)]\naccuracy_score(y_test_array, y_test_pred_array)\n", "intent": "The following cell will evaluate the workflow on the test data.\n"}
{"snippet": "test_review = np.array(['listen to this song again and again '])\ntest_review_vector = vectorizer.transform(test_review)\nprint(clf.predict_proba(test_review_vector))\nclf.predict(test_review_vector)\n", "intent": "A classified **neutral** review\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    current_grams = [gram_matrix(feats[idx], normalize=True) for idx in style_layers]\n    for item in zip(current_grams, style_targets, style_weights):\n        style_loss += item[2] * torch.sum((item[0] - item[1])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = np.round(clf.predict(test_df)).astype(int)\n", "intent": "Using the parameters optimised above, retrain on all the data so we don't miss anything. Then make predictions on the\ntest set.\n"}
{"snippet": "score = classifier.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy is \", round(score[1], 4))\nprint(\"Loss is \", round(score[0], 4))\n", "intent": "Now let us evalutate this with the test data we have.\n"}
{"snippet": "p_default= log_cls.predict([1,1000])\nprint ('Probability of Default for a balance of $1,000.00:\\n\\t',p_default)\np_default= log_cls.predict([1,2000])\nprint ('Probability of Default for a balance of $2,000.00:\\n\\t',p_default)\n", "intent": "Once the coefficients have been estimated, it is a simple matter to compute the probability of default for any given credit card balance.\n"}
{"snippet": "lm.predict(X)[0:5]\n", "intent": "We can calculate the predicted prices $\\hat{Y}_i$ using lm.predict.\n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "$$S = \\sum^{N}_{i=1}(y_i - \\hat{y}_i)^2$$\n"}
{"snippet": "yd_logsal = lass1.predict(X_test.reshape(1,-1))\nsalary = np.exp(yd_logsal)\nsalary\n", "intent": "Okay, let's predict an annual salary for Yu Darvish, arguably the top free agent starting pitcher in this year's class:\n"}
{"snippet": "Res50_predictions = [np.argmax(Res50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_1 = 100*np.sum(np.array(Res50_predictions)==np.argmax(test_targets, axis=1))/len(Res50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_1)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "np.random.seed(100)\nest_r, ncases = evaluateRejectionSampling(explorer, X=Xtest, a=actions_test, r=rewards_test, online=False)\nreal_r = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), explorer.predict(Xtest)])\nprint('Test set Rejection Sampling mean reward estimate (old policy)')\nprint('Estimated mean reward: ', est_r)\nprint('Sample size: ', ncases)\nprint('----------------')\nprint('Real mean reward: ', real_r)\n", "intent": "We can also evaluate the exploration policy with the same method:\n"}
{"snippet": "print('Will I pass?', clf.predict([[0.6, 0.5, 0.8]])[0] == 1)\n", "intent": "Now, if I were to enter my own data, I could see whether or not I'll pass!\n"}
{"snippet": "def log_likelihood(clf, x, y):\n    prob = clf.predict_log_proba(x)\n    rotten = y == 0\n    fresh = ~rotten\n    return prob[rotten, 0].sum() + prob[fresh, 1].sum()\n", "intent": "We use the log-likelyhood as the score here. We'll go into this later...\n"}
{"snippet": "lr.score(Xtest, ytest) \nprint('R-Square: %.2f' % r2_score(ytest, ypred))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "lm.evaluate(x, y, batch_size=2)\n", "intent": "---\nNow check out what the loss is for the initial internal weights:\n"}
{"snippet": "lm.evaluate(x, y)\n", "intent": "and evaluate the loss again:\n"}
{"snippet": "trn_features = model.predict(trn_data, batch_size=batch_size, verbose=1)\nval_features = model.predict(val_data, batch_size=batch_size, verbose=1)\n", "intent": "...and save them out using bcolz helpers we created\n"}
{"snippet": "print(\"Accuracy: %0.3f\" % gb_mod_ovr.score(X_test,y_test))\nprint(\"F1 Score: %0.3f\" % metrics.f1_score(y_test,gb_mod_ovr.predict(X_test),average=\"weighted\"))\n", "intent": "We can confirm that the OneVsRest method does not significantly impact model performance.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis=1)) / len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "base_clf = DummyClassifier(strategy='most_frequent',random_state=0)\nAcc_Score = cross_val_score(base_clf,X,data['airline_sentiment'], scoring='f1_micro',cv=10)\nprint(\"Accuracy for predicting majority class %0.3f\" % np.mean(Acc_Score))\nbase_clf = DummyClassifier(strategy='stratified',random_state=0)\nAcc_Score = cross_val_score(base_clf,X,data['airline_sentiment'], scoring='accuracy',cv=10)\nprint(\"Accuracy for random prediction %0.3f\" % np.mean(Acc_Score))\n", "intent": "Baseline : DummyClassifier() \n"}
{"snippet": "score = model.evaluate(X_test, Y_test)\n", "intent": "Evaluate model on test data.\nFinally, we can evaluate our model on the test data:\n"}
{"snippet": "scores3 = cross_validation.cross_val_score(logreg, features_array, target, scoring='precision', cv=3)\nprint scores3.mean(), scores3.std(), scores3.min(), scores3.max()\nscores5 = cross_validation.cross_val_score(logreg, features_array, target, scoring='precision', cv=5)\nprint scores5.mean(), scores5.std(), scores5.min(), scores5.max()\nscores10 = cross_validation.cross_val_score(logreg, features_array, target, scoring='precision', cv=10)\nprint scores10.mean(), scores10.std(), scores10.min(), scores10.max()\n", "intent": "We can also use optimize scores like precision, recall and f1 scores\n"}
{"snippet": "print('Homogeneity score:', metrics.homogeneity_score(circles_y, dbscan.labels_))\n", "intent": "With 2 clusters it looks like it's clustering the way we intended it to. Let's evaluate the homogenity score next.\n"}
{"snippet": "script = (dml(scriptPredict).input(data=testData, C=1, Hin=28, Win=28, W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3, W4=W4, b4=b4)\n                            .output(\"predictions\"))\npredictions = ml.execute(script).get(\"predictions\").toNumPy()\nprint (classification_report(testData[:,0], predictions))\n", "intent": "Use trained model and predict on test data, and evaluate the quality of the predictions for each digit.\n"}
{"snippet": "print \"test precision: {:.2f}%\".format(precision_score(ytest,ytest_pred) * 100)\nprint \"test recall: {:.2f}%\".format(recall_score(ytest,ytest_pred) * 100)\n", "intent": "<a id='test-precision-recall'></a>\n<span style='color:orange;font-weight:bold;font-size:1.5em'>more accurate Precision and Recall</span>\n"}
{"snippet": "test_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 8\nno_feature = 1500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4 Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 3\nno_feature = 1500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\nR^2 of .90 is good. It is high, but not too high to be worried.\n"}
{"snippet": "score = cross_val_score(regr, X_train, y_train, cv=6)\nprint(score)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\nThe model is robust.\n"}
{"snippet": "logit_clf.predict_proba(X_train)[:, 1][:10]\n", "intent": "What are the predicted probabilities on the training data (probability of being `1`) with our **Logit** classifier?\n"}
{"snippet": "preds = sgd_clf.predict(normalized_X)\npreds\n", "intent": "Oh we were right 97% of the time, not bad! What if we want to get the predictions for each sample instead of the score?\n"}
{"snippet": "preds = logit_clf.predict(normalized_X)\npreds\n", "intent": "Oh we were right 96% of the time, not bad! What if we want to get the predictions for each sample instead of the score?\n"}
{"snippet": "probas = logit_clf.predict_proba(normalized_X)\nprobas[:10]\n", "intent": "Great! Let's also obtain and observe the probabilities given by each classifier!\n"}
{"snippet": "mean_absolute_error(y, y_hat)\n", "intent": "$$MAE = \\frac{1}{N} \\sum_{n=1}^N \\left| y_n - \\hat{y}_n \\right|$$\n"}
{"snippet": "mean_squared_error(y, y_hat)\n", "intent": "$$MSE = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$\n"}
{"snippet": "np.sqrt(mean_squared_error(y, y_hat))\n", "intent": "$$RMSE = \\sqrt{MSE}$$\n"}
{"snippet": "metrics.roc_auc_score(true_labels, predicted_labels)\n", "intent": "AUC score can only be used for binary classification problems\n"}
{"snippet": "roc_auc = roc_auc_score(df_results['target'], df_results['scores'])\nprint('The AU ROC of our classifier is = %.3f' % roc_auc)\n", "intent": "Let's calculate then the AU ROC of classifier:\n"}
{"snippet": "grid_search.predict(X_test)[:10]\n", "intent": "We can use the fitted grid search to predict.\n"}
{"snippet": "clf.predict(test_data[10:20])\n", "intent": "This modifies the classifier object so that it now can estimate the targets of new, given values. Let's test it!\n"}
{"snippet": "clf.predict(iris.data[-1:])\n", "intent": "And now, check that the last one gets the correct class.\n"}
{"snippet": "cross_validation.cross_val_score(model, Xtrain, ytrain, cv = 3).mean()\n", "intent": "Now let's take a look at its cross validation score.  We'll set it to only 3-fold cross validation to help with runtime.\n"}
{"snippet": "score = myNetwork.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "**Question 5: Evaluate your model** \n"}
{"snippet": "def rmse_cv(model, scoring, X, y, CV=5):\n    rmse= np.sqrt(cross_val_score(estimator=model, X=X, y=y, scoring=scoring, cv=CV))\n    return(rmse)\n", "intent": "<p>We also need a cross validation routine, that uses the scorer to compute a cross validation error. </p>\n"}
{"snippet": "test_pred = clf.predict(test_data[new_features])\nfalse_pos = len(test_data[(test_pred == 1) & (test_pred != test_data[target])])\nfalse_pos\n", "intent": "Calculate the number of **false positives** made by the model.\n"}
{"snippet": "decision_tree_model.predict_proba(sample_validation_data)\n", "intent": "For each row in the **sample_validation_data**, what is the probability (according **decision_tree_model**) of a loan being classified as **safe**? \n"}
{"snippet": "print accuracy_score(train_data[target], big_model.predict(train_data[new_features]))\nprint accuracy_score(validation_data[target], big_model.predict(validation_data[new_features]))\n", "intent": "Now, let us evaluate **big_model** on the training set and validation set.\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1-best_alpha)* X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = stack_lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = stack_lr.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits,shuffle=True, random_state=skf_seed)\n    NNF = NearestNeighborsFeats(n_jobs=20, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv =skf)\n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "lr = Class_Fit(clf = linear_model.LogisticRegression)\nlr.grid_search(parameters = [{'C':np.logspace(-2,2,20)}], Kfold = 5)\nlr.grid_fit(X = X_train, Y = Y_train)\nlr.grid_predict(X_test, Y_test)\n", "intent": "Logistic regression classifier\n"}
{"snippet": "rf = Class_Fit(clf = ensemble.RandomForestClassifier)\nparam_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [20, 40, 60, 80, 100],\n               'max_features' :['sqrt', 'log2']}\nrf.grid_search(parameters = param_grid, Kfold = 5)\nrf.grid_fit(X = X_train, Y = Y_train)\nrf.grid_predict(X_test, Y_test)\n", "intent": "Random Forest Classifier-\n"}
{"snippet": "gb = Class_Fit(clf = ensemble.GradientBoostingClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\ngb.grid_search(parameters = param_grid, Kfold = 5)\ngb.grid_fit(X = X_train, Y = Y_train)\ngb.grid_predict(X_test, Y_test)\n", "intent": "Gradient Boosting Classifier-\n"}
{"snippet": "classifiers = [(lr, 'Logostic Regression'),(knn, 'k-Nearest Neighbors'),(rf, 'Random Forest'),(gb, 'Gradient Boosting')]\nfor clf, label in classifiers:\n    print(format(label))\n    clf.grid_predict(X, Y)\n", "intent": "Applying different classifiers on the test dataset as done for training dataset.\n"}
{"snippet": "predictions = votingC.predict(X)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, predictions)))\n", "intent": "Combining different classifiers by using method votingC to get the most accurate predictions.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Available metrics are: \\n{}\".format(model.metrics_names))\nprint(score)\n", "intent": "Now we should evaluate our model.\n"}
{"snippet": "predictions = predict(parameters, X)\nprint(predictions.shape)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "An even more precise metric is called the F1 score: the harmonic mean of precision and recall. \n<img src=\"F1.png\">\n"}
{"snippet": "print(linearsvm.predict(X_test))\n", "intent": "Now that we have trained our model using the training model, let's use the test set and see how our model performs:\n"}
{"snippet": "_, acc_train = model.evaluate(x_train, y_train_onehot)\n_, acc_dev = model.evaluate(x_dev, y_dev_onehot)\nprint('Train set accuracy: {}'.format(acc_train))\nprint('Validation set accuracy: {}'.format(acc_dev))\n", "intent": "First, check accuracy on training and validation set. \n"}
{"snippet": "train_dfs = np.split(pruned_df_train,[-1],axis=1)\npred = skmodel.predict(train_dfs[0])\nprint('Accuracy Score: ', accuracy_score(train_dfs[1],pred))\n", "intent": "Confusion Matrix for train data\n"}
{"snippet": "prediction=model.predict(X_gtest)\nprediction=prediction.reshape(1000,32,32,4)\npred=np.zeros((1000,32,32,3),dtype=np.float64)\nfor i in range(0,1000):\n    for j in range(0,32):\n        for k in range(0,32):\n            pred[i][j][k]=center[np.argmax(prediction[i][j][k])]        \n", "intent": "**Answer** Test and training error were plotted.\n"}
{"snippet": "train_pred = rf.predict(training_dataframe)\ntrainerror = mean_squared_error(training_dataframe_label,train_pred)\ntrainerror\n", "intent": "Computed the out of bag error and the test error in above code.\n"}
{"snippet": "acc = accuracy_score(total_df_test['blood'],prediction)\nprint(\"Accuracy for Supervised learning using L1 penalized SVM: \",acc)\n", "intent": "calculating accuracy \n"}
{"snippet": "pred = kmeans1.predict(test_df)\naccuracy_kmeans_test = accuracy_score(test_label,pred)\nprint(\"Accuracy of test data sing K-means is: \",accuracy_kmeans_test)\n", "intent": "test data close to center1 is cluster1_testdata and data close to center2 is stored in cluster2_testdata.\n"}
{"snippet": "scores_accur = cross_val_score(log, Xe_train, ye_train, \n                         cv=3, scoring='accuracy').mean()\nscores_logloss = cross_val_score(log, Xe_train, ye_train, \n                         cv=3, scoring='log_loss').mean()\nprint([scores_accur,scores_logloss])\n", "intent": "One-hot encoding, not scaled\n"}
{"snippet": "scores_accur = cross_val_score(log, scale(X_train), y_train, \n                         cv=3, scoring='accuracy').mean()\nscores_logloss = cross_val_score(log, scale(X_train), y_train, \n                         cv=3, scoring='log_loss').mean()\nprint([scores_accur,scores_logloss])\n", "intent": "Label encoding, scaled\n"}
{"snippet": "my_func.classifier_summary(y_train, lr_rfe.predict(X_train));\n", "intent": "Our mean F2 cross validiation score is 0.001 less than our original model utilizing all features. As for the full train vs. validation scores:\n"}
{"snippet": "y_train_pred = lr_best.predict(X_train_rfe)\ny_val_pred = lr_best.predict(X_val_rfe)\n", "intent": "Fit time has been drastically improved with the addition of regularization.\n"}
{"snippet": "pca_cvs = cross_val_score(lr_pca, X_train, y_train, scoring=f2_score)\nmy_func.print_cvs(pca_cvs, 'f2')\n", "intent": "We can see even with 99.9% of variance accounted for, we have still managed to reduce the number of features/components from 44 to 36.\n"}
{"snippet": "preds = lm.predict(X)\nexplained = np.sum((y.mean() - preds) ** 2)\nresiduals = np.sum((y - preds) ** 2)\nvar = np.sum((y - y.mean()) ** 2)\nr_squared = explained / var\nr_squared\n", "intent": "The price starts at 62.34 and goes down by -2.16 for every point of increase in student-teacher ratio.\n"}
{"snippet": "y_pred=logit.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscore = cross_val_score(rf,doc_train_matrix, target_train, cv = 3)\n", "intent": "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)\n"}
{"snippet": "residual= np.sum((bos.PRICE - lm.predict(X)) ** 2)  \nprint (residual )\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "print ( np.mean((bos.PRICE - lm.predict(X)) ** 2)  )\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "df_possible_cities['Predicted Sales'] = reg.predict(df_possible_cities[features])\n", "intent": "Using linear model fit to training set in previous sections.\n"}
{"snippet": "predictions = model.predict(test_data, batch_size=128, verbose=1)\n", "intent": "We can now use the model to predict the classes of the images contained in the original test dataset.\n"}
{"snippet": "prediction = classifier.predict(np.array([test_data[0]], dtype=float), as_iterable=False)\nprint(\"Predicted %d, Label: %d\" % (prediction, test_labels[0]))\ndisplay(0)\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "predictions = predict(parameters, X)\nprint(predictions)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "VGG19_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_generic]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'confusiont matrix \\n {} \\n'.format(confusion_matrix(\n        y_test, preds))\nprint 'accuracy = {0:0.3f}'.format(accuracy_score(y_test, preds))\n", "intent": "If the meaning of confusion matrix is not clear, look at [Confusion Matrix page](https://en.wikipedia.org/wiki/Confusion_matrix)\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom sklearn.metrics import make_scorer, mean_squared_error\ndef rmsle(y, y_pred):\n     return np.sqrt((( (np.log1p(y_pred*price_scale)- np.log1p(y*price_scale)) )**2).mean())\nscorer = make_scorer(mean_squared_error, False)\ndef rmse_cv(model, X, y):\n     return (cross_val_score(model, X, y, scoring=scorer)).mean()\n", "intent": "<a id='2.1.'></a>\n[back to top](\n"}
{"snippet": "from sklearn import metrics\nfm_index = metrics.fowlkes_mallows_score(foundlabels, labels) \nfm_index\n", "intent": "Calculating the Fowlkes-Mallows index to check how well the consensus clustering agrees with the expert/true cluster\n"}
{"snippet": "mutinf = metrics.adjusted_mutual_info_score(foundlabels, labels)  \nmutinf\n", "intent": "And the mutual information between the \"true\" clustering and the consensus clustering\n"}
{"snippet": "test_loss, test_acc = model.evaluate(data, labels)\n", "intent": "Run a test on all of the data to get a final accuracy score\n"}
{"snippet": "inceptionV3_predictions = [np.argmax(inceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in \n                           test_inceptionV3]\ntest_accuracy = 100*np.sum(np.array(inceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import sklearn.metrics as met\nY_pred = model.predict(X_test)\nprint(met.confusion_matrix(Y_test,Y_pred, labels=[0,1]))\nprint(met.f1_score(Y_test,Y_pred))\n", "intent": "While the score is not terribly bad, the confusion matrix have lots of false neatives\n"}
{"snippet": "predict=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "per2=dtree2.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,per2))\nprint('\\n')\nprint(confusion_matrix(y_test,per2))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "scores_rfc = cross_val_score(rfc, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_rfc)\nprint(scores_rfc.mean())\n", "intent": "**Random Forest classifier**\n"}
{"snippet": "scores_dtree_2 = cross_val_score(dtree_2, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_dtree_2)\nprint(scores_dtree_2.mean())\n", "intent": "**DecisionTreeClassifier**\n"}
{"snippet": "def newton_raphson(y, X, w, iters) :\n    for i in range(iters) :\n         n = random.randint(0,num_examples-1)\n        w = w - hinge_loss(y, w) / f_prime(w)\n        if i % 10 == 0 :\n            cost = calculate_primal_objective(y, X, w, lambda_)\n            acc = calculate_accuracy(y, X, w)\n            print(acc)\n            print(\"l=\" +str(cost)+\"acc=\"+str(acc))\n    return x\n", "intent": "Implement stochastic gradient descent: Pick a data point uniformly at random and update w based on the gradient for the n-th summand of the objective\n"}
{"snippet": "x=1\nmean_squared_error(list([x]),list([2]))\n", "intent": "The slope needed to update this weight is indeed 6\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "x = np.load(\"/media/yuens/WIN10-ENTERTENMENT/Kaggle/Intel-CCS/train_add_seg_224/Type_3_seg_224/100-resnet-152.npy\")\nxg_test = xgb.DMatrix(x)\npred = bst.predict(xg_test)\npred_label = np.argmax(pred_prob, axis=1)\nprint(\"pred:{}\".format(pred))\nprint(\"pred_label:{}\".format(pred_label))\n", "intent": "load model and single sample feature, then make prediction on single model using trained boost model.\n"}
{"snippet": "in_sample_accuracy = (clf.predict(images)==labels).mean()\nprint('In sample Accuracy {:.2%}, In sample Error {:.4f} '.format(in_sample_accuracy,1-in_sample_accuracy))\nout_of_sample_accuracy = (clf.predict(images_test)==labels_test).mean()\nprint('Out of sample Accuracy {:.2%}, Out of sample Error {:.4f}'.format(out_of_sample_accuracy,1-out_of_sample_accuracy))\n", "intent": "Let's see how well it does.\n"}
{"snippet": "lr_scores = cross_val_score(lr, X_test, Y_test, cv=10)\nprint('Cross-Validation Scores:', lr_scores)\nprint('Mean Cross-Validation Score: {:.2%} +/- {:.2%}'.format(lr_scores.mean(),lr_scores.std()))\n", "intent": "**Note: You're running regression on a classification problem, so you should still be able to produce a classification report.**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.\n    for i in range(len(style_layers)):\n        idx = style_layers[i]\n        loss += style_weights[i] * tf.reduce_sum(((style_targets[i] - gram_matrix(feats[idx])) **2))\n    return tf.reduce_sum(loss)\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfinal_model = grid_search_lasso.best_estimator_   \ny_te_estimation = final_model.predict(X_te)\nfinal_mse = mean_squared_error(y_te, y_te_estimation)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n", "intent": "Choose among grid_search_rr, grid_search_lr, and grid_search_enr, the model with best performance\n"}
{"snippet": "dt.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(features, axis=0)))for features in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test Accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "result = tf_predictor.predict(valid_set[0][0:100])\nclusters = [r.label['closest_cluster'].float32_tensor.values[0] for r in result]\n", "intent": "OK, a single prediction works.\nLet's do a whole batch and see how well the clustering works.\n"}
{"snippet": "labels = kmeans.predict(scaleddf)\n", "intent": "In the next lines of code we are gonna create a numpy arrays of labels to attach to our dataframe\n"}
{"snippet": "print('AUC score LR: {:.3f}'.format(roc_auc_score(y_test, y_predict_proba_high_lr)))\nprint('AUC score RF: {:.3f}'.format(roc_auc_score(y_test, y_predict_proba_high_rf)))\n", "intent": "AUC provides an aggregate measure of performance across all possible classification thresholds.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "Accuracy of CNN is higher when using image augmentation\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "Note:\n* MLP accuracy on test set ~40%\n* CNN accuracy on test set ~68%\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\naccuracy = 100*score[1]\nprint('Test accuracy: %.4f%%' % accuracy)\n", "intent": "Try out the model on the images in the Testing Set we find that the model gets 98.25% accuracy since MLP is a good solution here.\n"}
{"snippet": "vgg16_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) \n                     for feature in test_vgg16]\ntest_accuracy = 100*np.sum(np.array(vgg16_predictions)==\n                           np.argmax(test_targets, axis=1))/len(vgg16_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Note: The accuracy of 46.65% is considered impressive when you consider random guessing would result in less than 1% (1 in 133 chance)\n"}
{"snippet": "Resnet50_predictions = [np.argmax(CustomResnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\n    print('Maximum error is {:.8f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0, dtype = tf.float32)\n    for i in range(len(style_layers)):\n        feat_gram = gram_matrix(feats[style_layers[i]])\n        style_loss += tf.reduce_sum(tf.pow(style_targets[i]-feat_gram,2)) * style_weights[i]\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\n    print('Error is {:.8f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "test_x = np.asanyarray(test[['ENGINESIZE']])\ntest_y = np.asanyarray(test[['CO2EMISSIONS']])\ntest_y_ = regr.predict(test_x)\nprint (\"Residual Sum of Squares : %.2f\"\n      % np.mean((test_y_ - test_y)**2))\nprint('Variance Score : %.2f' % regr.score(test_x,test_y))\n", "intent": "**Evaluate the model with the Test data**\n"}
{"snippet": "kfold = KFold(n_splits =10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n", "intent": "I'll be using 10 fold cross validation to evaluate the model \n"}
{"snippet": "print (\"Class predictions according to GraphLab Create:\" )\nprint (sentiment_model.predict(sample_test_matrix))\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "print (sentiment_model.predict_proba(sample_test_matrix))\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "predictions = lm.predict( X_test)\n", "intent": "Predicting Test Data\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "Evaluating the Model\n"}
{"snippet": "Y_pred = rf.predict(X_test)\nY_over_pred = rf_over.predict(X_test)\nY_under_pred = rf_under.predict(X_test)\nY_comb_pred = rf_comb.predict(X_test)\n", "intent": "Let's now get some predictions for the test set and looking at the classification reports for each\n"}
{"snippet": "from sklearn import metrics\ntestDatacounts = trainDatavectorizer.transform(testData['message'].values)\ntestDatapredictions = trainDataclassifier.predict(testDatacounts) \ntdr = [(metrics.accuracy_score(testData['class'].values,testDatapredictions))]\nprint (tdr) \n", "intent": "Next, also the Test data is \"vectorized\". \n"}
{"snippet": "preds = LR.predict(X[N:,:])\nmetrics.roc_auc_score(y[N:],preds)\n", "intent": "Lets evaluate with AUC as we did with the MNIST dataset\n"}
{"snippet": "probxgbrf=[(1*x + 1.6*y+0.6*z)/3.2\n           for x, y,z in zip(probxgb, probxgbf,problr)]\nroc_auc_score(test_y, probxgbrf)\n", "intent": "Combining probabilities of multiple models. Stacking\n"}
{"snippet": "print(metrics.classification_report(ytest, y_model))\n", "intent": "We can take a look at the classification report for this classifier:\n"}
{"snippet": "print(\"abs error: {}\".format(metrics.mean_absolute_error(y_test,y_predictions)))\nprint(\"squared error: {}\".format(metrics.mean_squared_error(y_test,y_predictions)))\nprint(\"root mean squared error: {}\".format(np.sqrt(metrics.mean_squared_error(y_test,y_predictions))))\n", "intent": "Using metrices module of sklearn to get absolute error, squared error and mean squared error\n"}
{"snippet": "bagging_classifier_results = bt.predict_log_proba(X_bt_predict)\nbagging_classifier_max = np.argmax(bagging_classifier_results, axis=1)\n", "intent": "*Predict S1/S2/NOISE from features provided*\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import fbeta_score\nprint(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\nprint(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\nprint(\"F =\", fbeta_score(y_test, clf.predict(X_test), beta=1)) \n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$\n$$Recall = \\frac{TP}{TP + FN}$$\n$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nimport sys\nprint \"make a confusion matrix on the best initial model\"\ncm = confusion_matrix(dev_test_label, model_logistic.predict(dev_test_vec))\nfor row in cm:\n    for col in row: sys.stdout.write(\"%4d \"%col)\n    sys.stdout.write(\"\\n\")\nprint\nprint \"classes:\"\nprint str(model_logistic.best_estimator_.classes_)\n", "intent": "Error Analysis \n<ul>\n<li>Use confusion matrix to identify common errors</li>\n<li>Look at which recipes are causing the confusion</li>\n</ul>\n"}
{"snippet": "print('accuracy:', round(sparse_lreg.score(X_test, y_test), 3))\nprint('f1:', round(f1_score(y_test, sparse_lreg.predict(X_test), average='weighted'), 3))\n", "intent": "Accuracy on tfidf data\n"}
{"snippet": "print('accuracy:', round(dense_lreg.score(X_glove_test, y_test), 3))\nprint('f1:', round(f1_score(y_test, dense_lreg.predict(X_glove_test), average='weighted'), 3))\n", "intent": "Accuracy on means of embedded words\n"}
{"snippet": "print('accuracy:', round(dense_lreg.score(X_glove_weighted_test, y_test), 3))\nprint('f1:', round(f1_score(y_test, dense_lreg.predict(X_glove_weighted_test), average='weighted'), 3))\n", "intent": "Accuracy on weighted embedded words\n"}
{"snippet": "real_rate = df[['PassengerId','Survived']]\nreal_rate = real_rate.set_index('PassengerId').T.to_dict(orient='list')\nreal_rate = {x: real_rate[x][0] for x in real_rate} \nmts.accuracy_score(list(real_rate.values()), list(predictions.values()))\n", "intent": "Once you have finished implementing the function `predict`, you can use the next code cell in order to check your accuracy.\n"}
{"snippet": "def predict(theta, X):\n", "intent": "Making predictions in the same way as Ex 2.1\n"}
{"snippet": "predictions_lr_1 = logreg_classiffier_1.predict(test_df.iloc[:,2:])\n", "intent": "Let's test the model's fit on the test data and check the quality of our fit\n"}
{"snippet": "def get_auc_gini(actual, prediction):\n    auc_score = roc_auc_score(actual, prediction)\n    gini = 2 * auc_score - 1\n    print(\"AUC score: {0}, Gini: {1}\".format(auc_score, gini))\n", "intent": "Let's create a functionality for calculating the AUC score and Gini coefficient\n"}
{"snippet": "for th in [0.01, 0.05, 0.10, 0.20, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5]:\n    preds = [1 if el > th else 0 for el in probas_clocks] \n    y_test = _batches.classes \n    roc_auc = roc_auc_score(y_test, probas_clocks)\n    f1 = f1_score(y_test, preds)\n    acc = accuracy_score(y_test, preds)\n    print(\"th: {} :: ROC AUC S: {} -- F1 S: {} -- ACC: {}\".format(th, roc_auc, f1, acc))\n", "intent": "We need to fine-tune the theshold for the labeling. We will base on the validation results.\nIt appears that the optimal threshold is around 0.01\n"}
{"snippet": "pred_train = model.predict_proba(df_train.iloc[:, 1:])\nscore = log_loss(df_train['y'], pred_train)\nprint 'Logloss for the training set: ' + str(score)\n", "intent": "With the trained model, we can determine its performance on the training set.\n"}
{"snippet": "pred_train = model.predict_proba(df_test.iloc[:, 1:])\nscore = log_loss(df_test['y'], pred_train)\nprint 'Logloss for the validation set: ' + str(score)\n", "intent": "Now we can apply the model to the test set.\n"}
{"snippet": "pred = lr.predict(X_test)\n", "intent": "Predict and Evaluete\n"}
{"snippet": "target_names = ['Conservative Direction', 'Liberal Direction']\nprint(classification_report(y_test, yhat, target_names=target_names))\n", "intent": "The below classification report provides additional metrics for my model\n"}
{"snippet": "test_input = np.array([3.7, 1.26, 70.1, 1.0])\ntest_output = result.predict(test_input)\nprint('Probability of Female: {0:.2f}%'.format(test_output[0]*100.))\n", "intent": "Here is how we can make predictions with our model.\n"}
{"snippet": "y_pred_test = model2.predict(X_test)\nprint 'root mean squared error on test data: {}'.format(np.sqrt(mean_squared_error(y_test,y_pred_test)))\n", "intent": "Great! This is comparable to the cross validation R2 so we know we are not overfitting! \n"}
{"snippet": "((model.predict(x_test[0:1]) - x_test[0:1])**2).sum()\n", "intent": "To get the output use the .predict method feeding the NN with an input\n"}
{"snippet": "train_score = mean_squared_error(trainY_original.T,\n                                trainPredict.T,\n                                multioutput = 'raw_values')\ntrain_score = np.sqrt(train_score.sum()) * 1./train_score.shape[0]\nprint('Train Score: %.2f RMSE' % (train_score))\ntest_score = mean_squared_error(testY_original.T,\n                                testPredict.T,\n                                multioutput = 'raw_values')\ntest_score = np.sqrt(test_score.sum()) * 1./test_score.shape[0]\nprint('Test Score: %.2f RMSE' % (test_score))\n", "intent": "+ Compute mean square error\n"}
{"snippet": "train_score = mean_squared_error(trainY_original_stf.T,\n                                trainPredict_stf.T,\n                                multioutput = 'raw_values')\ntrain_score = np.sqrt(train_score.sum()) * 1./train_score.shape[0]\nprint('Train Score: %.2f RMSE' % (train_score))\ntest_score = mean_squared_error(testY_original_stf.T,\n                                testPredict_stf.T,\n                                multioutput = 'raw_values')\ntest_score = np.sqrt(test_score.sum()) * 1./test_score.shape[0]\nprint('Test Score: %.2f RMSE' % (test_score))\n", "intent": "+ Compute mean square error\n"}
{"snippet": "train_score = mean_squared_error(trainY_original_m_stf.T,\n                                trainPredict_m_stf.T,\n                                multioutput = 'raw_values')\ntrain_score = np.sqrt(train_score.sum()) * 1./train_score.shape[0]\nprint('Train Score: %.2f RMSE' % (train_score))\ntest_score = mean_squared_error(testY_original_m_stf.T,\n                                testPredict_m_stf.T,\n                                multioutput = 'raw_values')\ntest_score = np.sqrt(test_score.sum()) * 1./test_score.shape[0]\nprint('Test Score: %.2f RMSE' % (test_score))\n", "intent": "+ Compute mean square error\n"}
{"snippet": "train_score = mean_squared_error(YtrainpredPersitence_inv.T,\n                                YtrainpredPersitence_original_inv.T,\n                                multioutput = 'raw_values')\ntrain_score = np.sqrt(train_score.sum()) * 1./train_score.shape[0]\nprint('Train Score: %.2f RMSE' % (train_score))\ntest_score = mean_squared_error(YtestpredPersitence_inv.T,\n                                YtestpredPersitence_original_inv.T,\n                                multioutput = 'raw_values')\ntest_score = np.sqrt(test_score.sum()) * 1./test_score.shape[0]\nprint('Test Score: %.2f RMSE' % (test_score))\n", "intent": "+ Compute mean square error\n"}
{"snippet": "predictions = np.array(model.predict(testX)).argmax(axis=1)\nactual = testY.argmax(axis=1)\ntest_accuracy = np.mean(predictions == actual, axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "After you're satisified with the training output and accuracy, you can then run the network on the **test data set** to measure it's performance! \n"}
{"snippet": "ResNet50_2_predictions = [np.argmax(ResNet50_model2.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_2_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\nmodel.reset_states()\nprint(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Now, let's see the final accuracy of the model on the training set.\n"}
{"snippet": "y_pred = np.exp(res3.predict(df_test))\nprint(y_pred.min())\nprint(y_pred.max())\n", "intent": "Maximum predicted duration is only ~7900 sec.\n"}
{"snippet": "correct = 0\nfor i in range(len(X)):\n    predict_me = np.array(X[i].astype(float))\n    predict_me = predict_me.reshape(-1, len(predict_me))\n    prediction = kmeans.predict(predict_me)\n    if prediction[0] == y[i]:\n        correct += 1\nprint(correct/len(X))\n", "intent": "Let's see how well the model is doing by looking at the percentage of passenger records that were clustered correctly.\n"}
{"snippet": "print mean_squared_error(testY, sum(reg.predict(testX) for reg in regressors) / len(regressors))\n", "intent": "Averaging total mse. We could use different coefficient to mix results\n"}
{"snippet": "pred = testX.dot(w_optimal)\nroc_auc_score(testY, pred)\n", "intent": "Now predict output of logistic regression for the test sample and compute AUC\n"}
{"snippet": "from sklearn.metrics import precision_score,recall_score,accuracy_score,confusion_matrix\nprint( precision_score(y_test,y_preds))\nprint( recall_score(y_test,y_preds))\nprint( accuracy_score(y_test,y_preds))\nconfusion_matrix(y_test,y_preds)\n", "intent": "d. Provide the summary of your model below, and use it as necessary to answer the following questions.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "Let's test out the model on some testing data:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_rmse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_rmse)\nlin_rmse\n", "intent": "Let's measure this model's RMSE on the whole training set:\n"}
{"snippet": "from sklearn.metrics import f1_score\nX = [list(d.getdata()) for _, d in validate_data]\ny_true = [l.isalpha() for l, _ in validate_data]\ny_pred = [True] * len(y_true)\nf1_score(y_true, y_pred)\n", "intent": "To test out a hypothesis, I am going to compare the above LogisticRegression model using the binary classification against an all positive model.  \n"}
{"snippet": "import numpy as np\nfrom sklearn import metrics\nlr_y_pred_prob = logreg.predict_proba(test_dtm)[:, 1]\ny_test_binary = np.where(y_test==5, 1, 0)\nprint 'Logistic Regression: ', metrics.roc_auc_score(y_test_binary, lr_y_pred_prob)\nknn_y_pred_prob = knn.predict_proba(test_dtm)[:, 1]\nprint 'k-Nearest Neighbors: ', metrics.roc_auc_score(y_test_binary, knn_y_pred_prob)\nct_y_pred_prob = treeclf.predict_proba(test_dtm)[:, 1]\nprint 'Classification Tree: ', metrics.roc_auc_score(y_test_binary, ct_y_pred_prob)\n", "intent": "<h4> 5.4 Calculate the AUC for the 3 models </h4>\n"}
{"snippet": "import numpy as np\nfrom sklearn import metrics\nlr_y_pred_prob = logreg.predict_proba(X_test)[:, 1]\ny_test_binary = np.where(y_test==5, 1, 0)\nprint 'Logistic Regression: ', metrics.roc_auc_score(y_test_binary, lr_y_pred_prob)\nknn_y_pred_prob = knn.predict_proba(X_test)[:, 1]\nprint 'k-Nearest Neighbors: ', metrics.roc_auc_score(y_test_binary, knn_y_pred_prob)\nct_y_pred_prob = treeclf.predict_proba(X_test)[:, 1]\nprint 'Classification Tree: ', metrics.roc_auc_score(y_test_binary, ct_y_pred_prob)\n", "intent": "<h4> 6.5 Calculate the AUC for the 3 models </h4>\n"}
{"snippet": "numpyMatrix = y.as_matrix()\nrecommit_yes = np.count_nonzero(numpyMatrix)\nypred = np.zeros(21373)\nnull_score = 1.0 - 6514.0/21373.0\nprint ('Null model accuracy: %f' %null_score)\nnull_auc = metrics.roc_auc_score(y, ypred)\nprint ('Null model AUC: %f' %null_auc)\n", "intent": "<h3>Null Model</h3>\n"}
{"snippet": "overall_5 = np.count_nonzero(y)\nprint overall_5\nypred = np.ones(131960)\nnull_score = 119739.0/131960.0\nprint ('Null model accuracy: %f' %null_score)\nnull_auc = metrics.roc_auc_score(y, ypred)\nprint ('Null model AUC: %f' %null_auc)\n", "intent": "<h3>Null Model</h3>\n"}
{"snippet": "distress_y = np.count_nonzero(y)\nprint 'Number of people who have financial distress: %d' %distress_y\nnull_ypred = np.ones(120269)\nnull_score = 1 - 8357/120269.0\nprint ('Null model accuracy: %f' %null_score)\nnull_auc = metrics.roc_auc_score(y, null_ypred)\nprint ('Null model AUC: %f' %null_auc)\n", "intent": "<h3>Null Model</h3>\n"}
{"snippet": "distress_y = np.count_nonzero(y)\nprint 'Number of people who have financial distress: %d' %distress_y\nnull_ypred = np.ones(120269)\nnull_score = 1 - 8357/120269.0\nprint ('Null model accuracy: %f' %null_score)\nnull_auc = metrics.roc_auc_score(y, null_ypred)\nprint ('Null model AUC: %f' %null_auc)\n", "intent": "<h2>Null Model</h2>\n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "The liner regression model is not satifying. So we try the decision tree model on the training dataset.\n"}
{"snippet": "train_predict  = train_fit.predict(train_data)\n", "intent": "Calculate the predictions, $\\hat{y} = round(\\hat{p})$\n"}
{"snippet": "train_proba    = train_fit.predict_proba(train_data)\n", "intent": "The probabilities are returned by the `predict_proba` method.\n"}
{"snippet": "print(\"Logistic\", classification_report(college_label_median, y_train_pred))\nprint(\"Random Forest\", classification_report(college_label_median, pred_rf))\nprint(\"SVM\", classification_report(college_label_median, pred_svm))\n", "intent": "In this part, we are going to compare three models we've built to find the best one.\n"}
{"snippet": "test_pred = rf_clf.predict(college_test)\n", "intent": "The final step is to evaluate the model on the testing data.\n"}
{"snippet": "y_train_pred = cross_val_predict(clone_clf, college, college_label_4)\nconfusion_matrix(college_label_4,y_train_pred)\n", "intent": "Then we use confusion matrix to evaluate the performance of the model.\n"}
{"snippet": "pred = cross_val_predict(rf_clf, college, college_label_4, cv=3)\n", "intent": "Then we also use cross-validation to see how well this model performs.\n"}
{"snippet": "y_scores = cross_val_predict(rf_clf, college, college_label_4, cv=3, method = 'predict_proba')\n", "intent": "Then we plot ROC curve for each class, micro-average ROC curve and macro-average ROC curve.\n"}
{"snippet": "pred_rf_reduced = cross_val_predict(rf_clf, college_reduced, college_label_median, cv=3)\npred_rf = cross_val_predict(rf_clf, college, college_label_median, cv=3)\n", "intent": "We would like to evaluate th model performance on training data.\n"}
{"snippet": "print('Random Forest with 13 PC\\n' + classification_report(college_label_median, pred_rf_reduced))\nprint('Random Forest without PCA\\n' + classification_report(college_label_median, pred_rf)) \n", "intent": "Then we compare the Random Forest model with 13 Principal Components with the model without PCA.\n"}
{"snippet": "clf_pred = cross_val_predict(clf, college_centered, college_label_median)\n", "intent": "** Performance on training set **\n"}
{"snippet": "clf_pred_test = clf.predict(college_test_centered)\n", "intent": "** Performance on test set **\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = []\n    for layer, target, weight in zip(style_layers, style_targets, style_weights):\n        style_loss.append(weight * tf.reduce_sum(tf.squared_difference(target, gram_matrix(feats[layer]))))\n    return tf.reduce_sum(style_loss) \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "elastic_predict = model_elas.predict(X_train)\nget_rmse(elastic_predict, y)\n", "intent": "Here we provided a range of alpha and l1_ratio, let's see the result.\n"}
{"snippet": "pred_proba = model.predict_proba(X_test)\npred_proba[:5]\n", "intent": "Predict the class probabilities for the *Test* set\n"}
{"snippet": "print 'Accuracy: ',accuracy_score(y_test,pred)\nprint 'F1: ',f1_score(y_test,pred)\nprint 'Precision: ',precision_score(y_test,pred)\nprint 'Recall: ',recall_score(y_test,pred)\nprint 'AUROC: ',roc_auc_score(y_test,pred)\n", "intent": "Evaluate the *Test* set\n"}
{"snippet": "score = cross_val_score(clf,X,y, cv=8)\nprint 'Average accuracy: ',np.mean(score)\n", "intent": "Cross validate the test set\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint classification_report(pred,y_test)\n", "intent": "Check the Classification Report\n"}
{"snippet": "print 'Best accuracy: ',model.best_score_\nprint 'Best parameters: ',model.best_params_\npred = model.best_estimator_.predict(X_test)\nprint 'Accuracy: ',accuracy_score(y_test,pred)\nprint 'F1: ',f1_score(y_test,pred)\nprint 'Precision: ',precision_score(y_test,pred)\nprint 'Recall: ',recall_score(y_test,pred)\nprint 'AUROC: ',roc_auc_score(y_test,pred)\n", "intent": "The following shows the best parameters and best score\n"}
{"snippet": "pred = model.best_estimator_.predict(X_test)\nprint 'Accuracy: ',accuracy_score(y_test,pred)\nprint 'F1: ',f1_score(y_test,pred)\nprint 'Precision: ',precision_score(y_test,pred)\nprint 'Recall: ',recall_score(y_test,pred)\nprint 'AUROC: ',roc_auc_score(y_test,pred)\n", "intent": "Fit a new kNN model with the optimal parameters found in gridsearch. \n"}
{"snippet": "pred = model.best_estimator_.predict(X_test)\ncnf_matrix=confusion_matrix(y_test, pred)\nplot_confusion_matrix(cnf_matrix, classes=model.best_estimator_.classes_)\nprint classification_report(y_test,pred)\n", "intent": "The C parameter has increased to 10 which implies less regularisation\n"}
{"snippet": "y_pred = classifier.predict(x_test)\ny_pred = (y_pred > 0.5)\n", "intent": "Predict the test set results\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        G = style_targets[i]\n        A = gram_matrix(model.extract_features()[style_layers[i]])\n        style_loss += style_weights[i]*(tf.reduce_sum((G-A)**2))\n    return(style_loss)    \n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Now that the model is trained, let's evaluate it on the training set\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "Let's compute the same scores for the linear regression model\n"}
{"snippet": "scikitErrors = (regr.predict(xPoints) - yPoints)\nnativeErrors = (np.sum(theta*xPoints, axis= 1) - yPoints)\n", "intent": "Our null hyothesis being that the two error distributions come from the same bigger distribution\n"}
{"snippet": "y_predicted = grid_search.predict(docs_test)\n", "intent": "Predict the outcome on the testing set and store it in a variable named y_predicted.\n"}
{"snippet": "print(metrics.classification_report(y_test, y_predicted,\n                                    target_names=dataset.target_names))\n", "intent": "Print the classification report.\n"}
{"snippet": "X_test_reduced = pca.transform(X_test)\ny_pred = rnd_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n", "intent": "Evaluate the classifier on the test set: how does it compare to the previous classifier?*\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred=model.predict(x_test)\n", "intent": "Now let's predict using the trained model.\n"}
{"snippet": "def softmax_score(param_matrix, data):\n    return data.dot(param_matrix)\ndef softmax_function(logits):\n    return np.exp(logits) / np.sum(np.exp(logits), axis = 1, keepdims = True)\n", "intent": "We need to implement the softmax score, softmax function, cross entropy cost, and gradient descent\n"}
{"snippet": "tests = [\"meet sexy singles today who want sex sexy hot babes\", \"I was wondering what time the event started?\"]\nex_tests = count_vectorizer.transform(tests)\npredictions = clf.predict(ex_tests)\npredictions\n", "intent": "Let's test this classifier on some really basic examples\n"}
{"snippet": "model.predict(dataset, steps=30)\n", "intent": "And to predict the output of the last layer in inference for the data provided, as a `NumPy` array:\n```python\nmodel.predict(x, batch_size=32)\n```\n"}
{"snippet": "scores = model.evaluate(test_data, test_labels)\nprint(scores)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n"}
{"snippet": "print clf.best_params_\nprint\nprint metrics.classification_report(y_test,classifier.predict(X_test))\nprint \nprint metrics.accuracy_score(y_test,y_pred)\n", "intent": "Print Best Model and Metrics\n"}
{"snippet": "RF_cv = sk.cross_validation.cross_val_score(RF,pitch_career[['W','L','G','GS','CG','SHO','SV','IP','ER','IP','R','HR','BB','SO','season_count']], pitch_career.inducted)\nRF_cv.mean()\n", "intent": "Unsurprisingly, The preliminaary run putn ERA and Wins as the most important features.  Complete Games and Innings Pitched are unexpected \n"}
{"snippet": "y_test_prediction = model.predict(X_test)\ny_test_prediction.shape\n", "intent": "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:\n"}
{"snippet": " def mic(x, y):\n    m = MINE()\n    m.compute_score(x, y)\n    return (m.mic(), 0.5)\n", "intent": "$$I(X,Y)=\\sum_{x\\in X}\\sum_{y\\in Y}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$$\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ndef scoring_function(pipeline):\n    scores = cross_val_score(pipeline, features, target, scoring='f1_micro', cv=5)\n    return scores.mean(), scores.var()\n", "intent": "We define a function to run our pipeline and calculate the mean score and variance:\n"}
{"snippet": "print(\"Number of correct classifiers:\", round(accuracy_score(y_test, y_pred, normalize=False)))\n", "intent": "Return the number of correctly classified samples. \n"}
{"snippet": "prediction_input_fn = lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\npredictions = linear_regressor.predict(input_fn=prediction_input_fn)\npredictions = np.array([item['predictions'][0] for item in predictions])\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nroot_mean_squared_error = math.sqrt(mean_squared_error)\nprint('Mean Squared Error (on training data): %0.3f' % mean_squared_error)\nprint('Root Mean Squared Error (on training data): %0.3f' % root_mean_squared_error)\n", "intent": "Let's make predictions on that training data, to see how well our model fit it during training.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ntrain_pred = [tv_train.mean()]*len(tv_train)\ntrain_pred\ntrain_rmse = mean_squared_error(tv_train, train_pred)**0.5\nprint(\"RMSE on the Training Set is: \" + str(train_rmse) + \"\\n\")\ntest_pred = [tv_train.mean()]*len(tv_test) \ntest_pred\ntest_rmse = mean_squared_error(tv_test, test_pred)**0.5\nprint(\"RMSE on the Test Set is: \" + str(test_rmse) + \"\\n\")\n", "intent": "Now we can calculate the RMSE for the raw averages:\n"}
{"snippet": "from sklearn import metrics\nmetrics.r2_score(y_test,prediction)\n", "intent": "There is no linear relationship\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_y, predict_y, target_names=['Not Survived', 'Survived']))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$ \n$$Recall = \\frac{TP}{TP + FN}$$ \n$$F1 = \\frac{2TP}{2TP + FP + FN}$$ \n"}
{"snippet": "y_pred = clf.predict(X_test)\ny_pred\n", "intent": "Lastly, we predict the new labels, based on the words in the test set.\n"}
{"snippet": "model = xgb.train(params, dtrain, num_boost_round=999)\ndtest = xgb.DMatrix(X_test)\npredictions = model.predict(dtest)\n", "intent": "Lastly, with the final set of parameters we are able to make predictions and then submit the results on Kaggle to see our score.\n"}
{"snippet": "print (np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = model.predict(X_test_pca)\nprint (\"Overall prediction accuracy:\", model.score(X_test_pca, y_test))\nprint \nprint (classification_report(y_test, y_pred, target_names=target_names))\nprint (confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n", "intent": "Let's evaluate this 'best' model.\n"}
{"snippet": "result = model.predict(data, batch_size=32)\nprint(result.shape)\n", "intent": "And to *predict* the output of the last layer in inference for the data provided,\nas a NumPy array:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, explained_variance_score\npred_sqrt = model_competitions.predict(start=0, end=comps.index.get_loc(max(comps.index)))\npred = np.square(pred_sqrt)\nprint('Mean error:', (pred - comps).mean())\nprint('Mean absolute error:', mean_absolute_error(comps, pred))\nprint('Explained variance score:', explained_variance_score(comps, pred))\n", "intent": "Finally, here are some relevant error metrics for the model applied to the existing data:\n"}
{"snippet": "forecast_set = clf.predict(X_lately)\ndf['Forecast'] = np.nan\n", "intent": " X_lately variable contains the most recent features, which we're going to predict against.\n"}
{"snippet": "def squared_error(ys_orig,ys_line):\n    return sum((ys_line - ys_orig) * (ys_line - ys_orig))\ndef coefficient_of_determination(ys_orig,ys_line):\n    y_mean_line = [mean(ys_orig) for y in ys_orig]\n    squared_error_regr = squared_error(ys_orig, ys_line)\n    squared_error_y_mean = squared_error(ys_orig, y_mean_line)\n    return 1 - (squared_error_regr/squared_error_y_mean)\n", "intent": "11 Regression - How to Program R Squared\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nCV_score = cross_val_score(ridge, X, y, cv=10, scoring='neg_mean_absolute_error').mean()\nCV_score_root = cross_val_score(ridge_root, X1, y1, cv=10, scoring='neg_mean_absolute_error').mean()\nprint('CV_score=',CV_score,'and','CV_score_root=',CV_score_root)\n", "intent": "Compare the cross validation scores with and without square rooted num_voted_users\n"}
{"snippet": "score = Xception_model.evaluate(test_Xception, test_targets, verbose=0) \nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nRMSE = mean_squared_error(y_test,price_predictions)**0.5\nprint RMSE\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "lm.predict({'per_capita_income':[11550]}) \n", "intent": "The predict function will go the other way and give us our estimated home value for any income:\n"}
{"snippet": "labels = bestModel.predict(training_data)\nimgLabels = np.array(labels.collect()).reshape(2,76,87)\ndraw_image(imgLabels[0,:,:])\n", "intent": "<div class=\"alert alert-info\">\nComplete the source code below to visualize the result of clustering.\n</div>\n"}
{"snippet": "def total_variation_loss(x):\n    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return backend.sum(backend.pow(a + b, 1.25))\nloss += total_variation_weight * total_variation_loss(combination_image)\n", "intent": "Instead of just using style and content loss, we add a total variation loss to regularize content and style loss to encourage spatial smoothness\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X)) ** 2))\nprint(np.sum((bos.PRICE - lm.predict(X)) ** 2) / len(bos.PRICE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Xd, yd = squtils.mk_tagging_data(feature_encoder, size_f, \n                                 mk_data_generator(subset='dev'))\npd = classifier.predict(Xd)\nprint(sklearn.metrics.accuracy_score(yd, pd))\n", "intent": "Get the development data and evaluate its classification results.\n"}
{"snippet": "rss = np.mean((y-lReg.predict(X))**2)\nrss  \n", "intent": "**Checking RSS -** $$ mean([y-f(x)]^2) $$\n"}
{"snippet": "from sklearn import metrics \nprint(\"Mean Abs Error\", metrics.mean_absolute_error(y_test, y_predict))\nprint(\"Sqred Abs Error\", metrics.mean_squared_error(y_test, y_predict))\nprint(\"\", lReg.score(X_test, y_test))\n", "intent": "$$ ACC = \\frac{TP + TN}{TP + TN + FP + FN} $$\n$$ P = \\frac{TP}{TP + FP} $$\n$$ R = \\frac{TP}{TP + FN} $$\n"}
{"snippet": "print(accuracy_score(y_test, y_pred))\n", "intent": "**Accuracy**  \nAccuracy measures a fraction of the classifier's predictions that are correct.  \n"}
{"snippet": "def xVal_score(clf, X, y, K):\n    cv = KFold(n_splits=2)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print(scores)\n    print(\"Accuracy Mean : %0.3f\" %np.mean(scores))\n    print(\"Std : \", np.std(scores))\n    print(\"Standard Err : +/- {0:0.6f} \".format(sem(scores)))\n", "intent": "    cross_val_score uses the KFold or StratifiedKFold strategies by default    \n"}
{"snippet": "def xVal_score(clf, X, y, K):\n    cv = KFold(n=X.shape[0], n_folds=K, shuffle=True, random_state=True)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print(scores)\n    print(\"Accuracy Mean : %0.3f\" %np.mean(scores))\n    print(\"Std : \", np.std(scores)*2)\n    print(\"Standard Err : +/- {0:0.3f} \".format(sem(scores)))\n", "intent": "    cross_val_score uses the KFold or StratifiedKFold strategies by default    \n"}
{"snippet": "Dog_predictions = [np.argmax(Dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Dog_predictions)==np.argmax(test_targets, axis=1))/len(Dog_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.predict_proba(np.array([5])), model.predict_proba(np.array([35]))\n", "intent": "Just for fun, let's predict the probability of winning the point given a 5 second pull time and a 35 second pull time. \n"}
{"snippet": "cv_score(clf, Xtestlr, ytestlr)\n", "intent": "The GridSearchCV tool gave us the same best value of C.\n"}
{"snippet": "pred_y = linreg.predict(test_x)\nsklearn.metrics.mean_squared_error(test_y, pred_y)\n", "intent": "Evaluate the performance\n"}
{"snippet": "r = 0\nfor i in range(len(X)):\n    if (clf2.predict(X)[i] == y[i]): r+=1\nprint(r*100/len(y))\n", "intent": "Now call the ``predict`` method, and find the classification of ``X_new``.\n"}
{"snippet": "from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:, i])\n    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n", "intent": "Compute Precision-Recall and plot curve\n"}
{"snippet": "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_score.ravel())\naverage_precision[\"micro\"] = average_precision_score(y_test, y_score, average=\"micro\")\n", "intent": "Compute micro-average ROC curve and ROC area\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error([np.mean(y_test)]*len(y_test), y_test))\n", "intent": "Good, already did this\n"}
{"snippet": "print(mean_squared_error(y_test_predict, y_test))\n", "intent": "Evaluate our prediction.\n"}
{"snippet": "mean_absolute_error(y_tr,GPI(X_train_scaled)) \n", "intent": "We prepare the model.\n"}
{"snippet": "preds = clf.predict(valid_X)\n", "intent": "We predict the validation set.\n"}
{"snippet": "print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))\n", "intent": "Let's see also the classification report for the validation set.\n"}
{"snippet": "print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))\n", "intent": "Let's also plot the classification report for the validation set.\n"}
{"snippet": "print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))\n", "intent": "The classification report for the validation set.\n"}
{"snippet": "test_X = test_df[predictors]\npred_Y = clf.predict(test_X)\n", "intent": "<a href=\"\nFirst, we predict for test data using the trained model.\n"}
{"snippet": "preds = gs_clf.predict(valid_X)\n", "intent": "Let's predict with the validation data.\n"}
{"snippet": "print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))\n", "intent": "Let's check the validation classification report.\n"}
{"snippet": "test_X = test_df[predictors]\npred_Y = gs_clf.predict(test_X)\n", "intent": "First, we predict for test data using the trained model.\n"}
{"snippet": "predictions = xgbm.predict(x_test)\n", "intent": "With the fitted second level model we do the prediction for the test data.\n"}
{"snippet": "predicted = model2.predict(X_test)\nprobs = model2.predict_proba(X_test)\n", "intent": "Obtain list of predicted values and predicted probabilities\n"}
{"snippet": "import sklearn.metrics\nprint sklearn.metrics.r2_score(target,linear_titanic_model_1.predict(features))\n", "intent": "Let's use another component of sklearn to check the model performance = sklearn.metrics\n"}
{"snippet": "cluster = kmeans.fit_predict(tDfsubset)\n", "intent": "Now let's fit the data\n"}
{"snippet": "dbcluster = dbscan.fit_predict(tDfsubset) \n", "intent": "Fit predict the cluster for each observation\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, rf.predict(X_test)))\n", "intent": "We construct confusion matrix to visualize predictions made by a classifier and evaluate the accuracy of a classification.\n"}
{"snippet": "accuracy_score(Y,np.repeat(0,Y.shape))  \n", "intent": "Based on frequency counts, note that classes are unbalanced. So if we classifiy all wines as 0 we still can still get accuracy of 86%. \n"}
{"snippet": "result.predict(np.array([[1,38,5],[1,38,6]]))\n", "intent": "**Predict using statsmodels **\n"}
{"snippet": "model_balanced.evaluate(test_balanced)\n", "intent": "The validation accuracy has decreased. Recall has increased.\nWe should investigate via cross validation\n"}
{"snippet": "final_model_predictions = [np.argmax(final_model.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(final_model_predictions)==np.argmax(test_targets, axis=1))/len(final_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Resnet50_breed_predictions = [np.argmax(Resnet50_breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_breed_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = list()\nfor x in test_X:\n    yhat = model_persistence(x)\n    predictions.append(yhat)\ntest_score = mean_squared_error(test_y, predictions)\nprint('Test MSE: %.3f' % test_score)\n", "intent": "walk-forward validation\n"}
{"snippet": "print(model.predict(np.expand_dims(x_train[0], axis=0)))\n", "intent": "Let's use this model to predict a value for the first training instance we vizualized.\n"}
{"snippet": "evaluation_metrics = linear_classifier.evaluate(predict_validation_input_fn)\nprint(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\nprint(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])\n", "intent": "**Calculate accuracy and plot ROC curve**\n"}
{"snippet": "ensemb=4 \npred_test=[] \npred_test_final=np.zeros((test_selected_x.shape[0],1)) \nfor x in range(ensemb):\n     pred_test.append(model_nn_final(train_selected_x, train_selected_y, test_selected_x, test_selected_y, x, opt_result.best_params_[\"clf__dropout_rate\"]))\n     pred_test_final=pred_test[x]+pred_test_final\npred_test_final=pred_test_final/ensemb        \nauc_test_final = roc_auc_score(test_selected_y, pred_test_final)\nprint(auc_test_final)\n", "intent": "This will also save all the ensembled average models and weight matrix.\n"}
{"snippet": "y_subreddit = gs_subreddit.predict_proba(X)[:,1]\ny_body = gs_body.predict_proba(X_test)[:,1]\nX_combined = [[y1, y2] for y1, y2 in zip(y_subreddit, y_body)]\nX_combined = np.array(X_combined)\nX_combined = scaler.transform(X_combined)\npredictions = gs.predict_proba(X_combined)[:,1]\n", "intent": "We prepare the final solution:\n"}
{"snippet": "dataset = np.array([[1,1],[2,3],[4,3],[3,2],[5,5],[6,6]])\nX = dataset[:, 0].reshape(-1,1) \ny = dataset[:, 1].reshape(-1,1)\ndef predict(dataset):\n    b0 = 0.5\n    b1 = 0.8\n    prediction = b0 + b1*(dataset[:, 0].reshape(-1,1))\n    return prediction\npredict(dataset)\n", "intent": "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6,6]]\ncoef = [0.5, 0.8]\n"}
{"snippet": "print( sentiment_model.predict( sample_test_matrix ) )\n", "intent": "    Based on analysis of the sample_test_data the model works! Review 11 is the most positive and has the highest score.\n"}
{"snippet": "correctly_classified = 0\ntest_sentiments_predictions = sentiment_model.predict( test_matrix )\nfor i in range( len( test_data ) ):\n    if test_data.loc[ i, 'sentiment' ] == test_sentiments_predictions[ i ]:\n        correctly_classified += 1\nprint( \"%.2f\" % ( correctly_classified / len( train_data ) ) )\n", "intent": "In order to calcualte accuracy I will use the following formula:\n$$\naccuracy = \\frac{\\text{\n$$\n"}
{"snippet": "predictions = knn.predict(X_test)\nprint(\"Predictions are: \",'\\n'*2, predictions)\n", "intent": "**Predicting values with the 'predict' method using your KNN model and X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "print(\"With K = 31\",'\\n')\nprint(confusion_matrix(y_test,predictions),'\\n')\nprint(classification_report(y_test,predictions))\n", "intent": "**Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.**\n"}
{"snippet": "r_squared = sklearn.metrics.r2_score(y, y_pred)\nr_squared\n", "intent": "$R^2$ is ~70%. This is not \"amazing\" by any means, but it's a respectable tally for such a simple attempt.\n"}
{"snippet": "train_predict(clf,X_train[:100],y_train[:100],X_test,y_test)\n", "intent": "* data set size = 100\n"}
{"snippet": "train_predict(clf,X_train[:200],y_train[:200],X_test,y_test)\n", "intent": "* data set size = 200\n"}
{"snippet": "train_predict(clf,X_train[:300],y_train[:300],X_test,y_test)\n", "intent": "* data set size = 300\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test[:,1], rounded))\n", "intent": "This report gives us information on precision, recall, etc.:\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test[:, 1], lr_scores)\n", "intent": "Let's see how well the logistic regression did:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    out = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        idx = style_layers[i]\n        out += style_weights[i] * tf.reduce_sum(np.square(gram_matrix(feats[idx]) - style_targets[i]))\n    return out\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    pass\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "pred_prob = N.predict_proba(X_test)\npred = N.predict(X_test)\n", "intent": "Now you can predict the probabilities that data points belong to each category using the zero-one cost function:\n"}
{"snippet": "print(\"\\n\\n ************ START Model Prediction **************  \\n\")\nimages_to_plot = []\nprint(\"==============> WITH THE test dataset\")\nimage_to_predict = (test_X_list[0])[np.newaxis]\nprint(\"test dataset image shape : \", image_to_predict.shape)\nprint(\"Label Prediction on the test dataset image : \", model.predict(image_to_predict))\nprint(\"True Label Value : \", test_Y_list[0])\nimages_to_plot.append(test_X_list[0])\n", "intent": "<a name=\"prediction_on_Model\"></a>\n<a name=\"prediction_on_dataset\"></a>\n"}
{"snippet": "model = DNN(net)\nmodel.load('model.tflearn')\nmodel.predict(image_to_predict)\n", "intent": "<a name=\"Generate_labels_cVAE\"></a>\n"}
{"snippet": "report = sk.metrics.classification_report(df.true_lang_id, df.predicted_lang_id, target_names=dict_index)\noverall_accuracy = metrics.accuracy_score(df.true_lang_id, \n                                df.predicted_lang_id,)\ndf_report = classifaction_report(report)\ndf_report = df_report.merge(df_tested_lang, left_on='lang', right_on='Wiki').drop(['Wiki','Language'], axis=1).set_index('lang')\ndf_report.head(10)\n", "intent": " Lets see accuracy and true labels where give to each language\n"}
{"snippet": "predictions = pipeline.predict(testing_data.text.values)\nprint('Total reviews classified:', len(training_data))\nprint('Score:', f1_score(testing_data.michelin_rated, predictions, pos_label=0, average='weighted', labels=[0,1]))\nprint('Confusion matrix:\\n', confusion_matrix(testing_data.michelin_rated, predictions))\n", "intent": "And checking the performance\n"}
{"snippet": "cross_val_score(grid, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Cross validation is an efficient measure of accuracy:\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_classifier, X_train, y_train, cv=3)\nconf_matrix = confusion_matrix(y_train, y_train_pred)\nconf_matrix\n", "intent": "Cofusion matrices are awesome!\n"}
{"snippet": "from jupytalk.benchmark import timeexec\nmeasures_dl += [timeexec(\"keras.mobilenet\", \"model.predict(array_images[0])\",\n                         context=globals(), repeat=3, number=10)]\n", "intent": "Let's measure time.\n"}
{"snippet": "prep = p.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = t.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from pandas import datetime\nstartyear=datetime(2009,1,1)\nendyear=datetime(2012,1,1)\npredict=results.predict(start=startyear,end=endyear)\npredict\n", "intent": "8\\. Compute prediction for years 2009-2012 and analyze their fit against actual values. (1 point)\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_score = cross_val_score(svc, fmri_masked, conditions)\nprint(cv_score)\n", "intent": "Cross-validation with scikit-learn\n...................................\nScikit-learn has tools to perform cross-validation easier:\n"}
{"snippet": "from sklearn.cross_validation import LeaveOneLabelOut, cross_val_score\ncv = LeaveOneLabelOut(session)\ncv_scores = cross_val_score(anova_svc, X, conditions)\nclassification_accuracy = cv_scores.mean()\nprint(\"Classification accuracy: %.4f / Chance level: %f\" %\n      (classification_accuracy, 1. / len(np.unique(conditions))))\n", "intent": "Obtain prediction scores via cross validation\n-----------------------------------------------\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngrid = GridSearchCV(anova_svc, param_grid={'anova__k': k_range}, verbose=1)\nnested_cv_scores = cross_val_score(grid, X, y)\nprint(\"Nested CV score: %.4f\" % np.mean(nested_cv_scores))\n", "intent": "Nested cross-validation\n-------------------------\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncv_scores_ovo = cross_val_score(svc_ovo, X, y, cv=5, verbose=1)\ncv_scores_ova = cross_val_score(svc_ova, X, y, cv=5, verbose=1)\nprint('OvO:', cv_scores_ovo.mean())\nprint('OvA:', cv_scores_ova.mean())\n", "intent": "Now we compute cross-validation scores\n----------------------------------------\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = [0.0]\n    for i in range(0, len(style_layers)):\n        gram_p = gram_matrix(feats[style_layers[i]])\n        style_loss += content_loss(style_weights[i], gram_p, style_targets[i])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predicted_gbm = clf_GradBoost.predict(X_test)                 \ncf_test_gbm = mt.confusion_matrix(y_test, predicted_gbm)\nprint 'Confusion Matrix'\nprint cf_test_gbm\nprint ('Precision = %0.4f' % mt.precision_score(y_test, predicted_gbm))\nprint ('Recall = %0.4f' % mt.recall_score(y_test, predicted_gbm))\nprint ('Accuracy = %0.4f' % mt.accuracy_score(y_test, predicted_gbm))\n", "intent": "As Random Forest, we assess the variable importances and find the same first two variables: var38 and var15.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "test_preds = np.argmax(model.predict([user_id_test, item_id_test, item_meta_test]),axis=1)\nprint(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\nprint(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))\n", "intent": "The network seems to overfit way more than before. Let us compute some more undersantable metrics:\n"}
{"snippet": "y_llda_predicted = llda.predict(enlarged_x)\nprint(\"The class values associated to the points are \"+str(y_llda_predicted) )\n", "intent": "Now let us make the prediction using the model. Determine whether the prediction is correct. \n"}
{"snippet": "prediction = cross_val_predict(lr,breast_cancer_data.data, breast_cancer_data.target,cv=5)\n", "intent": "We estimate the classifier accuracy using k-fold cross-validation with k=5. The result of cross-validation will be the predictions for all instances\n"}
{"snippet": "mpl_fashion_prediction = mpl_clf.predict(X_fashion_test)\n", "intent": "The model is used to compute the predictions on the test data. \n"}
{"snippet": "dt_test_predictions = dt.predict(test_data)\nlda_test_predictions = lda.predict(test_data)\nlg_test_predictions = lg.predict(test_data)\n", "intent": "We predict the class of the samples in the test data with the three classifiers.\n"}
{"snippet": "dt_acc =  accuracy_score(test_class,dt_test_predictions)\nlda_acc =  accuracy_score(test_class,lda_test_predictions)\nlg_acc =  accuracy_score(test_class,lg_test_predictions)\nprint(\"Accuracy for the decision tree :\",dt_acc)\nprint(\"Accuracy for LDA :\",lda_acc)\nprint(\"Accuracy for logistic regression:\",lg_acc)\n", "intent": "Finally, we compute the accuracy using the three classifiers and print it. \n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images. If it's working, we should be able to get an accuracy better than random (>1%).\n"}
{"snippet": "def predict(X_data, y_data):\n    rate = 0.003\n    sess = tf.get_default_session()\n    predict_operation = tf.argmax(logits, 1)\n    num_example = len(y_data)\n    prediction = sess.run(predict_operation, feed_dict={cons_soft_holder: 0, x: X_data, y: y_data,dropprob_holder: 1.0,is_training: False,rate_holder:rate})\n    return  prediction\n", "intent": "**Define prediction function**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Now we predict the test data using the above created classifier.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "The accuracy score of the model can be given as;\n"}
{"snippet": "precision_score(y_test, y_pred, average='macro')\n", "intent": "The precision score of the model is given as\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['T', 'F']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n", "intent": "Below is a detailed classification report for the model. This is achieved using scikit-learn.\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred)\n", "intent": "The accuracy it gets is pretty high,\neven though it's the \"fold\"\nthat maximizes the F1 score (micro-average):\n"}
{"snippet": "print(metrics.classification_report(y_test, rf_y_pred,\n                                    labels=y_counts[y_counts >= 10].index))\n", "intent": "Below are the same statistics created for the LSA model ($k=200$),\nfor comparison.\n"}
{"snippet": "print(f\"{sum(rf_model.predict(X_train) != y_train)} out of {len(y_train)}\")\n", "intent": "That's all about the testing set.\nHow many training examples are misclassified?\n"}
{"snippet": "lasso_model_mse = mean_squared_error(y_predict, y_test)\nmath.sqrt(lasso_model_mse)\n", "intent": "Compares to linear regression rmse of _5108_\n"}
{"snippet": "ridge_model_mse = mean_squared_error(y_predict, y_test)\nmath.sqrt(ridge_model_mse)\n", "intent": "Compares to linear regression rmse of _5108_\n"}
{"snippet": "predicted = clf_svc_pipeline.predict(twenty_test.data)\n", "intent": "Pass the test data through the transformation pipeline\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc_svm = accuracy_score(twenty_test.target, predicted)\nacc_svm\n", "intent": "Get the prediction accurancy, how many predicted are equal to the actual\n"}
{"snippet": "y_pred_svm = clf_svm.predict(X_test)\nacc_svm = accuracy_score(y_test, y_pred_svm)\n", "intent": "Evaluate the model trained without grid search\n"}
{"snippet": "y_pred_best_svm = best_svc.predict(X_test)\nacc_best_svm = accuracy_score(y_test, y_pred_best_svm)\n", "intent": "Evaluate the grid searched model\n"}
{"snippet": "xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\nscores = cross_val_score(xgb_model, X, y, scoring=\"neg_mean_squared_error\", cv=5)\ndisplay_scores(np.sqrt(-scores))\n", "intent": "Cross-validation using `cross_val_score`\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint('{:20s} Recall\\tPrecision\\tAUC'.format('Method'))\nfor method in doub_scores:\n    recall = (doub_calls['Demuxlet'] & doub_calls[method]).sum() / doub_calls['Demuxlet'].sum()\n    precision = (doub_calls['Demuxlet'] & doub_calls[method]).sum() / doub_calls[method].sum()\n    roc = roc_auc_score(doub_calls['Demuxlet'], doub_scores[method])\n    print('{:20s} {:.3f}\\t{:.3f}\\t\\t{:.3f}'.format(method, recall, precision, roc))\n", "intent": "Compute some metrics to compare predictions to ground truth.  \nFor this dataset, log transformation gives better recall without hurting precision.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint('{:20s}  Recall   Precision   ROC'.format('Method'))\nfor method in doub_scores:\n    recall = sum(doub_calls['Demuxlet'] & doub_calls[method]) / sum(doub_calls['Demuxlet'])\n    precision = sum(doub_calls['Demuxlet'] & doub_calls[method]) / sum(doub_calls[method])\n    roc = roc_auc_score(doub_calls['Demuxlet'], doub_scores[method])\n    print('{:20s}  {:.3f}    {:.3f}       {:.3f}'.format(method, recall, precision, roc))\n", "intent": "Compute some metrics to compare predictions to ground truth.  \nFor this dataset, log normalization improves recall without hurting precision.\n"}
{"snippet": "def compute_rmse(actual, predicted):\n  return np.sqrt(np.mean((actual-predicted)**2))\ndef print_rmse(model):\n  print \"Train RMSE = {0}\".format(compute_rmse(df_train[TARGET_COL], model.predict(df_train.iloc[:,FEATURE_COLS].values)))\n  print \"Valid RMSE = {0}\".format(compute_rmse(df_valid[TARGET_COL], model.predict(df_valid.iloc[:,FEATURE_COLS].values)))\nprint_rmse(model)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "Y = T.tensor3('target')  \ndef l2_loss(t):\n    return T.sqr(t - Y).sum() / 2\n", "intent": "Now we have our network, we need to set up the objective function. Here, we simply use L-2 loss as the objective.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Let's check the accuracy of the k-means:\n"}
{"snippet": "p = Y.value_counts()*1.0 / X.shape[0]\nN = X.shape[0]\npredictions = np.zeros((N,2))\nfor i in xrange(N):\n    predictions[i,:] = p\na = accuracy_score(Y, np.argmax(predictions,axis=1))\nll = log_loss(Y, predictions)\nauc = roc_auc_score(np.c_[Y==0,Y==1], predictions)\nprint 'Log-loss: {ll} Accuracy: {acc} AUC:{auc}'.format(acc=a,ll=ll,auc=auc)\n", "intent": "Use frequency of outcome variable for prediction. \n"}
{"snippet": "new_prediction = classifier.predict(\n    sc.transform(np.array(ex_user.values)))\nnew_prediction_tf = (new_prediction > 0.5)\nprint('Customer will retain:', new_prediction_tf, new_prediction)\n", "intent": "So, with the following features, the user will not retain after 6 months at 10.38% chance of retaining.\n"}
{"snippet": "def D_loss(x,g_x):\n    with tf.variable_scope(\"x_probs\") as scope: \n        D_x = Discriminator_probs(x,0.5)\n    with tf.variable_scope(\"Gx_probs\") as scope: \n        D_g_x = Discriminator_probs(g_x,0.5)\n    x_loss = -tf.reduce_sum(tf.log(D_x[:,0]),axis=0)\n    g_x_loss = -tf.reduce_sum(tf.log(D_x[:,1]),axis=0)\n    return x_loss+g_x_loss\n", "intent": "(2) Discriminator loss\n"}
{"snippet": "def get_optimal_n_estimators(model, X_new, y_new): \n    validation_loss = np.zeros(model.get_params()['n_estimators'])\n    for i , preds in enumerate(model.staged_predict(X_new)):\n        validation_loss[i] = mean_squared_error(y_new, preds)\n    optimal_tree = np.argmin(validation_loss)\n    optimal_loss = validation_loss[optimal_tree]\n    return optimal_tree, optimal_loss\n", "intent": "To finish the model I will pair back the number of trees in the model using the staged_predict method to minimize test error and combat overfitting.\n"}
{"snippet": "test_score(pipeline_tfidf_ngram_range_min_df_stopwords[2])\n", "intent": "The best model is still the one in the previous task, which is using LogisticRegression with L2 penalty.\n"}
{"snippet": "x_test = np.array(['my love'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint rmse(y_test, y_pred)\n", "intent": "The predictions can be obtained as follows:\n"}
{"snippet": "rmse(y_test, tree.predict(X_test))\n", "intent": "Check the error on the test set:\n"}
{"snippet": "pred_test = estimator.predict(x_new)\n", "intent": "predictions are risk scores on an arbitrary scale, which means you can \nusually only determine the sequence of events, but not their exact time.\n"}
{"snippet": "del cnn\ncnn2 = ConvNN(random_seed=123)\ncnn2.load(epoch=20, path='./tflayers-model/')\nprint(cnn2.predict(X_test_centered[:10, :]))\n", "intent": "After the training is finished, the model can be used to do prediction on the test dataset, as follows:\n"}
{"snippet": "preds = cnn2.predict(X_test_centered)\nprint('Test Accuracy: %2f%%' % (100*np.sum(y_test==preds)/len(y_test)))\n", "intent": "Finally, we can measure the accuracy of the test dataset as follows:\n"}
{"snippet": "gs = GridSearchCV(estimator=pipe_svc, \n                  param_grid=param_grid, \n                  scoring='accuracy', cv=2)\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), \n                                      np.std(scores)))\n", "intent": "In scikit-learn, we can perform nested cross-validation as follows: \n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n", "intent": "Those scoring metrics are all implemented in scikit-learn and can be imported from the *scikit-metrics* module as shown in the following snippet: \n"}
{"snippet": "mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\nclf_labels += ['Majority voting']\nall_clf = [pipe1, clf2, pipe3, mv_clf]\nfor clf, label in zip(all_clf, clf_labels):\n    scores = cross_val_score(estimator=clf, \n                             X=X_train, y=y_train, \n                             cv=10, scoring='roc_auc')\n    print('Accuracy: %0.2f (+/- %0.2f) [%s]'\n          % (scores.mean(), scores.std(), label))\n", "intent": "Now let's move on to the more exciting part and combine the individual classifiers for majority rule voting in our *MajorityVoteClassifier*: \n"}
{"snippet": "test_predictions_subset = simple_model.predict(test_matrix_word_subset)\naccuracy = accuracy_score(test_amazon_baby['sentiment'], test_predictions_subset)\nprint('The accuracy for the simple model on the test data is %s.' % accuracy)\n", "intent": "It is the sentiment_model that has the higher accuracy on the training set.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_fold, shuffle=True) \n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf)  \n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "def muffin_or_cupcake(flour, sugar):\n    if(model.predict([[flour, sugar]]))>=0:\n        print('You\\'re looking at a muffin recipe!')\n    else:\n        print('You\\'re looking at a cupcake recipe!')\n", "intent": "__Step 5:__ Predict New Case\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\nfrom scipy.stats import sem\ndef evaluate_cross_validation(clf, X, y, K):\n    cv = KFold(len(y), K, shuffle=True, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=cv)\n    print (scores)\n    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n        np.mean(scores), sem(scores)))\n", "intent": "Perform 5-fold cross-validation\n"}
{"snippet": "new_measurement = [[5.4,4.3,1.3,1]]\nnew_label = km.predict(new_measurement)\nprint(new_label)\n", "intent": "* When we train the model Kmeans remembers the centroids of the cluster.\n* Thus, new points can be easily and quickly assigned to existing clusters\n"}
{"snippet": "new = np.array([[6.8,3.2,5.7,2.4],\n               [6.7,3.0,5.6,2.4],\n               [5.0,3.3,1.5,0.2],\n               [5.3,3.4,1.4,0.1]])\ntarget_variables = knn.predict(new)\nfor i,j in zip(new,target_variables):\n    print (i,iris['target_names'][j])\n", "intent": "<p class=\"grey\"> Supervised Learning With Scikit </p>\n<hr class=\"red\">\n"}
{"snippet": "print (\"Dummy\")\nprint(classification_report(y_test, pred_dummy))\nprint (\"Decision Tree\")\nprint(classification_report(y_test, pred_tree))\nprint (\"Logistic Regression\")\nprint(classification_report(y_test, pred_logreg))\n", "intent": "<p class=\"grey\"> Check out Wiki link  </p>\n- - -\nhttps://en.wikipedia.org/wiki/Confusion_matrix\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(reg,X,y,cv=5)\nprint(cv_scores)\n", "intent": "<p class=\"grey\"> 5-fold, 10-fold, k-fold  </p>\n- - -\n"}
{"snippet": " def softmax_loss(x, y):\n    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    N = x.shape[0]\n    eps = 1e-8\n    loss = -np.sum(np.log(probs[np.arange(N), y] + eps)) / N\n    dx = probs.copy()\n    dx[np.arange(N), y] -= 1\n    dx /= N\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "x_test = np.array(['Today was not a blast'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "ypred = clf_smote.predict(Xtestlr)\nprint(metrics.classification_report(ytestlr, ypred))\nprint(metrics.confusion_matrix(ytestlr, ypred))\n", "intent": "Now, let us predict and generate a classification report\n"}
{"snippet": "def predict_tags_clf1(clf1, X):\n    Y_pred = clf1.predict(X)\n    return Y_pred\n", "intent": "Function to predict tags using clf1.\n"}
{"snippet": "df_test['Loan_Status'] = model.predict(df_test[predictor_var])\n", "intent": "After running the Decision Tree algorithm, we apply the model on the training set for prediction\n"}
{"snippet": "df_test['Loan_Status_Logic'] = model.predict(df_test[predictor_var])\n", "intent": "Logic regression prediction\n"}
{"snippet": "df_test['Loan_Status_DT'] = model.predict(df_test[predictor_var])\n", "intent": "Decision tree prediction\n"}
{"snippet": "df_test['Loan_Status_RF'] = model.predict(df_test[predictor_var])\n", "intent": "Random forest prediction\n"}
{"snippet": "df_test['Loan_Status_NB'] = model.predict(df_test[predictor_var])\n", "intent": "Naive bayes prediction\n"}
{"snippet": "y_pred_train = classifier.predict(X_train)\ny_pred_test = classifier.predict(X_test)\ny_pred_test\n", "intent": "All models are wrong but some are useful...\n"}
{"snippet": "from sklearn import metrics\ncm = metrics.confusion_matrix(y_train, y_pred_train)\nprint(\"Confusion Matrix\")\nprint(\"{0}\".format(metrics.confusion_matrix(y_train, y_pred_train)))\nprint(\"\")\nprint(\"Classification Report\")\nprint(metrics.classification_report(y_train, y_pred_train))\n", "intent": "checking for over fitting by comparing trained and tested matrix\n"}
{"snippet": "model.predict([[25,4000,300,2,-1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,1,0]])\n", "intent": "So the above values mean that the probability that the person will subscribe to the term deposit is 0.03.\n"}
{"snippet": "model.predict([[40,5000,400,2,8,50,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0,0,0,1]])\n", "intent": "So the above values mean that the probability that the person will subscribe to the term deposit is 0.829\n"}
{"snippet": "print(classification_report(y_pred, y_test))\n", "intent": "Performance measures\n"}
{"snippet": "cv_scores = cross_val_score(logit, X_train_new, y_train, cv=time_split, \n                            scoring='roc_auc', n_jobs=1) \n", "intent": "**Performing time series cross-validation, we see an improvement in ROC AUC.**\n"}
{"snippet": "print(\"MSE train: {0:.4f}, test: {1:.4f}\".\\\n      format(mean_squared_error(y_train, y_train_pred), \n             mean_squared_error(y_test, y_test_pred)))\n", "intent": "printing the MSE and the R^2 values \n"}
{"snippet": "label = {0:'Sitting', 1:'Sitting down', 2:'Standing', 3:'Standing up', 4:'Walking'}\nsf['predicted_class'] = model.predict(sf)\ngl.evaluation.confusion_matrix(sf['class'].apply(lambda x: label[x]), sf['predicted_class'].apply(lambda x: label[x])).print_rows(num_rows=20)\n", "intent": "And for comparison sake, below is the confusion matrix using the model that achieved the highest accuracy (~99.6%).\n"}
{"snippet": "for clf in clfs:\n    y_train_pred = clf.predict(union[training_filter][features])\n    print(type(clf.best_estimator_) if isinstance(clf, sklearn.grid_search.GridSearchCV) else type(clf))\n    print(sklearn.metrics.classification_report(y_train_pred, union[training_filter].Survived))\n", "intent": "How well have we trained against our own training set. \n"}
{"snippet": "def evaluate(logits, labels):\n    return tf.reduce_mean(tf.cast(\n        tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), \n        tf.float32\n    ))\nwith tf.name_scope('Accuracy_SGD'):\n    accuracySGD = evaluate(modelSGD, y)\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "precision = precision_score(y_true=test_data['sentiment'], \n                            y_pred=model.predict(test_matrix))\nprint(\"Precision on test data: %s\" % precision)\nrecall = recall_score(y_true=test_data['sentiment'],\n                      y_pred=model.predict(test_matrix))\nprint(\"Recall on test data: %s\" % recall)\n", "intent": "$$\n[\\text{recall}] = \\frac{[\\text{\n$$\n"}
{"snippet": "evaluate_classification_error(small_data_decision_tree_subset_20, train_data, target)\n", "intent": "Now, let us compare the classification error of the model small_data_decision_tree_subset_20 on the entire test set `train_data`:\n"}
{"snippet": "sample_validation_features = validation_features.loc[sample_validation_data.index]\nprint(\"Prediction with ensemble method (max_depth = 6, estimators: 5):, \\n\", ensemble_classifier.predict(sample_validation_features))\n", "intent": "For each row in the **sample_validation_data**, write code to predict whether or not the loan is classified as a safe loan.\n"}
{"snippet": "print(\"ACCURACY: \", (test_data['sentiment'] == sentiment_model.predict(test_matrix)).sum()/len(test_data))\n", "intent": "Let's compute the classification accuracy of the sentiment_model on the test_data.\n"}
{"snippet": "print(\"ACCURACY (Sentiment Model, TEST DATA): \", (test_data['sentiment'] == sentiment_model.predict(test_matrix)).sum()/len(test_data))\nprint(\"ACCURACY (Simple Model, TEST DATA): \", (test_data['sentiment'] == simple_model.predict(test_matrix_word_subset)).sum()/len(test_data))\n", "intent": "Now, we will repeat this excercise on the `test_data`. Start by computing the classification accuracy of the sentiment_model on the `test_data`:\n"}
{"snippet": "model_1_rss_train = np.sum((model_1_y_train - model_1.predict(model_1_x_train))**2)\nprint(\"RSS of Model 1 (train data): %.4g\" % model_1_rss_train)\nmodel_2_rss_train = np.sum((model_2_y_train - model_2.predict(model_2_x_train))**2)\nprint(\"RSS of Model 2 (train data): %.4g\" % model_2_rss_train)\nmodel_3_rss_train = np.sum((model_3_y_train - model_3.predict(model_3_x_train))**2)\nprint(\"RSS of Model 3 (train data): %.4g\" % model_3_rss_train)\n", "intent": "First use your functions from earlier to compute the RSS on TRAINING Data for each of the three models.\n"}
{"snippet": "model_1_rss_test = np.sum((model_1_y_test - model_1.predict(model_1_x_test))**2)\nprint(\"RSS of Model 1 (test data): %.4g\" % model_1_rss_test)\nmodel_2_rss_test = np.sum((model_2_y_test - model_2.predict(model_2_x_test))**2)\nprint(\"RSS of Model 2 (test data): %.4g\" % model_2_rss_test)\nmodel_3_rss_test = np.sum((model_3_y_test - model_3.predict(model_3_x_test))**2)\nprint(\"RSS of Model 3 (test data): %.4g\" % model_3_rss_test)\n", "intent": "Now compute the RSS on on TEST data for each of the three models.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        idx = style_layers[i]\n        G = style_targets[i].view(1, -1)\n        A = gram_matrix(feats[idx]).view(1, -1)\n        layerwise_style_loss = (G - A).pow(2).sum()\n        style_loss += style_weights[i] * layerwise_style_loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = Classifier.predict(X_test)\n", "intent": "**Now predict values for the testing data**\n"}
{"snippet": "print(\"The accuracy rate for the model is: {:.3f}\".format(sklearn.metrics.accuracy_score(y_test,y_pred)))\n", "intent": "**Creating a classification report for the model**\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "**Predicting the test result**\n"}
{"snippet": "print(\"The accuracy score for this model is: {:.3f}\".format(sklearn.metrics.accuracy_score(y_test,y_pred)))\n", "intent": "** Classification metrics**\n"}
{"snippet": "print(confusion_matrix(y_pred,y_test))\nprint (classification_report(y_pred,y_test ,digits=3))\nprint(np.mean(abs(y_pred-y_test)))\n", "intent": "We can evaluate our model on the test set. Below are reported the condusion matrix, the precision/recall and F1 scores. \nThe error rate is 29%.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    dim = sess.run(tf.shape(gram), {model.image: style_img_test})\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.zeros(1, tf.float32)\n    for i in range(len(style_layers)):\n        loss += content_loss(style_weights[i], gram_matrix(feats[style_layers[i]]), style_targets[i])\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "lm.predict(X)[10:20]\n", "intent": "Here we try to run some prediction by input the tenure value and expecting Total Charges output.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    N = len(style_layers)\n    loss = 0\n    for i in range(N):\n        gram = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i]*tf.reduce_sum((gram-style_targets[i])**2)\n    return loss   \n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "model.evaluate(x_test_p,y_test)\n", "intent": "We can now evaluate on the test set\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ndef class_error(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    label_dict = {0: '0-FAIL', 1:'1-PASS'}\n    print '      ', label_dict.values()\n    for i in range(len(cm[:,0])):\n        print 'True', label_dict[i], cm[i,:]\n    return cm\ncm = class_error(y_test, y_pred)\n", "intent": " $C_{i, j}$ is equal to the number of observations known to be in group i but predicted to be in group j.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "Once the model has learned from the training data, we can make predictions on the test data using predict.\n"}
{"snippet": "result = estimator.evaluate(input_fn_train, steps=1)\nfor key,value in sorted(result.items()):\n    print('{}: {:0.2f}'.format(key, value))\n", "intent": "our results on training:\n"}
{"snippet": "probas = model.predict(x_test)\nprobas[:3]\n", "intent": "and for our held out records we also get softmax predictions, so a probability of every category for a given record\n"}
{"snippet": "y_pred = xgb_2.predict(X_test)\n", "intent": "After training a model its time to evaluate it on the test data\n"}
{"snippet": "Y = tree.predict(X)\nfill_age_df.loc[fill_age_df['AgeBand'] == -1, 'AgeBand'] = Y\n", "intent": "We can now predict the age band for passengers with missing data:\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['Bot', 'Not Bot']\nprint(classification_report(test_label, pred, target_names=target_names))\n", "intent": "Here we display the classification report using sklearn metrics\n"}
{"snippet": "sum(my_svc.predict(Xscale) == y) / len(y)\n", "intent": "What's the accuracy? Try changing the `C` parameter.\n"}
{"snippet": "y_pred = TPOT_xgb.predict(X_val)\nTPOT_xgb_RMSE = np.sqrt(mean_squared_error(y_val, y_pred))\nTPOT_xgb_CV = rmse_cv(XGBRegressor(learning_rate=0.1,max_depth=9, min_child_weight=4, n_estimators=100, nthread=1, subsample=0.88, silent=False, seed=0), X, y)\nTPOT_xgb_CV = np.average(TPOT_xgb_CV)\nprint(f'RMSE: {TPOT_xgb_RMSE}, CV: {TPOT_xgb_CV}')\n", "intent": "Lets compare with the RMSE function that we used earlier:\n"}
{"snippet": "i = 999\nprint(' '.join(predict(img_codes[i])))\nprint(' '.join(captions[i][0]))\n", "intent": "Prediction examples for one of validation samples.\n"}
{"snippet": "def compute_loss(X, y, w):\n    s = 1 - y * expand(X).dot(w)\n    s[s < 0] = 0\n    return np.sum(s) / len(s)\ndef compute_grad(X, y, w):\n    return -(y * ((1 - y * expand(X).dot(w)) > 0 )).dot(expand(X))\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "net.evaluate()\ny_pred = net.forward(X[-1000:])\naccuracy_score(y[-1000:], from_categorical(y_pred))\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nclas = []\nclas.append(len(train[train[target] == 0]))\nclas.append(len(train[train[target] == 1]))\nclas = np.array(clas)\nprint('number of data points with sickness = true: ' + str(len(sick)))\nprint('number of data points with sickness = false: ' + str(len(no_sick)))\npreds = [np.argmax(clas)]*len(test)\nprint('majority class accuracy percentage = ' + str(accuracy_score(preds, test[target])))\n", "intent": "Let's run a classifier which classifies all datapoints according to the majority class. \n"}
{"snippet": "prediction=nb.predict(x_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "iris_predictor.predict([-15.819178720771802,8.7759971528627,-22.8046864614815,11.864868080360699,-9.09236053189517,-2.38689320657655,-16.5603681078199,0.9483485947860579,-6.31065843275059,-13.0888909176936,9.81570317447819,-14.0560611837648,0.777191846436601,-13.7610179615936,-0.353635939812489,-7.9574472262599505,-11.9629542349435,-4.7805077876172,0.652498045264831,0.992278949261366,-2.35063374523783,1.03636187430048,1.13605073696052,-1.0434137405139001,-0.10892334328197999,0.657436778462222,2.1364244708551396,-1.41194537483904,-0.3492313067728856]) \n", "intent": "Invoking prediction:\n"}
{"snippet": "print \"Classification Error of MultinomialNB:\", 1 - metrics.accuracy_score(ytt, y_pred_mnb)\nprint \"Classification Error of KNN:\", 1 - metrics.accuracy_score(ytt, y_pred_knn)\nprint \"Classification Error of LinearSVC:\", 1 - metrics.accuracy_score(ytt, y_pred_svc)\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "print \"F-measure of MultinomialNB:\", metrics.f1_score(ytt, y_pred_mnb, average='weighted')\nprint \"F-measure of KNN:\", metrics.f1_score(ytt, y_pred_knn, average='weighted')\nprint \"F-measure of LinearSVC:\", metrics.f1_score(ytt, y_pred_svc, average='weighted')\n", "intent": "**F-measure:** \n- 2 * P * R / (P + R) \n"}
{"snippet": "payload = X_test\nground_truth = y_test\nprediction = predictor.predict(payload).decode('utf-8')\nprint(prediction)\n", "intent": "After deployment, you should pass data as numpy array to predictor instance and get predicted lables and probabilities.\n"}
{"snippet": "from sklearn.metrics.cluster import v_measure_score\nprediction_arr = np_res[0]\nground_truth = y_test\nprint(\"DAAL Accuracy on Iris Train Set: \", str(v_measure_score(prediction_arr, ground_truth)))\n", "intent": "Compute the accuracy of trained model on test dataset 'y_test'.\n"}
{"snippet": "confusion_matrix(LinearSVC.predict(xtestScaledVB[columnMask]),ytest)\n", "intent": "* Now that is much better. Let's try the testing data\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(\n    model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(\n    dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nmodel=None\nclean_mem()\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(\n    VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(\n    np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: {0:.4f} %'.format(test_accuracy))\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "top_predictions = [np.argmax(top_model.predict(\n    np.expand_dims(feature, axis=0))) for feature in test_top]\ntest_accuracy = 100*np.sum(\n    np.array(top_predictions)==np.argmax(test_targets, axis=1))/len(top_predictions)\nprint('Test accuracy: {0:.4f} %'.format(test_accuracy))\nclean_mem()\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dtResults[\"RF\"] = np.round(bestrf_grid.predict(trainX.iloc[0:20])*4)/4\ndtResults\n", "intent": "How does this compare to our decision trees?\n"}
{"snippet": "def skin_color_score(X, cluster_labels):\n    skin = X.skin\n    W = skin[cluster_labels == 0]\n    B = skin[cluster_labels == 1]\n    white_count = (skin > 4).sum()\n    black_count = (skin < 4).sum()\n    return ((W > 4).sum() / white_count + (B < 4).sum() / black_count) / 2\n", "intent": "Given a classification/clustering computes the correlation with the skin color.\n"}
{"snippet": "predictions = dbn.predict(trY)\nprint classification_report(teY, predictions)\nprint 'The accuracy is:', accuracy_score(teY, predictions)\n", "intent": "Performance of the Classification Model:\n"}
{"snippet": "predictions = dbn.predict(trY)\nprint classification_report(teY, predictions)\nprint 'The accuracy is:', accuracy_score(teY, predictions)\n", "intent": "Performance of the updated Classification Model:\n"}
{"snippet": "train_pred, train_std = gp_model.predict(train_X, return_std=True)\n", "intent": "Look at the $R^2$ and RMSE of the trained model on train_Y.\n"}
{"snippet": "test_hidden_states = hmm.predict(test_X)\n", "intent": "Look at how the HMM_GP model performs on unseen test data\n"}
{"snippet": "fxg_test = xgb.DMatrix(test)\nfold_preds = np.around(xgclassifier.predict(fxg_test, ntree_limit=xgclassifier.best_iteration), decimals = 1)\ntest_preds += fold_preds\n", "intent": "Run the predictor on the __actual__ test data provided by the contest.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.\n    for i in range(len(style_layers)):\n        layer_id = style_layers[i]\n        feature = feats[layer_id]\n        A = gram_matrix(feature)\n        G = style_targets[i]\n        fmp = tf.subtract(G,A)**2\n        loss += style_weights[i] * tf.reduce_sum(fmp)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "preds = np.expm1(lasso_model.predict(model_test))\n", "intent": "The above shows the top and bottom 10 attributes with their respective coefficients in the Lasso model.\n"}
{"snippet": "predict_fn = lambda x: clfs_best_config[0][-1].predict_proba(x).astype(float)\n", "intent": "Create a function that will return the probability that the model (in our case we chose the logistic regression model) selects a certain class.\n"}
{"snippet": "seed = 7\nkfold = KFold(n_splits = 10, random_state = seed)\npredict = cross_val_predict(model, X, Y)\nresult = cross_val_score(model, X, Y, cv = kfold)\nprint(\"Mean accuracy is: \", result.mean())\nprint(predict)\n", "intent": "Now, we apply cross-validation with no. of folds = 10\n"}
{"snippet": "predictions = {}\nfor model in models:\n    label = model.replace('model','prediction')\n    predictions[label] = models[model].predict(test_data[1])\n", "intent": "We utilize the <code>predict</code> function to predict the target values for individual models.\n"}
{"snippet": "print (classification_report(test_data[0], predictions['prediction_4_char']))\nprint (accuracy_score(test_data[0], predictions['prediction_4_char']))\n", "intent": "<u>Classification report and accuracy score for a 4-gram character model</u>\n"}
{"snippet": "print (classification_report(test_data[0], predictions['prediction_2_char']))\nprint (accuracy_score(test_data[0], predictions['prediction_2_char']))\n", "intent": "<u>Classification report and accuracy score for a 2-gram character model</u>\n"}
{"snippet": "print (classification_report(test_data[0], predictions['prediction_1_char']))\nprint (accuracy_score(test_data[0], predictions['prediction_1_char']))\n", "intent": "<u>Classification report and accuracy score for a 1-gram character model</u>\n"}
{"snippet": "print (classification_report(test_data[0], predictions['prediction_2_word']))\nprint (accuracy_score(test_data[0], predictions['prediction_2_word']))\n", "intent": "<u>Classification report and accuracy score for a 2-gram word model</u>\n"}
{"snippet": "print (classification_report(test_data[0], predictions['prediction_1_word']))\nprint (accuracy_score(test_data[0], predictions['prediction_1_word']))\n", "intent": "<u>Classification report and accuracy score for a 1-gram word model</u>\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\ny_pred = rf.predict(y_train)\nprint(classification_report(y_test, y_pred))\ncm= confusion_matrix(y_test,y_pred)\nprint('confusion matrix:')\nprint(cm)\n", "intent": "very promising results. As long as the classifier is intrepretable, it should reveal meaningful insight. Lets look at the metrics just to be sure.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(logreg, X, Y, cv=skf, scoring='mean_squared_error', n_jobs=-1)\n", "intent": "The above loop can also be written more succinctly using [cross_val_score](http://scikit-learn.org/stable/modules/cross_validation.html\n"}
{"snippet": "X_train, y_train = get_sampled_data(train_dataset,train_labels,100)\ny_predict = lr.predict(X_test)\nprint(accuracy_score(y_predict,y_test))\nprint(x)\n", "intent": "Let's try 1000 samples\n"}
{"snippet": "X_train, y_train = get_sampled_data(train_dataset,train_labels,2500)\ny_predict = lr.predict(X_test)\nprint(accuracy_score(y_predict,y_test))\n", "intent": "Now with 2500 samples\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for i in range(len(style_layers)):\n        loss += content_loss(style_weights[i], gram_matrix(feats[style_layers[i]]), style_targets[i])\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_testing = model.predict(x_testing, verbose = 1)\n", "intent": "**Time to predict!**\n"}
{"snippet": "def kFoldValidation(data, prVal, modelParam={}):\n    N_folds = 5\n    skf = StratifiedKFold(data.click, n_folds=N_folds, shuffle=True, random_state=1234)\n    res = []\n    for train_index, test_index in skf:\n        res.append(prVal(data.ix[data.index[train_index]], data.ix[data.index[test_index]], modelParam))\n    res = np.concatenate(res, axis=1)    \n    print 'ROC AUC =', roc_auc_score(res[0,:], res[1,:]),\n    return res\n", "intent": "At the beginning I'll use 5-fold validation to check models:\n"}
{"snippet": "prediction = knn.predict(X_new)\nprint('Prediction:{}'.format(prediction))\nprint('Prediction target name: {}'.format(iris_dataset['target_names'][prediction]))\n", "intent": "To make a prediction, we call the *predict* method of the *knn* object.\n"}
{"snippet": "dt_mse = mean_squared_error(tree_sample_df['Actual_Shares'],tree_sample_df['Predicted_Shares'] )\ndt_rmse = np.sqrt(dt_mse)\nprint(dt_mse)\nprint(dt_rmse)\n", "intent": "The predictions for the first few rows match exactly with the actual values. Lets check the error metric for the predicted shares.\n"}
{"snippet": "test_model = rf_grid.best_estimator_\ntest_predictions = test_model.predict(test_X)\ntest_predictions_mse = mean_squared_error(test_Y,test_predictions)\ntest_predictions_rmse = np.sqrt(test_predictions_mse)\ntest_predictions_rmse\n", "intent": "We can evaluate the model with the Test data.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(actual_shares, predictions)\n", "intent": "Model seems to be predicting higer shares than the actual values. Lets calculate the R2 score.\n"}
{"snippet": "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\nreg.predict(sample_house)\n", "intent": "Lets try to predict the value of a sample house.\n"}
{"snippet": "reg.predict(35)\n", "intent": "Lets try to predict the brain weight of a Kangaroo through our trained model. The average brain weight of a Kangaroo is 56g with 35Kg body weight.\n"}
{"snippet": "reg.predict(21.07931)\n", "intent": "Lets try to predict the life expectancy in Laos from the country's 21.07931 mean BMI of males in the country.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(labels, predictions)\nprint('leave-one-out accuracy score:', acc)\n", "intent": "The output of the previous code block shows that we obtain a good accuracy. We can measure this using the accuracy score:\n"}
{"snippet": "folds = StratifiedKFold(n_splits=10,shuffle=True,random_state=random_seed)\nscores = {}\nfor model_name in models:\n    clf = models[model_name];\n    score = cross_val_score(clf,x,y,cv=folds)\n    scores[model_name]=(np.average(score),np.std(score))\n    print('\\t%26s\\t%.3f +/- %.3f'%(model_name,scores[model_name][0],scores[model_name][1]))\n", "intent": "For each model, we apply 10-fold stratified crossvalidation and compute the average accuracy and the corresponding standard deviation\n"}
{"snippet": "Yhat = lm.predict(X)\nYhat[0:5]\n", "intent": "we can optput a prediction\n"}
{"snippet": "Y_hat = lm.predict(Z)\n", "intent": "First lets make a prediction\n"}
{"snippet": "ypipe=pipe.predict(Z)\nypipe[0:4]\n", "intent": "Similarly, we can normalize the data, perform a transform and produce a prediction simultaneously\n"}
{"snippet": "Yhat = lm.predict(X)\nYhat[0:4]\n", "intent": "We can predict the output i.e., \"yhat\" using the predict method, where X is the input variable:\n"}
{"snippet": "mean_squared_error(df['price'], Yhat)\n", "intent": "we compare the predicted results with the actual results\n"}
{"snippet": "Y_predict_multifit = lm.predict(Z)\n", "intent": "we produce a prediction\n"}
{"snippet": "mean_squared_error(df['price'], Y_predict_multifit)\n", "intent": "we compare the predicted results with the actual results\n"}
{"snippet": "mean_squared_error(df['price'], p(x))\n", "intent": "We can also calculate the MSE:\n"}
{"snippet": "yhat=lm.predict(new_input)\nyhat[0:5]\n", "intent": "Produce a Prediction\n"}
{"snippet": "print metrics.classification_report(df_test[\"class\"], predicted, \ntarget_names=[\"Spam\",\"Ham\"])\n", "intent": "https://www.codeproject.com/Articles/1232758/Detect-E-mail-Spam-Using-Python\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint ('accuracy = {0}%'.format(accuracy))\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"migo.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "def sse(mod, X, y):\n    predictions = mod.predict(X)\n    sq_error = [\n        (pred - actual) ** 2 for (pred, actual) in zip(predictions, y)]\n    RSS = sum(sq_error)\n    return(RSS)\n", "intent": "NOTE: sklearn models had a `residues_` member, but it is deprecated, and will be removed in 0.19, so a function is provided here\n"}
{"snippet": "predict_test_2D= logistic_classifier_2D.predict(rpca_test_2D)\nprint (\"Accuracy score of 2 component Randomized PCA is:\", accuracy_score(test_target, predict_test_2D))\n", "intent": "From the output, we know that the accuarcy score for 3 principle component is around 86.97%.\n"}
{"snippet": "scikit_prediction = regr.predict(10)\nprint custom_prediction\nprint scikit_prediction[0][0]\n", "intent": "Now let's compare the results of our custom implemention against sci-kit:\n"}
{"snippet": "yHat = mlpr.predict(A)\nmpl.plot(A, y, c='\nmpl.plot(A, yHat, c='\nmpl.show()\n", "intent": "With the MLP network trained, prediction can be performed and the results plotted using matplotlib.\n"}
{"snippet": "def purity_score(clusters, classes):\n    A = np.c_[(clusters,classes)]\n    n_accurate = 0.\n    for j in np.unique(A[:,0]):\n        z = A[A[:,0] == j, 1]\n        x = np.argmax(np.bincount(z))\n        n_accurate += len(z[z == x])\n    return n_accurate / A.shape[0]\n", "intent": "<h4>Performance - Purity:</h4>\n"}
{"snippet": "text_clf.predict_proba([\"Reader, you fucking rock!  Thank you so much for buying my book!\"])[0][1]\n", "intent": "Hmm... maybe the model is over-relying on \"thanks\" and \"fuck\" and doesn't really get the context?  Let's test that idea!\n"}
{"snippet": "client_data = [[5, 17, 15], \n               [4, 32, 22], \n               [8, 3, 12]]  \nprint reg.predict(client_data)\nfor i, price in enumerate(reg.predict(client_data)):\n    print \"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price)\n", "intent": "**Answer: ** Parameter 'max_depth' is 1 for the optimal model.\n"}
{"snippet": "lng.predict(X_new)\n", "intent": "___\nscore return coef of R^2: (1 - u/v) (bestscore is 1.0)\n- u: ((y_true - y_pred) ** 2).sum()\n- v: ((y_true - y_true.mean()) ** 2).sum()\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores_DT = cross_val_score(tree_re, train_data, train_labels,\n                        cv = 10, scoring = \"neg_mean_squared_error\")\nmse_CV_DT = np.sqrt(-scores_DT)\n", "intent": "???cross-validation\n"}
{"snippet": "Y_pred = model_dict['Random Forest'].predict(X_test)\n", "intent": "The best model of this time is ** Random Forest **.\nTherefore, we will predict saleprice using random forest.\n"}
{"snippet": "def predictBelegung(df):\n    features = df[featurevector].values\n    prediction = int(classifier.predict([features]))\n    return prediction\n", "intent": "Here we predict it for the whole dataset\n"}
{"snippet": "def gram_matrix_test(correct):\n    features = model.extract_features()[5]\n    print type(features)\n    print features.shape\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in xrange(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2)\n    return style_loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "DogResNet50_predictions = [np.argmax(DogResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResNet50]\ntest_accuracy = 100*np.sum(np.array(DogResNet50_predictions)==np.argmax(test_targets, axis=1))/len(DogResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG19_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogVGG19Data]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "mse_gd_manual=mean_squared_error(y_test, y_pred_manual)\n", "intent": "Let's judge the model's performance using the standard metric used for regression problems: Mean Squared Error\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test accuracy:', score[1])\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint('Train accuracy:', score[1])\n", "intent": "Train and Test accuracy \n"}
{"snippet": "print(\"****** Test Data ********\")\ny_pred = model.predict_classes(test)\nprint(\"Accuracy of the CNN model on the test set is : %.3f\\n\" % accuracy_score(y_pred, test_labels))\nprint(\"Classification report of CNN : \\n\", metrics.classification_report(test_labels, y_pred))\nprint(\"************* Confusion Matrix ********** \")\npd.crosstab(np.array(test_labels), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Use the test dataset to evaluate the model\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint predicted\n", "intent": "Now we need to predict class labels for the test set. \nWe will also generate the class probabilities.\n"}
{"snippet": "print metrics.accuracy_score(y_test, predicted)\nprint metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "The model is taking as a positive ECB hering predicted case all the probabilities above 9%\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test, predicted)\n", "intent": "In the next cells we can observe the confusion matrix and a classification report with other metrics.\n"}
{"snippet": "model.predict_proba(np.array([16,23,0,1,14,3,2,1,1,13,17,9,10,4,4]))\n", "intent": "Now we are going to predict the probability of an ECB hearing for a random square not present in the dataset. \n"}
{"snippet": "predicted = mireg.predict(X_test)\n", "intent": "The model is taking as a positive ECB hering predicted case all the probabilities above 61%.\n"}
{"snippet": "print(\"target values for D:\")\nprint(D[1])\nprint(\"prediction on D:\")\nprint(predict(D[0]))\n", "intent": "Printing of the target values (the classes) and the prediction by our model.\n"}
{"snippet": "result = predict(D[0]) - D[1]\nerror = 0\nfor index in result:\n    if result[index] != 0:\n        error += 1\ncorrect_guesses = N - error\naccuracy = (N - error)*100/N\nprint\nprint \"correct predictions = %f over %i examples\" % (correct_guesses, N)\nprint \"accuracy = %i%%\" % accuracy\n", "intent": "Calculate the errors, i.e. the numbers of examples that have not been classified correctly and output the accuracy result.\n"}
{"snippet": "print('Real value of y_test[5]: '+str(y_test[5]) + ' -> predict value: ' + str(lr.predict(x_test.iloc[[5],:])))\nprint('Real value of y_test[5]: '+str(y_test[5]) + ' -> predict value: ' + str(rfr.predict(x_test.iloc[[5],:])))\nprint('Real value of y_test[5]: '+str(y_test[5]) + ' -> predict value: ' + str(dt.predict(x_test.iloc[[5],:])))\nprint('\\n')\nprint('Real value of y_test[50]: '+str(y_test[50]) + ' -> predict value: ' + str(lr.predict(x_test.iloc[[50],:])))\nprint('Real value of y_test[50]: '+str(y_test[50]) + ' -> predict value: ' + str(rfr.predict(x_test.iloc[[50],:])))\nprint('Real value of y_test[50]: '+str(y_test[50]) + ' -> predict value: ' + str(dt.predict(x_test.iloc[[50],:])))\n", "intent": "<ul><li>These are the regression estimates for samples with 5 and 50 indexes:</li></ul>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_train, cl.predict(X_train))\n", "intent": "Get the accuracy score for the training data.\n"}
{"snippet": "scoreCV=cross_val_score(cl,X_train,y_train,cv=10,scoring='accuracy') \nscoreCV\n", "intent": "Now perform cross-validation (10 folds)\n"}
{"snippet": "from sklearn.metrics import classification_report \nprint(classification_report(y_test, cl.predict(X_test)))\n", "intent": "The model seems to be over-fitting. Also check the classification performance for the test data\n"}
{"snippet": "scoreCV=cross_val_score(cl,X_train,y_train,cv=10,scoring='accuracy') \nscoreCV\n", "intent": "Perform cross validation and get accuracy scores across folds\n"}
{"snippet": "from sklearn.metrics import classification_report \nprint(classification_report(y_test, cl.predict(X_test)))\n", "intent": "Also show accuracy at predicting test set.\n"}
{"snippet": "def calculate_hinge_loss(clf,X2,y2):\n    return loss, loss_per_data\nloss,loss_per_data = calculate_hinge_loss(clf,X2,y2)\nprint('hinge loss per data point is: \\n{}'.format(loss_per_data))\nprint('the hinge loss is: {}'.format(loss))\n", "intent": "Now, let's write a function that returns the value of $(1 - y_k f(x_k;x_c))_+$ for each $k$ (as variable `loss_per_data`) as well as $L(x_c)$.\n"}
{"snippet": "from sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_fscore_support,precision_recall_curve\nypred_l=clf_l.predict(Xtest_l)\n", "intent": "** Using the regression to get the predicted values **\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ytest_l,ypred_l))\n", "intent": "** Precision and Recall Metrics **\nPrecision = 0.65995525727069348\nRecall = 0.62855113636363635\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n", "intent": "Now we'll visualize the AE's performance.\n"}
{"snippet": "y_pred = treereg.predict(X_test)\ny_pred\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each observation?\n"}
{"snippet": "print(\"Precision : %.3f\" % precision_score(y, y_hat))\n", "intent": "<img src=\"https://github.com/basilhan/figures/blob/master/Precision.png?raw=true\">\n"}
{"snippet": "print(\"Recall : %.3f\" % recall_score(y, y_hat))\n", "intent": "<img src=\"https://github.com/basilhan/figures/blob/master/Recall.png?raw=true\">\n"}
{"snippet": "DogInceptionV3_predictions = [np.argmax(DogInceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogInceptionV3]\ntest_accuracy = 100*np.sum(np.array(DogInceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(DogInceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_hat = mdl.predict(np.expand_dims(X,1))\nY_hat.shape\n", "intent": "Now, predicting. Will output a tensor of size *(N_CHUNKS, 12)* since it's a 12 classes trained model\n"}
{"snippet": "r_a_score = roc_auc_score(y_train_sm, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)\n", "intent": "Lastly, we will compute the size of this AUC space, known as the ROC-AUC score.\n"}
{"snippet": "predictions = cross_val_predict(ran, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)\n", "intent": "The confusion matrix provides a method for quantitatively evaluating model performance. Let's see what it can tell us below.\n"}
{"snippet": "f1_score(y_train, predictions)\n", "intent": "Calculating the harmonic mean of precision and recall (e.g. the F1 score) can provide another useful accuracy measurement.\n"}
{"snippet": "r_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)\n", "intent": "Lastly, we will compute the size of this AUC space, known as the ROC-AUC score.\n"}
{"snippet": "xgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\n", "intent": "Lasso and XGboost worked best so lets take predictions from them\n"}
{"snippet": "logisticRegr.predict(X_test[0].reshape(1,-1))\n", "intent": "Uses the information the model learned during the model training process\n"}
{"snippet": "print (metrics.accuracy_score(y_test, predicted))\nprint (metrics.roc_auc_score(y_test, predicted))\n", "intent": "**Evaluation Metrics**\n"}
{"snippet": "print (metrics.confusion_matrix(y_test, predicted))\nprint (metrics.classification_report(y_test, predicted))\n", "intent": "**Confusion matrix and a Classification report with other metrics**\n"}
{"snippet": "model.predict_proba(np.array([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 25, 3, 1, 4,16]).reshape(1,-1))\n", "intent": "**Predicting the Probability of an Affair**\n"}
{"snippet": "sep.predict(X_test, y_test)\n", "intent": "Now use the SEPClass predict method to make predictions on the test set and print out a report card\n"}
{"snippet": "sepDT.predict(X_test, y_test)\n", "intent": "Now use the SEPClass predict method to make predictions on the test set and print out a report card\n"}
{"snippet": "sepLR.predict(X_test, y_test)\n", "intent": "Now use the SEPClass predict method to make predictions on the test set and print out a report card\n"}
{"snippet": "test_predictions = est.predict(data_test)\nprint 'gradient boosting score: {}'.format(mape(targets_test, test_predictions))\ntest_predictions = bagging_est.predict(data_test)\nprint 'gradient boosting with bagging score: {}'.format(mape(targets_test, test_predictions))\n", "intent": "Test data's prediction MAPE score:\n"}
{"snippet": "final_est = search_params.best_estimator_\ntest_predictions = final_est.predict(data_test_original)\nprint(mape(targets_test, test_predictions))\n", "intent": "Test data's prediction MAPE score:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=0\n    for (i,style_layer) in enumerate(style_layers):\n        gram = gram_matrix(feats[style_layer])\n        style_loss+=style_weights[i]*tf.reduce_sum(tf.square(gram-style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "mlp_eval_results = dict()\nt1 = time()\nfor name, metric in zip(mlp.metrics_names, \n                        mlp.evaluate(x=X_test[:fraction], y=y_test[:fraction], verbose=0)):\n    mlp_eval_results.update({name:metric})\n    print('Model {}: {:.4f}'.format(name, metric))\nmlp_eval_results.update({'eval_time': time() - t1})\nprint('Evaluation time: {:.2f} seconds'.format(mlp_eval_results['eval_time']))\n", "intent": "Check model performance on test data:\n"}
{"snippet": "cnn_retrain_eval_results = dict()\nt1 = time()\nfor name, metric in zip(cnn.metrics_names, \n                        cnn.evaluate(x=X_test, y=y_test, verbose=0)):\n    cnn_retrain_eval_results.update({name:metric})\n    print('Model {}: {:.4f}'.format(name, metric))\ncnn_retrain_eval_results.update({'eval_time': time() - t1})\nprint('Evaluation time: {:.2f} seconds'.format(cnn_retrain_eval_results['eval_time']))\n", "intent": "Evaluate model on test data:\n"}
{"snippet": "y_pred_test_lin = lin_reg.predict(X_test)\nlin_rmse_test = np.sqrt(mean_squared_error(y_pred_test_lin, y_test))\nlin_rmse_test\n", "intent": "And on the test set:\n"}
{"snippet": "scores_tree = cross_val_score(tree_reg, X_train, y_train,\n                             scoring='neg_mean_squared_error', cv=10)\nrmse_scores_tree = np.sqrt(-scores_tree)\nprint('mean:', rmse_scores_tree.mean(), 'and', 'std:', rmse_scores_tree.std())\n", "intent": "This is quite impressive! But it could also be that the model is overfitting, so let's do cross validation.\n"}
{"snippet": "y_test_tree_pred = tree_reg.predict(X_test)\ntree_mse_test = mean_squared_error(y_test, y_test_tree_pred)\ntree_rmse_test = np.sqrt(tree_mse_test)\ntree_rmse_test\n", "intent": "Yes, our fears are real. It overfit.\n"}
{"snippet": "print \"\"\nprint \":: Confusion Matrix\"\nprint \"\"\nprint confusion_matrix(label_test, svm_detector.predict(msg_test))\nprint \"\"\nprint \":: Classification Report\"\nprint \"\"\nprint classification_report(label_test, svm_detector.predict(msg_test))\n", "intent": "**CONFUSION MATRIX AND CLASSIFICATION REPORT**\n"}
{"snippet": "y_test = list(target)\npred = list(labels)\ny_train = list([x[0] for x in test_original])\npred_train = list(rf.predict(test))\ncategories = [0.0, 1.0]\n", "intent": "First of all prepare the test and training set labels and the predictions performed:\n"}
{"snippet": "model.evaluate(x_test, y_test)\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "SVM_C_28_trans_rot_15_aug=joblib.load('SVM_C_28_trans_rot_15_aug.pkl')\ny_pred_test = SVM_C_28_trans_rot_15_aug.predict(X_test)\nprint(accuracy_score(y_test,y_pred_test))\n", "intent": "Training for this will take even longer, roughly an hour and a half for training and prediction.\n"}
{"snippet": "my_model.evaluate(test_InceptionV3, test_targets)[1]\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(mnist_labels_train_5, prediction), recall_score(mnist_labels_train_5, prediction)\n", "intent": "Concrete Metrics\n---\n1. `precision_score`: models gets the right answer percentage\n2. `recall_score`: model detects the true classifier\n"}
{"snippet": "labels_scores = cross_val_predict(sgd_clf, mnist_train, mnist_labels_train_5, cv=3, method=\"decision_function\")\nfrom sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(mnist_labels_train_5, labels_scores)\n", "intent": "Thresholds for `decision_function`\n---\nBalancing the precision and recall might be important in the application.\n- Precision and Recall Plots\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(mnist_labels_train_5, labels_scores)\n", "intent": "- Integrate under the ROC curve. Purely random is 0.5, perfect is 1.0 `roc_auc_score` (Area Under Curve)\n"}
{"snippet": "sgd_train_predict = cross_val_predict(sgd_clf, mnist_train, mnist_labels_train, cv=3)\nsgd_train_cm = confusion_matrix(mnist_labels_train, sgd_train_predict)\n", "intent": "- Confusion Matrices\n"}
{"snippet": "knn.predict([[3,5,4,2]])\n", "intent": "<h2>Making predictions</h2>\n"}
{"snippet": "x_new = [[3,5,4,2], [5,4,3,2]]\nknn.predict(x_new)\n", "intent": "<h2>Model evaluation metrics for regression</h2>\n"}
{"snippet": "new_test = test[features]\nprediction_count=new_rfr.predict(new_test)\n", "intent": "Now on to our prediction\n"}
{"snippet": "print messages['message'][0]\nprint 'predicted:', spam_detector.predict(tfidf4)[0]\nprint 'expected:', messages.label[3]\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "print svm_detector.predict([\" c!edit ?\"])[0]\nprint svm_detector.predict([\"WINNER! Credit for free!\"])[0]\n", "intent": "So apparently, linear kernel with `C=1` is the best parameter combination.\nSanity check again:\n"}
{"snippet": "input_msg = 'WINNER credit for free'\nprint 'before:', svm_detector.predict([input_msg])[0]\nprint 'after:', svm_detector_reloaded.predict([input_msg])[0]\n", "intent": "The loaded result is an object that behaves identically to the original:\n"}
{"snippet": "print messages['message'][3]\nprint 'predicted:', spam_detector.predict(tfidf4)[0]\nprint 'expected:', messages.label[3]\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_y, \n                            predict_y, \n                            target_names=['Not Survived', 'Survived']))\n", "intent": "Display the classification report:\n$$Precision = \\\\frac{TP}{TP + FP}$$\n$$Recall = \\\\frac{TP}{TP + FN}$$\n$$F1 = \\\\frac{2TP}{2TP + FP + FN}$$\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum((actual-preds)**2)/len(preds) \nprint(mse(y_test, DTR_pred))\nprint(mean_squared_error(y_test, DTR_pred))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "Xtest_count = vect.transform(Xtest)\nXtest_tfidf = tfidf.transform(Xtest_count)\ny_pred = clf.predict(Xtest_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "CPU Time: \nAWS Time: \n"}
{"snippet": "def precision(actual, preds):\n    TP = np.sum(actual[preds == 1])\n    return TP/np.sum(preds == 1) \nprint(precision(y_test, naive_bayes_pred))\nprint(precision_score(y_test, naive_bayes_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    TP = np.sum(actual[preds == 1])\n    return TP/np.sum(actual == 1) \nprint(recall(y_test, naive_bayes_pred))\nprint(recall_score(y_test, naive_bayes_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    rec = recall(preds, actual)\n    pre = precision(preds, actual)\n    return 2*(rec*pre)/(rec + pre) \nprint(f1(y_test, naive_bayes_pred))\nprint(f1_score(y_test, naive_bayes_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "BC_prediction = BC_model.predict(testing_data)\nRFC_prediction = RFC_model.predict(testing_data)\nABC_prediction = ABC_model.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score, make_scorer\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "def L2_loss(w):\n    global x_vals\n    global y_noisy_vals\n    return np.sum((w * x_vals - y_noisy_vals)**2)/(2 * y_noisy_vals.shape[0])\nfor w1 in [-1, 0, 1]:\n    y_pred = w1 * x_vals\n    assert L2_loss(w1) == L2_loss_example(y_pred, y_noisy_vals)\n", "intent": "We are now going to modify our L2 code function from before to make this step slightly clearer:\n"}
{"snippet": "SSreg = np.mean((regr.predict(x_test) - y_test) ** 2)\nSStot = np.mean((y_test - np.mean(y_test)) ** 2)\nprint(1-SSreg/SStot)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\n100 * accuracy_score(y_test, clf.predict(X_test))\n", "intent": "Check the predictions on test data.\n"}
{"snippet": "X_data_trans = model_transfer.predict(x_data) \nX_data_trans.shape\n", "intent": "its interesting to note the improved accuracy of the model in the validation set\n"}
{"snippet": "learner.evaluate(X_test, y_test, metric='accuracy', get_loss=True)\n", "intent": "We can easily evaluate how well algorithms generalize using **learner.evaluate**\n"}
{"snippet": "predicted = gs_clf.predict(docs_new)\nfor doc, category in zip(docs_new, predicted):\n    print('\"{}\" => {}'.format(doc, twenty_train.target_names[category]))\n", "intent": "__Note:__ This one includes cross-validation.\n"}
{"snippet": "testdm = dfdict[minerror[1]]['test_x']\ntestrats = dfdict[minerror[1]]['test_y']\ntestpred = regr.predict(testdm)\ntestdm.head()\nvaldm = dfdict[minerror[1]]['val_x']\nvalrats = dfdict[minerror[1]]['val_y']\nvalpred = regr.predict(valdm)\npredictions['ridge2'] = testpred\npredictions_valid['ridge2'] = valpred\n", "intent": "We again find the test set corresponding to the best k value and compare results:\n"}
{"snippet": "ensemble_predictions = valreg.predict(dfensembletest.drop('y',axis=1))\ntestactual = dfensembletest['y']\n", "intent": "There are our data frames and linear regression to determine the weightings. Here is the ensemble prediction using those weightings.\n"}
{"snippet": "def modelTest(clf, train, labels):\n    cv = KFold(n_splits=5,shuffle=True,random_state=45).split(train)\n    r2 = make_scorer(r2_score)\n    r2_val_score = cross_val_score(clf, train, labels, cv=cv, scoring=r2)\n    scores=[r2_val_score.mean()]\n    return scores\n", "intent": "Here, it'd better to test the goodness of models by using cross validation\n"}
{"snippet": "topredict = scaler.transform(X_remain_vec)\ndf_pollingdata_Pos_only.loc[df_pollingdata_Pos_only['TopicName']=='notopic','TopicName'] = clftop.predict(topredict)\n", "intent": "Assign the predicted labels to the questions\n"}
{"snippet": "preds=model.predict( test_input, batch_size=32, verbose=0)\n", "intent": "Lets make some predictions\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nX = to_vectors(sents)\ny = np.array([1 if l=='pos' else 0 for l in labels])\nscores = cross_val_score(clf, X, y, cv=3, scoring='f1')\nscores\n", "intent": "3-fold cross-validation with sklearn\n"}
{"snippet": "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))))\nprint('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\nprint('Decision Tree Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, tree.DecisionTreeClassifier))))\nprint('Ridge Regression Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.RidgeClassifier))))\nprint('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, svm.SVC))))\nprint('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\nprint('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))))\nprint('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.LogisticRegression))))\nprint('Gender based Classifier:\\n {}\\n'.format(metrics.classification_report(y, [index[1]<0 for index in X])));\nprint('Dumb Classifier:\\n {}\\n'.format(metrics.classification_report(y, [0 for index in y.tolist()]))); \n", "intent": "Look at f1 scores using the classification report:\n"}
{"snippet": "y_pred = model.predict(X_test, batch_size=len(X_test), verbose=1)\ny_pred = np.argmax(y_pred, axis=1)\n", "intent": "Get predictions from the test set to visualize the results.\n"}
{"snippet": "log_lik = ed.evaluate('log_likelihood', data={x_pred: x_train})\nmse = ed.evaluate('mean_squared_error', data={x_pred: x_train})\nprint(f'Log likelihood is: {log_lik:0.2f}')\nprint(f'Mean squared error is: {mse:0.2f}')\n", "intent": "Evaluate log likelihood and mean squared error.\n"}
{"snippet": "logistic.predict_proba(X)[::60]\n", "intent": "This is a pretty nice score, we didn't even do features engineering to get there :)\n"}
{"snippet": "propensity = [b for [a,b] in logistic.predict_proba(X)]\n", "intent": "While the score above is nice, it's not really informative, what we want here is the propensity score measure.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(clf_grid3.predict(X_val), y_val)\n", "intent": "We use the third grid search to plot a confusion matrix\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint(np.mean(y_pred == y_test))\n", "intent": "Now you have to see how well it does on the test data\n"}
{"snippet": "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C=1, penalty='l1')\ny_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n", "intent": "When you are happy with the quality, create predictions for *test* set, which you will submit to Coursera.\n"}
{"snippet": "def data_scorer(model, features, labels, folds):\n    recall = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"recall\", n_jobs = -1))\n    precision = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"precision\", n_jobs = -1))\n    accuracy = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"accuracy\", n_jobs = -1))\n    f1 = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"f1\", n_jobs = -1))\n    auc = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"roc_auc\", n_jobs = -1))\n    return accuracy, recall, precision, f1, auc\n", "intent": "This function calculates and returns the recall, precision, accuracy, F-score, and AUC of a particular model. \n"}
{"snippet": "print(\"RMSE:\",(np.sqrt(metrics.mean_squared_error(data_train[target].values, y_train_predict))))\n", "intent": "With the Predicted Item Outlet Sale values in hand let's calculate the RMSE value for our DT_Model\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\naccuracy = 100*score[1]\nprint('Test accuracy: %.4f%%' % accuracy)\n", "intent": "Let's see how our model does before any training.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\naccuracy = 100*score[1]\nprint('Test accuracy: %.4f%%' % accuracy)\n", "intent": "Time to test our network.\n"}
{"snippet": "cross_val_score(lr, X_train, y_train, cv = 10)\n", "intent": "This classifier yielded the same results as the previous one. This is due to the imbalanced dataset. \n"}
{"snippet": "predictions_log = pipe.predict_proba(val_data)[:,1]\n", "intent": "Create prediction on the validation data:\n"}
{"snippet": "loss = log_loss(val_labels, predictions_log)\nacc = accuracy_score(val_labels, np.rint(predictions_log))\nprint \"Validation scores for the Logsitic regression\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)\n", "intent": "Calculate validation scores:\n"}
{"snippet": "predictions = val_data[\"word_share\"]\nloss = log_loss(val_labels, predictions)\nacc = accuracy_score(val_labels, np.rint(predictions))\nprint \"Validation scores for the benchmark model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)\n", "intent": "Now let's calculate the benchmark model scores in order to be able to compare them to our logistic regression scores:\n"}
{"snippet": "model.load_weights(best_model_path)\npredictions = model.predict([q1_val_data, q2_val_data], batch_size=batch_size, verbose=1)\npredictions += model.predict([q2_val_data, q1_val_data], batch_size=batch_size, verbose=1)\npredictions /= 2\npredictions[predictions == 1] = 0.9999999\nutility.save_pickle(predictions, file_directory, \"lstm_preds_on_val.pkl\")\nloss = log_loss(val_labels, predictions)\nacc = accuracy_score(val_labels, np.rint(predictions))\nprint \"Validation scores of Lstm model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)\n", "intent": "Load the trained model and calculate logloss and accuarcy on the validation set:\n"}
{"snippet": "model.load_weights(best_model_path)\npredictions = model.predict([q1_test_data, q2_test_data], batch_size=batch_size, verbose=1)\npredictions += model.predict([q2_test_data, q1_test_data], batch_size=batch_size, verbose=1)\npredictions /= 2\npredictions[predictions == 1] = 0.9999999\nutility.save_pickle(predictions, file_directory, \"final_lstm_preds_on_val.pkl\")\nloss = log_loss(test_labels, predictions)\nacc = accuracy_score(test_labels, np.rint(predictions))\nprint \"Test scores of Lstm model\\n LogLoss: {:.4f}\\n Accuracy: {:.2f} \".format(loss, acc)\n", "intent": "Load the trained model and calculate logloss and accuarcy on the validation set:\n"}
{"snippet": "print \"Logistic regression recall:\", recall_score(y_test, lr_pred)\nprint \"Logistic regression precision:\", precision_score(y_test, lr_pred)\nprint \"Logistic regression F1 score:\", f1_score(y_test, lr_pred)\n", "intent": "> ...but prediction isn't so good in this case because of the unbalanced classes\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100 * np.sum(np.array(ResNet50_predictions) == np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('R^2 Score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "scores = cross_val_score(regr, X_train, y_train, cv=10)\nprint (scores)\nprint ('\\nAs the result of 10-fold cross validation shows the model is robust')\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "def MSE(predictions, labels):\n    return sklearn.metrics.mean_squared_error(labels, predictions)\n", "intent": "- Train the models above and predict the ratings. Then calculate the MSE of each model in order to evaluate the performance.\n"}
{"snippet": "accuracy_score(y_pred, y_test)\n", "intent": "Let's verify this score:\n"}
{"snippet": "X_new = .3\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\nprint(y_pred)\n", "intent": "Now we have an ensemble of three trees and can make predictions on a new instance by simply adding up the predictions of each tree.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(X, X_reduced)\n", "intent": "We can then compute the reconstruction pre-image error:\n"}
{"snippet": "ResNet50_predictions = [np.argmax(\n    model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(\n    np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_Model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "embedding_model.predict(np.array([[wordvec.vocab['python'].index]]))[0][0] == wordvec['python']\n", "intent": "Looks good, right? But let's not waste our time when the computer could tell us definitively and quickly:\n"}
{"snippet": "test_loss, test_acc = model.evaluate(x_test, y_test)\nprint('Accuracy of the model: {}'.format(test_acc))\n", "intent": "And in the end, we can test our model on the test set.\n"}
{"snippet": "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test))\n", "intent": "After training we can evaluate our model on the test set.\n"}
{"snippet": "x_test = np.array(['not feeling very happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'Apr 3 2011', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "y_pred = forest_fit.predict(X_test)\n", "intent": "Since we now have multiple predictions for each sample, we can use their spread as an estimate of the uncertainty in the mean prediction:\n"}
{"snippet": "model.evaluate()\n", "intent": "The initial generator's samples are random noise and the initial discriminator guesses that all samples are ~50% likely to be real:\n"}
{"snippet": "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n", "intent": "Evaluate model performance:\n"}
{"snippet": "classes = model.predict_classes(X_test, batch_size=32)\nproba = model.predict_proba(X_test, batch_size=32)\n", "intent": "generate predictions on new data:\n"}
{"snippet": "matrix['pred1'] = lm1.predict(matrix[['x1','x2']])\nmatrix['pred2'] = lm2.predict(matrix[['x1']])\nmatrix['pred3'] = lm3.predict(matrix[['x2']])\n", "intent": "Just one point has changed almost all coefficients of estimations. See Residual plots for every regressions.\n"}
{"snippet": "prob = est.predict(df[['Lag1','Lag2','Lag3','Lag4','Lag5','Volume']])\nprob.head()\n", "intent": "To get estimated probabilities, use **est.predict**\n"}
{"snippet": "pred2 = fit3.predict(X_test2)\n", "intent": "**Linear Regression**\n"}
{"snippet": "y_pred = lr.predict(X_test)  \n", "intent": "We perform predictions for the test set:\n"}
{"snippet": "ppv = cross_val_score(lr, X, Y, cv=10, scoring='precision')\nprint('PPV = {0:.2f} +/- {1:.2f}'.format(ppv.mean(),ppv.std()))\nrec = cross_val_score(lr, X, Y, cv=10, scoring='recall')\nprint('REC = {0:.2f} +/- {1:.2f}'.format(rec.mean(),rec.std()))\nacc = cross_val_score(lr, X, Y, cv=10, scoring='accuracy')\nprint('ACC = {0:.2f} +/- {1:.2f}'.format(acc.mean(),acc.std()))\nf1 = cross_val_score(lr, X, Y, cv=10, scoring='f1')\nprint('F1 = {0:.2f} +/- {1:.2f}'.format(f1.mean(),f1.std()))\n", "intent": "Let's see how it works:\n"}
{"snippet": "predicted_y=model0.predict(test)\n", "intent": "In the cell below we use the parameters we trained using our training data to score our unseen test data.\n"}
{"snippet": "predictionA = modelA.predict_proba(np_inputs)\npredictionB = modelB.predict_proba(np_inputs)\npredictionC = modelC.predict_proba(np_inputs)\nprint(predictionA)\nprint(predictionB)\nprint(predictionC)\n", "intent": "We then find the probably of each plant being in each group.\n"}
{"snippet": "resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0)))\n                        for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==\n                           np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "test_pred=averaged_models.predict(df_test2.values)\n", "intent": "We will go with LightGBM model\n"}
{"snippet": "import pylab as pl\npl.scatter(y_test,random_forest.predict(X_test))\n", "intent": "The optimal model should print the line:\n\\begin{align}\ny = 1*x\n\\end{align}\nThat means that the model predicts exactly the correct value.\n"}
{"snippet": "import pylab as pl\npl.scatter(y_test,svr_rbf.predict(X_test))\n", "intent": "The optimal model should print the line:\n\\begin{align}\ny = 1*x\n\\end{align}\nThat means that the model predicts exactly the correct value.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(x = test_images,\n                                     y = test_labels_onehot)\nprint('test_loss: {}'.format(test_loss))\nprint('test_acc: {}'.format(test_acc))\n", "intent": "Keras FAQ: https://keras.io/getting-started/faq/\n"}
{"snippet": "pipeline.predict(['this movie is truly good'])\n", "intent": "Probemos nuestro clasificador inicial\n"}
{"snippet": "pipeline.predict(['this movie is great'])\n", "intent": "Probemos nuestro clasificador inicial\n"}
{"snippet": "pipeline.predict(['this movie is good'])\n", "intent": "Probemos nuestro clasificador inicial\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, pred)\n", "intent": "18% women have been predicted to have an affair\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, pred_LR_Test))\nprint(classification_report(y_test, pred_LR_Test))\n", "intent": "Accuracy is 73% which is same as when predicting on same data\n"}
{"snippet": "test_sample = np.array([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 25, 3, 1, 4, 16]).reshape(1, -1)\nLR.predict_proba(test_sample)\n", "intent": "So the accuracy holds up to 73%\n"}
{"snippet": "pred = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = sum(np.array(pred) == np.argmax(test_targets, axis=1))/len(test_targets) *100\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(MyResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = predict(net2, md.val_dl)\npreds = np.argmax(preds, axis=1)\nplots(x_imgs[:8], titles=preds[:8])\n", "intent": "Now we can check our predictions:\n"}
{"snippet": "import numpy as np\nprint(\"Slope: {0}\".format(reg_model.coef_[0][0]))\nprint(\"Intercept: {0}\".format(reg_model.intercept_[0]))\nprint(\"Training Set Score: {0}\".format(reg_model.score(train_X,train_y)))\nmean_sq_err_train = np.mean((reg_model.predict(train_X) - train_y) ** 2)\nmean_sq_err_test = np.mean((reg_model.predict(test_X) - test_y) ** 2)\nprint(\"Mean square error train: {0}\".format(mean_sq_err_train[0]))\nprint(\"Mean square error test: {0}\".format(mean_sq_err_test[0]))\n", "intent": "We print out slope and intercept, and we calculate the training set score, as well as the Mean Squared Error for training and test predictions.\n"}
{"snippet": "print(\"Slope: {0}\".format(model_2A.coef_[0][0]))\nprint(\"Intercept: {0}\".format(model_2A.intercept_[0]))\nprint(\"Training Set Score: {0}\".format(model_2A.score(train_2A_X,train_2A_y)))\nmean_sq_err_train_2A = np.mean((model_2A.predict(train_2A_X) - train_2A_y) ** 2)\nmean_sq_err_test_2A = np.mean((model_2A.predict(test_2A_X) - test_2A_y) ** 2)\nprint(\"Mean square error train: {0}\".format(mean_sq_err_train_2A[0]))\nprint(\"Mean square error test: {0}\".format(mean_sq_err_test_2A[0]))\n", "intent": "We print/compute the slope, intercept, training set score, and the mean squared errors:\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('R-squared: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lgr, X_train, y_train, cv = 10)\nprint(scores)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "y_sub=df_test_ini[['id']]\ny_sub_rf=y_sub[['id']]\ny_sub_rf['status_group']=clf_rf.predict(df_test)\n", "intent": "In this step the model developed above is used to predict the labels of the given testing dataset as follows:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Now we can re-run predictions on this grid object just like we would with a normal model.\n"}
{"snippet": "example_measures = np.array([[4,2,1,1,1,2,3,2,1], [4,2,1,2,2,2,3,2,1]])\nexample_measures = example_measures.reshape(2,-1)\nprediction = clf.predict(example_measures)\nprint(prediction)\n", "intent": "What if we had 2 samples we would like to predict for? Easy - we reshape the arrays and feed it to the model.\n"}
{"snippet": "example_measures = example_measures.reshape(len(example_measures),-1)\nprediction = clf.predict(example_measures)\nprint(prediction)\n", "intent": "Actually, there's an even easier way that will work for any amount of values we want predicted.\n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "Let's evaluate our KNN model.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Now we can predict values for the testing data.\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the X_test values and evaluate our model.\n"}
{"snippet": "print(classification_report(y_test,rfc_pred))\n", "intent": "Now let's create a classification report from the results.\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "Let's see how this model does.\n"}
{"snippet": "grid_pred = grid.predict(X_test)\n", "intent": "Since this isn't default, we can see some improvement, though maybe not much (there isn't much room for improvement here).\n"}
{"snippet": "df['predicted'] = clf.predict(X)\nprint(df[df['predicted'] ==0]['survived'].describe())\nprint(df[df['predicted'] ==1]['survived'].describe())\n", "intent": "Let's take a look at the 2 clusters using describe()\n"}
{"snippet": "preds = nb.predict(X_test)\n", "intent": "Time to see how our model did.\n"}
{"snippet": "pipe_preds = pipe.predict(X_test)\n", "intent": "Let's get some predictions and see how they compare.\n"}
{"snippet": "sample = cv.transform(['do not eat here'])\nprint(nb.predict(sample))\n", "intent": "Looks like it helped increase accuracy a bit this time.\nLet's try out the original algorithm with a simple, new sentence.\n"}
{"snippet": "lm.predict(X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "metrics.explained_variance_score(y_test, predicted)\n", "intent": "We're also interested in knowing how much variance our model explains.\n"}
{"snippet": "forecast_set = clf.predict(X_Lately)\n", "intent": "Now we are ready to start predicting for the next 35 days.\n"}
{"snippet": "test_metrics = estimator.evaluate(input_fn = test_input_func,steps=1000)\n", "intent": "In order to see how our model did on the test/eval dataset, we run ```estimator.evaluate```.\n"}
{"snippet": "estimator.predict(input_fn = predict_input_func)\n", "intent": "We make the predictions. If you would like to see them right away, wrap the line below in ```list(<your code>)```.\n"}
{"snippet": "predictions = []\nfor x in estimator.predict(input_fn = predict_input_func):\n    predictions.append(x['predictions'])\n", "intent": "We read the predictions into an array.\n"}
{"snippet": "cross_val_score(nb, data, target, cv=10)\n", "intent": "Hitting all the positve reviews correctly, but almost missing all the negative reviews. \n"}
{"snippet": "pred_labels = final_model.predict(test_selected_features)\nprint(classification_report(test_labels, pred_labels))\nprint(confusion_matrix(test_labels,pred_labels))\n", "intent": "Compute metrics and confusion matrix:\n"}
{"snippet": "pred1 = model.predict(X_test)\npred1.shape\n", "intent": "Now predict test labels using method *'predict'* and evaluate prediction.\n"}
{"snippet": "print(model.metrics_names)\nprint(model.evaluate(X_test, y_test,verbose=0))\n", "intent": "Method *evaluate* returns values of all metrics the model calculates. Their names are in the attribute *.metrics_names*.\n"}
{"snippet": "def find_cars(image, clf=clf):\n    windows = tuple(sliding_windows(image))\n    window_images = np.array([window[1] for window in windows])\n    window_boxes = np.array([window[0] for window in windows])\n    cars = clf.predict(window_images)\n    boxes = window_boxes[cars==1.]\n    return boxes, draw_boxes(image, boxes)\nboxes, boxed_image = find_cars(image)\n", "intent": "find_cars is the first version of the function to detect vehicles inside an image. It uses the sliding_windows function with the classifier.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,fitted.predict(X_test),labels=[0, 1, 2])\n", "intent": "Classifier performs very well (**>99%**) on test data, too\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_pred\n", "intent": "Input known test data set to the model and see how it preidicts.\n"}
{"snippet": "y_predicted_train_SVC=modelSVC.predict(X_train)\ny_predicted_test_SVC=modelSVC.predict(X_test)\n", "intent": "Evaluating the model\n"}
{"snippet": "y_predicted_train_SVCScaled=modelSVCScaled.predict(X_train_scaled)\ny_predicted_test_SVCScaled=modelSVCScaled.predict(X_test_scaled)\nprint('train set sensitivity =',recall_score(y_train,y_predicted_train_SVCScaled))\nprint('train set accuracy =',accuracy_score(y_train,y_predicted_train_SVCScaled))\nprint('train set precision =',precision_score(y_train,y_predicted_train_SVCScaled))\nprint('test set sensitivity =',recall_score(y_test,y_predicted_test_SVCScaled))\nprint('test set accuracy =',accuracy_score(y_test,y_predicted_test_SVCScaled))\nprint('test set precision =',precision_score(y_test,y_predicted_test_SVCScaled))\n", "intent": "Evaluating the model\n"}
{"snippet": "def check_accuracy(y_actual, y_predict):\n    accuracy = metrics.accuracy_score(y_actual, y_predict)\n    return accuracy\n", "intent": "1) Write a function that computes the accuracy of the predicted values\n"}
{"snippet": "knn.predict_proba(x_test) \n", "intent": "3) The KNeighborsClassifier also has a [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test5.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "import itertools\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npred_labels = model.predict(val_images)\ny_test_class = np.argmax(val_labels,axis=1)\ny_pred_class = np.argmax(pred_labels,axis=1)\naccuracy = accuracy_score(y_test_class, y_pred_class)\nprint(accuracy)\n", "intent": "Lets see how the model performs on the test data. Note, there is no augmentation of images here.\n"}
{"snippet": "print classification_report(y_test,gbc.predict(X_test))\n", "intent": ">Perhaps even more impressively, the model's recall and f1 score are both an improvement on the random forest classifier.\n"}
{"snippet": "rfc2_cross_val = cross_val_score(rfc2, X_train, y_train, cv=10)\n", "intent": ">We can see that this new random forest, with the new parameters, outperforms the original random forest model.\n"}
{"snippet": "rfc3_cross_val = cross_val_score(rfc3, X_train, y_train, cv=10)\n", "intent": ">At best, this model with class weights balanced seems comparable to the previous model, while at worst it may even be a hair worse.\n"}
{"snippet": "gbc2_cross_val = cross_val_score(gbc2, X_train, y_train, cv=10)\n", "intent": ">We can see that this model outperforms the original gradient boosted model. It performs extremely well in fact.\n"}
{"snippet": "theta_min = np.matrix(res.x)\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint('accuracy = %d%%' % accuracy)\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "**Classification** accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y = learn.predict()\nx,_ = next(iter(md.val_dl))\n", "intent": "Visualizing results\n"}
{"snippet": "predicted_y = model.predict_classes(x, verbose=0)\nprint(classification_report(y, predicted_y))\n", "intent": "Precision tells us what fract of data is actually correct.\nRecall tell us how many predictions we made were actually correct.\n"}
{"snippet": "print('Target weights: {:}'.format(w_f))\nprint('Target in-sample error: {:.2%}'.format(in_sample_error(z, y, f)))\nprint('Target out-of-sample error: {:.2%}'.format(estimate_out_of_sample_error(P_x, phi, P_f, f)))\n", "intent": "With the metrics below we can see that the quality of the final hypothesis depends on the number of samples we have as predicted by learning theory.\n"}
{"snippet": "predict_unclass=model.predict(y)\nprint(predict_unclass)\n", "intent": "**_Model Implementation_**<br><br>\nThe model is now used to conduct the sentiment analysis.\n"}
{"snippet": "prediction_outputs = m.predict(file_to_process)\nprobs = prediction_outputs['probs'][0]\noriginal_shape = prediction_outputs['original_shape']\nprobs = probs[:, :, 1]  \nprobs = probs / np.max(probs)  \npage_bin = page_make_binary_mask(probs)\nbin_upscaled = cv2.resize(page_bin.astype(np.uint8, copy=False),\n                          tuple(original_shape[::-1]), interpolation=cv2.INTER_NEAREST)\n", "intent": "Predict each pixel's label\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.zeros([1])\n    for idx, layer in enumerate(style_layers):\n        style_current = gram_matrix(feats[layer])\n        style_loss += style_weights[idx] * tf.reduce_sum(tf.squared_difference(style_current, style_targets[idx]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_results = exported_pipeline.predict(test_features)\nincorrect_predictions = []\nfor index, digit in test_labels.iteritems():\n    if digit != test_results[index]:\n        incorrect_predictions.append(index)\nfor index in range(20):\n    index = incorrect_predictions[index]\n    print_prediction(index, test_features, test_labels)\n", "intent": "I'd like to see some examples of numbers the model gets wrong. Let's print out the first 20 incorrect digits.\n"}
{"snippet": "rgr_probs = rgr.predict_proba(X_test)\nrgr_probs_isdem = zip(*rgr_probs)[1]\nrgr_fpr, rgr_tpr, _ = roc_curve(y_test,rgr_probs_isdem,pos_label=1)\nrgr_auc = auc(rgr_fpr,rgr_tpr)\n", "intent": "For each, draw the ROC curve and calculate the AUC.\n"}
{"snippet": "target_predicted = logreg.predict(features_test)\n", "intent": "under the hood this implements c++ library that doesn't map directly to numpy array memory mapping; implies copies the arrays in memory\n"}
{"snippet": "visualize.plot_loss(net)\n", "intent": "Image from [Deep learning for computational biology](http://onlinelibrary.wiley.com/doi/10.15252/msb.20156651/full)\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for i in range(len(style_layers)):\n        gram_feat = gram_matrix(feats[style_layers[i]], normalize=True)\n        style_loss += style_weights[i] * tf.reduce_sum((gram_feat - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.cpu().numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.4f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.0\n    for i in range(len(style_layers)):\n        loss += style_weights[i] * torch.sum((gram_matrix(feats[style_layers[i]]) - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "(predict(X, pos) == y).mean()\n", "intent": "And from this we can just compute for the accuracy. We perform predictions, compare an equivalence to the ground-truth value `y`, and get the mean.\n"}
{"snippet": "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n", "intent": "Then, you can evaluate the model's performance in one line:\n"}
{"snippet": "classes = model.predict_classes(X_test, batch_size=32)\nproba = model.predict_proba(X_test, batch_size=32)\n", "intent": "Or you can generate predictions on new data:\n"}
{"snippet": "predictions = model.predict(X_test)\nlabels = primary_site_encoder.classes_.tolist()\n", "intent": "Use the trained model to predict all the test samples and generate a confusion matrix\n"}
{"snippet": "pred_train = regr.predict(train_x)\n", "intent": "Predictions on train data:\n"}
{"snippet": "math.sqrt(mean_squared_error(train_y, pred_train ))\n", "intent": "Evaluating the model:\n"}
{"snippet": "pred_test = regr.predict(test_x)\n", "intent": "Evaluating the model with non previously seen data:\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'test/styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])    \n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    i = 0\n    style_loss = 0.0\n    for idx in style_layers:\n        a = gram_matrix(feats[idx])\n        loss_idx = torch.sum(torch.pow(a - style_targets[i], 2)) * style_weights[i]\n        style_loss += loss_idx\n        i += 1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = linear_regression_model.predict(x)\n", "intent": "Get predictions and coefficients\n"}
{"snippet": "x_reg_line = np.linspace(x.min() - 0.01, x.max() + 0.01, 1000)\nreg_line = linear_regression_model.predict(x_reg_line[:, np.newaxis])\n", "intent": "Plot data and regression line\n"}
{"snippet": "prediction = knn.predict(X_new)\nprediction\n", "intent": "To make prediction we call the predict method of the knn object\n"}
{"snippet": "print 'F1 score (all ones)', metrics.f1_score(data.y, np.ones(data.shape[0]))\n", "intent": "Some to compare all further classification attempts with:\n- all predictions are 1\n- dummy classifier\n"}
{"snippet": "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n", "intent": "This approach gives a different best value for C. Lets see how does this model perform on the test data.\n"}
{"snippet": "hint(\"Making prediction...\")\nYva_ = model.predict(Xva)\nhint(\"Done\")\n", "intent": "Making prediction on the validation set.\n"}
{"snippet": "cocoEval.evaluate()\ncocoEval.accumulate()\n", "intent": "This step might take a few minutes, so please be patient.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(rfr, X_train, y_train)\n", "intent": "***Accuracy Score:*** 77%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn, X_train, y_train)\n", "intent": "***Accuracy Score:*** 74%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(lclf2, X_train, y_train)\n", "intent": "<br><br>\n***Accuracy Score:*** 78.2%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(rfr2, X_train, y_train)\n", "intent": "***Accuracy Score:*** 78.7%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn2, X_train, y_train)\n", "intent": "***Accuracy Score:*** 75%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(lclf3, X_train, y_train)\n", "intent": "<br><br>\n***Accuracy Score:*** 80.2%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(rfr3, X_train, y_train)\n", "intent": "***Accuracy Score:*** 77.6%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn3, X_train, y_train)\n", "intent": "***Accuracy Score:*** 75.9%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(lclf4, X_train, y_train)\n", "intent": "<br><br>\n***Accuracy Score:*** 83.4%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(rfr4, X_train, y_train)\n", "intent": "<br><br>\n***Accuracy Score:*** 81.2%\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn4, X_train, y_train)\n", "intent": "<br><br>\n***Accuracy Score:*** 79.7%\n"}
{"snippet": "from sklearn import metrics\ny_pred = knn.predict(X)\nmetrics.accuracy_score(y, y_pred)\n", "intent": "***Accuracy Score:*** 80.8%\n"}
{"snippet": "mse = np.mean((bos.PRICE - lr.predict(X)) ** 2)\nmse\n", "intent": "Note: As the actual housing prices increase, we come to see some errors in the predicted price.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nprint(\"Gaussian Mixed Model: %s\" % silhouette_score(X, pred_gmm))\nprint(\"K-nearest neigbour: %s\" % silhouette_score(X, pred))\n", "intent": "By visually comparing the result of KMeans and GMM clustering, which one was better able to match the original?\n"}
{"snippet": "def report_accuracy(model, test_tensors):\n    predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n    test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n    print('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndef precision_recall(y_pred, y_true):\n    rec_per_species = [accuracy_score(y_pred[y_true==k], y_true[y_true==k])\n                       for k in np.unique(y_true)]\n    rec = np.mean(rec_per_species)\n    acc_per_species = [accuracy_score(y_pred[y_pred==k], y_true[y_pred==k])\n                       for k in np.unique(y_true) if (y_pred==k).any()]\n    prec = np.mean(acc_per_species)\n", "intent": "As an example evaluation metric, the average species-level precision and recall are used below.\n"}
{"snippet": "result = knn.predict(Xtest)\nprint result\nprint iris.target_names[result]\n", "intent": "Once $X$ is properly formatted, go ahead and do the prediction.  \nCan you output the *name* rather than the index of the iris type?\n"}
{"snippet": "test_proj = [[1000, 11, 1]]\nclass_code_1 = clf.predict(test_proj) \ntest_proj = [[15000, 30.088918, 2.000000]]\nclass_code_2 = clf.predict(test_proj) \nclass_code_1, class_code_2\n", "intent": "Just to make sure nothing too crazy was going on with our classifier, we checked if within some reasonable bounds, projects would succeed.\n"}
{"snippet": "best_model = best_linear_model\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...?\n"}
{"snippet": "best_model = best_kernel_model\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...?\n"}
{"snippet": "score = cross_val_score(mass_svr, X, y_mass, cv = k_fold, scoring='neg_mean_squared_error')\nprint ('Mass:', -score.mean().round(5),'+/-', score.std().round(3))\nscore = cross_val_score(smax_rr, X, y_smax, cv = k_fold, scoring='neg_mean_squared_error')\nprint ('Smax:', -score.mean().round(5),'+/-', score.std().round(3))\nscore = cross_val_score(u2_gpr_1, X, y_u2, cv = k_fold, scoring='neg_mean_squared_error')\nprint ('  U2:', -score.mean().round(6),'+/-', score.std().round(5))\n", "intent": "for Mass: <b>SVR</b>.\nfor Smax: <b>Ridge Regression</b>.\nfor U2: <b>GPR (RBF)</b>.\n"}
{"snippet": "model_input = morris_sampler.sample(problem, N=1000, num_levels=4, grid_jump=2)\nmodel_output_mass =  reg_mass.predict(model_input)\nmodel_output_smax =  reg_smax.predict(model_input)\nmodel_output_u2 =  reg_u2.predict(model_input)\nMI_mass = morris_analyzer.analyze(problem,model_input, model_output_mass)\nMI_smax = morris_analyzer.analyze(problem, model_input,model_output_smax)\nMI_u2 = morris_analyzer.analyze(problem, model_input,model_output_u2)\n", "intent": "<br/> <!--Intentionally left blank-->\n"}
{"snippet": "Y_test = gbm.predict(xgb.DMatrix(X_test))\n", "intent": "- Predicting Y_test\n"}
{"snippet": "clf = trainRawNB(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Confussion Matrix: \")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n", "intent": "We train the algorithm and predict over the test set. We also check performance over the clean test set.\n"}
{"snippet": "ut = np.array([[1,0],[0,1]])\ny_nbus = nbusXlabel(X_testAtt, clf, ut)\nxxyy = ACRAparPosterior(X_testAtt, clf, var = 0.5)\ny_ACRA = ACRAlabel(xxyy, ut)\nprint(\"Confussion Matrix NB: \")\nprint(confusion_matrix(y_test, y_nbus))\nprint(\"Accuracy Score NB: \", accuracy_score(y_test, y_nbus))\nprint(\"Confussion Matrix ACRA: \")\nprint(confusion_matrix(y_test, y_ACRA))\nprint(\"Accuracy Score ACRA: \", accuracy_score(y_test, y_ACRA))\n", "intent": "Now we calculate ACRA and utility sensitive naive bayes labels for a given utility.\n"}
{"snippet": "clf = trainRawNB(X_train, y_train)\nut = np.array([[1,-1],[-10,1]])\ny_pred = nbusXlabel(X_test, clf, ut)\nprint(\"Confussion Matrix: \")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n", "intent": "We train the algorithm and predict over the test set. We also check performance.\n"}
{"snippet": "ut = np.array([[1,-1],[-10,1]])\ny_nbus = nbusXlabel(X_testAtt, clf, ut)\nxxyy = ACRAparPosterior(X_testAtt, clf, var = 0.5)\ny_ACRA = ACRAlabel(xxyy, ut)\nprint(\"Confussion Matrix NB: \")\nprint(confusion_matrix(y_test, y_nbus))\nprint(\"Accuracy Score NB: \", accuracy_score(y_test, y_nbus))\nprint(\"Confussion Matrix ACRA: \")\nprint(confusion_matrix(y_test, y_ACRA))\nprint(\"Accuracy Score ACRA: \", accuracy_score(y_test, y_ACRA))\n", "intent": "Now we calculate ACRA and utility sensitive naive bayes labels for a given utility.\n"}
{"snippet": "import numpy as np\nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "import numpy as np\nVGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def test_train_log_loss(estimator, X_test, X_train, y_test, y_train):\n    res_test = []\n    res_train = []\n    for s_test, s_train in zip(estimator.staged_decision_function(X_test), \n                               estimator.staged_decision_function(X_train)):\n        res_test.append(log_loss(y_test, transform(s_test)))\n        res_train.append(log_loss(y_train, transform(s_train)))\n    return res_test, res_train\n", "intent": "Losses at given learning rate\n"}
{"snippet": "def predict_pg13(series):\n    pred = []\n    for i in series:\n        pred.append('PG-13')\n    return pred\ny11_pg13pred = predict_pg13(y11_test)\nprint(\"Baseline Accuracy: \" + str(accuracy_score(y11_test, y11_pg13pred)))\n", "intent": "Make a baseline stupid predictor that always predicts the label that is present the most in the data. Calculate its accuracy on a test set.\n"}
{"snippet": "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\nfpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n", "intent": "**Challenge 2**\nFor each, draw the ROC curve and calculate the AUC.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n", "intent": "---\n* Classification Report\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, \\\n                    recall_score, f1_score\naccuracy = accuracy_score(y_pred, y_test)\nprecision = precision_score(y_pred, y_test, average='weighted')\nrecall = recall_score(y_pred, y_test, average='weighted')\nf1 = f1_score(y_pred, y_test, average='weighted')\nprint('Accuracy score is {0:.2f}'.format(accuracy))\nprint('Precision score is {0:.2f}'.format(precision))\nprint('Recall score is {0:.2f}'.format(recall))\nprint('F1 score is {0:.2f}'.format(f1))\n", "intent": "---\n* **Accuracy**, **Precision**, **Recall**, **F1**\n* Note that ROC-Curve and Precision-Recall-Curve are restricted to binary classification.\n"}
{"snippet": "from sklearn.metrics import classification_report\nclass_rep = classification_report(y_test, y_pred)\nprint('classification report: \\n {}'.format(class_rep))\n", "intent": "---\n* Classification Report\n"}
{"snippet": "kmeans_colors = clf.predict(color_data)\n", "intent": "Now we can replace every color in the image with the nearest cluster center.\n"}
{"snippet": "X_test = np.array([[a,b] for a in np.linspace(0,3,50) for b in np.linspace(1,4,50)])\ny_test = reg.predict(X_test)\n", "intent": "Let's create a grid of test data to visualize the decision boundary found by our logistic regression model:\n"}
{"snippet": "a = model.predict(inputs, batch_size=1) \nb = bbox_util.detection_out(a)\n", "intent": "Run the two operations again, timing them. This is redundant and solely to quantify the inference time.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature,axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets,axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant([0.])\n    j = 0\n    for i in style_layers:\n        gram_diff = gram_matrix(feats[i]) - style_targets[j]\n        style_loss += tf.reduce_sum(gram_diff*gram_diff)*style_weights[j]\n        j += 1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "lof.contamination = 0.02 / 100\ninliers = lof.fit_predict(X_scaled)\n", "intent": "We are getting a nice elbow shape curve for LOF score at 0.02% of datapoint.\n<br>\nSetting the contamination ratio = 0.02%\n"}
{"snippet": "y_pred = reg.predict(X_test)\nprint(performance_metric(y_test, y_pred))\n", "intent": "**Answer: **\nBefore we dive into the qualitative issues with the model, let's look at how well the model performs by executing the code below:\n"}
{"snippet": "predictionVec = initialLinearMod.predict(langFrame)\n", "intent": "Let us first study how well our model is performing on the dataset. We will use the error metric root mean-squared error (RMSE).\n"}
{"snippet": "y_pred = log_reg.predict(X_test)\nprint('Accuracy: {:.4f}'.format(accuracy_score(Y_test,y_pred)))\n", "intent": "Based on the ROC AUC score, simple logistic regression produces strong results on the cross validation set\n"}
{"snippet": "y_pred = log_reg.predict(df_test_final)\nprint('Accuracy: {:.4f}'.format(accuracy_score(df_test_final_Y,y_pred)))\n", "intent": "Although the test set returned slightly lower ROC AUC score, logistic regression still produces a strong result\n"}
{"snippet": "roc_auc_score(titanic['Survived'], lm.predict(feature_set))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "sns.distplot(y_test - lm.predict(X_test), bins = 50)\n", "intent": "You should have gotten a very good model with a good fit. Let's quickly explore the residuals to make sure everything was okay with our data. \n"}
{"snippet": "def predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n    return pred\n", "intent": "Lets start mnaking predictions\n"}
{"snippet": "hide_code\nmlp_model.load_weights('weights.best.mlp.hdf5')\nmlp_score = mlp_model.evaluate(x_test, y_test, verbose=0)\nprint(\"MLP Model Accuracy: \", mlp_score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "from sklearn import model_selection\ncross_auc=model_selection.cross_val_score(svc,x_train, y_train, cv=5)\n", "intent": "lets check accuracy of training fit through 8 folds\n"}
{"snippet": "prediction = lm.predict(X_test)\n", "intent": "** Using lm.predict() to predict off the X_test set of the data.**\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print(\">\", pair[0])\n        print(\"=\", pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = \" \".join(output_words)\n        print(\"<\", output_sentence)\n        print(\"\")\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "final_pred = svm_clf.predict(X_test_tfidf)\nfinal_pred\n", "intent": "From all the above models, we see that SVM gives us the best result, thus we do predictions on the test data using the SVM Classifier\n"}
{"snippet": "p_test = model.predict(x_test, verbose=1)\n", "intent": "Let's predict the images.\n"}
{"snippet": "def predict(parameters, X):\n    A2, cache = forward_pass(X, parameters)\n    predictions = np.round(A2)\n    return predictions\n", "intent": "To make predictions, we have defined the function predict wich takes learned parmeters and applyes it to calculate predictions.\n"}
{"snippet": "Y_predictions_test = predict(parameters_final, X_test)\nY_predictions_train = predict(parameters_final, X_train)\n", "intent": "Here we will make predictions on our test and training set, and we will print the accuracy.\n"}
{"snippet": "average_precision_score(actual, predicted)\n", "intent": "Implementation from sklearn\n"}
{"snippet": "preds = model.predict(X_test)\npreds[1:10]\n", "intent": "We can make predictions on new data using:\n"}
{"snippet": "preds = model.predict(X_test)\nround(100*sum(preds == y_test)/float(len(y_test)), 3)\n", "intent": "With the model trained, we can then predict new data:\n"}
{"snippet": "predictions = clf.predict(...)\nacc = accuracy_score(tes...t_y, predictions)\nprint('Accuracy on Test set =', acc)\n", "intent": "Apply the same approach to find the accuracy score on the test set.\n"}
{"snippet": "print('Train-set performance --> loss: {:.4f}, accuracy: {:.4f}'.format(*model.evaluate(x_train, y_train, verbose=0)))\nprint('Test-set performance --> loss: {:.4f}, accuracy: {:.4f}'.format(*model.evaluate(x_test, y_test, verbose=0)))\n", "intent": "First, let's evaluate our model on the train and test sets.\n"}
{"snippet": "from minepy import MINE\nm = MINE()\nx = np.random.uniform(-1,1,1000)\nm.compute_score(x,x**2)\nprint m.mic()\n", "intent": "https://en.wikipedia.org/wiki/Maximal_information_coefficient\n"}
{"snippet": "kf = cross_validation.KFold(len(X_train), n_folds=5)\nscores = cross_validation.cross_val_score(clf,\n                                          X, \n                                          Y,\n                                          cv=kf,\n                                          scoring='accuracy') \nprint scores\nprint(\"\\nAccuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n", "intent": "<h5> k-folding </h5>\n"}
{"snippet": "predictions = pipeline.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = torch.sum(torch.zeros(1)).type(dtype)\n    for idx, sl in enumerate(style_layers):\n        style_loss.add_((gram_matrix(feats[sl]) - style_targets[idx]).pow(2).sum() * style_weights[idx])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\ndef performance_metric(y_true, y_predict):\n    error = mean_absolute_error(y_true, y_predict)\n    return error\ntry:\n    total_error = performance_metric(y_train, y_train)\n    print \"Successfully performed a metric calculation!\"\nexcept:\n    print \"Something went wrong with performing a metric calculation.\"\n", "intent": "For regression as an example:\n"}
{"snippet": "print(recall_score(labels_test, pred))\n", "intent": "The recall_score is one of the many methods of evaluating the code.\n"}
{"snippet": "item_prediction = predict(train_data_matrix, item_similarity, type='item')\nuser_prediction = predict(train_data_matrix, user_similarity, type='user')\n", "intent": "Let's predict ratings for item-based CF and User-based CF.\n"}
{"snippet": "probs = RNN_predict(seq_len,x,rnn_size,weights,biases)\n", "intent": "Get ready for session\n"}
{"snippet": "def calculate_score(doc1, new_doc):\n    return np.dot(topic_distribution(doc1), topic_distribution(new_doc))\n", "intent": "To calculate the final \"closeness score\", I use the simple dot product of individual topic scores:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nResnet50_test_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Resnet50 Test accuracy: %.4f%%' % Resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = clf.predict(x_test)\ni=0\nfor row in y_test.iterrows():\n    prediction = predictions[i]\n    i+=1\n    actual = row[1]['usage']\n    print(prediction,actual)\nprint(list(y_test['usage']))\nprint(list(predictions))\nmetrics.accuracy_score(list(y_test['usage']), predictions)\n", "intent": "<h4>Check predictions</h4>\n"}
{"snippet": "y_pred = predict_svm(gist_train, gist_test, y_train, y_test)\ny_pred = [class_mapping[elem] for elem in y_pred]\ny_t = [class_mapping[elem] for elem in y_test]\nprint('Classification report:')\nprint(classification_report(y_t, y_pred))\n", "intent": "SVM classification:\n"}
{"snippet": "y_pred = model.predict(x_test) \ny_prob = model.predict_proba(x_test) \n", "intent": "**Step -6 :Evaluation model**\n"}
{"snippet": "res2 = trained_model.evaluate(val_rdd,2800,[MAE()])\n", "intent": "* output the final MAE on validation dataset using BigDL API\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint(f1_score(y_test, predict(X_test)))\n", "intent": "Now, we can apply our metric of choice (f1 score) to the above predictor and see what we get.\n"}
{"snippet": "y_train_pred_pipeline = best_pipeline.predict(X_train_scaled)\ny_test_pred_pipeline  = best_pipeline.predict(X_test_scaled)\nprint(\"Training F1 Score: {:.2f}\".format(f1_score(y_train_scaled, y_train_pred_pipeline)))\nprint(\"Testing  F1 Score: {:.2f}\".format(f1_score(y_test_scaled,  y_test_pred_pipeline)))\n", "intent": "With the parameters of the learner laid out, we move to see how this learner performs on the testing set.\n"}
{"snippet": "from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\ntest_predict = model.predict(test_texts)\nprecision = str(round(precision_score(test_labels, test_predict, average='macro'),2))\nrecall = str(round(recall_score(test_labels, test_predict, average='macro'),2))\naccuracy = str(round(accuracy_score(test_labels, test_predict),2))\nprint('Precision',precision )\nprint('Recall',recall )\nprint('Accuracy',accuracy )\n", "intent": "Evaluation using precision, recall and accuracy\n"}
{"snippet": "prediction_ridge_01 = regr_01.predict(X_mat_ridge_norm)\nmin_cost_ridge_01 = mean_squared_error(prediction_ridge_01, Y_mat_ridge)\nprint (\"Minimum cost achieved : \", min_cost_ridge_01)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction_ridge_1 = regr_1.predict(X_mat_ridge_norm)\nmin_cost_ridge_1 = mean_squared_error(prediction_ridge_1, Y_mat_ridge)\nprint (\"Minimum cost achieved : \", min_cost_ridge_1)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction_ridge_10 = regr_10.predict(X_mat_ridge_norm)\nmin_cost_ridge_10 = mean_squared_error(prediction_ridge_10, Y_mat_ridge)\nprint (\"Minimum cost achieved : \", min_cost_ridge_10)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction_lasso_0001 = regr_000l.predict(X_mat_lasso_norm)\nmin_cost_lasso_0001 = mean_squared_error(prediction_lasso_0001, Y_mat_lasso)\nprint (\"Minimum cost achieved : \", min_cost_lasso_0001)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction_lasso_001 = regr_00l.predict(X_mat_lasso_norm)\nmin_cost_lasso_001 = mean_squared_error(prediction_lasso_001, Y_mat_lasso)\nprint (\"Minimum cost achieved : \", min_cost_lasso_001)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction_lasso_01 = regr_0l.predict(X_mat_lasso_norm)\nmin_cost_lasso_01 = mean_squared_error(prediction_lasso_01, Y_mat_lasso)\nprint (\"Minimum cost achieved : \", min_cost_lasso_01)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction_lasso_1 = regr_l.predict(X_mat_lasso_norm)\nmin_cost_lasso_1 = mean_squared_error(prediction_lasso_1, Y_mat_lasso)\nprint (\"Minimum cost achieved : \", min_cost_lasso_1)\n", "intent": "Minimum Cost achieved\n"}
{"snippet": "prediction2_ridge_01_train = regr2_01.predict(X2_train_16[:, 1:])\nmin_cost2_ridge_01_train = mean_squared_error(prediction2_ridge_01_train, Y2_train_16)\nprint (\"Minimum cost achieved on training dataset : \", min_cost2_ridge_01_train)\n", "intent": "Minimum Cost achieved on training dataset\n"}
{"snippet": "prediction2_ridge_01_valid = regr2_01.predict(X2_valid_16[:, 1:])\nRMSE2_ridge_01_valid = math.sqrt((1/len(Y2_valid_16))*((np.square(prediction2_ridge_01_valid-Y2_valid_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE2_ridge_01_valid)\n", "intent": "Root Mean Square Error using lambda 0.1 on validation set\n"}
{"snippet": "prediction2_ridge_1_train = regr2_1.predict(X2_train_16[:, 1:])\nmin_cost2_ridge_1_train = mean_squared_error(prediction2_ridge_1_train, Y2_train_16)\nprint (\"Minimum cost achieved on training dataset using lambda 1 : \", min_cost2_ridge_1_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 1.0\n"}
{"snippet": "prediction2_ridge_1_valid = regr2_1.predict(X2_valid_16[:, 1:])\nRMSE2_ridge_1_valid = math.sqrt((1/len(Y2_valid_16))*((np.square(prediction2_ridge_1_valid-Y2_valid_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE2_ridge_1_valid)\n", "intent": "Root Mean Square Error using lambda 1 on validation set\n"}
{"snippet": "prediction2_ridge_10_train = regr2_10.predict(X2_train_16[:, 1:])\nmin_cost2_ridge_10_train = mean_squared_error(prediction2_ridge_10_train, Y2_train_16)\nprint (\"Minimum cost achieved on training dataset using lambda 10 : \", min_cost2_ridge_10_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 10.0\n"}
{"snippet": "prediction2_ridge_10_valid = regr2_10.predict(X2_valid_16[:, 1:])\nRMSE2_ridge_10_valid = math.sqrt((1/len(Y2_valid_16))*((np.square(prediction2_ridge_10_valid-Y2_valid_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE2_ridge_10_valid)\n", "intent": "Root Mean Square Error using lambda 10 on validation set\n"}
{"snippet": "prediction2_ridge_01_test = regr2_01.predict(X2_test_16[:, 1:])\nRMSE2_ridge_01_test = math.sqrt((1/len(Y2_test_16))*((np.square(prediction2_ridge_01_test-Y2_test_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE2_ridge_01_test)\n", "intent": "Root mean square error on test dataset using lambda 0.1\n"}
{"snippet": "prediction2_lasso_01_train = regr2_0l.predict(X2_train_16[:, 1:])\nmin_cost2_lasso_01 = mean_squared_error(prediction2_lasso_01_train, Y2_train_16)\nprint (\"Minimum cost achieved on training dataset using lambda 0.1: \", min_cost2_lasso_01)\n", "intent": "Minimum Cost achieved using lambda 0.1\n"}
{"snippet": "prediction2_lasso_01_train = regr2_01.predict(X2_train_16[:, 1:])\nRMSE2_lasso_01_train = math.sqrt((1/len(Y2_train_16))*((np.square(prediction2_lasso_01_train-Y2_train_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set using lambda 0.1: \", RMSE2_lasso_01_train)\n", "intent": "Root Mean Square Error on training dataset using lambda 0.1\n"}
{"snippet": "prediction2_lasso_01_valid = regr2_01.predict(X2_valid_16[:, 1:])\nRMSE2_lasso_01_valid = math.sqrt((1/len(Y2_valid_16))*((np.square(prediction2_lasso_01_valid-Y2_valid_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set using lambda 0.1: \", RMSE2_lasso_01_valid)\n", "intent": "Root Mean Square Error on validation dataset using lambda 0.1\n"}
{"snippet": "prediction2_lasso_1_train = regr2_l.predict(X2_train_16[:, 1:])\nmin_cost2_lasso_1 = mean_squared_error(prediction2_lasso_1_train, Y2_train_16)\nprint (\"Minimum cost achieved on training dataset using lambda 1.0: \", min_cost2_lasso_1)\n", "intent": "Minimum Cost achieved using lambda 1.0\n"}
{"snippet": "prediction2_lasso_1_train = regr2_1.predict(X2_train_16[:, 1:])\nRMSE2_lasso_1_train = math.sqrt((1/len(Y2_train_16))*((np.square(prediction2_lasso_1_train-Y2_train_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set using lambda 1.0: \", RMSE2_lasso_1_train)\n", "intent": "Root Mean Square Error on training dataset using lambda 1.0\n"}
{"snippet": "prediction2_lasso_1_valid = regr2_1.predict(X2_valid_16[:, 1:])\nRMSE2_lasso_1_valid = math.sqrt((1/len(Y2_valid_16))*((np.square(prediction2_lasso_1_valid-Y2_valid_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set using lambda 1: \", RMSE2_lasso_1_valid)\n", "intent": "Root Mean Square Error on validation dataset using lambda 1\n"}
{"snippet": "prediction2_lasso_10_train = regr2_l0.predict(X2_train_16[:, 1:])\nmin_cost2_lasso_10 = mean_squared_error(prediction2_lasso_10_train, Y2_train_16)\nprint (\"Minimum cost achieved on training dataset using lambda 10.0: \", min_cost2_lasso_10)\n", "intent": "Minimum Cost achieved using lambda 10.0\n"}
{"snippet": "prediction2_lasso_10_train = regr2_10.predict(X2_train_16[:, 1:])\nRMSE2_lasso_10_train = math.sqrt((1/len(Y2_train_16))*((np.square(prediction2_lasso_10_train-Y2_train_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set using lambda 1.0: \", RMSE2_lasso_10_train)\n", "intent": "Root Mean Square Error on training dataset using lambda 10.0\n"}
{"snippet": "prediction2_lasso_10_valid = regr2_10.predict(X2_valid_16[:, 1:])\nRMSE2_lasso_10_valid = math.sqrt((1/len(Y2_valid_16))*((np.square(prediction2_lasso_10_valid-Y2_valid_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set using lambda 10: \", RMSE2_lasso_10_valid)\n", "intent": "Root Mean Square Error on validation dataset using lambda 10\n"}
{"snippet": "prediction2_lasso_01_test = regr2_01.predict(X2_test_16[:, 1:])\nRMSE2_lasso_01_test = math.sqrt((1/len(Y2_test_16))*((np.square(prediction2_lasso_01_test-Y2_test_16)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on test dataset: \", RMSE2_lasso_01_test)\n", "intent": "Root mean square error on test dataset using lambda 0.1\n"}
{"snippet": "prediction_tf_ridge_01_train = regr_tf_01.predict(X_train_tf[:, 1:])\nmin_cost_tf_ridge_01_train = mean_squared_error(prediction_tf_ridge_01_train, Y_train_tf)\nprint (\"Minimum cost achieved on training dataset using lambda 0.1 : \", min_cost_tf_ridge_01_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 0.1\n"}
{"snippet": "prediction_tf_ridge_01_valid = regr_tf_01.predict(X_valid_tf[:, 1:])\nRMSE_tf_ridge_01_valid=math.sqrt((1/len(Y_valid_tf))*((np.square(prediction_tf_ridge_01_valid-Y_valid_tf)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_ridge_01_valid)\n", "intent": "Root Mean Square Error using lambda 0.1 on validation set\n"}
{"snippet": "prediction_tf_ridge_1_train = regr_tf_1.predict(X_train_tf[:, 1:])\nmin_cost_tf_ridge_1_train = mean_squared_error(prediction_tf_ridge_1_train, Y_train_tf)\nprint (\"Minimum cost achieved on training dataset using lambda 1 : \", min_cost_tf_ridge_1_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 1\n"}
{"snippet": "prediction_tf_ridge_1_valid = regr_tf_1.predict(X_valid_tf[:, 1:])\nRMSE_tf_ridge_1_valid = math.sqrt((1/len(Y_valid_tf))*((np.square(prediction_tf_ridge_1_valid-Y_valid_tf)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_ridge_1_valid)\n", "intent": "Root Mean Square Error using lambda 1 on validation set\n"}
{"snippet": "prediction_tf_ridge_10_train = regr_tf_10.predict(X_train_tf[:, 1:])\nmin_cost_tf_ridge_10_train = mean_squared_error(prediction_tf_ridge_10_train, Y_train_tf)\nprint (\"Minimum cost achieved on training dataset using lambda 10 : \", min_cost_tf_ridge_10_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 10\n"}
{"snippet": "prediction_tf_ridge_10_valid = regr_tf_10.predict(X_valid_tf[:, 1:])\nRMSE_tf_ridge_10_valid = math.sqrt((1/len(Y_valid_tf))*((np.square(prediction_tf_ridge_10_valid-Y_valid_tf)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_ridge_10_valid)\n", "intent": "Root Mean Square Error using lambda 10 on validation set\n"}
{"snippet": "prediction_tf_ridge_10_test = regr_tf_10.predict(X_test_tf[:, 1:])\nRMSE_tf_ridge_10_test = math.sqrt((1/len(Y_test_tf))*((np.square(prediction_tf_ridge_10_test-Y_test_tf)).sum(axis=0)))\nprint (\"Root Mean Square Error achieved on test set: \", RMSE_tf_ridge_10_test)\n", "intent": "Root mean square error on test dataset using lambda 10\n"}
{"snippet": "prediction_tf_lasso_01_train = regr_lasso_tf_01.predict(X_train_tf[:, 1:])\nmin_cost_tf_lasso_01_train = mean_squared_error(prediction_tf_lasso_01_train, Y_train_tf)\nprint (\"Minimum cost achieved on training dataset using lambda 0.1 : \", min_cost_tf_lasso_01_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 0.1\n"}
{"snippet": "prediction_tf_lasso_01_valid = regr_lasso_tf_01.predict(X_valid_tf[:, 1:])\nRMSE_tf_lasso_01_valid = math.sqrt(mean_squared_error(prediction_tf_lasso_01_valid, Y_valid_tf))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_lasso_01_valid)\n", "intent": "Root Mean Square Error using lambda 0.1 on validation set\n"}
{"snippet": "prediction_tf_lasso_1_train = regr_lasso_tf_1.predict(X_train_tf[:, 1:])\nmin_cost_tf_lasso_1_train = mean_squared_error(prediction_tf_lasso_1_train, Y_train_tf)\nprint (\"Minimum cost achieved on training dataset using lambda 1 : \", min_cost_tf_lasso_1_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 1\n"}
{"snippet": "prediction_tf_lasso_1_valid = regr_lasso_tf_1.predict(X_valid_tf[:, 1:])\nRMSE_tf_lasso_1_valid = math.sqrt(mean_squared_error(prediction_tf_lasso_1_valid, Y_valid_tf))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_lasso_1_valid)\n", "intent": "Root Mean Square Error using lambda 1 on validation set\n"}
{"snippet": "prediction_tf_lasso_10_train = regr_lasso_tf_10.predict(X_train_tf[:, 1:])\nmin_cost_tf_lasso_10_train = mean_squared_error(prediction_tf_lasso_10_train, Y_train_tf)\nprint (\"Minimum cost achieved on training dataset using lambda 10 : \", min_cost_tf_lasso_10_train)\n", "intent": "Minimum Cost achieved on training dataset using lambda 10\n"}
{"snippet": "prediction_tf_lasso_10_valid = regr_lasso_tf_10.predict(X_valid_tf[:, 1:])\nRMSE_tf_lasso_10_valid = math.sqrt(mean_squared_error(prediction_tf_lasso_10_valid, Y_valid_tf))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_lasso_10_valid)\n", "intent": "Root Mean Square Error using lambda 10 on validation set\n"}
{"snippet": "prediction_tf_lasso_01_test = regr_lasso_tf_01.predict(X_test_tf[:, 1:])\nRMSE_tf_lasso_01_test = math.sqrt(mean_squared_error(prediction_tf_lasso_01_test, Y_test_tf))\nprint (\"Root Mean Square Error achieved on validation set: \", RMSE_tf_lasso_01_test)\n", "intent": "Root mean square error on test dataset using lambda 0.1\n"}
{"snippet": "features = []\nfor image_path in tqdm(images):\n    img, x = get_image(image_path);\n    feat = feat_extractor.predict(x)[0]\n    features.append(feat)\n", "intent": "So, we generated those NumPy arrays above before for 1 image... now let's generate these same arrays for all 20k.\nThis takes a while.\n"}
{"snippet": "prediction = np.argmax(model.predict(x = X_test, batch_size=64), axis=1)\nhmap(Y_test, prediction)\n", "intent": "Let us now test the model against our test set.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx, feat_idx in enumerate(style_layers):\n        gram = gram_matrix(feats[feat_idx])\n        diff_sq = (gram-style_targets[idx])**2\n        style_loss += style_weights[idx]*diff_sq.sum()\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)\n", "intent": "RandomizedSearchCV model\n"}
{"snippet": "print \"Accuracy = %.3f\" % (metrics.accuracy_score(decision_tree.predict(X), Y))\nX2 =test_df.drop(['churndep'],axis = 1)\nY2 = test_df['churndep']\nprint \"Accuracy = %.3f\" % (metrics.accuracy_score(decision_tree.predict(X2), Y2))\n", "intent": "6\\. Using the classifier built in 2.2, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?\n"}
{"snippet": "preds = model.predict(img) \nprint(\"[Info]Image is predicted\")\nprint(decode_predictions(preds)) \nLabel_no,class_label,prob = decode_predictions(preds)[0][0]\nprint('\\n',' [Info]It predicted',class_label,' with probability ',prob)\n", "intent": "will be downloaded for the first time. This json file contains class names. \n"}
{"snippet": "mn_scores = cross_val_score(mn_clas, X2, Y, cv=5)\nsvc_scores = cross_val_score(svc_clas, X2, Y, cv=5)\nrf_scores = cross_val_score(rf_clas, X2, Y, cv=5)\n", "intent": "Note that the number of features of X2 has increased by 1 since we are using the `useful` column as a feature for training.\n"}
{"snippet": "mn_scores = cross_val_score(mn_clas, X4, Y, cv=5)\nsvc_scores = cross_val_score(svc_clas, X4, Y, cv=5)\nrf_scores = cross_val_score(rf_clas, X4, Y, cv=5)\n", "intent": "Note that the features of X4 has increased by 2 since now we have the `useful` and `cool` columns as features for training.\n"}
{"snippet": "score = model.evaluate(x_train_total, y_train_total, verbose=1)\nprint('Train score:', score[0])\nprint('Train accuracy:', score[1])\npred_test = model.predict(Xtest)\n", "intent": "Use the new model to predict the test data, fisrt to see the train accuaracy and then the test result\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We now try out our model on the test dataset of dog images. Good test accuracy!\n"}
{"snippet": "def RocCurve(predprobs, actual_y):\n    pos_probs = []\n    for i in predprobs:\n        pos_probs.append(i[1])\n    fpr, tpr, threshold = roc_curve(actual_y, pos_probs)\n    score = roc_auc_score(actual_y, pos_probs, average = 'weighted')\n    return fpr, tpr, threshold, score\n", "intent": "Define function to return Roc parameters and AUC score\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "Here, I'll do an elementary test performance, and a little bit of model evaluation. \n"}
{"snippet": "def lin_predict(x, y, x_predict, alpha, bias):\n    m, n = x_predict.shape\n    y_predict = np.zeros(m)\n    for i in range(m):\n        y_predict[i] = np.sum(alpha * y * np.dot(x_predict[i], x.transpose()).transpose())\n    return np.sign(y_predict + bias)\n", "intent": "This is simple function that predicts y values of new data. I spend a lot of time trying to make write it in matrix form as it is now.\n"}
{"snippet": "def gauss_predict(x, y, x_predict, alpha, bias, sigma):\n    m, n = x_predict.shape\n    y_predict = np.zeros(m)\n    for i in range(m):\n        temp = x - x_predict[i]\n        temp =np.exp(-np.linalg.norm(temp, axis = 1) ** 2 / 2*sigma ** 2)\n        y_predict[i] = np.sum(alpha * y * temp)\n    return np.sign(y_predict + bias)\n", "intent": "Same as lin_predict\n"}
{"snippet": "trainScore = math.sqrt(mean_squared_error(y_train[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n", "intent": "Calculate root mean squared error\n"}
{"snippet": "DogX_predictions = [np.argmax(DogX_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogX]\ntest_accuracy = 100*np.sum(np.array(DogX_predictions)==np.argmax(test_targets, axis=1))/len(DogX_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\nestimator = KerasRegressor(build_fn=reg_model,nb_epoch=100,batch_size=30,verbose=0)\nkfold   = KFold(n_splits=5,random_state=seed)\nresults = cross_val_score(estimator,X,Y,cv=kfold)\nprint(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n", "intent": "Prepare data and perform 5 fold cross validation. The number of epochs and batch size were also adjusted for the lowest RMSE.\n"}
{"snippet": "test_predictions = trigramwocs_pipelineFit.transform(test_set)\ntest_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\ntest_roc_auc = evaluator.evaluate(test_predictions)\nprint \"Accuracy Score: {0:.4f}\".format(test_accuracy)\nprint \"ROC-AUC: {0:.4f}\".format(test_roc_auc)\n", "intent": "And finally, let's try this model on the final test set.\n"}
{"snippet": "breed_Resnet50_predictions = [np.argmax(breed_Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(breed_Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(breed_Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dtest = xgb.DMatrix(X_test, missing = np.nan)\npreds = clf.predict(dtest)\nprint preds[:10]\n", "intent": "Now, making the predictions\n"}
{"snippet": "y_ = model.predict(X_train)\n(y_ == y_train).sum()/y_.shape[0]\n", "intent": "Just to compare let's look at the same metric but for the train data\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) \\ len(correct))\nprint ('accuracy = {0}%'.format(accuracy))\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "y_pred = lr.predict(X)\n", "intent": "Now we'll use this classifier to predict labels for the data\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensor_resized]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in xception_test]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dt_predictions = decisionTree.predict(test_set)\nprint(\"Decision tree f1 on test set:\", accuracy_score(test_labels,dt_predictions))\ndt_predictions = dt_predictions.tolist()\nlr_predictions = logisticRegressor.predict(test_set)\nprint(\"Logistic regression f1 on test set:\", accuracy_score(test_labels,lr_predictions))\nlr_predictions = lr_predictions.tolist()\nmlp_predictions = mlp.predict(test_set, test_labels)\nprint(\"MLP f1 on test set:\", accuracy_score(test_labels,mlp_predictions))\n", "intent": "<h3>10: Print accuracy of all classifiers on the global test set</h3>\n"}
{"snippet": "print(\"The test set contains\",len(test_labels[test_labels == 1]),\"positive examples and\",len(test_labels[test_labels == 0]),\"negative examples\")\nprint(\"Bagging accuracy on test set:\", accuracy_score(test_labels,combined_predictions))\n", "intent": "<h3>12: Print accuracy</h3>\n"}
{"snippet": "print clf.predict([[10, 1, 4, 0, 0, 0]])\n", "intent": "Predict by decision tree\n"}
{"snippet": "clf_labels = ['Logistic Regression', 'Decision Tree', 'KNN']\nprint('10-fold cross validation:\\n-------------------------')\nfor clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n    scores = cross_val_score(estimator=clf, \n                             X=X_train, \n                             y=y_train, \n                             cv=10, \n                             scoring='roc_auc')\n    print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\" \\\n          % (scores.mean(), scores.std(), label))\n", "intent": "Let's see how these three classifiers do individually on our dataset. The metric used is the area under the classifier's ROC curve (ROC AUC).\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_naivepred=naive_clf.predict(X_test)\ny_decpred=dec_clf.predict(X_test)\nnaivematrix=confusion_matrix(y_test, y_naivepred)\ndecmatrix=confusion_matrix(y_test, y_decpred)\nprint(naivematrix)\n", "intent": "Now we will create confusion matrix to get an insight of performance of both the model.\n"}
{"snippet": "min_samples_split = 13\nmax_depth = 7\nrandom_state = 1\nmodel = DecisionTreeClass(min_samples_split = min_samples_split, random_state = random_state, max_depth = max_depth)\nclf = model.learn(train[features], train['high_income'])\ntest_predictions = model.predict(clf, test[features])\nauc_score_test = model.compute_score(test['high_income'], test_predictions)\ntrain_predictions = model.predict(clf, train[features])\nauc_score_train = model.compute_score(train['high_income'], train_predictions)\nprint(\"train score {0} \\n test_score {1}\".format(auc_score_train, auc_score_test))\n", "intent": "It seem we reduce a bit overfitting of our previous model.\nNow let's play with other parameters\n"}
{"snippet": "min_samples_split = 100\nmax_depth = 2\nrandom_state = 1\nmodel = DecisionTreeClass(min_samples_split = min_samples_split, random_state = random_state, max_depth = max_depth)\nclf = model.learn(train[features], train['high_income'])\ntest_predictions = model.predict(clf, test[features])\nauc_score_test = model.compute_score(test['high_income'], test_predictions)\ntrain_predictions = model.predict(clf, train[features])\nauc_score_train = model.compute_score(train['high_income'], train_predictions)\nprint(\"train score {0} \\n test_score {1}\".format(auc_score_train, auc_score_test))\n", "intent": "We aren't overfitting anymore since both AUC valeus are about the same. Let's tweak the parameters more aggressively, and see what happens!\n"}
{"snippet": "print(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "with sess.as_default():\n    yhat_cnn = np.argmax(cnn.predict(X_test), axis=1)\n    acc_cnn = mt.accuracy_score(y_test,yhat_cnn)\n    cm = mt.confusion_matrix(y_test,yhat_cnn)\n    print(cm)\n    print(acc_cnn)\n", "intent": "We saved our CNN to a file called cnn_81.h5, named after the accuracy it achieved on the validation set.\n"}
{"snippet": "predictor.predict(timeseries[:-prediction_length], quantiles=[0.10, 0.5, 0.90])\n", "intent": "Now we can use the `predictor` object to generate predictions.\n"}
{"snippet": "print(accuracy_score(gender_valid, classifier2_check.predict(doc_avg_valid))) \nprint(confusion_matrix(gender_valid, classifier2_check.predict(doc_avg_valid))) \n", "intent": "Now 100% accuracy and perfect confusion matrix, probably even more overfitted. Let's see how it does on the post-hoc validaton data: \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    losses = 0\n    for i in range(len(style_layers)):\n        gram_current_img = gram_matrix(feats[style_layers[i]]) \n        sq_err = tf.reduce_sum((gram_current_img - style_targets[i]) ** 2)\n        loss = style_weights[i] * sq_err\n        losses += loss\n    return losses\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "predictions = model.predict(dvalid)\n", "intent": "As the training and validation errors are decreasing with number of iterations we can say that there is no overfitting of data\n"}
{"snippet": "nn_predictions=[np.argmax(nn_model.predict(np.expand_dims(feature, axis=0))) for feature in test_nn]\ntest_accuracy = 100*np.sum(np.array(nn_predictions)==np.argmax(test_targets, axis=1))/len(nn_predictions)\nprint('Test accuracy: %.3f%%' % test_accuracy)\nnn_accuracy_dict[neural_net] = '%.3f%%'%test_accuracy\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model_d2v_09_es.evaluate(x=validation_vecs_ugdbow_tgdmm, y=y_validation)\n", "intent": "If I evaluate the model I just run, it will give me the result as same as I got from the last epoch.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "Create a simpler classifier that classifies every image in 'not-5' class:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_5, y_train_pred)\n", "intent": "**Precision** (calculated from Confusion Matrix) is the accuracy of the positive predictions:\n"}
{"snippet": "recall_score(y_train_5, y_train_pred)\n", "intent": "**Recall** is the sensitivity or true positive rate (TPR):\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "**F1 score**: combine precision and recall into a single metric (harmonic mean)\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')\nmy_y_scores = y_scores[:,1]\n", "intent": "**Choosing an optimal threshold:**\n"}
{"snippet": "y_train_pred_90 = (my_y_scores > 130000)\nprecision_score(y_train_5, y_train_pred_90)\n", "intent": "Aim for a 90% precision\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n", "intent": "Calculating the **area under the curve (AUC)**\n"}
{"snippet": "forest_clf.predict_proba([some_digit])\n", "intent": "Can list probabilities the classifier assigned to each instance:\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Evaluate classifiers**\n"}
{"snippet": "prediction = classifier.predict(X_test)\nprint prediction[0]\nprint y_test[0]\n", "intent": "And now we can predict the outputs of the test set.\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(\n        lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_train, pred_train_dt_pca))\nprint(classification_report(y_train, pred_train_lr_pca))\nprint(classification_report(y_train, pred_train_rf_pca))\nprint(classification_report(y_train, pred_train_dt_int))\nprint(classification_report(y_train, pred_train_lr_int))\nprint(classification_report(y_train, pred_train_rf_int))\n", "intent": "Ideally, we should choose the model with the highest accuracy as well as the acceptable precision and recall.\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on **30 epochs**) performs on the test set.\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on **300 epochs**) performs on the test set.\n"}
{"snippet": "giant_rand_scores = []\ngiant_rand_var = []\nn_est = [10, 20, 50, 100, 200]\nfor n in n_est:\n    randcla.set_params(n_estimators=n)\n    sco = cross_val_score(randcla, X_giant_train, y_giant_train, scoring=prec_scorer, cv=5)\n    giant_rand_scores.append(sco.mean())\n    giant_rand_var.append(sco.var())\n    print (np.around(sco.mean(),decimals=3), '+/-', np.around(100*sco.var()/sco.mean(), decimals=3), '%')\n", "intent": "**2.1.1** Using precision as a quality metrics\n"}
{"snippet": "randcla.set_params(n_estimators=50)\nsco = cross_val_score(randcla, X_giant_train, y_giant_train, scoring='accuracy', cv=5)\nprint ('RF accuracy', np.around(sco.mean(),decimals=3), '+/-', np.around(100*sco.var()/sco.mean(), decimals=3), '%')\n", "intent": "**2.1.2** Checking accuracy as a quality metrics for 50 trees\n"}
{"snippet": "scores = cross_val_score(RidModel, XtestTFMatrix, y=y_test)\nprint('CV Scores:{}\\nMean score:{}'.format(scores, scores.mean()))\n", "intent": "* Moving forward, we should use TFID rather than count vectorizer\n"}
{"snippet": "scores = cross_val_score(Model2, wordX_test.as_matrix(), y=y_test)\nprint('Scores:{}\\nMean score:{}'.format(scores, scores.mean()))\n", "intent": "* We have successfully improved the score of our model to predict salary incrementally\n* Success rate is now 21.33%\n"}
{"snippet": "logit_cv_scores = cross_val_score(logit, X_train, y_train, cv=skf)\nprint u'Scross-val score ', logit_cv_scores\n", "intent": "Evaluate Logistic Regression on cross-validation and pending sample\n"}
{"snippet": "from sklearn.metrics import log_loss\nlog_loss(y_test, y_test_pred_bi, eps=1e-15, normalize=True, labels=range(1, 10))\n", "intent": "log_loss makes more sense for probability prediction\n"}
{"snippet": "r2_score(y_test, clf.predict(X_test))\n", "intent": "Coefficient of Determination:\n"}
{"snippet": "r2_score(y_test, clf2.predict(X_test))\n", "intent": "Coefficient of Determination:\n"}
{"snippet": "r2_score(y_test, clf3.predict(X_test))\n", "intent": "Coefficient of Determination:\n"}
{"snippet": "zip (y_test, clf2.predict(X_test))\n", "intent": "Lasso Implementation\n"}
{"snippet": "zip (y_test, clf3.predict(X_test))\n", "intent": "Optimized Lasso Implementation\n"}
{"snippet": "from kmodes.kmodes import KModes\ndata['portal_id']=pd.Categorical(data['portal_id'])\ndata['cheapest_total_fare']=pd.Categorical(data['cheapest_total_fare'])\ncategorical_vars = ['origin','destination', 'portal_id', 'cheapest_total_fare']\ndata_categotical = data.loc[:,categorical_vars]\nkm = KModes(n_clusters=6, init='Huang', n_init=20, verbose=1)\ndata['clusters_categorical'] = km.fit_predict(data_categotical)\n", "intent": "**PCA doesn't provide significant improvement of variance**\n**For categorical variables we could use the k-mode method:**\n"}
{"snippet": "prediction = [np.argmax(bottleneck.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy = 100 * np.sum(np.array(prediction) == np.argmax(test_targets, axis= 1)) / len(prediction)\nprint('Test accuracy: {}%'.format(round(test_accuracy)))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "evaluation_lstm = lstm_model.evaluate(x_test, y_test, verbose=1)\nprint(\"\\nLSTM -> {}: {}, {}: {}\".format(lstm_model.metrics_names[0], evaluation_lstm[0], lstm_model.metrics_names[1],evaluation_lstm[1]))\n", "intent": "Using the best model you obtained, evaluate its performance using the data from the test set.\n"}
{"snippet": "regressor_pred = regressor.predict(X_test)\nregressor_error = abs(regressor_pred - y_test['PTS'].values)\n", "intent": "<h3>6. Compare our Linear Regression Model with the Random Forest Regressor</h3>\n"}
{"snippet": "model.predict(x_test)\n", "intent": "The approach achieves an accuracy of approximately 88 %\n"}
{"snippet": "print('The normalized mutial information score between the partitions determined by a resolution of 5 and 6 in the louvain algorithm:')\nnormalized_mutual_info_score(list(EC_louvain5.values()), list(EC_louvain6.values()))\n", "intent": "Using a resolution of 5 or 6 assignes every node to the exact same parition:\n"}
{"snippet": "print('The normalized mutial information score between the partitions determined by a resolution of 5 and 6 in the louvain algorithm:')\nnormalized_mutual_info_score(list(louvain5.values()), list(louvain6.values()))\n", "intent": "Using a resolution of 5 or 6 assignes every node to the exact same parition:\n"}
{"snippet": "errors1, coeffs1, idxmat = ox.crossvalidation_loop(\n    myfit, myeval, data, K=K, random_state=42)\nprint('{2:>30s}: {0:.4f}\\n{3:>30s}: {1:.4f}'.format(\n    *ox.crossvalidation_error(errors1), \n    'CV error of the baseline model', \n    'CV std dev'))\n", "intent": "Second, run the cross-validation procedure. \n"}
{"snippet": "errors2, coeffs2, _ = ox.crossvalidation_loop(\n    myfit, myeval, data, idxmat=idxmat)\nprint('{2:>30s}: {0:.4f}\\n{3:>30s}: {1:.4f}'.format(\n    *ox.crossvalidation_error(errors2), \n    'CV error of the baseline model', \n    'CV std dev'))\n", "intent": "Second, conduct the cross-validation procedure. We will use exactly the same reshuffled blocks `idxmat` that have been used above.\n"}
{"snippet": "out = model.predict(im)\nfor index in np.argsort(out)[0][::-1][:10]:\n    print(\"%01.4f - %s\" % (out[0][index], synsets[index].replace(\"\\n\",\"\")))\n", "intent": "And now predict the class label from the VGG-19 model:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model_step5.predict(np.expand_dims(tensor, axis=0))) for tensor in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lrPred = lrModel.predict(X_test_robustScaler_lr.values).clip(0,20)\nlgbmPred = lgbmModel.predict(X_test_robustScalerTree.values).clip(0,20)\nnnPred = nnModel.predict(X_test_standardScaler.values).clip(0,20)\n", "intent": "These are the meta features\n"}
{"snippet": "lrFinalPred = lrModel.predict(X_test_robustScaler_lr).clip(0,20)\nlgbmFinalPred = lgbmModel.predict(X_test_robustScalerTree).clip(0,20)\nnnFinalPred = nnModel.predict(X_test_standardScaler).clip(0,20)\n", "intent": "Get predictions for the base learners\n"}
{"snippet": "lrMetaPred = lr_meta.predict(preds_test).clip(0,20)\nlgbMetaPred = lgb_meta.predict(preds_test).clip(0,20)\n", "intent": "Get predictions for the meta learners\n"}
{"snippet": "code_tr=encoder.predict(x_tr)\n", "intent": "Let's extract the feature vector for each example\n"}
{"snippet": "if debug:\n    print('predicted:', human_detect_model.predict(tfidf4)[0])\n    print('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "all_predictions = human_detect_model.predict(messages_tfidf)\nif debug:\n    print(all_predictions)\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset. Let's beginby getting all the predictions:\n"}
{"snippet": "def partition_skin_length(data):\n    return (len(data[data['skintone'] == 0]), len(data[data['skintone'] == 2]))\ndef cluster_lengths(data):\n    return (len(data[data['cluster'] == 0]), len(data[data['cluster']== 1]))\ndef get_cluster(data, cnum):\n    return data[data['cluster'] == cnum]\ndef cluster_skintone(data, cnum):\n    return get_cluster(data, cnum).skintone.mean()\ndef mix_score(data):\n", "intent": "We can't seem to plot the cluster with the code from the examples of the sklearn documentation...\n"}
{"snippet": "from time import time \nt0 = time()\nresults = cross_val_score(estimator, X_train, Y_train, cv=kfold)\ntt = time() - t0\nprint(\"\\n\\nClassifier trained in {} seconds\".format(round(tt, 3)))\n", "intent": "Now we can evaluate our model (estimator) on our dataset (X_train and Y_train) using a 10-fold cross validation procedure (kfold).\n"}
{"snippet": "scores = cross_val_score(estimator=pipeline_log,\n                        X=X2,\n                        y=y,\n                        cv=4,\n                         scoring='roc_auc',\n                        n_jobs=1)\nprint(\"average score: \", np.mean(scores))\nprint(\"variance: \", np.std(scores))\n", "intent": "Now lets train and cross validate. Ill take the average score and standard deviation to how well the model generalizes.\n"}
{"snippet": "ypredict=tree.predict(xtest)\nnum_mis=sum(ypredict!=ytest)\nprint num_mis, \"of\", len(ytest), \"are misclassified\" \n", "intent": "Use tree to predict\n"}
{"snippet": "def get_loss(model):\n    model['loss'] = {}\n    model['loss']['softmax'] = tf.nn.softmax_cross_entropy_with_logits_v2(logits=model['logits']['logits'], labels=model['logits']['correct_label'])\n    model['loss']['cross_entropy_loss'] = tf.reduce_mean(model['loss']['softmax'])\n    model['loss']['optimizer'] = tf.train.AdamOptimizer(learning_rate=model['placeholders']['learning_rate'])\n    model['loss']['train_op'] = model['loss']['optimizer'].minimize(model['loss']['cross_entropy_loss'])\n", "intent": "Create loss functions and training operation\n"}
{"snippet": "predicted_sales_lin = model1.predict(test_data_lin)\npredicted_sales_quad = model2.predict(test_data_quad)\n", "intent": "Now that we have estimated the coefficients, we can now use them !\n"}
{"snippet": "VGG19_breed_predictions = [np.argmax(VGG19_breed_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_breed_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def sse(y, y_pred): return ((y-y_pred)**2).sum()\ndef loss(y,a,b,x): return sse(y, lin(a, b, x))\ndef avg_loss(y, a, b, x): return np.sqrt(loss(y, a, b, x) / n)\n", "intent": "We can then evaluate out prediction as before utilising the sum of squared errors, which will be out loss function.\n"}
{"snippet": "a_guess = -1.\nb_guess = 1.\navg_loss(y, a_guess, b_guess, x)\n", "intent": "We then initialise the algorithm by chosing some random values of a and b and compute our loss.\n"}
{"snippet": "def gen_features_mid(dirn):\n    gen = (features_mid[i:min(i + 128, n)] for i in range(0, len(features_mid), 128))\n    for i, batch in tqdm(enumerate(gen)):\n        features_mid2.append(rn_top_avg.predict(batch[:, :, ::dirn]))\n        if (i % 100 == 99):\n            features_mid2.flush()\n    features_mid2.flush()\n", "intent": "Define funtion to generate features. That is go through what is contained in the bcolz array a batch at a time.\n"}
{"snippet": "vec = ft_model.predict(np.expand_dims(new_im, 0))\n", "intent": "Get vector for the new image\n"}
{"snippet": "def print_example():\n    seed_string = \"ethics is a basic foundation of all that\"\n    for i in range(320):\n        x = np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis, :]\n        preds = model.predict(x, verbose = 0)[0][-1]\n        preds = preds / np.sum(preds)\n        next_char = choice(chars, p = preds)\n        seed_string = seed_string + next_char\n    print(seed_string)\n", "intent": "We define a function such as to print sample predictions.\n"}
{"snippet": "def get_next(inp):\n    idxs = [char_indices[c] for c in inp]\n    arrs = [np.array(i)[np.newaxis] for i in idxs]\n    p = model.predict(arrs)\n    i = np.argmax(p)\n    return chars[i]\n", "intent": "We define a function that returns the predictions for a given input sequence of characters.\n"}
{"snippet": "def get_next(inp):\n    idxs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n    p = model.predict(idxs)\n    return chars[np.argmax(p)]\n", "intent": "Again we define a function to predict the next chracter of a sequence.\n"}
{"snippet": "def get_next_keras(inp):\n    idxs = [char_indices[c] for c in inp]\n    arrs = np.array(idxs)[np.newaxis,:]\n    p = model.predict(arrs)[0]\n    return chars[np.argmax(p)]\n", "intent": "We can then test the model. Again, define a function returning the predicted character.\n"}
{"snippet": "def get_nexts(inp):\n    idxs = [char_indices[c] for c in inp]\n    arrs = [np.array(i)[np.newaxis] for i in idxs]\n    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n    print(list(inp))\n    return [chars[np.argmax(o)] for o in p]\n", "intent": "Again define a model returning predictions.\n"}
{"snippet": "def get_nexts_keras(inp):\n    idxs = [char_indices[c] for c in inp]\n    arr = np.array(idxs)[np.newaxis,:]\n    p = model.predict(arr)[0]\n    print(list(inp))\n    return [chars[np.argmax(o)] for o in p]\n", "intent": "And we can then test by defining the test function\n"}
{"snippet": "def style_loss(x, targ): return K.mean(K.square(gram_matrix(x) - gram_matrix(targ)))\n", "intent": "Note that we again calculate our own loss.\n"}
{"snippet": "loss = sum(style_loss(l1[0], l2[0]) for l1,l2 in zip(layers, targs))\ngrads = K.gradients(loss, model.input)\nstyle_fn = K.function([model.input], [loss]+grads)\nevaluator = Evaluator(style_fn, shp)\n", "intent": "Define loss, grads, funktion and evaluator\n"}
{"snippet": "p = top_model.predict(arr_lr[7846:7847])\n", "intent": "We can then do a prediction\n"}
{"snippet": "w = [0.025, 0.8, 0.15, 0.025]\ndef tot_loss(x):\n    loss = 0; n = len(style_targs)\n    for i in range(n):\n        loss += mean_sqr_b(gram_matrix_b(x[i + n]) - gram_matrix_b(style_targs[i]))\n        loss += mean_sqr_b(x[i] - x[i + n]) * w[i]\n    loss += total_variation_loss(x[n * 2]) * 1e-7\n    return loss\n", "intent": "Finally define the combined loss function, combining style loss, content loss and total variation loss.\n"}
{"snippet": "predictions_log=logistic.predict(x_test)\nlogistic_score=accuracy_score(y_test, predictions_log)\nprint (\"logistic\")\nprint (\"train size: \",train_size,\"has accuracy:\", logistic_score)\n", "intent": "now make the predictions and calculate the score\n"}
{"snippet": "print('Precision: {:.5f}.'.format(precision_score(july_labels['label'], july_labels['predicted_label'])))\nprint('Recall: {:.5f}.'.format(recall_score(july_labels['label'], july_labels['predicted_label'])))\nprint('F1 Score: {:.5f}.'.format(f1_score(july_labels['label'], july_labels['predicted_label'])))\nprint('ROC AUC Score: {:.5f}.'.format(roc_auc_score(july_labels['label'], july_labels['probability'])))\n", "intent": "Let's look at the four performance metrics.\n"}
{"snippet": "pred = model.predict(test.drop('A', axis = 1))\nprint 'OS prediction accuracy: {}'.format(float(sum(test.A == pred.A)) / len(test))\n", "intent": "c) Use the model to predict \"A\" using the testing dataset. Report the OS prediction accuracy. (10pts) \n"}
{"snippet": "print \"Class predictions according to sklearn:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "print \"Class predictions according to sklearn:\" \nprint \"\\tP(y=-1|x,w) \\t P(y=1|x,w)\"\nprint sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "writecsv(DMaxmodel.predict(newtest[training_features]), newtest, 'logistic.csv')\n", "intent": "We submit the testing prediction dataset to Kaggle in the mini-contest. It gave an accuracy of 0.50236 shown on Kaggle.\n"}
{"snippet": "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\nprint(\"\\nFinal test set Loss:     %6.6f\" % (loss))\nprint(\"Final test set Accuracy: %6.6f\" % (accuracy))\n", "intent": "Use the testing set to test your model, clearly calculating and displaying the error rate.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(logreg, X, y, scoring='accuracy', cv=10)\nprint scores\nprint scores.mean()\n", "intent": "Moving along, let's calculate a mean accuracy with cross validation using 10 folds. \n"}
{"snippet": "def wasserstein_loss(y_true, y_pred):\n    return K.mean(y_true * y_pred)\ndef gradient_penalty_loss(y_true, y_pred, averaged_samples,\n                          gradient_penalty_weight):\n    gradients = K.gradients(K.sum(y_pred), averaged_samples)\n    gradient_l2_norm = K.sqrt(K.sum(K.square(gradients)))\n    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n    return gradient_penalty\n", "intent": "Define new custom loss functions not included among the standard keras ones:\n"}
{"snippet": "from tensorflow.python.keras.applications import ResNet50\nmy_model = ResNet50(weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\ntest_data = read_and_prep_images(img_paths)\npreds = my_model.predict(test_data)\n", "intent": "**ResNet model with pretrained weights file**\n"}
{"snippet": "none_predicted = np.array([1 for i in xrange(df_testing.shape[0])])\nnone_precision = precision_score(expected, none_predicted)\nprint(none_precision)\n", "intent": "Another good baseline to compare against is the \"all label\" (label is always 1).\n> Here, let's compare to a prediction where all employers survive.\n"}
{"snippet": "basic_model.evaluate( test_images, test_labels, batch_size=1024, verbose=1)\n", "intent": "    The first value gives the loss and the second value is the accuracy of the model on the training data\n"}
{"snippet": "model.evaluate( test_images, test_labels, batch_size=1024, verbose=1) \n", "intent": "    The first value gives the loss and the second value is the accuracy of the model on the training data\n"}
{"snippet": "loss_real = tf.losses.mean_squared_error(output_tf[:,0:1], output[:,0:1])\nloss_imag = tf.losses.mean_squared_error(output_tf[:,1:2], output[:,1:2])\nloss = tf.reduce_mean(loss_real + loss_imag + regularizer*beta)\ntrain = tf.train.AdamOptimizer(learning_rate).minimize(loss)\nr = make_r(output_tf, output)\ninit = tf.global_variables_initializer()\n", "intent": "Here we reduce the mean of loss_real, loss_imag and our regularizer value times some beta defined above to scale it to the value we need. \n"}
{"snippet": "print(classification_report(y_test,pred))\n", "intent": "So, the default KNN classificaton model is giving an accuracy of 90%\n"}
{"snippet": "def show_stats(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    print('===============================================================================================')\n    print('Model:', model)\n    accuracy = classifier.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, classifier.predict(X_test))\n    print(\"Accuracy={:.2f}%\".format(accuracy*100))\n    print(\"M.A.E={:.2f}\".format(mae))\n    print('===============================================================================================')\n", "intent": "Generic function to print results of a model training\n"}
{"snippet": "predictions = classifier.predict(X_to_forecast)\npredictions\n", "intent": "Predicting values for X_to_forecast\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        loss = style_weights[i] * tf.reduce_sum(tf.squared_difference(gram, style_targets[i]))\n        style_loss += loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "mse = np.mean((boston_df.PRICE - lm.predict(X)) ** 2)\nprint (mse)\n", "intent": "Notice that as the price increases, the prediction error also tends to increase. Calculating the mean square error for the prediction, we get:\n"}
{"snippet": "print(\"Trace of normalized confusion matrix\")\nprint(sum(np.diagonal(normalized_M)))\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy\")\nprint(accuracy_score(y_test, y_pred))\n", "intent": "The **accuracy** is the number of data points correctly classified. It is the normalized confusion matrix' **trace**.\n"}
{"snippet": "sim_preds_em = sklearn.mixture.GMM(n_components=3).fit_predict(sim_points)\nplot_simulated(sim_points, sim_preds_em, sim_labels, 'EM on mixture of Gaussians')\n", "intent": "Now that we understand we can cluster data using expectation maximization on a mixture of Gaussians, let's try running it on a test dataset.\n"}
{"snippet": "combo = ComboPredict(booster)\nassert all(fold == 1 for _, fold, _ in booster.models)\ntrain, test = booster.make_cv_split(1, returnxgb=False)\noverall_cls_factor = 0.4\nscore_train = combo.predict_score(train[0], overall_cls_factor)\nscore_test = combo.predict_score(test[0], overall_cls_factor)\n", "intent": "We demonstrate the use of `ComboPredict` to calculate scores:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n", "intent": "Defining a method that calculates root mean square error (rmse) \n"}
{"snippet": "y_train_pred_ridge=model_ridge.predict(X_train)\ny_test_pred_ridge=model_ridge.predict(X_test)\ny_train_pred_lasso=model_lasso.predict(X_train)\ny_test_pred_lasso=model_lasso.predict(X_test)\n", "intent": "The predictions (train and test data) are calculated separately from each model. \n"}
{"snippet": "from sklearn.metrics import r2_score\nprint('\\nR^2 train model Lasso:',r2_score(y_train,y_train_pred_lasso))\nprint('R^2 test model Lasso:', r2_score(y_test,y_test_pred_lasso))\nprint('\\nR^2 train model Ridge:',r2_score(y_train,y_train_pred_ridge))\nprint('R^2 test model Ridge:', r2_score(y_test,y_test_pred_ridge))\n", "intent": "Calculating r2 score (Coeficient of determination) for each model.\n"}
{"snippet": "train_batches = data.batches('train', batch_size=batch_size)\ntest_set = data.batches('test', batch_size=None)[0]\nfor epoch in range(num_epochs):\n    for batch in train_batches:\n        model.update(input=batch['input'], output=batch['output'])\n    hat_y = model.predict(input=test_set['input'])\n    accuracy = 100*np.mean(hat_y == test_set['output'])\n    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))\n", "intent": "> After you have ensured that your Backpropagation algorithm is correct, you can train a model with the data we have.\n"}
{"snippet": "clf.predict(digits.data[-1:])\n", "intent": "Once the estimator is trained we can predict the target attribute for new instances:\n"}
{"snippet": "predicted_y = tree_class.predict(X=example)\npredicted_y\n", "intent": "Can you describe in words employee 7, 8 and 9? Who do you think will leave or stay?\n"}
{"snippet": "def binary_error(data_x, guess_x):\n", "intent": "2: a)\n3: a)\n4: c)\n5: d)\n6: b)\n"}
{"snippet": "n_folds = 10\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=1).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "Got this from a kaggler- it's pretty handy, letting us select the number of folds and letting us shuffle the data when we run it\n"}
{"snippet": "def accuracy(y_true, y_pred, normalize=True, sample_weight=None):\n    return accuracy_score(y_true, y_pred, normalize, sample_weight)\n", "intent": "1) Write a function that computes the accuracy of the predicted values\n"}
{"snippet": "conf = knn.predict_proba(x_test)\nlc = np.where((conf[:,0] >= 0.45) & (conf[:,0] <= 0.55))\ncountries.iloc[lc]\n", "intent": "3) The KNeighborsClassifier also has a [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "train_labels = model.predict(x_train)\ntrain_labels = np.reshape(train_labels, -1).astype(int)\nprint(train_labels)\nprint(y_train)\nprint(\"Accuracy: {}\".format(np.sum(train_labels == y_train) / len(y_train)))\n", "intent": "* Check overfiting/underfiting\n"}
{"snippet": "model_predict(model = model, test = X_train, true_y = y_train)\n", "intent": "<p>Training accuracy</p>\n"}
{"snippet": "clean_valid = data_cleansing(valid)\nX_valid, y_valid, vectorizer = vectorize_data(clean_title=clean_valid, vectorizer=vectorizer, vectorize=False, table=valid)\npredict_valid, accur = model_predict(model = model, test = X_valid, true_y = y_valid)\n", "intent": "<H6>Cleaning and predicting validation set</H6>\n"}
{"snippet": "clean_test = data_cleansing(blindset)\nX_test, vectorizer = vectorize_data(clean_title = clean_test, vectorizer = vectorizer, vectorize = False, table = None)\ny_test = model_predict(model = model, test = X_test)\n", "intent": "<H6>Cleaning and predicting test set</H6>\n"}
{"snippet": "result = forest.predict(blindset_features)\noutput = ['no']*num_tables\n", "intent": "<H4>Predicting for blind set</H4>\n"}
{"snippet": "val_pred = est.predict(X_test)\n", "intent": "<H4>Predicting for cv set</H4>\n"}
{"snippet": "predict(2,3,True)\n", "intent": "We can apply nlogarithm, to create better model.\n"}
{"snippet": "with open('./models/sf/simple_linear.pkl', 'rb') as f:\n    simple_linear = pickle.load(f)\nsqft_input = np.array([sqft]).reshape(1,1)\npredicted_price_sl = simple_linear.predict(sqft_input).squeeze()\nprint(\"simple linear predicted price: ${}M\".format(predicted_price_sl/1e6))\n", "intent": "Compare with our simple linear model from before\n"}
{"snippet": "codes = encoder.predict(x.T)\ncodes[0]\n", "intent": "We use ``encoder`` to compress the training data to 30 dimensional vectors.\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_pred\n", "intent": "Make predictions of what each observation in the test data set should be and comepare them to their actual label. \n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n   * Also known as \"Misclassification Rate\"\n"}
{"snippet": "logreg.predict(X_test)[0:10]\n", "intent": "By default there is a 0.5 classification threshold:\n* Class 1 is predicted if probability > 0.5\n* Class 0 is predicted if probability < 0.5\n"}
{"snippet": "print(classification_report(y, training_preds, labels=[0,1,2], target_names=['class 0', 'class 1', 'class 2']))\n", "intent": "The `classification_report` function will easily give us some other metrics:\n"}
{"snippet": "import warnings\nwarnings.filterwarnings(\"ignore\")\nprint('Precision Score' , precision_score(y,y_pred))\n", "intent": "*  Find precision score of the prediction \n"}
{"snippet": "print('Precision Score' , precision_score(y,y_pred))\n", "intent": "*  Find precision score of the prediction \n"}
{"snippet": "print('Majority-Class Classifier: {:.2f}'.format(metrics.accuracy_score(y, [0 for ii in y.tolist()])))\n", "intent": "Like always, let's choose randomly guessing the majority as the baseline.\n"}
{"snippet": "y_pred = LogReg.predict(X_test)\n", "intent": "*** d) predicting value ***\n"}
{"snippet": "accuracy = metrics.accuracy_score(testTargets, predictions)\nprint \"The model produced {0}% accurate predictions.\".format(accuracy*100)\nprint \" \"\ny_true = testTargets\ny_pred = results\nprint(classification_report(y_true, y_pred))\n", "intent": "Generate some statistics on the effectiveness of our model\n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "Fitting the classifier on the train data\n"}
{"snippet": "predict_results = classifier.predict(\n    input_fn=lambda: titanic_input_fn(test_path, False, 1))\nprint(\"Predictions on test file\")\ni=892\nfor prediction in predict_results:\n    print(prediction[\"class_ids\"][0])\n    i += 1\nprint(i)\n", "intent": "Predict on test set\n"}
{"snippet": "def evaluate(words, supervised_training, bsize=100):\n    batch_words = np.random.choice(words, size=bsize, replace=False)\n    lines_ix = inp_voc.to_matrix(batch_words)\n    trans_ix = s.run(supervised_training.greedy_translations,\n                     {supervised_training.input_sequence: lines_ix})\n    batch_trans = out_voc.to_lines(trans_ix)\n    distances = list(map(get_distance, batch_words, batch_trans))\n    return np.array(distances, dtype='float32')\n", "intent": "Some functions to launch training easier\n"}
{"snippet": "y_chap = knnOpt.predict(X_test2)\ny_chapC = np.round(y_chap)\ntable=pd.crosstab(y_chapC,Y_test)\nprint(table)\n", "intent": "**Exercice** :\nIf k >= 13, relaunch the GridSearch for higher parameters in the grid.\n"}
{"snippet": "X_train_norm = normX.transform(DF_X[VarToUse].as_matrix())\nnnetPredTrain = Predict(X_train_norm,W,b)\nX_test_norm = normX.transform(DF_X_test[VarToUse].as_matrix())\nnnetPredTest = Predict(X_test_norm,W,b)\n", "intent": "** Prediction on training and test sets ** :\n"}
{"snippet": "X_train_norm = normX.transform(DF_X[VarToUse].as_matrix())\nnnetPredTrain = Predict(X_train_norm,\\\n                       Whidden,bhidden,Wout,bout)\nX_test_norm = normX.transform(DF_X_test[VarToUse].as_matrix())\nnnetPredTest = Predict(X_test_norm,\\\n                       Whidden,bhidden,Wout,bout)\n", "intent": "** Prediction on training and test sets ** :\n"}
{"snippet": "KerasPredTrain = model.predict(normX.transform(DF_X[VarToUse].as_matrix()))\nKerasPredTrain = KerasPredTrain.reshape(Y.shape)\nKerasPredTest = model.predict(normX.transform(DF_X_test[VarToUse].as_matrix()))\nKerasPredTest = KerasPredTest.reshape(Y_test.shape) \n", "intent": "The prediction is done this way :\n"}
{"snippet": "def Compute_Reconstruction_error(model,X_querry):\n    X_reconstr=model.predict(X_querry)\n    Error=np.linalg.norm(X_reconstr-X_querry,axis=1)/np.linalg.norm(X_querry,axis=1)\n    return Error\nError_train= Compute_Reconstruction_error(model,X_train_norm)\nprint('Accuracy train : ',100-100*np.median(Error_train),'%')\nError_test= Compute_Reconstruction_error(model,X_test_norm)\nprint('Accuracy test : ',100-100*np.median(Error_test),'%')\n", "intent": "Let us compute the reconstruction accurary defined as R= 1-norm(X_reconstr-X_init)/norm(X_init)\n"}
{"snippet": "def predict(x_in):\n    return sess.run(y, feed_dict={x: [x_in]})\n", "intent": "Demonstrate saving and restoring a model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Creating a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = logmodel.predict(X_test)\npredictions\n", "intent": "** Predicting values for the testing data.**\n"}
{"snippet": "np.sqrt(mean_squared_error(y_dev, 0.5 * (lasso.predict(dev) + xgb_reg.predict(dev))))\n", "intent": "reference score 0.13213255344648153\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n", "intent": "Let's evaluate our model performance by calculating the residual sum of squares and the explained variance score (R^2).\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlinear_reg_mse = mean_squared_error(y_train , housing_predictions)\nlinear_reg_rmse = np.sqrt(linear_reg_mse)\nlinear_reg_rmse\n", "intent": "to make an initial assessment of efficacy, have a look at the root mean squared error on the training data itself\n"}
{"snippet": "final_model = grid_search.best_estimator_\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n", "intent": "with the final model picked (through rf gridCV) we can evalute on the test set. Recall our X_test is already prepared above.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, predictions)\nMSE = mean_squared_error(y_test, predictions)\nRMSE = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"MAE: %.11f\") % (MAE)\nprint(\"MSE: %.11f\") % (MSE)\nprint(\"RMSE: %.11f\") % (RMSE)\n", "intent": "Let's evaluate our model performance by calculating the residual sum of squares.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\npred = clf.predict(digits.data)\nprint(\"Accuracy: {}\".format(accuracy_score(digits.target, pred)))\nprint(classification_report(digits.target, pred))\n", "intent": "Now let's evaluate the robustness of our model\n"}
{"snippet": "y_pred = clf.predict(X_test) \naccuracy_score(y_test, y_pred)\n", "intent": "The prediction on training data was perfect (why?).  But how will it do on the test data set?\n"}
{"snippet": "def calculate_joint_score(perFeatureScores):\n    driverScores = perFeatureScores\n    featureCols = [col for col in driverScores if col.startswith('Harsh')]\n    driverScores['metric'] = (driverScores[featureCols].sum(axis = 1) / 3.0)\n    driverScores = driverScores.sort_values('metric')\n    driverScores['rank'] = (driverScores.metric.rank(method=\"min\")-1)/((len(perFeatureScores)-1)*1.0)\n    return driverScores\ndriverScores = calculate_joint_score(perFeatureScores)\n", "intent": "The commulative metric is the sum (or weighted sum) of CDFs per event type. here we assume that features have equal weights.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "To help our analysis, we will also look at the accuracy score of our model.\n"}
{"snippet": "sklearn.metrics.accuracy_score(tar_test, predictions)\n", "intent": "Display the accuracy score\n"}
{"snippet": "sklearn.metrics.accuracy_score(tar_test, predictions_r)\n", "intent": "Print the accuracy score\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ntrain_error = mean_squared_error(tar_train, model.predict(pred_train))\ntest_error = mean_squared_error(tar_test, model.predict(pred_test))\nprint ('training data MSE')\nprint(train_error)\nprint ('')\nprint ('test data MSE')\nprint(test_error)\n", "intent": "Print the mean squared error from training and test data\n"}
{"snippet": "best = clf.best_estimator_\nprint('Best estimator: ', best)\nscores = model_selection.cross_val_score(best, X_test, y_test, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Then we can check the accuracy on the **Test Set**:\n"}
{"snippet": "y_train_predicted = model.predict(X_train)\ntrainScore = sqrt(mean_squared_error(y_train, y_train_predicted))\nprint('Train Score: %.2f RMSE' % (trainScore))\ny_test_predicted = model.predict(X_test)\ntestScore = sqrt(mean_squared_error(y_test, y_test_predicted))\nprint('Test Score: %.2f RMSE' % (testScore))\ny_hat = model.predict(X)\n", "intent": "**And last we predict the output for the training set and test set.**\n"}
{"snippet": "y_train_pred = mlp.get_outputs(train_set)\ny_test_pred = mlp.get_outputs(test_set)\ny_train_pred = y_train_pred.ravel()\ny_test_pred = y_test_pred.ravel()\ny_test, y_test_pred = same_len_trans(y_test, y_test_pred)\ny_train, y_train_pred = same_len_trans(y_train, y_train_pred)\nscore_train = sqrt(mean_squared_error(y_train, y_train_pred))\nprint('Train Score: %.2f RMSE' % (score_train))\nscore_test = sqrt(mean_squared_error(y_test, y_test_pred))\nprint('Test Score: %.2f RMSE' % (score_test))\n", "intent": "**And last we predict the output for the training set and test set.**\n"}
{"snippet": "accuracy_score = classifier.evaluate(x=mnist.test.images.astype(np.float32), \n                                     y=np.argmax(mnist.test.labels, axis=1).astype(np.int64))[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n", "intent": "The accuracy is similar to the example above.\n"}
{"snippet": "print('L2 (default) regularization.')\nshow_report(logreg_C, X_train, y_train, 'classification', 'train')\nshow_report(logreg_C, X_test, y_test, 'classification', 'test')\n", "intent": "After cross_validation compared with logreg_C accuracy was a bit lower, will stick with logreg_C.\nApplying regularizations for logreg_C.\n"}
{"snippet": "X_all = child_wishes.drop('target', axis=1).values\npredicted_probs = model.predict_proba(X_all)\n", "intent": "Looks good! We see significant progress being made during training. Now, let's unleash the beast!\n"}
{"snippet": "    model.load_weights(filepath = '.mdl_wts.hdf5')\n    score = model.evaluate(Xcv_more, Ycv_more, verbose=2)\n    print('CV loss:', score[0])\n    print('CV accuracy:', score[1])\n    pt = model.predict(Xcv_more)\n    mse = (np.mean((pt-Ycv_more)**2))\n    print('CV MSE: ', mse)\n    predA_test = model.predict(Xtest) \n", "intent": "Load the best weights, check scores, and predict values\n"}
{"snippet": "y_test = model.predict(word_seq_test)\n", "intent": "Let's make predictions on the test data:\n"}
{"snippet": "test_predictions = simple_cnn.predict(test_images)\n", "intent": "Here we make predictions on the output and export the CSV so we can submit\n"}
{"snippet": "ridge_pred = np.expm1(ridge.predict(X_valid))\n", "intent": "After `log1p`-transformation, we need to apply an inverse  `expm1`-trasformation to predictions.\n"}
{"snippet": "ridge_valid_mae = mean_absolute_error(y_valid, ridge_pred)\nridge_valid_mae\n", "intent": "As we can see, the prediction is far from perfect, and we get MAE $\\approx$ 1.3 that corresponds to $\\approx$ 2.7 error in \n"}
{"snippet": "mean_absolute_error(y_valid, .6 * lgb_pred + .4 * ridge_pred)\n", "intent": "Now let's mix predictions. We's just pick up weights 0.6 for Lgbm and 0.4 for Ridge, but these are typically tuned via cross-validation. \n"}
{"snippet": "pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\n", "intent": "Now iterate through classifiers and save the results\n"}
{"snippet": "scores_rfc = cross_val_score(rfc, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_rfc)\nprint(scores_rfc.mean())\n", "intent": "for Random Forest classifier\n"}
{"snippet": "scores_dtree_2 = cross_val_score(dtree_2, X_train_all_sc, y_train_all_sc, cv=10, scoring='accuracy')\nprint(scores_dtree_2)\nprint(scores_dtree_2.mean())\n", "intent": "for DecisionTreeClassifier\n"}
{"snippet": "y_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)\n", "intent": "Getting 65% accuracy with only LinearSVC. Try different ensemble models to get more accurate model. \n"}
{"snippet": "def acc_score(model):\n    return np.mean(cross_val_score(model,x_train,y_train,cv=k_fold,scoring=\"accuracy\"))\n", "intent": "We first create these functions to allow us the ease of computing relevant scores and plotting of plots.\n"}
{"snippet": "def predict(x, t, x_test, K=1):\n    w_hat = get_w_hat(x, t, K)\n    X_test = get_X(x_test, K)\n    predictions = X_test.dot(w_hat)\n    return predictions\n", "intent": "<div class=\"alert alert-info\">\nWrite a function that, when given `x`, `t` and `x_test`, computes `w_hat` and makes predictions at `x_test`.</div>\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds.shape\n", "intent": "We'll grab the **predictions for each individual tree**, and look at one example.\n"}
{"snippet": "y_pred = lm.predict(x_test.reshape(-1,1))\n", "intent": "We conclude that increasing the Critic Score by 0.03 increases the Global Sales unit by 1.\n"}
{"snippet": "from sklearn.metrics import cohen_kappa_score\nlabeler1 = [2, 0, 2, 2, 0, 1]\nlabeler2 = [0, 0, 2, 2, 0, 2]\ncohen_kappa_score(labeler1, labeler2)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\n<img src=\"https://i.stack.imgur.com/kYNd6.png\">\n"}
{"snippet": "X_test = X_test.values\npredictions = model.predict(X_test)\n", "intent": "We pass `X_test` into `predict` of our `LinearRegression` model:\n"}
{"snippet": "assess = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Evaluated accuracy: %.2f%%\" % (scores[1] * 100))\n", "intent": "Now we can assess the accuracy of our trained model through the \"evaluate\" method:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out the model on the test dataset of dog images.  Ensure test accuracy is greater than 1%.\n"}
{"snippet": "i = [0,1,2,3]\nx = X_test[i,:]\nprint x\nyhat = predictor.predict(x)\nfirst_four =  np.array(zip(yhat,y_test[i]))\nprint first_four\n", "intent": "Now that we trained `predictor` we can use it to provide predictions on any example `x`. \n"}
{"snippet": "predictions = model.predict(X)\nrounded = [round(x[0]) for x in predictions]\nprint(rounded)\n", "intent": "Basic rounding operations of the sigmoid result\n"}
{"snippet": "print (\"Predicted %d, Label: %d %\"classifier.predict(test_data[0]), test_labels[0])\ndisplay(0)\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "loss = create_loss(prediction, y)\ntrain_step = tf.train.AdamOptimizer(eta).minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\naccuracy = 100.0 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\ntrain_writer = tf.summary.FileWriter(log_dir + '/trainLenet', sess.graph)\ntest_writer = tf.summary.FileWriter(log_dir + '/testLenet')\ntf.global_variables_initializer().run()\n", "intent": "Redefine the loss etc:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_predict = clf.predict(X_train)                   \nprint 'Training accuracy = ', accuracy_score(y_train, y_predict)\nclass_labels = clf.get_classes()     \nprint 'Class labels=', class_labels\nconfusion_matrix = confusion_matrix(y_train, y_predict, class_labels)\nprint 'Confusion matrix [known in lines, predicted in columns]=\\n', confusion_matrix\nbalance_classification_rate = 1/2 * ( float(confusion_matrix[1,1])/float(confusion_matrix[1,0] + confusion_matrix[1,1]) ) + ( float(confusion_matrix[0,0])/float(confusion_matrix[0,0] + confusion_matrix[0,1]))                              \nprint 'Balance Classification Rate = ', balance_classification_rate\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "import libscores\nY_train, C = libscores.onehot(y_train)                                   \nprint 'Dimensions Y_train=', Y_train.shape, 'Class labels=', C\nassert((class_labels==C).all()) \nfrom libscores import bac_metric \nfrom libscores import pac_metric \ny_predict_proba = clf.predict_proba(X_train)      \nprint 'Training balanced accuracy = ', bac_metric(Y_train, y_predict_proba, task='binary.classification')\nprint 'Training probabilistic accuracy = ', pac_metric(Y_train, y_predict_proba, task='binary.classification')\n", "intent": "ADVANCED: Sklearn does not have multi-class metrics, this shows how libscore metrics work.\n"}
{"snippet": "score = Xception.evaluate(test_Xception, test_targets)\nprint('\\n', 'Accuracy:', score[1] * 100, '%')\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted = cross_val_predict(model, X, Y, cv=kfold)\nr_squared = r2_score(Y, predicted, multioutput='variance_weighted')\nprint 'R-squared = ',r_squared\n", "intent": "R^2, Close to one is better\n"}
{"snippet": "predicted = model.predict(X)\nr_squared = r2_score(Y, predicted, multioutput='variance_weighted')\nprint 'R-squared = ',r_squared\n", "intent": "R^2, Close to one is better\n"}
{"snippet": "def predict_best_classifier(x_test):\n    threshold = .45\n    y_prob = logistic.predict_proba(x_test)[:, 1]\n    y_pred = np.zeros((x_test.shape[0],1)).reshape(-1,)\n    y_pred[y_prob > threshold] = 1\n    return y_pred, y_prob\n", "intent": "Implement classifier on test data, return predicted class label and predicted probability.\n"}
{"snippet": "predicted=g.predict(x)\n", "intent": "Predict using the model and store in a list for later use\n"}
{"snippet": "y = mod.predict(val_iter)\nassert y.shape == (4000, 26)\n", "intent": "To predict with module, we can call predict(). It will collect and return all the prediction results.\n"}
{"snippet": "CNN_model.evaluate(x = X_test, y = y_test)\n", "intent": "And the winner is...\n"}
{"snippet": "net.evaluate()\npredictions = net.forward(X_test).argmax(axis=1)\nprint (predictions == y_test).mean()\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "preds = model_keras.predict([test_mean, test_std])\n", "intent": "So we get a very similar CRPS compared to the 25 day rolling window. This suggests that the seasonality is not that important.\n"}
{"snippet": "for model,n in zip(models, np.arange(len(models))):\n    start_time =time.time()\n    kfold = KFold(n_splits=kFold_n_splits, random_state=seed)\n    cv_score = cross_val_score(model[1], X_train, Y_train, cv=kfold, scoring=scoring)\n    time_taken = round((time.time() - start_time),3)\n    name=model[0]\n    results.loc[n]=[name, cv_score.mean(), time_taken]\n    msg = \"%s: %.3f%% - %s seconds\" % (name, cv_score.mean(), time_taken)\n    print(msg)\n", "intent": "Executing cross validation steps for each model enabled above and storing results.\n"}
{"snippet": "np.mean((bos.PRICE-lm.predict(X))**2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Rsquare = 1 - np.sum((bos.PRICE-lm.predict(X))**2)/np.sum((bos.PRICE - np.mean(bos.PRICE)) ** 2)\nRsquare\n", "intent": "** Rsquare **: Compute Co-efficient of determination or Rsq. \n"}
{"snippet": "predictions = model.predict(x_test_norm)\npredictions = np.argmax(predictions, axis= 1)\n", "intent": "The test data is normalised.\n"}
{"snippet": "t0 = time()\nsgd = sgd_cv.best_estimator_\nscores = cross_val_score(sgd, train_x_encoded, train_y, cv=10, scoring=rmsle_score)\nprint(\"10 fold cv completed in {0}\".format(time() - t0))\n", "intent": "We have found our optimal parameters. Lets use them on the whole data set and see how we're doing.\n"}
{"snippet": "for c in [0,1,2,3]:\n    indices = [i for i,x in enumerate(y_test) if x == c]\n    _X_test = X_test[indices]\n    _y_test = y_test[indices]\n    _y_hat = lr.predict(_X_test)\n    print('Accuracy of class {} (ours): '.format(c),accuracy_score(_y_test,_y_hat))\n    _y_hat_sk = lr_sk.predict(_X_test)\n    print('Accuracy of class {} (sk):   '.format(c),accuracy_score(_y_test,_y_hat_sk))\n    print('Difference: ', (accuracy_score(_y_test,_y_hat)-accuracy_score(_y_test,_y_hat_sk)),'\\n')\n", "intent": "Next we wanted to see how the accuracy was for each class.\n"}
{"snippet": "import time\nindex = 50\ntest_image = X_test_split[index]\ntest_result = y_test_split[index]\nstart_time = time.time()\nfeature = generate_feature_vector(test_image)\nscaled_feature = X_scaler.transform(feature)\nresult = svc.predict(scaled_feature)\ntime_svc = time.time() - start_time\nprint(time_svc)\n", "intent": "To benchmark our models, we need to calculate the time investment for each function.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = model.predict(X_test_scaled)\nMSE = mean_squared_error(y_test_scaled, predictions)\nr2 = model.score(X_test_scaled, y_test_scaled)\nprint(f\"MSE: {MSE}, R2: {r2}\")\n", "intent": "- Use mean squared error to determine accuracy of model on test data\n"}
{"snippet": "LabelsAndPredictions = testSet.map(lambda v: (v.label, model.predict(v.features)))\nLabelsAndPredictions.cache()\nerrorRate = LabelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(LabelsAndPredictions.count())\nprint(\"Training Error = \" + str(errorRate))\n", "intent": "Let's test how well the model performs....\n"}
{"snippet": "mean_prediction = np.mean(Y)\nmean_predictions = np.array([mean_prediction for _ in range(len(test_ratings))])\nprint(\"mean absolute error of predicting the training mean: %0.3f\" % metrics.mean_absolute_error(test_ratings, mean_predictions))\n", "intent": "Let's compare the baseline model with predicting the mean of the training set.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('ResNet50 test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nn_validation = int(len(X) * validation_split)\ny_predicted = model.predict(X[-n_validation:])\nprint roc_auc_score(y[-n_validation:], y_predicted)\n", "intent": "Bonus (optional): let's check the area under the receiver operating characteristic curve (ROC AUC) so we can compare to other work.\n"}
{"snippet": "import numpy as np\ndef print_indicator(data, model, class_names, bar_width=50):\n    probabilities = model.predict(np.array([data]))[0]\n    print(probabilities)\n    left_count = int(probabilities[1] * bar_width)\n    right_count = bar_width - left_count\n    left_side = '-' * left_count\n    right_side = '-' * right_count\n    print class_names[0], left_side + '\n", "intent": "Let's test the model by defining a function that evaluates a single image and prints a corresponding indicator.\n"}
{"snippet": "y_pred=model.predict(X_test)\ny_pred\n", "intent": "We then apply the model to our test data and predict car prices.\n"}
{"snippet": "R2 = r2_score(y_test,y_pred)\nMAE = mean_absolute_error(y_test, y_pred)\nprint (\"R^2: \",R2)\nprint (\"MAE: \",MAE)    \n", "intent": "Finally, let us determine the evaluation measures R<sup>2</sup> and Mean Absolute Error (MAE).\n"}
{"snippet": "predictions=logReg.predict(X_test)\n", "intent": "We then apply the model to the test data\n"}
{"snippet": "accuracy=sklearn.metrics.accuracy_score(y_test,predictions)\nfscore=sklearn.metrics.f1_score(y_test,predictions)\n", "intent": "As evaluation metrics we use accuracy and F1-Score\n"}
{"snippet": "num_rows=test.shape[0]\nones_test=[1]*num_rows\nX_testt=np.stack((ones_test,test[\"sqft_living\"]))\nX_test=np.transpose(X_testt)\ny_pred_test=np.matmul(X_test,thetas)\ny_test=test[\"price\"]\ny_pred_test=y_pred_test.reshape((-1))\nr2= r2_score(y_test, y_pred_test)\nprint (\"R2: \",r2)\n", "intent": "Now let us check how the obtained parameter perform on the test set.\n"}
{"snippet": "num_rows=test.shape[0]\nonestest=[1]*num_rows\nxstest=test[\"sqft_living\"]\nX_test=stack_xn(onestest,xstest,2)\ny_pred_test=np.matmul(X_test,thetas)\ny_test=test[\"price\"]\ny_pred_test=y_pred_test.reshape((-1))\nr2= r2_score(y_test, y_pred_test)\nprint (\"R2: \",r2)\n", "intent": "Again, we determine the R2 score on the test set. \n"}
{"snippet": "num_rows=test.shape[0]\nones_test=[1]*num_rows\nxstest=test[\"sqft_living\"]\nX_test=stack_xn(ones_test,xstest,10)\ny_pred_test=np.matmul(X_test,thetas)\nr2= r2_score(y_test, y_pred_test)\nprint (\"R2: \",r2)\n", "intent": "<font color=\"red\">Again determine the $R^2$ on the test set, which is rather poor on average.</font>\n"}
{"snippet": "num_rows=test.shape[0]\nones_test=[1]*num_rows\nxstest=test[\"sqft_living\"]\nX_test=stack_xn(ones_test,xstest,10)\ny_pred_test=np.matmul(X_test,thetas)\ny_test=test[\"price\"]\ny_pred_test=y_pred_test.reshape((-1))\nr2= r2_score(y_test, y_pred_test)\nprint (\"R2: \",r2)\n", "intent": "Let us determine the $R^2$ using regularization.\n"}
{"snippet": "*YOUR CODE*\nr2= r2_score(y_test, *YOUR CODE*)\nprint (\"R2: \",r2)\n", "intent": "Now let us check how the obtained parameter perform on the test set.\n"}
{"snippet": "y_prediction = model.predict(x_val)\n", "intent": "Now we use the predict method to feed our validation data into the decision tree to get the predicted target values.\n"}
{"snippet": "PredictedValues = model.predict({'month_input': MonthCodeInput_Test, 'year_input': YearCodeInput_Test,\n                                 'dayofweek_input': DayOfWeekCodeInput_Test, 'cont_input': ContinuousInput_Test},\n          batch_size=100)\nprint(PredictedValues)\n", "intent": "Model will now be used to make predictions on the *Test* features.\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\nrfc_pred\n", "intent": "Predict the class of not.fully.paid for the X_test data.\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "** As expected, the classification report card is bad**\n"}
{"snippet": "grid_pred = grid.predict(X_test)\ngrid_pred\n", "intent": "Then you can re-run predictions on this grid object just like you would with a normal model\n"}
{"snippet": "predictions = linear.predict(X_test)\nprint (\"Type of the predicted object:\", type(predictions))\nprint (\"Size of the predicted object:\", predictions.shape)\n", "intent": "**Prediction using the linear model**\n"}
{"snippet": "max_mi_col = None\nmax_mi = -1\nfor col in df.columns:\n    cur_mi = mutual_info_score(df['outcome'], df[col])\n    if max_mi < cur_mi:\n        max_mi = cur_mi\n        max_mi_col = col\nprint(col + ': ' + str(max_mi))\n", "intent": "Which attribute has the highest Mutual Information with the 'outcome' attribute?\n"}
{"snippet": "print(cross_val_score(mdl, train_x, train_y, cv=5))\n", "intent": "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat.\n"}
{"snippet": "cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)\n", "intent": "To compute the score method of an estimator, the sklearn exposes a helper function:\n"}
{"snippet": "heuristic.evaluate('discriminatt-test.txt', threshold=0.1)\n", "intent": "But what's really interesting about this simple heuristic is how it performs on the previously held-out test set.\n"}
{"snippet": "print(accuracy_score(y_test,prediction)*100) \n", "intent": "we got accuracy 0.80,it is good not bad\n"}
{"snippet": "predictions = regr_3.predict(test)\n", "intent": "<p><a name=\"model3assumptions\"></a></p>\n"}
{"snippet": "def cost_function(features, labels, weights):\n    observations = len(labels)\n    predictions = predict(features, weights)\n    class1_cost = -labels*np.log(predictions)\n    class2_cost = (1-labels)*np.log(1-predictions)\n    cost = class1_cost - class2_cost\n    cost = cost.sum()/observations\n    return cost\n", "intent": "Calculation of cost function\n"}
{"snippet": "def update_weights(features, labels, weights, lr):\n    N = len(features)\n    predictions = predict(features, weights)\n    gradient = np.dot(features.T,  predictions - labels)\n    gradient /= N\n    gradient *= lr\n    weights -= gradient\n    return weights\n", "intent": "To minimize the cost, Gradient Descent is used. \n"}
{"snippet": "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\nprint(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\nprint(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\n", "intent": "The Accuracy is between 97%-98%. Based on this output, model should be extremely accurate.\n"}
{"snippet": "np.mean(cross_val_score(dummy_pipeline, emails, y, scoring='accuracy', cv=10))\n", "intent": "Check how the dummy classifier performs, which will predict the most frequent class.\n"}
{"snippet": "np.mean(cross_val_score(count_vect_eng_pipeline, emails['stripped_metadata'], y, scoring='accuracy', cv=10))\n", "intent": "Checking the accuracy of a classifier, with 10-fold cross validation, and stripped metadata\n"}
{"snippet": "y_predicted = cross_val_predict(count_vect_eng_pipeline, emails['stripped_metadata'], y, cv=10)\nconfusion_matrix(y, y_predicted)\n", "intent": "The confusion matrix:\n"}
{"snippet": "ss = ShuffleSplit(n_splits=1, train_size=0.8)\ncross_val_score(pipeline, df, y, scoring=\"neg_mean_absolute_error\", cv=ss)\n", "intent": "<h1><code>cross_val_score</code> explained</h1>\n"}
{"snippet": "kf = KFold(n_splits = 10)\nnp.mean(cross_val_score(pipeline, df, y, scoring=\"neg_mean_absolute_error\", cv=kf))\n", "intent": "<h2>$k$-Fold Cross Validation in scikit-learn</h2>\n"}
{"snippet": "np.mean(cross_val_score(pipeline, df, y, scoring=\"neg_mean_absolute_error\", cv=10))\n", "intent": "<ul>\n    <li>But $k$-fold cross-validation is so common, there's a shorthand:</li>\n</ul>   \n"}
{"snippet": "ss = StratifiedShuffleSplit(n_splits=1, train_size=0.8)\nnp.mean(cross_val_score(pipeline, df, y, scoring=\"accuracy\", cv=ss))\n", "intent": "<h1>Stratification in scikit-learn</h1>\n"}
{"snippet": "y_predicted = cross_val_predict(pipeline, df, y, cv=10) \nconfusion_matrix(y, y_predicted)\n", "intent": "<h1>Confusion Matrices in scikit-learn</h1>\n"}
{"snippet": "y_pred = myNaiveBayes(X_train,y_train).predict(X_test)\naccuracy = confusion_matrix(y_test,y_pred)\nprint(accuracy)\n", "intent": "Now let's try with all the classes as a test\n"}
{"snippet": "y_pred = myNaiveBayes(X_train,y_train).predict(X_test)\naccuracy = confusion_matrix(y_test,y_pred)\nprint(accuracy)\n", "intent": "That's a mess. Lots of data, luckily we can ignore all the zeros\n"}
{"snippet": "accuracy_score(knn_predictions, rand_forest_predictions)\n", "intent": "In 99.7% of all cases the knn classifier agrees with the choice of the voting classifier.\n"}
{"snippet": "predictions = model.predict(digits_test_scaled)\n", "intent": "After 12 training epochs the model sits at 99.7% training set accuracy and 99.0% dev set accuracy. This beats my previous models by a lot.\n"}
{"snippet": "pred_train = model.predict([input_sequences_train[:10], target_sequences_train[:10]])\npred_val = model.predict([input_sequences_val[:10], target_sequences_val[:10]])\n", "intent": "Let's see how it translates some of the training sentences.\n"}
{"snippet": "M1_test_y_pred = clf.predict(test_X_scaled)\n", "intent": "** Evaluate Model 1 on the test set **\n"}
{"snippet": "loss = tf.losses.mean_squared_error(y, y_hat)\n", "intent": "Specify a loss function (mean squared error):\n"}
{"snippet": "from sklearn.metrics import classification_report\ntarget_names = ['Foreground', 'Background']\nprint(classification_report(Y_test, pred, target_names=target_names))\n", "intent": "We can create a classification report:\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['cluster'],kmean.labels_))\nprint(classification_report(df['cluster'],kmean.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions=logmodel.predict(X_test)\npredictions\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "Well this was a lot of work for a small improvement in accuracy... Let's see more details about how the algorithm is performing\n"}
{"snippet": "cluster_labels = cluster_model.predict(data_for_clustering_matrix)\ndf_pivoted['cluster'] = cluster_labels\n", "intent": "Lets score the dataframe (cluster our customers) with our cluster model \n"}
{"snippet": "metrics = model.evaluate(X_normalized_test, y_one_hot_test)\nfor metric_i in range(len(model.metrics_names)):\n    metric_name = model.metrics_names[metric_i]\n    metric_value = metrics[metric_i]\n    print('{}: {}'.format(metric_name, metric_value))\n", "intent": "Evaluate the new model on test data:\n"}
{"snippet": "print('Score: ',poly_lin_model.score(Xtest, ytest))\nprint(\"r squared test (training): \", r2_score(ytrain, ytrainpred))\nprint(\"r squared test (prediction): \", r2_score(ytest, ytestpred))\n", "intent": "We can see that PolynomialFeatures(4) is most effective\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndef answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    knn = answer_five()\n    y_pred = answer_seven()\n    accuracy = accuracy_score(y_test, y_pred)\n    return accuracy\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\nprint (knn.predict(X_new))\nknn_predict_list = [k_classifier.predict(X_new) for k_classifier in knn_list]\nprint (knn_predict_list)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "print (\"start\")\nstartTime= datetime.now()\ntrn_features = model.predict(trn_data, batch_size=batch_size)\ntimeElapsed=datetime.now()-startTime\nprint('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\nstartTime= datetime.now()\nval_features = model.predict(val_data, batch_size=batch_size)\ntimeElapsed=datetime.now()-startTime\nprint('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n", "intent": "...and their 1,000 imagenet probabilties from VGG16--these will be the *features* for our linear model:\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\nprint (\"preds:\", type(preds))\nprint (\"probs:\", type(probs))\nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "startTime= datetime.now() \ntrn_features = model.predict(trn_data, batch_size=batch_size)\ntimeElapsed=datetime.now()-startTime \nprint('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\ntimeElapsed=datetime.now()-startTime \nval_features = model.predict(val_data, batch_size=batch_size)\nprint('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n", "intent": "...and their 1,000 imagenet probabilties from VGG16--these will be the *features* for our linear model:\n"}
{"snippet": "from datetime import datetime \nstartTime= datetime.now() \npreds = lm.predict_classes(val_features, batch_size=batch_size)\ntimeElapsed=datetime.now()-startTime \nprint('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\ntimeElapsed=datetime.now()-startTime \nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\nprint('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "preds2 = model.predict_classes(val_data, batch_size=batch_size)\nprobs2 = model.predict_proba(val_data, batch_size=batch_size)[:,0]\nprobs2[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "print(classification_report(y_test, prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print(classification_report(y_test, prediction))\nprint('\\n')\nprint(classification_report(y_test, pred))\nprint('\\n')\nprint(confusion_matrix(y_test, prediction))\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\n", "intent": "**What performed better the random forest or the decision tree?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(college_data['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(confusion_matrix(college_data['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(y_test, prediction))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "prediction = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = model.predict(X_test)\nprint(predictions[0:5])\n", "intent": "This score is the R squared value of the model for this dataset, \nwhich measures what portion of total variation is explained by the model.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, model.predict(X_test))  \n", "intent": "Sci-kit learn also provides convenience functions for computing mean squared error.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.\n    for idx in range(len(style_layers)):\n        style_current = gram_matrix(feats[style_layers[idx]])\n        style_loss += style_weights[idx] * tf.reduce_sum((style_current - style_targets[idx]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, idx in enumerate(style_layers):\n        loss_l = tf.reduce_sum((style_targets[i] - gram_matrix(feats[idx]))**2)\n        style_loss += style_weights[i] * loss_l\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "error = svm_model.predict(train_std)\nprint error[error == -1].size\n", "intent": "To assess the error of the model I make a prediction for the training set. \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncvs = cross_val_score(rfc, train_std, target, \n                    scoring='accuracy', cv=10).mean()\n", "intent": "I use 10-fold cross validation.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncvs = cross_val_score(svc, train_std, target, \n                    scoring='accuracy', cv=10).mean()\n", "intent": "I use 10-fold cross validation.\n"}
{"snippet": "acc_score = metrics.accuracy_score(y_pred,y)\nprint(acc_score)\n", "intent": "* our accuarcy are 24 Percent with KNN because this is regreesion module and I have trying Classifier so it will not give you the best result\n"}
{"snippet": "metrics.accuracy_score(y,y_new_pred)\n", "intent": "* Accuarcy = 24 percent\n* 24 percent is very low as compared to Logistic regression(91 percent)\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(data['Cluster'],kmeans.labels_))\nprint(classification_report(data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "def z_score(x):\n    return (x - x.mean()) / x.std()\n", "intent": "create a function that would evaluate the Z-scores since we don't want to worry about conflicts introduced by different unit systems. \n"}
{"snippet": "X_train = z_score(X_train)\ny_train = z_score(y_train)\nX_test  = z_score(X_test)\ny_test  = z_score(y_test)\nfor i in range(0, len(camp)):\n    X_train_camp[i] = z_score(X_train_camp[i])\n    y_train_camp[i] = z_score(y_train_camp[i])\n    X_test_camp[i]  = z_score(X_test_camp[i])\n    y_test_camp[i]  = z_score(y_test_camp[i])\nX_test.head()\n", "intent": "After the splitting process has been done, we need to normalize the training and testing datasets correspondingly.\n"}
{"snippet": "cross_accuracy = metrics.r2_score(y, y_cross_pred)\nprint('Cross-Predicted Accuracy:', cross_accuracy)\n", "intent": "There's much more data points in this plot as there're ten validation samples being trained\n"}
{"snippet": "def content_loss(content, combination):\n    return K.sum(K.square(combination - content))\n", "intent": "* The content loss is the (scaled, squared) Euclidean distance between feature representations of the content and combination images.\n"}
{"snippet": "def total_variation_loss(x):\n    a = K.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n    b = K.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n", "intent": "You can experiment with reducing the total_variation_weight and play with the noise-level of the generated image.\n"}
{"snippet": "print(((bos.PRICE - lm.predict(X))**2).mean())\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_train_pred_90 = (y_scores > 70000)\nrecall_score(y_train_5, y_train_pred_90)\n", "intent": "The idea is you want to hit the area where the two plots intersect so that neither values are such a low place. \n"}
{"snippet": "predicted_labels = xgb.predict(X_test)\naccuracy = accuracy_score(y_test, predicted_labels)\nconfusion = confusion_matrix(y_test, predicted_labels).tolist()\ntarget_names = list(iris.target_names)\nprint(\"Accuracy\", accuracy)\nprint(\"Confusion Matrix\", confusion, \"\\n\")\nprint(classification_report(y_test, predicted_labels, target_names=target_names))\n", "intent": "Again, now with the trained model we can now predict the label for our test set and see how well the model performed.\n"}
{"snippet": "X_new = \nX_new_poly = poly_transformer.transform() \ny_pred_new = .predict() \n", "intent": "Use your final model to make predictions on the new data.\nNote that you must first transform the features with a 3rd-degree polynomial, as before:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"car_YOLO.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "x_test = np.array(['I want to have love you'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "MSE = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(MSE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = model.predict(x_test) \ny_prob = model.predict_proba(x_test) \n", "intent": "**Step-7: Evaluation ** \n"}
{"snippet": "y_pred_log = model_log.predict(x_test) \ny_pred_knn = model_knn.predict(x_test) \ny_pred_svm = model_svm.predict(x_test) \ny_pred_dt = model_dt.predict(x_test) \ny_pred_rf = model_rf.predict(x_test) \n", "intent": "**Step - 6 : Evaluation**\n"}
{"snippet": "y_pred_log = model_log.predict(x_test) \ny_pred_knn = model_knn.predict(x_test) \ny_pred_svm = model_svm.predict(x_test) \ny_pred_dt = model_dt.predict(x_test) \ny_pred_rf = model_rf.predict(x_test) \ny_pred_nb = model_rf.predict(x_test) \n", "intent": "**Step - 6 : Evaluation**\n"}
{"snippet": "NBPred = clf.predict(test_X)\ntest_error = len([(a, b) for a, b in zip(NBPred, test_y) if a != b]) / len(test_y)\ntest_error\n", "intent": "from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(train_X, train_y)\n"}
{"snippet": "from sklearn.metrics import log_loss\ndef calculate_logloss(y_true, y_pred):\n    loss_cal = log_loss(y_true, y_pred)\n    return loss_cal\n", "intent": "Taken from https://www.kaggle.com/tj2552/similarity-techniques-nlp\n"}
{"snippet": "predictions = mlp.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,predictions))\n", "intent": "Now that we have a model it is time to use it to get predictions! We can do this simply with the predict() method off of our fitted model:\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy_Inception = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Inception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG_19_predictions = [np.argmax(VGG_19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG_19]\ntest_accuracy_VGG_19 = 100*np.sum(np.array(VGG_19_predictions)==np.argmax(test_targets, axis=1))/len(VGG_19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_VGG_19)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "cnn1_pred_cv_y_mfccs_1D = cnn1.predict(conv_cv_X_mfccs_1D, batch_size=32)\ncnn1_pred_cv_y_mfccs_1D = utils.one_hot_encode(cnn1_pred_cv_y_mfccs_1D)\ncnn1_pred_cv_y_mfccs_1D.shape\n", "intent": "This CNN architecture should get to 0.3 accuracy within 50-70 epochs and then start to overfit.\n"}
{"snippet": "print \"Accuracy when predicting num of mos >=50 with virus = \", metrics.accuracy_score(train_label,train['NumMosquitos']>=50)\n", "intent": "The number of mosquitos is not included in the test dataset, use number of mosquitos to predict the detection of virus.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)\n", "intent": "To get a measure of our model's performance we need to evaluate it using the test samples:\n"}
{"snippet": "import os\ndef savePredictions(model):\n    predictions = model.predict(inception_test_output, batch_size=32, verbose=0)\n    save_dir = 'predictions/'\n    i = 0\n    while os.path.exists(os.path.join(save_dir, \"predictions_%s.npy\" % i)):\n        i += 1\n    np.save(os.path.join(save_dir, \"predictions_%s.npy\" % i), predictions)\n", "intent": "The following function saves the predictions of the test set as a file.\n"}
{"snippet": "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n", "intent": "We can also evaluate our performance using **evaluate**.\n"}
{"snippet": "classes = model.predict(x_test, batch_size=128)\n", "intent": "We can generate predictions on new data using **predict**.\n"}
{"snippet": "def cifar10_loss(logits, labels):\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=labels, logits=logits, name='cross_entropy_per_example')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n  tf.add_to_collection('losses', cross_entropy_mean)\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n", "intent": "Use the code provided by tensorflowt directly.\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Predict the class of vehicle for the X_test data.\n"}
{"snippet": "perturbed_samples = [get_perturbed_sample(sample) for i in range(5000)]\nperturbed_predictions = text_clf.predict(perturbed_samples)\n", "intent": "Get a lot of perturbed samples and predict on them\n"}
{"snippet": "new_pert_samples = [get_perturbed_sample(sample) for i in range(1000)]\nprint(\"Score: {:.1%}\".format(explainer.score(new_pert_samples, text_clf.predict(new_pert_samples))))\n", "intent": "How well does it approximate the black-box model?\n"}
{"snippet": "p = explainer.predict_proba([sample])[0]\nfor c in explainer.steps[2][1].classes_:\n    print(\"> {}\".format(twenty_train.target_names[c]))\n    print(f\"probability: {p[c]:.1%}\"); print(\"-\"*20)\n    for w, v in zip(important_words, explainer.steps[2][1].coef_[c]):\n        print(f\"{w}: {v:.3}\")\n    print()\n", "intent": "Look at the explanation \n"}
{"snippet": "regression1_scores = cross_val_score(model_regression1, X_train, y_train, cv = 10)\nprint(\"cross validation scores: \\n\",regression1_scores)\nprint(\"testing score: {}\".format(model_regression1.score(X_test, y_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "regression2_scores = cross_val_score(model_regression2, X_pg_train, y_pg_train, cv = 10)\nprint(\"cross validation scores: \\n\",regression2_scores)\nprint(\"testing score: {}\".format(model_regression2.score(X_pg_test, y_pg_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "regression3_scores = cross_val_score(model_regression3, X_genre_train, y_genre_train, cv = 10)\nprint(\"cross validation scores: \\n\",regression3_scores)\nprint(\"testing score: {}\".format(model_regression3.score(X_genre_test, y_genre_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "regression4_scores = cross_val_score(model_regression4, X_occ_train, y_occ_train, cv = 10)\nprint(\"cross validation scores: \\n\",regression4_scores)\nprint(\"testing score: {}\".format(model_regression4.score(X_occ_test, y_occ_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "regression5_scores = cross_val_score(model_regression5, X_timespan_train, y_timespan_train, cv = 10)\nprint(\"cross validation scores: \\n\",regression5_scores)\nprint(\"testing score: {}\".format(model_regression5.score(X_timespan_test, y_timespan_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "regression6_scores = cross_val_score(model_regression6, X_pagerank_train, y_pagerank_train, cv = 10)\nprint(\"cross validation scores: \\n\",regression6_scores)\nprint(\"testing score: {}\".format(model_regression6.score(X_pagerank_test, y_pagerank_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "dt_scores = cross_val_score(model_dt, X_train, y_train, cv = 10)\nprint(\"cross validation scores: \\n\",dt_scores)\nprint(\"testing score: {}\".format(model_dt.score(X_test, y_test)))\nprint(\"\\n\")\n", "intent": "performing 10-fold CV and checking the effectiveness of the model on test data\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i, l in enumerate(style_layers):\n        style_weight = style_weights[i]\n        style_gram = style_targets[i]\n        content_features = feats[l]\n        content_gram = gram_matrix(content_features)\n        style_loss += style_weight * tf.reduce_sum(tf.square(content_gram - style_gram))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The accuracy of this model is slightly worse than that of the original model (77.48%)\n"}
{"snippet": "labels = data.map(lambda x: bestModel.predict(x))\nimgLabels = labels.toarray().reshape(2, 76, 87)\ndraw_image(imgLabels[0, :, :]);\n", "intent": "<div class=\"alert alert-info\">\nComplete the source code below to visualize the result of clustering.\n</div>\n"}
{"snippet": "best_k = 20\nbest_model = models[ks.index(best_k)]\ntraining_labels = training_data.map(lambda x: best_model.predict(x))\ntest_labels  = test_data.map(lambda x: best_model.predict(x))\nimg_training_labels = training_labels.toarray().reshape(2, 76, 87)\nimg_test_labels = test_labels.toarray().reshape(2, 76, 87)\n", "intent": "<div class=\"alert alert-success\">\nLet us keep $k=20$ and plot the cluster labels with the selected model.\n</div>\n"}
{"snippet": "print(mean_squared_error(Y_test,RF.predict(X_test)))\n", "intent": "MSE compared with predicting the median\n"}
{"snippet": "np.vstack((RF.predict(X_test),Y_test)).T\n", "intent": "Check predictions with ground truth\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    losses = []\n    for i, n in enumerate(style_layers):\n        loss = style_weights[i] * tf.reduce_sum((gram_matrix(feats[n]) - style_targets[i]) ** 2)\n        losses.append(loss)\n    return tf.reduce_sum(losses)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "MSE = np.sum((bos.PRICE - lm.predict(X)) ** 2) / len(bos.PRICE)\nprint (MSE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "f1 = model_selection.cross_val_score(model, \n                                     Xtrain, y=ytrain, \n                                     scoring='f1', \n                                     cv=kf_indices)\nprint([\"%.3f\" % value for value in f1])\n", "intent": "scikit-learn allows us to directly compute the cross-validated F1 score (on the train test) of this model:\n"}
{"snippet": "f1 = model_selection.cross_val_score(model, \n                                     Xtrain, y=ytrain, \n                                     scoring='f1', \n                                     cv=kf_indices)\nprint([\"%.3f\" % value for value in f1])\n", "intent": "and compute the cross-validated F1 score (on the train test) of this model:\n"}
{"snippet": "f1 = model_selection.cross_val_score(model, \n                                     Xtrain, y=ytrain_hard, \n                                     scoring='f1', \n                                     cv=kf_indices)\nprint([\"%.3f\" % value for value in f1])\n", "intent": "and compute the cross-validated F1 score (on the train test) of a decision tree:\n"}
{"snippet": "nmse = model_selection.cross_val_score(model, \n                                      Xtrain_std, y=ytrain, \n                                      scoring='neg_mean_squared_error', \n                                      cv=kf_indices)\nprint([\"%.3f\" % value for value in nmse])\n", "intent": "scikit-learn allows us to directly compute the cross-validated MSE (on the train test) of this model:\n"}
{"snippet": "import numpy as np\ny_true = np.array(valor_verdade)\ny_scores = np.array(valor_previsto)\nmean_absolute_error(y_true, y_scores)\n", "intent": "MEAN ABSOLUTE ERROR \n"}
{"snippet": "sqrt(mean_squared_error(y_true, y_scores))\n", "intent": "ROOT MEAN SQUARED ERROR\n"}
{"snippet": "y_predictions = nbayes.predict(X_testcv)\n", "intent": "*using X_testcv : which is vectorized such that the dimensions are matched*\n"}
{"snippet": "print(\"Accuracy % :\",metrics.accuracy_score(y_test, y_predictions)*100)\nprint(\"Precision Score: \", precision_score(y_test, y_predictions, average='micro'))\nprint(\"Recall Score: \",recall_score(y_test, y_predictions, average='micro') )\nprint(\"F1 Score: \",f1_score(y_test, y_predictions, average='micro') )\n", "intent": "** Printing out the Accuracy, Precision Score, Recall Score, F1 Score **\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Inception V3 Test accuracy: %.4f%%' % test_accuracy)\nResnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Resnet 50 Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "dt_scores = cross_val_score(decision_tree_classifier, all_inputs, all_classes, cv=10)\nsb.boxplot(dt_scores)\nsb.stripplot(dt_scores, jitter=True, color='white')\n", "intent": "<img src=\"iris_dtc.png\" />\n"}
{"snippet": "def predict(x, w):\n    return w[0] + np.dot(x, w[1:])\n", "intent": "In this step, we will optimize the least squares cost function using gradient descent and Newton's method.\n"}
{"snippet": "least_squares = lambda w : np.sum((predict(X, w) - normed_Y)**2)\nw_init = np.random.randn(normed_X.shape[1] + 1, 1) * 0.1\nmax_iters = 5000\nalpha = 10**-4\nw_history = gradient_descent(least_squares, w_init,alpha, max_iters, beta=0.5, version='normalized')\niters = range(max_iters)\ncosts = [least_squares(w) for w in w_history]\n", "intent": "First we experiment with the gradient descent optimizer. Here unnormalized input features and normalized output are used. \n"}
{"snippet": "print(mean_squared_error(bos.PRICE, pred))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def loss_poisson(target, predictions):\n    return tf.reduce_mean(tf.nn.log_poisson_loss(target, predictions))\n", "intent": "We have to create our own loss function:\n"}
{"snippet": "labels_test.count(1)  \nlabels_dummy = [0 for _ in xrange(len(labels_test))] \nacc = accuracy_score(labels_test, labels_dummy) \nacc\n", "intent": "**Accuracy of a Biased Identifier**   \nUdacity: If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?\n"}
{"snippet": "data_dict = init()\nfeatures_list = ['poi','salary']\nmy_dataset = data_dict\nlabels, features = split_data(my_dataset, features_list)\nclf = classify()\nevaluate(clf, my_dataset, features_list)\n", "intent": "Now we could modify ```poi_id.py``` as below\n"}
{"snippet": "n_neighbors = 4\nclf_k = classify(features_train, labels_train, n_neighbors)\nlabels_pred_k = clf_k.predict(features_test)\nacc_k = accuracy_score(labels_test, labels_pred_k)\nprint('Accuracy: {}'.format(acc_k))\n", "intent": "Let us try increasing neighbors..\n"}
{"snippet": "from sklearn.metrics import r2_score\ntarget_predictions = reg.predict(feature_test)\nscore = r2_score(target_test, target_predictions)  \nscore\n", "intent": "**Step 2: Predict and calculate the score**\nwith test data..\n"}
{"snippet": "from sklearn.metrics import r2_score\ntarget_predictions = reg.predict(ages_test)\nscore = r2_score(net_worths_test, target_predictions)  \nscore\n", "intent": "**Score of Regression with Outliers**\nWhat is the score you get when using your regression to make predictions with the test data?\n"}
{"snippet": "y_predict = rf.predict(X_test)\nprint \"Confusion matrix:\\n\\n\", confusion_matrix(y_test, y_predict)\nprint \"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_predict)\n", "intent": "Look at the confusion matrix and classification report.\n"}
{"snippet": "activity_id = list(merged_test_df.activity_id)\ntest_dataset = merged_test_df.drop(['people_id','activity_id','date_x','date_y'], 1)\ntest_mat = np.array(test_dataset, dtype='int')\nprint(test_mat.shape)\nans = model.predict(test_mat)\nprint(ans.shape)\n", "intent": "To predict the submission\n"}
{"snippet": "activity_id = list(merged_test_df.activity_id)\ntest_dataset = merged_test_df.drop(['people_id','activity_id','date_x','date_y'], 1)\ntest_mat = np.array(test_dataset, dtype='int')\nprint test_mat.shape\nans = model.predict(test_mat)\nprint ans.shape\n", "intent": "To predict the submission\n"}
{"snippet": "num_folds = 3\nnum_instances = n\nseed = 7\niterations = 1000\nresults = cross_validation.cross_val_score(model, X, Y, cv=cv)\nprint(results.mean())\n", "intent": "To train our classifier with sklearn.svm\n"}
{"snippet": "scores_all_lr = cross_validation.cross_val_score(llr, clean_data, target, cv=5)\nscores_ss_lr = cross_validation.cross_val_score(llr1, lr_data, target, cv=5)\nscores_rfe_lr = cross_validation.cross_val_score(llr2, rfe_data, target, cv=5)\nprint \"Accuracy of complete dataset: \" + \"{0:0.2f}\".format(scores_all_lr.mean()*100) +\"%\"\nprint \"Accuracy of Stability selection attributes: \" + \"{0:0.2f}\".format(scores_ss_lr.mean()*100) +\"%\"\nprint \"Accuracy of rfe attributes: \" + \"{0:0.2f}\".format(scores_rfe_lr.mean()*100) +\"%\"\n", "intent": "<h3>5-fold cross validation</h3>\n"}
{"snippet": "np.around(model.predict(x))\n", "intent": "Now let's check the output from our training set.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Model pre-trained by Resnet-50 test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('R^2 Score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(regr, X_train, y_train, cv=8)\nprint(scores)\nprint(\"The model is strong because of the high cross validation score.\")\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "from keras import backend as K\ndef max_margin_loss(y_true, y_pred):\n    loss_ = K.sum(K.maximum(0.0, 1.0 - y_pred[0] + y_pred[1]))\n    return loss_\n", "intent": "\\begin{equation*}\nloss = \\sum_i{max(0, 1 -p_i + n_i)}\n\\end{equation*}\n"}
{"snippet": "from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))\n", "intent": "Now, let's calculate the average precision score\n"}
{"snippet": "y_test_prediction = model.predict(X_test1)\n", "intent": "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:\n"}
{"snippet": "inception_pred = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ninception_acc = 100*np.sum(np.array(inception_pred)==np.argmax(test_targets, axis=1))/len(inception_pred)\nprint('Test accuracy: %.4f%%' % inception_acc)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds_lon = model_lon.predict(valid_set_lon.features, batch_size=1024)\npreds_lon.shape\n", "intent": "Note: The R2 and unexplained, total error do not agree with the Tensorboard scores, see next section.\n"}
{"snippet": "print(classification_report(y1_test,clf_rfc_pred))\n", "intent": "We have a much better prediction of customers who actually left and the ones who did not leave as well.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for idx in range(len(style_layers)):\n        loss = style_weights[idx]*tf.reduce_sum(tf.square(gram_matrix(feats[style_layers[idx]])- style_targets[idx]))\n        style_loss += loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2)\n", "intent": "The residual sum of squares: \n"}
{"snippet": "np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2)\n", "intent": "Mean squared error: \n"}
{"snippet": "feats_occ = extractor.predict(ims_acc, batch_size=128, verbose=0)\nfeats_occ.shape\n", "intent": "```ims_occ``` contains all images with the occluder set at different positions. Let's run these through our extractor:\n"}
{"snippet": "feats_frame_1 = base_net.predict(frame_1_samples,batch_size=1,verbose=2)\nfeats_frame_2 = base_net.predict(frame_2_samples,batch_size=1,verbose=2)\n", "intent": "**Exercise:** Use ```base_net``` to extract features from detections in both frames (using the ```predict``` method):\n"}
{"snippet": "y_pred = decoder_op\ny_true = X\ncost_m = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\ncost_sparse = 0.001*tf.reduce_sum(KL_Div(0.2,rho_hat))\ncost_reg = 0.0001* (tf.nn.l2_loss(weights['decoder_h1']) + tf.nn.l2_loss(weights\n['encoder_h1']))\ncost = tf.add(cost_reg,tf.add(cost_m,cost_sparse))\noptimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\ninit = tf.global_variables_initializer()\n", "intent": "Reconstructed output\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\npredictions = knn.predict(X_new)\nfor prediction in predictions:\n    print(f\"Predicted Iris type: {prediction} or {iris.target_names[prediction]}\")\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "feature_cols = ['TV', 'radio']\nX2 = data[feature_cols]\nrmse_no_advertising = np.sqrt(-cross_val_score(lm, X2, y, cv=10, scoring='neg_mean_squared_error')).mean()\nprint(rmse_no_advertising)\n", "intent": "The RMSE for all 3 features ( TV, radio, newspaper ) is 1.69.\n"}
{"snippet": "y_predicted = linear_regression_model.predict(x_input_series)\n", "intent": "For the 1000 new x values, predict the set of y values\n"}
{"snippet": "lrm = np.sqrt(-cross_val_score(linear_regression_model, x_series, y, cv=10, scoring='neg_mean_squared_error')).mean()\npoly3 = np.sqrt(-cross_val_score(poly_model_3, x_series, y, cv=10, scoring='neg_mean_squared_error')).mean()\npoly7 = np.sqrt(-cross_val_score(poly_model_7, x_series, y, cv=10, scoring='neg_mean_squared_error')).mean()\npoly5 = np.sqrt(-cross_val_score(poly_model_5, x_series, y, cv=10, scoring='neg_mean_squared_error')).mean()\npoly15 = np.sqrt(-cross_val_score(poly_model_15, x_series, y, cv=10, scoring='neg_mean_squared_error')).mean()\nprint(f\"Linear Regression error: {lrm}\")\nprint(f\"3rd Degree Polynomial error: {poly3}\")\nprint(f\"5th Degree Polynomial error: {poly5}\")\nprint(f\"7th Degree Polynomial error: {poly7}\")\nprint(f\"15th Degree Polynomial error: {poly15}\")\n", "intent": "Calculate the RMSE for each of the models and see how each on compares with the others.\n"}
{"snippet": "acc = (testdf['is_unloading_ship'] == clf.predict(testdf[['SENSOR_A', 'SENSOR_B']])).sum() / float(len(testdf))\nprint 'Accuracy: ', acc*100, '%'\n", "intent": "Not surprisingly, since identifying the inital label was easy. How well does it perform on the test set?\n"}
{"snippet": "def predict_loading_by_group(x):\n    preds = clf2.predict_proba(x[['SENSOR_A', 'SENSOR_B', 'TIME_FROM_UNLOADING']])\n    msk = preds[:,0] == preds[:,0].min()\n    preds[:,1][msk] = 1\n    preds[:,1][~msk] = 0\n    x['is_loading_train_pred'] = preds[:,1]\n    return x\n", "intent": "Not bad. But, I bet we can do better if we obtain the probabilities from each group and choose the one with the highest probability.\n"}
{"snippet": "Xc_predictions = [np.argmax(Xc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xc]\nXc_test_accuracy = 100*np.sum(np.array(Xc_predictions)==np.argmax(test_targets, axis=1))/len(Xc_predictions)\nprint('Test accuracy: %.4f%%' % Xc_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "[ \n    parameter_,\n    data_generator_sample_,\n] = evaluate([ \n    parameter, \n    data_generator.sample(),\n])\nprint(\"'parameter_' evaluated Tensor :\", parameter_)\nprint(\"'data_generator_' sample evaluated Tensor :\", data_generator_sample_)\n", "intent": "More generally, we can use our `evaluate()` function to convert between the Tensfflow `tensor` data type and one that we can run operations on:\n"}
{"snippet": "N = 100\nsecond_coin_flips = tfd.Bernoulli(name=\"second_flips\", \n                                  probs=0.5).sample(sample_shape=N, \n                                                    seed=5)\n[\n    second_coin_flips_,\n] = evaluate([\n    second_coin_flips,\n])\nprint(second_coin_flips_)\n", "intent": "Although *not everyone* flips a second time, we can still model the possible realization of second coin-flips:\n"}
{"snippet": "y_true = y_test_class\ny_pred = rfc.predict(X_test_class)\ncm = confusion_matrix(y_true=y_true, y_pred=y_pred)\nTN = cm[0][0]\nFN = cm[1][0]\nTP = cm[1][1]\nFP = cm[0][1]\nprint 'Precision (TP, FP): ', TP, FP, 'Prec = ', (TP)/(TP + FP)\nprint 'recall (TP, FN): ', TP, FN, 'Recall = ', (TP)/(TP + FN)\n", "intent": "**Confusion Matrix**\n"}
{"snippet": "prediction02 = nb.sk_naivebayes_predict()\nprint np.sum(nb.test_labels==prediction01)/len(nb.test_labels)\n", "intent": "Sk_learn's implementation\n"}
{"snippet": "print(\"confusion matrix :\")\nprint(confusion_matrix(iris.target , pred_y))\nprint(\"accuracy score :\")\nprint(accuracy_score(iris.target , pred_y))\n", "intent": "<h3>Checking ouput or predicted clusters with original dependent variable of iris data set.</h3>\n"}
{"snippet": "from lstm_model import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\nclass supervised_training:\n    input_sequence = tf.placeholder('int32',[None,None])\n    reference_answers = tf.placeholder('int32',[None,None])\n    logprobs_seq = model.symbolic_score(input_sequence, reference_answers)\n    crossentropy = - select_values_over_last_axis(logprobs_seq,reference_answers)\n    mask = infer_mask(reference_answers, out_voc.eos_ix)\n    loss = tf.reduce_sum(crossentropy * mask)/tf.reduce_sum(mask)\n    train_step = tf.train.AdamOptimizer().minimize(loss, var_list=[model.weights])\ninitialize_uninitialized(s)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images. \n"}
{"snippet": "Y_pred_train = linear_regression.predict(X_train)\nY_pred_test = linear_regression.predict(X_test)\n", "intent": "Now run a prediction on both the training set and the test set.\n"}
{"snippet": "sil_avg = silhouette_score(X, cluster_labels)\nsil_samples = silhouette_samples(X, cluster_labels)\nsil_avg,sil_samples\n", "intent": "This uses the original data to compute the average silhouette score\n"}
{"snippet": "sil_avg = silhouette_score(cluster_distance, cluster_labels)\nsil_samples = silhouette_samples(cluster_distance, cluster_labels)\nsil_avg,sil_samples\n", "intent": "This uses the distance matrix\n"}
{"snippet": "predicted_labels = model.predict(features_test)\npredicted_labels\n", "intent": "Compute the predicted labels-:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    n = len(style_layers)\n    for i in range(n):\n        gram = gram_matrix(feats[style_layers[i]])\n        if(i==0):\n            style_loss = style_weights[i]*torch.sum((gram-style_targets[i])**2)\n        else:\n            style_loss += style_weights[i]*torch.sum((gram-style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_loss, test_acc = network.evaluate(test_images, test_labels)\n", "intent": "Now let's check that our model performs well on the test set too:\n"}
{"snippet": "DogResnet50Data_predictions = [np.argmax(DogResnet50Data_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50Data]\ntest_accuracy = 100*np.sum(np.array(DogResnet50Data_predictions)==np.argmax(test_targets, axis=1))/len(DogResnet50Data_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Y_pred = pipeline.predict(X_test)\ny_prob_output = pipeline.predict_proba(X_test)\n", "intent": "<h3>Generating the probabilities</h3>\n"}
{"snippet": "y_pred = expm1(pl.predict(X_test))\nmean_absolute_error(y_test, y_pred)\n", "intent": "Performance is slightly better than using one-hot encoding and then applying feature agglomeration or truncated SVD\n"}
{"snippet": "y_pred = expm1(pl.predict(X_test))\nmean_absolute_error(y_test, y_pred)\n", "intent": "Before evaluating the predicted values needs to be converted using function **expm1**\n"}
{"snippet": "msePTRATIO = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint (msePTRATIO)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.zeros(1)\n    for idx, target, weight in zip(style_layers, style_targets, style_weights):\n        gram = gram_matrix(feats[idx])\n        loss += weight * tf.reduce_sum((gram-target)**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "precision_score(y_train, rf.predict(X_train))\n", "intent": "The model is doing alright in deciding whether a customer will actually leave.\n"}
{"snippet": "yhat_test = rforest.predict(X_test)\ntest_presision = precision_score(y_test, yhat_test)\ntest_recall = recall_score(y_test, yhat_test)\nprint(f\"Model Precision: {test_presision}\")\nprint(f\"Model Recall: {test_recall}\")\n", "intent": "To asses the final value of the model, let us compute the precision, recall and confussion matrix for our (unseen) test dataset\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "** Testing the model within the train dataset **\n"}
{"snippet": "from sklearn.metrics import recall_score, precision_score\nprint(\"Recall:\")\nprint(recall_score(y_train_5, y_train_pred))\nprint()\nprint(\"Precision:\")\nprint(precision_score(y_train_5, y_train_pred))\n", "intent": "$$\n    Recall = \\frac{TP}{TP + FN}\n$$\n$$\n    Precision = \\frac{TP}{TP + FP}\n$$\n$$\n    F1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}\n$$\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n", "intent": "A good classifier will have an $AUC \\approx 1$, a mediocre classifier will have an $AUC \\approx 0.50$ \n"}
{"snippet": "from scipy.stats import mode\nrandom_forest_preds = []\nfor xtslice in xtest:\n    model_preds = []\n    for model in decision_trees:\n        slice_pred = model.predict(xtslice.reshape(1, -1))\n        model_preds.append(slice_pred[0])\n    random_forest_preds.append(mode(model_preds).mode[0])\n", "intent": "Very low-efficient implementation of a random-forest classifier.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc_decision_tree = accuracy_score(tree_clf.predict(xtest), ytest)\nacc_random_forest = accuracy_score(random_forest_preds, ytest)\nprint(f\"Final accuracy for a decision tree: {acc_decision_tree:0.2%}\")\nprint(f\"Final accuracy for a random forest: {acc_random_forest:0.2%}\")\n", "intent": "**-- The Random Forest --**\n"}
{"snippet": "yhat_test = rforest2.predict(X_test_compress)\nprint(f\"{'*' * 10}Accuracy: {accuracy_score(y_test, yhat_test):0.2%}{'*' * 10}\")\n", "intent": "By compressing the MNIST dataset using PCA, random forest actually took **longer** to compute an answer.\n"}
{"snippet": "roc_df = get_roc_values('roc_data', 'y_true', 'y_score', conn)\ndb_auc_score = get_roc_auc_score(roc_df)\n", "intent": "We compare this to the ROC curve computed in database.\n"}
{"snippet": "Gender = 0 \nClass = 1 \nResult = decision_tree.predict([Gender, Class])\n", "intent": "What Gender and Class shall we give to our new passenger?\n"}
{"snippet": "net.evaluate(X_test, y_test)\n", "intent": "Let's see how our model performs on the test dataset.\n"}
{"snippet": "a = model.predict(np.array([[1,1,0,0]]))\nprint(model.predict(np.array([[1,1,0,0]])))\n", "intent": "Se publican los valores resultados de la NN\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint('MSE-DF1:', metrics.mean_squared_error(df1_test['MEDV'], y_pred_df1))\nprint('MSE-DF2:', metrics.mean_squared_error(df2_test['MEDV'], y_pred_df2))\n", "intent": "* How does the model perform on $ D_1 $ and $ D_2 $? Why?\n"}
{"snippet": "print('MSE-LINEAL_ESCALADO:', metrics.mean_squared_error(Y_pred_s1,Y_test))\nprint('MSE-MPL_ESCALADO:', metrics.mean_squared_error(Y_pred_sMLP,Y_test))\n", "intent": "* Train the following models:\n         * Report the mean square error on the test set\n"}
{"snippet": "preds = grid_roc_auc.predict(testDF)\n", "intent": "Interesting with the gamma turned on the AUC is bad\n"}
{"snippet": "preds = estimator.predict(X_test_matrix)\ntestScore = roc_auc_score(preds, y_test)\nprint(\"test AUC: \", testScore)\n", "intent": "dev set AUC is 0.81\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for idx, layer in enumerate(style_layers):\n        style_gram = gram_matrix(feats[layer])\n        loss += style_weights[idx] * tf.reduce_sum((style_gram - style_targets[idx])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Will this classifier correctly predict the 36000th digit we saw before?\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train_5, y_train_pred)\n", "intent": "It got it right!\nBelow 3-fold cross validation is done and a confusion matrix shows the performance of the model.\n"}
{"snippet": "y_pred = model.predict(x_test)\n", "intent": "Now, the first step is to compute our predictions in the test set. For this, we'll use the `predict` method from our model.\n"}
{"snippet": "preds = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df[\"Cluster\"],kmeans.labels_))\nprint(classification_report(df[\"Cluster\"],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "lm.predict(CLIENT_FEATURES)\n", "intent": "- **CHAS**     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- **RM**       average number of rooms per dwelling\n"}
{"snippet": "indices_to_project = [t[0] for t in sorted(enumerate(vars_), key=lambda x: x[1])[-3:]] \ndata_to_project = data.randomSplit([10, 90])[0]\nrdd0 = data.filter(lambda point: clusters.predict(point)==0)\nrdd1 = data.filter(lambda point: clusters.predict(point)==1)\n", "intent": "We will select only these 3 features to do the plot, and on 1% of the whole dataset.\n"}
{"snippet": "clusterLabelCount = labelsAndData.map(lambda row: ((clusters.predict(row[1]), row[0]), 1)).reduceByKey(add)\nfor item in clusterLabelCount.collect():\n    print(item)\n", "intent": "```python\nclusterLabelCount = ...\nfor item in clusterLabelCount:\n    print(item)\n```\n"}
{"snippet": "new_data1 = map_to_ints(raw_data)\nnew_data = new_data1.map(lambda x: LabeledPoint(x[0],x[1])).cache()\nmodel = DecisionTree.trainClassifier(new_data, numClasses=23, categoricalFeaturesInfo=get_dictionnary([True]*41),\n                                     impurity='entropy', maxDepth=15, maxBins=70)\npredictions = model.predict(new_data.map(lambda x: x.features))\nlabelsAndPredictions = new_data.map(lambda lp: lp.label).zip(predictions)\ntestErr = labelsAndPredictions.filter(lambda x: x[0] != x[1]).count()/new_data.count()\n", "intent": "We will now train our decision tree to see if it is possible to select less features and still get a good classification.\n"}
{"snippet": "from scipy.spatial.distance import cdist\nfrom operator import add\ndef model_error_1(data, model):\n    bModel = sc.broadcast(model)\n    return data.map(lambda x: np.linalg.norm(x - bModel.value.cluster_centers_[bModel.value.predict(x)])**2).reduce(add)\n", "intent": "```python\nfrom scipy.spatial.distance import cdist\ndef model_error_1(data, model):\n    ...\n    return data....\n```\n"}
{"snippet": "labels = data.map(lambda x: bestModel.predict(x))\nimgLabels = labels.toarray().reshape(2, 76, 87)\ndraw_image(imgLabels[0, :, :])\n", "intent": "```python\nlabels = bestModel.predict(...)\nimgLabels = labels...\ndraw_image(imgLabels[:,:,0])\n```\n"}
{"snippet": "indices_to_project = [t[0] for t in sorted(enumerate(vars_), key=lambda x: x[1])[-3:]] \ndata_to_project = data.randomSplit([1, 99])[0]\nrdd0 = data_to_project.filter(lambda point: clusters.predict(point)==0)\nrdd1 = data_to_project.filter(lambda point: clusters.predict(point)==1)\n", "intent": "We will select only these 3 features to do the plot, and on 1% of the whole dataset.\n"}
{"snippet": "from scipy.spatial.distance import cdist\nfrom operator import add\ndef model_error_1(data, model):\n    bModel = sc.broadcast(model)\n    return data.map(lambda x: np.linalg.norm(x - bModel.value.centers[bModel.value.predict(x)])**2).reduce(add)\n", "intent": "```python\nfrom scipy.spatial.distance import cdist\ndef model_error_1(data, model):\n    ...\n    return data....\n```\n"}
{"snippet": "X_train_array = fe.transform(X_train)\ny_pred = clf.predict_proba(X_train_array)\nground_truth = problem.Predictions(y_true=y_train)\nprediction = problem.Predictions(y_pred=y_pred)\n", "intent": "Transform the _whole_ (training + validation) dataframe into a numpy array and compute the prediction.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy for Resnet50 : %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "if best_kernel_model_gs.best_score_ > best_linear_model_gs.best_score_:\n    best_model = best_kernel_model\nelse:\n    best_model = best_linear_model\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...?\n"}
{"snippet": "results = []\nnames = []\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, x_train, y_train.values.ravel(), cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n", "intent": "We now have 6 models and accuracy estimations for each. We need to compare the models to each other and select the most accurate.\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "New observations are called \"out-of-sample\" data\nUses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": "Proportion of correct predictions\nCommon evaluation metric for classification problems\n"}
{"snippet": "tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)\ntree_rmse_scores = np.sqrt(-tree_scores) \n", "intent": "DecisionTreeRegressor\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores) \ndisplay_scores(lin_rmse_scores)\n", "intent": "LinearRegression model\n"}
{"snippet": "X_test_dtm = vect.transform(X_test)\ny_pred_class = log_reg_model.predict(X_test_dtm)\ny_pred_prob = log_reg_model.predict_proba(X_test_dtm)[:, 1]\n", "intent": "Which of the following ranges contains the accuracy of the selected_words_model on the test_data?\n"}
{"snippet": "model_1_test = house_test[model_1_input]\nmodel_2_test = house_test[model_2_input]\nmodel_3_test = house_test[model_3_input]\ntest_target = house_test['price']\nmodel_1_predict_on_test = lin_reg_model_1.predict(model_1_test)\nmodel_2_predict_on_test = lin_reg_model_2.predict(model_2_test)\nmodel_3_predict_on_test = lin_reg_model_3.predict(model_3_test)\nprint(\"Model 1 RSS on TRAINING Data: \", sqrt(mean_squared_error(test_target, model_1_predict_on_test)))\nprint(\"Model 2 RSS on TRAINING Data: \", sqrt(mean_squared_error(test_target, model_2_predict_on_test)))\nprint(\"Model 3 RSS on TRAINING Data: \", sqrt(mean_squared_error(test_target, model_3_predict_on_test)))\n", "intent": "Which model (1, 2 or 3) has lowest RSS on TESTING Data?\n"}
{"snippet": "from sklearn import metrics\ny_predict = sentiment_model.predict(test_matrix)\nprint(\"Accuracy score: \", metrics.accuracy_score(y_predict, test_data['sentiment']))\n", "intent": "What is the accuracy of the sentiment_model on the test_data? Round your answer to 2 decimal places (e.g. 0.76).\n"}
{"snippet": "y_predict_sentiment_model_test = sentiment_model.predict(test_matrix)\ny_predict_simple_model_test = simple_model.predict(test_matrix_word_subset)\nprint(\"Accuracy score for sentiment_model on the test data: \", metrics.accuracy_score(y_predict_sentiment_model_test, test_data['sentiment']))\nprint(\"Accuracy score for simple model on the test data: \", metrics.accuracy_score(y_predict_simple_model_test, test_data['sentiment']))\n", "intent": "Which model (sentiment_model or simple_model) has higher accuracy on the TEST set?\n"}
{"snippet": "print(\"Accuracy score for majority class classifier model on the test data: \", metrics.accuracy_score(y_predict_majority_class, test_data['sentiment']))\n", "intent": "Enter the accuracy of the majority class classifier model on the test_data. Round your answer to two decimal places (e.g. 0.76).\n"}
{"snippet": "sample_validation_predict_proba = decision_tree_model.predict_proba(sample_validation_data[features])\nsample_validation_predict_proba\n", "intent": "Which loan has the highest probability of being classified as a safe loan?\n"}
{"snippet": "validation_predict_big_model = big_model.predict(validation_data[features])\naccuracy_big_model = list(validation_predict_big_model == validation_data[target]).count(True) / len(validation_data[target]) * 1.0\nprint(\"the accuracy of the big_model on the entire validation_data: \", accuracy_big_model)\n", "intent": "How does the performance of big_model on the validation set compare to decision_tree_model on the validation set? Is this a sign of overfitting?\n"}
{"snippet": "evaluate_classification_error(my_decision_tree, test_data)\n", "intent": "Rounded to 2nd decimal point (e.g. 0.76), what is the classification error of my_decision_tree on the test_data?\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_new, validation_data)\n", "intent": "Now, let's use this function to evaluate the classification error of my_decision_tree_new on the validation_set. Your code should be analogous to\n"}
{"snippet": "evaluate_classification_error(my_decision_tree_old, validation_data)\n", "intent": "Now, evaluate the validation error using my_decision_tree_old.\n"}
{"snippet": "print(\"Validation data, classification error (model 1):\", evaluate_classification_error(model_1, validation_data))\nprint(\"Validation data, classification error (model 2):\", evaluate_classification_error(model_2, validation_data))\nprint(\"Validation data, classification error (model 3):\", evaluate_classification_error(model_3, validation_data))\n", "intent": "Which tree has the smallest error on the validation data?\n"}
{"snippet": "print(\"Validation data, classification error (model 4):\", evaluate_classification_error(model_4, validation_data))\nprint(\"Validation data, classification error (model 5):\", evaluate_classification_error(model_5, validation_data))\nprint(\"Validation data, classification error (model 6):\", evaluate_classification_error(model_6, validation_data))\n", "intent": "Calculate the accuracy of each model (model_4, model_5, or model_6) on the validation set. Your code should be analogous to\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncmat = confusion_matrix(y_true=test_data['sentiment'],\n                        y_pred=model.predict(test_matrix),\n                        labels=model.classes_)    \nprint(' target_label | predicted_label | count ')\nprint('--------------+-----------------+-------')\nfor i, target_label in enumerate(model.classes_):\n    for j, predicted_label in enumerate(model.classes_):\n        print('{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j]))\n", "intent": "The accuracy, while convenient, does not tell the whole story. For a fuller picture, we turn to the confusion matrix. \n"}
{"snippet": "print('linear r^2 score : {:.4f}'.format(linear_r.score(test_x, test_y)))\nprint('linear rmse score : {:.4f}'.format(metrics.mean_squared_error(test_y, linear_r.predict(test_x))))\n", "intent": "<br>\n<b>Model 1:</b> Linear Regression\n"}
{"snippet": "print('svr r^2 score : {:.4f}'.format(svr.score(test_x, test_y)))\nprint('svr rmse score : {:.4f}'.format(metrics.mean_squared_error(test_y, svr.predict(test_x))))\n", "intent": "<br>\n<b>Model 3:</b> Support Vector Machine\n"}
{"snippet": "print('knn r^2 score : {:.4f}'.format(knn_regression.score(test_x, test_y)))\nprint('knn rmse score : {:.4f}'.format(metrics.mean_squared_error(test_y, knn_regression.predict(test_x))))\n", "intent": "<br>\n<b>Model 4:</b> K Nearest Neighbor\n"}
{"snippet": "plot_results(history, rnn, x_train, y_train, x_test, y_test)\nscores = rnn.evaluate(x_test, y_test, verbose=0)\nprint('%s: %.2f%%' % (rnn.metrics_names[1], scores[1]))\n", "intent": "<br>\n<b>Model 5:</b> Recurrent Neural Network\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "The results from the test dataset is not better than the previous result. This might be the fact that we didn't test out many min_df for the model. \n"}
{"snippet": "predictions = mlp.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\n", "intent": "Now we can use the sklearn built-in metrics such as confusion matrix and classification report to measure how well our model can predict!\n"}
{"snippet": "y_pred2 = mlp.predict(X_test)\n", "intent": "Now we can use the sklearn built-in metrics such as confusion matrix and classification report to measure how well our model can predict!\n"}
{"snippet": "print(regr.predict(7.5)) \nprint(regr.predict(np.array([[30],[60]])))\n", "intent": "Now we can actually predict profit values by supplying our own population of city parameter!\n"}
{"snippet": "pred_labels = regr.predict(X_test)\nverify_dataset(test_dataset, pred_labels)\n", "intent": "The above value is the mean accuracy. So far, we've managed to classify 50% correctly!\n"}
{"snippet": "print_score(gbc_clf, X_train, y_train, X_test, y_test, train=True) \nprint_score(gbc_clf, X_train, y_train, X_test, y_test, train=False) \n", "intent": "print(grid_search.best_score_)\ngrid_search.best_estimator_.get_params()\n"}
{"snippet": "from sklearn.metrics import classification_report\ncls_rep = classification_report(y_test, y_pred)\nprint cls_rep\n", "intent": "Calculating classification report for the logistic regression (discussed in detail in the final report at the end of this notebook).\n"}
{"snippet": "C = confusion_matrix(y_test, gsdt.predict(X_test))\nshow_confusion_matrix(C, ['Class 0 = Bank Survived', 'Class 1 = Bank Failed'])\n", "intent": "Graphical representation of Confusion Matrix for Decision tree matrix\n"}
{"snippet": "X_=pd.Series(\"coding\")\ny_pred_t = model_con.predict_proba(X_)\ny_pred = model_con.predict(X_)\nprint(y_pred_t)\nprint(y_pred)\n", "intent": "Software Development Manager : 1\nSoftware Development Engineer : 2\nSenior Software Development Engineer : 95\n"}
{"snippet": "print('MSE:', metrics.mean_squared_error(te['IND_BOM_1_1'], rProba))\nprint('KS Test:', ksTest(te['IND_BOM_1_1'], rProba)[0])\nprint('ROC AUC:', metrics.roc_auc_score(te['IND_BOM_1_1'], rProba))\nprint('Accuracy:', metrics.accuracy_score(te['IND_BOM_1_1'], rClass))\nprint('Precision, Recall and FScore:')\nprint(metrics.precision_recall_fscore_support(te['IND_BOM_1_1'], rClass, average='binary')[:-1])\nprint('Confusion Matrix:')\nprint(metrics.confusion_matrix(te['IND_BOM_1_1'], rClass))\n", "intent": "Testing the statistical model\n"}
{"snippet": "cv_scores = cross_val_score(reg, X, y, cv=5)\n", "intent": "Compute 5-fold cross-validation scores: cv_scores\n"}
{"snippet": "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n", "intent": "Compute predicted probabilities: y_pred_prob\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\n", "intent": "Predict the labels of the test set:\n"}
{"snippet": "noised_test_y = test_y + np.random.normal()\nprint(mse(model.predict(train_X), train_y))\nprint(mse(model.predict(test_X), noised_test_y))\n", "intent": "By adding noise to the test set y, we're making a larger difference between the train set and the test set which cause a larger MSE.\n"}
{"snippet": "k_fold = 5\ncross_val_score(six_features_model, X, y, cv=k_fold)\n", "intent": "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat.\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Now we can use it to detect images of the number 5:\n"}
{"snippet": "precision_score(y_train_5, y_train_pred_90)\n", "intent": "Let's check these predictions precision and recall:\n"}
{"snippet": "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\nprecision_score(y_train_5, y_train_pred_forest)\n", "intent": "Try measuring the precision and recall scores (below): you should find 98.2% precision and 81.8% recall. Not\ntoo bad!\n"}
{"snippet": "print(history.history.keys())\nscore = model.evaluate(x_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1]) \nscore = model.evaluate(x_test, y_test)\nprint(\"\\n Testing Accuracy:\", score[1]) \n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "training_input_fn = estimator.inputs.numpy_input_fn(x={'x': mnist.train.images}, \n                                                  y=mnist.train.labels.astype(np.int32), \n                                                  shuffle=True,\n                                                  batch_size=150, \n                                                  num_epochs=5)\ndeep_model.train(input_fn=training_input_fn, steps=1000)\ninput_fn_eval = estimator.inputs.numpy_input_fn(x={'x': mnist.test.images}, shuffle=False)\npreds_mnist = list(deep_model.predict(input_fn=input_fn_eval))\npredictions_mnist = [p['class_ids'][0] for p in preds_mnist]\n", "intent": "Apply DNNClassifier to bare MNIST data wihtout filtering\n"}
{"snippet": "predSalary = regressor.predict(8) \nprint(\"For experience of 8 years: \"\"%.2f\" %predSalary,\"lakhs per annum\")\n", "intent": "<br>Since the model is trained, we can use it to predict results for us\n"}
{"snippet": "from math import sqrt\npredY = regressor.predict(X_test)\ndiff = []\nfor i in range(len(X_test)):\n    diff.append((Y_test[i] - predY[i])**2)\nprint(sqrt(sum(diff)/len(X_test))*100,\"%\")\n", "intent": "Let's check model performance using MAPE\n"}
{"snippet": "clf.predict_proba([[2., 2.], [-1., -2.]])\n", "intent": "Vector of probability estimates $P(y|x)$ per sample $x$:\n"}
{"snippet": "print(classification_report(label_train, log_reg_pages.predict(feat_train_pages)))\n", "intent": "Again, we see the 97% precision at 100% recall for the naive classifier.\n"}
{"snippet": "df['Demanda_uni_equil'][df.Semana == 11] = gbm.predict(df[features][df.Semana ==11])\n", "intent": "Do I have weeks with demand = 0 ? \n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"sf.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "EXAMPLES = ['17th of October 2000', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "variables = tf.trainable_variables()\nregularizer = tf.add_n([ tf.nn.l2_loss(v) for v in variables if 'b' not in v.name ])\nloss = tf.reduce_mean(tf.add(cross_entropy, tf.multiply(regularizer,beta)))\ntf.summary.histogram('loss', loss)\noptimiser = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n", "intent": "Insert L2 regularization as mentioned in the paper. Select all weight variables and exclude bias variables.\n"}
{"snippet": "kfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n", "intent": "The final step is to evaluate this baseline model. We will use 10-fold cross validation to evaluate the model.\n"}
{"snippet": "test_ili_predict = np.exp(flu_trend1.predict(queries_test))\nflu_test['ILI_predict'] = test_ili_predict\n", "intent": "which is not very good. Now, let us search predict the values and attach them to the test dataframe\n"}
{"snippet": "edx_min_index = edx_reg_mod.predict(X=edx_descriptors).argmin()\nstatedata.loc[edx_min_index]['state.name']\n", "intent": "Predict the country of least life expectancy\n"}
{"snippet": "edx_max_index = edx_reg_mod.predict(X=edx_descriptors).argmax()\nstatedata.loc[edx_max_index]['state.name']\n", "intent": "State with highest life expectancy\n"}
{"snippet": "edx_predictions = edx_reg_mod.predict(X=edx_descriptors)[:, 0]\ntwo_largest_indexes = np.argpartition(edx_predictions, -2)[-2:]\nstatedata.loc[two_largest_indexes]['state.name']\n", "intent": "As an exercise in computational efficiency, let us find the **two** countries predicted to have highest life expectancy.\n"}
{"snippet": "y_baseline_test, X_baseline_test = patsy.dmatrices('notFullyPaid ~ intRate', loans_test, return_type='dataframe')\npred_baseline_mod = baseline_mod.predict(exog=X_baseline_test)\n", "intent": "Make test set predictions for the bivariate model. What is the highest predicted probability of a loan not being paid in full on the testing set?\n"}
{"snippet": "y_test, X_test = patsy.dmatrices('violator ~ male + race + age + C(state) + '\n                       'timeServed + maxSentence + multipleOffenses + '\n                       'C(crime)', parole_test, return_type='dataframe')\nprecitions_mod1 = mod1.predict(exog=X_test)\nprecitions_mod1.max()\n", "intent": "Predict probabilities for parolees in the testing set. What is the maximal?\n"}
{"snippet": "m3_predict = m3.predict(songs_test.drop(['Top10', 'energy'], axis=1))\n", "intent": "What is the accuracy of Model 3 on the test set, using a threshold of 0.45?\n"}
{"snippet": "y_predict = dt_model.predict(x_test)\n", "intent": "Now, it's time to predict the testing target mapping to testing feature(s).\n"}
{"snippet": "inputs = [iris_inputs[0], iris_inputs[start_class_one], iris_inputs[start_class_two]]\nprint('Class predictions: {0}'.format(model.predict(inputs))) \nprint('Probabilities:\\n{0}'.format(model.predict_proba(inputs))) \n", "intent": "Now we can make some predictions using the trained model. We'll pull out some examples from our training data and see what the model says about them.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nprint(\"Mean absolute error = \" + str(mean_absolute_error(true, predicted)))\n", "intent": "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. \n"}
{"snippet": "cv_predict = cross_val_predict(m, X, y_int[:,0])\n", "intent": "OK, so the logistic regression baseline is basically equivalent to predicting the majority class. *Indeed, observe the predictions:*\n"}
{"snippet": "results = test.map(lambda lp: (lp.label, float(model.predict(lp.features))))\nprint (results.take(10))\ntype(results)                       \n", "intent": "Run it on the test data\n"}
{"snippet": "accuracy = accuracy_score(y_test, best_rfc.predict(X_test))\nprint \"Accuracy: \", accuracy\n", "intent": "Now let us find the accuracy\n"}
{"snippet": "print classification_report(y_test, best_rfc.predict(X_test))\n", "intent": "Precision\nPrecision is also called positive predictive value.\n"}
{"snippet": "roc = roc_auc_score(y_test, best_rfc.predict_proba(X_test)[:,1])\nprint \"AUC Score: \", roc\n", "intent": "AUC: Area Under the Curve\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n", "intent": "Part 1: Implement K-Fold Cross Validation, with 10 folds, on your Breast Cancer Model\n"}
{"snippet": "model.evaluate(p_x_test,p_y_test)\n", "intent": "** 3-1. Evaluate model on test data using model.evaluate. Print the model accuracy on test set. **\n"}
{"snippet": "scores = model.evaluate(x_test,y_test)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "** 4. Evaluate the model on Test data. **\n"}
{"snippet": "y_predicted = model.predict(X)\n", "intent": "Lets us see how well the model is predicting.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"myGiraffe.jpg\")\n", "intent": "In the above picture, the persons, umbrella and potted plants are detected. The car wasn't detected though.\n"}
{"snippet": "regression.predict(60)\n", "intent": "Now that we have fit our prediction model to our data lets do cool stuff to it.\n"}
{"snippet": "regr1.predict(80)[0]\n", "intent": "This is how we can predict specific values on our model:\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nmean_square_error = np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2)\nprint(\"Residual sum of squares: {}\".format(mean_square_error))\nprint('Variance score: {}'.format(regr.score(diabetes_X_test, diabetes_y_test)))\n", "intent": "Lets print some data about our Regression line.\n"}
{"snippet": "print(\"Inputed X: \", diabetes_X_test)\nprint(\"Predicted Y: \", regr.predict(diabetes_X_test))\n", "intent": "If we wanted to take a look at our X test inputs and their respective predictions we could do this:\n"}
{"snippet": "regr1.predict(80)\n", "intent": "This is how we can predict specific values on our model:\n"}
{"snippet": "from sklearn.externals import joblib\nbest_model_one_week_later = joblib.load(\"my_model.pkl\") \nbest_model_one_week_later.predict(X_test)\n", "intent": "You can save the final model in order to reuse it later. \n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Remark: we can observe all default parameters. We will explain them further on.  \n"}
{"snippet": "print(tree_clf.predict([[5, 1.5]]))\nprint(tree_clf.predict_proba([[5, 1.5]]))\n", "intent": "Suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. Let's predict its specie:\n"}
{"snippet": "X_dev, y_dev = load2d(full_train_dev = 'Dev')\nfor key in subgroup.keys():\n    subgroup[key]['y_dev_pred'] = subgroup[key]['model'].predict(X_dev)\ny_dev_pred = np.hstack((subgroup[key]['y_dev_pred'] for key in subgroup.keys()))\nscore = mean_squared_error(y_dev,y_dev_pred)\nprint(\"RMSE: \",np.sqrt(score) * 48)\n", "intent": "We then predict and calculate the score for this model:\n"}
{"snippet": "clf.sigma_ = clf.sigma_/100000 \nprint(\"The accuracy of GaussainNB after dividing its sigma by 100,000 (arrived at empirically) is\", \n      accuracy_score(y_true = dev_labels, y_pred = clf.predict(dev_data)))\n", "intent": "One thing that might help is to reduce the variation of the gaussian model. This can be done by using a very small sigma value.\n"}
{"snippet": "print('accuracy for logistic regression - version 1: {0:.2f}'.format(accuracy_score(y_test, model_lr_1.predict(X_test))))\nprint('accuracy for logistic regression - version 1:\\n {0}'.format(confusion_matrix(y_test, model_lr_1.predict(X_test))))\nprint('precision for logistic regression - version 1: {0:.2f}'.format(precision_score(y_test, model_lr_1.predict(X_test))))\nprint('recall for logistic regression - version 1: {0:.2f}'.format(recall_score(y_test, model_lr_1.predict(X_test))))\n", "intent": "The accuracy score for this model is 83%, which is better than the baseline model of 0.62%. Hooray\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature,axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_predicted = MLP.predict(X_test)\nprint(y_predicted)\nprint(y_test)\naccuracy = sum(y_test == y_predicted)/float(len(y_test))\nprint(accuracy)\n", "intent": "We then test our test data set by using the model to predict the correct digit. We compare this against the ground truth.\n"}
{"snippet": "MSE = (np.sum((bos.PRICE - lm.predict(X)) ** 2)) / len(bos)\nMSE\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error as mse\nmse(bos.PRICE, pred1)\nnp.sum((bos.PRICE - lm.predict(X)) ** 2)/len(X)\nnp.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_reg_train_pred = lm_reg.predict(X)\ny_cas_train_pred = lm_cas.predict(X)\ny_train_pred = np.expm1(y_reg_train_pred) + np.expm1(y_cas_train_pred)\ny_train_pred = np.log1p(y_train_pred)\nprint \"The RMSLE score on the training set was {:.2f}\".format(rmsle(y_train_pred, train['count']))\nprint \"This is compared to 0.65 on the test set\"\n", "intent": "The linear model scored 0.64739 on the Kaggle Public Scoreboard. \n"}
{"snippet": "y_pred = tree.predict(X_test).astype(int)\ntree_pred = open(\"RandomForestClassifierPred.csv\", \"wb\")\nopen_file_object = csv.writer(tree_pred)\nopen_file_object.writerow([\"PassengerId\",\"Survived\"])\nopen_file_object.writerows(zip(test_ids, y_pred))\ntree_pred.close()\n", "intent": "Public Kaggle score of 0.76555. \n"}
{"snippet": "y_pred = tree_opt.predict(X_test).astype(int)\ntree_opt_pred = open(\"RandomForestClassifierOptPred.csv\", \"wb\")\nopen_file_object = csv.writer(tree_opt_pred)\nopen_file_object.writerow([\"PassengerId\",\"Survived\"])\nopen_file_object.writerows(zip(test_ids, y_pred))\ntree_opt_pred.close()\n", "intent": "Public Kaggle score of 0.75598. \n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(logreg, X_titanic, y_titanic, cv=5)\nprint('Accuracy of logistic regression classifier with cross validation: {:.4f}'.format(scores.mean()))\n", "intent": "82% is correct but we can do better! Perhaps change kernel approximation\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(logreg_m, X_titanic, y_titanic, cv=5)\nprint('Accuracy of logistic regression classifier with cross validation: {:.4f}'.format(scores.mean()))\n", "intent": "80% is correct but we can do better! Perhaps change kernel approximation\n"}
{"snippet": "predictions = list()\nfor i in range(len(X_test)):\n    X= X_test[i,:]\n    X = X.reshape(1,3,1)\n    y = model.predict(X, 1)\n    y=y*(max_y-min_y)+min_y\n    predictions.append(y)\nprint (len(predictions))\n", "intent": "Next we make predictions, one sample at a time and rescale our prediction and add it to our list of predictions\n"}
{"snippet": "from sklearn import metrics\nmetrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "We can use the metrics module from scikit-learn to calculate the Silhouette Coefficient for our fitted K-Means estimator.\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\ndef rmse_cv(model,data):\n    rmse= np.sqrt(-cross_val_score(model, data, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)\n", "intent": "we also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning par\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "__Classification accuracy:__ percentage of correct predictions\n"}
{"snippet": "ada_predict = ada_1.predict(test_data)\ntest_labels_np = np.array(test_labels['count'])\nada_predict_int = (np.array(ada_predict)).astype('int')\nada_performance =  rmsle(test_labels_np,ada_predict_int)\nprint('RMSLE for adaboost',ada_performance)\n", "intent": "Now we test its accuracy on the test data:\n"}
{"snippet": "forest_6_predict_reg = forest_6_reg.predict(test_data_v4)\nforest_6_predict_cas = forest_6_cas.predict(test_data_v4)\nforest_6_predict = forest_6_predict_reg + forest_6_predict_cas\nforest_6_predict_int  = (np.array(forest_6_predict)).astype('int')\n", "intent": "**Predicting total** by summing predictions of individual models\n"}
{"snippet": "y_hat = optimalenet.predict(Xsub_test)\nresiduals = y_test - y_hat\n", "intent": "Some of these issues may be due to violations of the assumptions of linear regression modelling.\n"}
{"snippet": "homogeneity = homogeneity_score(y, k3_labels)\ncompleteness = completeness_score(y, k3_labels)\nvmeasure = v_measure_score(y, k3_labels)\nprint(homogeneity)\nprint(completeness)\nprint(vmeasure)\n", "intent": "3 clusters seems the best, which is also what makes most sense logically as this is the number of job categories we assigned \nearlier.\n"}
{"snippet": "lm.predict(60)\nlm.predict(80)\n", "intent": "or use lm.predict function\n"}
{"snippet": "trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n", "intent": "We can estimate the performance of the model on the test dataset once the model is fit.\n"}
{"snippet": "from sklearn.metrics import r2_score\ny_true = y_test.values\nr2_score(y_true,final_predictions)\n", "intent": "With the above error from the grid search, it is still not the lowest error we could have gotten. This can be adjusted with the hyperparameters.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=tf.constant(0.0)\n    for i in range(len(style_layers)):\n        style_loss+=style_weights[i]*tf.reduce_sum((gram_matrix(feats[style_layers[i]])-style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_raw_pred = model.predict(image)\ny_raw_pred.shape\n", "intent": "The last cell provides us the image pixels as a `numpy.array`. We then can try to predict its labels.\n"}
{"snippet": "y_raw_pred = model.predict(image)\npredicted_labels = np.argmax(y_raw_pred, axis=3)\npredicted_image = np.zeros(shape=np.append(predicted_labels.shape, 3), dtype=np.int8)\nfor i in range(nb_labels):\n    predicted_image[predicted_labels == i] = train_config[\"labels\"][i][\"color\"]\n", "intent": "Now it should be better. We first rebuild the output image, as previously.\n"}
{"snippet": "y_raw_pred = model.predict(image)\npredicted_labels = np.argmax(y_raw_pred, axis=3)\npredicted_image = np.zeros(shape=np.append(predicted_labels.shape, 3), dtype=np.int8)\nfor i in range(nb_labels):\n    predicted_image[predicted_labels == i] = train_config[\"labels\"][i][\"color\"]\n", "intent": "We can recompute the output image, and display it, as previously:\n"}
{"snippet": "if not os.path.isdir(save_directory):\n    os.makedirs(save_directory)\nmodel_path = os.path.join(save_directory, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n", "intent": "NOTE: Executing the above will train the model and will take some time. A few hours with a decent laptop via Jupyter\n"}
{"snippet": "def get_perf(model, X, y, thresh = 0.5):\n    probs = model.predict_proba(X)\n    preds =  [1 if p >= thresh else 0 for p in probs[:, 1]]\n    return (preds == y).sum() / len(y), probs, preds\n", "intent": "A helper function to return accuracy and predictions that allows us to vary the prediction threshold.\n"}
{"snippet": "scores = cross_val_score(rfc, X, y, cv=20, scoring='f1_weighted', n_jobs=4)\nprint(scores)\n", "intent": "To have an idea of the performance of the model, we can run cross validation (20-fold) on it :\n"}
{"snippet": "silhouette_score(X, km.labels_)\n", "intent": "Let's compute the silhouette score :\n"}
{"snippet": "predict_y_scaled_cnn = cnn_model.predict(predict_x_cnn, verbose=1)\npredict_y_scaled_dense = dense_model.predict(predict_x_dense, verbose=1)\n", "intent": "Below we'll perform predictions using the fitted models to confirm that they are properly setup for this operation.\n"}
{"snippet": "y_pred = regressor.predict(x_test)\n", "intent": "3.10 Predicting the test results\n"}
{"snippet": "lin_reg.predict(6.5)\n", "intent": "Predicting a new result with Linear Regression\n"}
{"snippet": "y_pred = regressor.predict(sc_x.transform([6.5]))\n", "intent": "Since we used feature scaling on X, we need to apply the same transformation to the single 6.5 value for prediction\n"}
{"snippet": "y_pred = regressor.predict(6.5)\n", "intent": "6.6 Predicting a new result\n"}
{"snippet": "y_pred = regressor.predict(6.5)\n", "intent": "7.6 Predicting a new result\n"}
{"snippet": "y_means = kmeans.fit_predict(X)\n", "intent": "We create a vector with each point assinged to a cluster\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(log_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Cross Validation Score**\n"}
{"snippet": "y_test_pred = log_clf.predict(X_test)\ny_test_pred\n", "intent": "**Plot Confusion Matrix against Test data**\n"}
{"snippet": "cross_val_score(log_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Scross Validation Score**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(svc_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Cross Validation Score**\n"}
{"snippet": "y_test_pred = svc_clf.predict(X_test)\ny_test_pred\n", "intent": "**Plot Confusion Matrix against Test data**\n"}
{"snippet": "cross_val_score(svc_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Scross Validation Score**\n"}
{"snippet": "y_test_pred = dt_clf.predict(X_test)\ny_test_pred\n", "intent": "**Plot Confusion Matrix against Test data**\n"}
{"snippet": "y_test_pred = knn_clf.predict(X_test)\ny_test_pred\n", "intent": "**Plot Confusion Matrix against Test data**\n"}
{"snippet": "cross_val_score(knn_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Scross Validation Score**\n"}
{"snippet": "cross_val_score(log_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Scross Validation Score**\n"}
{"snippet": "y_test_pred = clf.predict(X_test)\ny_test_pred\n", "intent": "**Plot Confusion Matrix against Test data**\n"}
{"snippet": "y_test_pred = rf_clf.predict(X_test)\ny_test_pred\n", "intent": "**Plot Confusion Matrix against Test data**\n"}
{"snippet": "cross_val_score(svc_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Cross Validation Score**\n"}
{"snippet": "cross_val_score(svc_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Calculate Cross-Validation Score **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This is not a better classifier. It performed better on the training set, but worse on the test data. \n"}
{"snippet": "y_pred = clf.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_pred, rownames=['True'], colnames=['Predicted'], margins=True, dropna = False)\n", "intent": "Evaluate the performance on the validation set\n"}
{"snippet": "y_pred = clf.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True, dropna = False)\n", "intent": "evaluate the performance on the test set\n"}
{"snippet": "scores = cross_val_score(clf, X_train_plus_valid, y_train_plus_valid, cv=10)\nprint('Score',scores)\nprint('Mean' , scores.mean())\nprint('Standard Deviation',scores.std())\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "y_pred = my_tuned_model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the performance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, y_pred) \n", "intent": "<b>R-squared of Linear Regression<b>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n", "intent": "<b>MSE of Linear Regression<b>\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n", "intent": "<b>MAE of Linear Regression<b>\n"}
{"snippet": "model_name=[]\nmse=[]\nr2=[]\nmae=[]\nmodel_name.append(\"Backward/MLR\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, y_pred) \n", "intent": "<b>R-squared of SVR<b>\n"}
{"snippet": "model_name.append(\"Backward/SVR\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, y_pred) \n", "intent": "<b>R-squared of Decision Tree<b>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n", "intent": "<b>MSE of Decision Tree<b>\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n", "intent": "<b>MAE of Decision Tree<b>\n"}
{"snippet": "model_name.append(\"Backward/Decision Tree\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, y_pred) \n", "intent": "<b>R-squared of Decision Forest<b>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n", "intent": "<b>MSE of Decision Forest<b>\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n", "intent": "<b>MAE of Decision Forest<b>\n"}
{"snippet": "model_name.append(\"Backward/Random Forest\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/MLR\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/SVR\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, y_pred) \n", "intent": "<b>R-squared of Decision tree<b>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred)\n", "intent": "<b>MSE of Decision tree<b>\n"}
{"snippet": "model_name.append(\"Forward/Decision Tree\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "model_name.append(\"Forward/Random Forest\")\nmae.append(mean_absolute_error(y_test, y_pred))\nr2.append(r2_score(y_test, y_pred))\nmse.append(mean_squared_error(y_test, y_pred))\n", "intent": "<b>Adding results to a table for summarization in the end</b>\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg,housing_prepared,housing_labels,\n                             scoring = \"neg_mean_squared_error\",cv=10) \nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "It is worse than the Linear Regression model! \n"}
{"snippet": "y_pred_prob = lr.predict_proba(X_test)[:,1]\n", "intent": "Hint: Generate prediction probabilities (y_pred_prob) using predict_proba() function.\n"}
{"snippet": "print(np.sum((bos.PRICE-lm.predict(X))**2)/len(bos.PRICE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "netout = predict(model, \"COCO_train2014_000000012138.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "from sklearn.mixture import GMM\ngmm =  GMM(n_components=3)\ny_gmm = gmm.fit_predict(good_data_pca)\ncenters = gmm.means_\nvisualize_clusters(good_data_pca, y_gmm, centers, \"GMM\")\n", "intent": "[[ go back to the top ]](\n"}
{"snippet": "train_X_pred = lr.predict(train_X)\ntest_X_pred = lr.predict(test_X)\n", "intent": "wow..this is just totally uninterpretable!\n"}
{"snippet": "af_fit_lm = af_lm.predict(af_X_train)\nsf_fit_lm = sf_lm.predict(sf_X_train)\naf_fit_test_lm = af_lm.predict(af_X_test)\nsf_fit_test_lm = sf_lm.predict(sf_X_test)\n", "intent": "R2, F - Tests, Adjusted R2, AIC, BIC \n"}
{"snippet": "af_fit_elastic_net = af_elastic_net.predict(af_X_train)\nsf_fit_elastic_net = sf_elastic_net.predict(sf_X_train)\naf_fit_test_elastic_net = af_elastic_net.predict(af_X_test)\nsf_fit_test_elastic_net = sf_elastic_net.predict(sf_X_test)\n", "intent": "R2, F - Tests, Adjusted R2, AIC, BIC \n"}
{"snippet": "af_fit_lasso = af_lasso.predict(af_X_train)\nsf_fit_lasso = sf_lasso.predict(sf_X_train)\naf_fit_test_lasso = af_lasso.predict(af_X_test)\nsf_fit_test_lasso = sf_lasso.predict(sf_X_test)\n", "intent": "R2, F - Tests, Adjusted R2, AIC, BIC \n"}
{"snippet": "af_fit_ridge = af_ridge.predict(af_X_train)\nsf_fit_ridge = sf_ridge.predict(sf_X_train)\naf_fit_test_ridge = af_ridge.predict(af_X_test)\nsf_fit_test_ridge = sf_ridge.predict(sf_X_test)\n", "intent": "R2, F - Tests, Adjusted R2, AIC, BIC \n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nprint mean_absolute_error(real_rating, predicted_rating)\n", "intent": "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. \n"}
{"snippet": "from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('Jaccard score : ', jaccard_similarity_score(y_test, y_hat_SVM))\nprint(classification_report(y_test, y_hat_SVM))\n", "intent": "The model shows a very high accurace based on the evaluation metrices. The Jaccard score is 1.0 and the F-1 score is 1.0, which is a maximum level. \n"}
{"snippet": "print('Jaccard score : ', jaccard_similarity_score(y_test, y_hat_Tree))\nprint(classification_report(y_test, y_hat_Tree))\n", "intent": "The model shows a very high accurace based on the evaluation metrices. The Jaccard score is 1.0 and the F-1 score is 1.0, which is a maximum level. \n"}
{"snippet": "model.evaluate(single_auto.predict(x_val), y_val)\n", "intent": "How well does a classifier after the vanilla autoencoder perform?\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_dog.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = regr.predict(Xts)\nRSS = np.mean((y_pred - yts) ** 2) / (np.std(yts) ** 2)\nprint(\"Normalized RSS is %f.\" % RSS)\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "yhat = logreg.predict(Xs)\nprint(\"Accuracy on training data = %f\" % np.mean(yhat == y))\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "scorer = make_scorer(mean_squared_error, greater_is_better = False)\ndef rmse_cv_train(model):\n    rmse= (np.sqrt(-cross_val_score(model, X_train, Y_train, scoring = scorer))).mean()\n    return(rmse)\ndef rmse_cv_test(model):\n    rmse= (np.sqrt(-cross_val_score(model, X_test, Y_test, scoring = scorer))).mean()\n    return(rmse)\n", "intent": "We will use RMSE to build a predictive regression model for cereal product ratings\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(school['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(school['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nprint(\"Accuracy score: \", accuracy_score(y_test, predictions))\nprint(\"Recall score: \", recall_score(y_test, predictions, average = 'weighted'))\nprint(\"Precision score: \", precision_score(y_test, predictions, average = 'weighted'))\nprint(\"F1 score: \", f1_score(y_test, predictions, average = 'weighted'))\n", "intent": "* This is a multi-class classification. So, for these evaulation scores, explicitly specify `average` = `weighted`\n"}
{"snippet": "a = np.array( [ 5000, 0, 0, 1, 0, 0, 0, 1, 0, 30] )\nmodel.predict_proba(a.reshape(1,-1))\n", "intent": "<h3>Rademacher</h3>\n"}
{"snippet": "a = np.array( [ 5000, 0, 0, 0, 0, 0, 1, 1, 0, 30] )\nmodel.predict_proba(a.reshape(1,-1))\n", "intent": "<h3>Veeravalli</h3>\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))\n", "intent": "**Evaluation metrics for binary classification**\n"}
{"snippet": "print(classification_report(y_test_mc, svm_mc_2_pred))\n", "intent": "Multi-class classsification report\n- reports the average metrics computed for each class\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions=lin_reg.predict(housing_prepared)\nlin_mse=mean_squared_error(housing_labels,housing_predictions)\nlin_rmse=np.sqrt(lin_mse)\nlin_rmse\n", "intent": "**finding error root mean square error**\n"}
{"snippet": "final_model= grid_search.best_estimator_\nx_test=strat_test_set.drop('median_house_value',axis=1)\ny_test=strat_test_set['median_house_value'].copy()\nx_test_prepared=full_pipeline.transform(x_test)\nfinal_predictions=final_model.predict(x_test_prepared)\nfinal_mse=mean_squared_error(y_test,final_predictions)\nfinal_rmse=np.sqrt(final_mse)\n", "intent": "<p><h3>**Final Model** </h3></p>\n>Now its time to evaluate the final model on the test set\n"}
{"snippet": "score = model.evaluate(X_test, Y_test)\nprint()\nprint('Test accuracy: ', score[1])\n", "intent": "    Get cumilative accuracy score\n    Get predictions on every data point in test set\n"}
{"snippet": "college_predictions = tree_reg.predict(college)\ntree_mse = mean_squared_error(college_labels, college_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Now the model is trained, let's evaluate it on the training set.\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\ncollege_predictions = lin_reg.predict(college)\nlin_mse = mean_squared_error(college_labels, college_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Use Scikit-Learn's **mean-squared_error** function to estimate the RMSE the regression model on training set:\n"}
{"snippet": "college_predictions = tree_reg.predict(college)\ntree_mse = mean_squared_error(college_labels, college_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Since the model is trained in the previous section, it is time to evaluate the model on the training set. \n"}
{"snippet": "college_predictions = tree_reg.predict(college)\ntree_mse = mean_squared_error(college_labels, college_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Now the model is trained, let's evaluate it on the training set:\n"}
{"snippet": "college_predictions = tree_reg.predict(college)\ntree_mse = mean_squared_error(college_labels, college_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n", "intent": "Now that the model is trained, let's evaluate it on the training set.\n"}
{"snippet": "y_pred = pois_results.predict(X_test)\n", "intent": "Interestingly, we can not reject the training set of being a poisson distribution.\n"}
{"snippet": "y_hat = dt.predict(X_test_imputed)\nprint 'Accuracy: ', accuracy_score(y_test.as_matrix(), y_hat)\nprint 'F1 score: ', f1_score(y_test.as_matrix(), y_hat)\nprint 'ROC AUC score: ', roc_auc_score(y_test.as_matrix(), y_hat)\n", "intent": "How does our decision tree perform? Compare its predictions on the transformed *test* data, with the actual/real target values.\n"}
{"snippet": "yts_pred=regr.predict(Xts)\nRSS_norm=np.mean((yts_pred-yts)**2)/((np.std(yts))**2)\nprint(RSS_norm)\n", "intent": "Measure and print the normalized RSS on the test data.  \n"}
{"snippet": "def assess_performance(predicted_proba):\n    accuracy = accuracy_score(np.argmax(y_val, axis=1), np.argmax(predicted_proba, axis=1))\n    multiclass_log_loss = log_loss(np.argmax(y_val, axis=1), predicted_proba)\n    print(\"Accuracy of {} on the validation set\".format(accuracy))\n    print(\"Multiclass log loss of {} on the validation set\".format(multiclass_log_loss))\nassess_performance(prediction)\n", "intent": "And we assess the accuracy and the multiclass logloss of our predictions.\n"}
{"snippet": "sklearn.metrics.log_loss(y,y_pred)\n", "intent": "Let's compare it with a log loss calculated with sklearn\n"}
{"snippet": "correct_predictions = np.where((holdout.predict(data.iloc[500:][keywords]) == data.iloc[500:].positive_sentiment), True, False)\npd.Series(correct_predictions).value_counts()\n", "intent": "And running our model on the holdout set gives:\n"}
{"snippet": "correct_predictions = np.where((holdout.predict(data.iloc[:500][keywords]) == data.iloc[:500].positive_sentiment), True, False)\npd.Series(correct_predictions).value_counts()\n", "intent": "A MUCH worse performance than on the training set:\n"}
{"snippet": "correct_predictions = np.where((holdout.predict(data[keywords]) == data.positive_sentiment), True, False)\npd.Series(correct_predictions).value_counts()\n", "intent": "Looks like textbook overfitting! Let's run the holdout model against the entire data set:\n"}
{"snippet": "print('-----grid search end------------')\nprint ('on all train set')\nscores = cross_val_score(grid_search.best_estimator_, X_train, y_train,cv=3,scoring='accuracy')\nprint scores.mean(),scores\nprint ('on test set')\nscores = cross_val_score(grid_search.best_estimator_, X_test, y_test,cv=3,scoring='accuracy')\nprint scores.mean(),scores\n", "intent": "**After grid searching for the best estimator, let's evaluate it further**\n- Accuracy\n- ROC_AUC\n"}
{"snippet": "print('-----grid search end------------')\nprint ('on all train set')\nscores = cross_val_score(grid_search.best_estimator_, X_train, y_train,cv=5,scoring='roc_auc')\nprint scores.mean(),scores\nprint ('on test set')\nscores = cross_val_score(grid_search.best_estimator_, X_test, y_test,cv=5,scoring='roc_auc')\nprint scores.mean(),scores\n", "intent": "** Accuracy for the train test is slightly lower for the train set than the Random Forest Model, but is higher for the test set**\n"}
{"snippet": "print(classification_report(y_train, grid_search.best_estimator_.predict(X_train) ))\nprint('test data')\nprint(classification_report(y_test, grid_search.best_estimator_.predict(X_test) ))\n", "intent": "** ROC_AUC is better for train and test sets compared to the RF Model**\n"}
{"snippet": "y_pred_train= lm.predict(X_train)\ny_pred_test = lm.predict(X_test)\n", "intent": "The predict() function that make predictions on new data. \n"}
{"snippet": "from xgboost import XGBClassifier\nxgb = XGBClassifier()\nscores = cross_val_score(model, X , y , cv=10)\nscores\n", "intent": "**With Default parameters XGBClassifier is giving 0.9690.**\n"}
{"snippet": "mod.predict_proba([\"URGENT! You have built a model - scroll down to see more\"])\n", "intent": "Notice the type is an `sklearn` model, just as we created. \n"}
{"snippet": "y_pred = regr.predict(X_test)\n", "intent": "Predict the 'mid' column corresponding to most probable failure day.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\n", "intent": "Let's evaluate the model.\n"}
{"snippet": "y_pred = model.predict(X_test)\nmse = sklearn.metrics.mean_squared_error(y_test, y_pred)\nprint('MSE: ' + str(mse))\n", "intent": "Check the model quality.\n"}
{"snippet": "y_pred = clf.best_estimator_.predict(X_test, ntree_limit= n_trees)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %.2f%%' % (accuracy * 100.0))\n", "intent": "Display the accuracy of the best parameter combination on the test set.\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: %.2f%%' % (accuracy * 100.0))\n", "intent": "Now, you are ready to evaluate the accuracy of the model trained on the reduced set of features.\n"}
{"snippet": "predicted = model.predict(test_data)\nprint('Evaluation report: \\n\\n%s' % metrics.classification_report(test_labels, predicted))\n", "intent": "Use **test data** to generate an evaluation report to check your **model quality**.\n"}
{"snippet": "guess = rfc.predict(test_cleaned.drop(['default payment next month'], axis = 1)).astype(int)\nsum(guess == test_solution)/guess.size\n", "intent": "Based on my results, Random Forest was the best performer. Let's see how it performed with the actual test set.\n"}
{"snippet": "fruit_prediction = knn.predict([[20, 4.3, 5.5]])\nlookup_fruit_name[fruit_prediction[0]]\n", "intent": "We use the predict function to predict some desired training example\n"}
{"snippet": "y_pred_nb = clf_nb.predict(X_test)\ny_pred_svm = clf_svm.predict(X_test)\n", "intent": "We will now predict using both model on our text data set and we will compare the results\n"}
{"snippet": "f1_nb = metrics.f1_score(y_test, y_pred_nb)\nf1_svm = metrics.f1_score(y_test, y_pred_svm)\n", "intent": "We will now compute the F1 score for both.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    l = []\n    for i in range(len(style_layers)):\n        G = gram_matrix(feats[style_layers[i]])\n        A = style_targets[i]\n        l.append(tf.reduce_sum(tf.square(G-A)) * style_weights[i])\n    return tf.reduce_sum(l)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom scipy.stats import sem\nscores = cross_val_score(pipeline, twenty_train_small.data,\n                         twenty_train_small.target, cv=3, n_jobs=3)\nscores.mean(), sem(scores) \n", "intent": "Such a pipeline can then be cross validated or even grid searched:\n"}
{"snippet": "start = time.time()\ny_pred=[]\nfor i in range(X_test.shape[0]):\n    y_pred.append(categories[np.random.randint(0,24)])\nprint(\"accuracy:\",accuracy_score(y_test,y_pred))\nend=time.time()\nprint(\"Execution time : {} seconds\".format(end-start))\n", "intent": "Baseline model: Assign the categories randomly.\n"}
{"snippet": "Inception_predictions = [np.argmax(inceptionModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\nInception_test_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % Inception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "NewResNet50_predictions = [np.argmax(NewResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(NewResNet50_predictions)==np.argmax(test_targets, axis=1))/len(NewResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred))\n", "intent": "Also known as \"Misclassification Rate\"\n"}
{"snippet": "print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred))\n", "intent": "How \"sensitive\" is the classifier to detecting positive instances?\nAlso known as \"True Positive Rate\" or \"Recall\"\n"}
{"snippet": "print(TP / float(TP + FP))\nprint(metrics.precision_score(y_test, y_pred))\n", "intent": "+ How \"precise\" is the classifier when predicting positive instances?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.0\n    for index in xrange(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[index]])\n        style_loss += content_loss(style_weights[index], gram, style_targets[index])\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = estimator.predict(X_test)\nprint(\"Prediction accuracy: {:.2f}%\".format(np.sum(y_pred == y_test) / len(y_pred) * 100))\n", "intent": "- Validation accuracy\n"}
{"snippet": "estimator.predict([me])\n", "intent": "<img src=\"https://media.tenor.co/images/6f829b9a646004774c9f3ebff45e8b39/raw\" align=left>\n"}
{"snippet": "class Baseline(BaseEstimator, RegressorMixin):\n    def fit(self, X, y):\n        self.value = y.mean()\n        return self\n    def predict(self, X):\n        if not hasattr(self, 'value'):\n            raise ValueError(\"Regressor not trained yet. Use fit method first.\")\n        target_shape = (X.shape[0], )\n        return np.ones(target_shape) * self.value\n", "intent": "---\nIn order to put our models to context we create a baseline model which represents \"coin toss\" decision.\n"}
{"snippet": "def mean_error(T, Y):\n    n = T.shape[0]\n    return np.sum(T * Y[: 1]) / n\nprem_x = np.array([[2,5], [6,2],\n              [1,9], [4,5],\n              [6,3], [7,4], [3, 4]])\nprem_t = np.array([1, 0, 1, 1, 0, 0, 0]).reshape(-1, 1)\nerror_baseline = 0.5\n", "intent": "Now that the models are implemented, we need to run a general test with random parameters to see whether they work good.\n"}
{"snippet": "results = pulse.evaluate(\n    input_fn=tf.estimator.inputs.numpy_input_fn(\n        x={'X': X_test}, y=y_test,\n        num_epochs=1, shuffle=False))\n", "intent": "Evaluate how well the trained network performs on the test data:\n"}
{"snippet": "X_new = np.array([[-1], [0], [+1]])\ny_new = fit_w.predict(X_new)\n", "intent": "A `LinearRegression` fit can apply what is has learned to predict new data:\n"}
{"snippet": "print ('Recall: ', recall_score(Y_test, all_one_predictions))\n", "intent": "What would the recall value be for this predictor?\n"}
{"snippet": "all_zero_predictions = [0]*len(X_test)\nprint ('Recall: ', recall_score(Y_test, all_zero_predictions))\nprint ('Precision: ', precision_score(Y_test, all_zero_predictions))\n", "intent": "It might also be illustrative to consider the case where the predictor claims that all instances are negative. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nimport math\ny_pred = reg.predict(X_test)  \nmse = mean_squared_error(y_test, y_pred)  \nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test,y_pred)\nprint ('mse = {}, rmse = {} \\nmae = {} r2 = {}'.format(mse,math.sqrt(mse), mae, r2))\n", "intent": "Let's print some other metrics\n"}
{"snippet": "ECE_USA=np.array([1, 1500,1000, 41,80,1000,800,10000,4000,500,1500,40,90, 15,30,11000,80, 0.75]) \ny_pred = reg.predict(ECE_USA.reshape(1,-1))\nprint(y_pred)\n", "intent": "\\8- Use your Regression model for predicting the number of applicants of a new college \"ECE_USA\" whose data is provided below:\n"}
{"snippet": "fbeta_score( y_true=y_train, y_pred=y_pred, beta=my_beta, pos_label='Yes')\n", "intent": "And looking at the score we see there is no improvement.\n"}
{"snippet": "fbeta_score(y_true=y_train, y_pred=y_pred, beta=my_beta, pos_label='Yes')\n", "intent": "And looking at the score we see there is no improvement.\n"}
{"snippet": "scores = cross_val_score(dt_gs_clf, X_train, y_train, scoring=my_fbeta_score, cv=10)\nshow_cv_summary(scores)\n", "intent": "We shouldn't forget we must validate the \n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = clf.predict(test_x).astype(int)\ntest_y_logistic = logistic.predict(test_x).astype(int)\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint (\"Accuracy = %.2f\" % (accuracy_score(test_y, predict_y_logistic)))\n", "intent": "View the Confusion Matrix:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pre = model.predict(Phi)\nacc = str(accuracy_score(Y, y_pre))\nnumber = str(np.argmax(w))\nprint(\"number: \" + number)\nprint(\"accuracy: \" + acc)\n", "intent": "<font color='red'>**Write your answer here ...**</font> (as a *markdown* cell)\n"}
{"snippet": "inception_predictions = [np.argmax(new_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "diabetes_y_pred = regr.predict(diabetes_X_test)\n", "intent": "Make predictions using the testing set\n"}
{"snippet": "print(regr.predict(.08))\n", "intent": "Ok. lets make some new predictions. \n"}
{"snippet": "print(regr.predict(.05))\n", "intent": "Ok. lets make some new predictions. \n"}
{"snippet": "full_prob = np.array([tree.predict_proba(X_test) for tree in fit.estimators_])[:,:,1]\n", "intent": "Calculate the QSO probability predicted by each tree, for each test object, using both models (FULL / SMALL):\n"}
{"snippet": "cv = cross_validation.ShuffleSplit(X.shape[0], test_size = 0.2, random_state = 0, n_iter = 100)\nscores = cross_validation.cross_val_score(clf, X, Y, scoring = \"mean_squared_error\", cv = cv)\nprint(\"   Mean error of %0.2f (+/- %0.2f)\" % (math.sqrt(-scores.mean()), math.sqrt(scores.std() if scores.std() >= 0 else -scores.std())))\n", "intent": "This part will output the root of the MSE (square root of the Mean Squared Error) from the cross validation algorithm.\n"}
{"snippet": "print(\"Prediction Probabilities: {}\".format(nb_model.predict_proba([features_test[0]])))\n", "intent": "How confident is the model of that prediction? Let's get the prediction proabilities for that point.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\nnever_5_clf = Never5Classifier()\ncvs_never_5 = cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\nprint(cvs_never_5)\n", "intent": "<br>\nAll digits are classified as not 5.\n<br>\n10% of images are not 5 so guessing an image is not 5 will make it right about 90% of the time.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision = round(precision_score(y_true=y_train_5, y_pred=y_train_pred), 3)\nrecall = round(recall_score(y_true=y_train_5, y_pred=y_train_pred), 3)\nprint(\"Model claims an image representing 5 is correct in\", round(precision, 3))\nprint(\"Model detects only\", round(recall, 3), \"of the 5s\")\n", "intent": "<br>\nRecall - sensitivity or true positive rate (TPR)\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")[:,1]\n", "intent": "Compute precision and recall for all possible threshold\n"}
{"snippet": "y_train_pred_90 = (y_scores > 120000)\nprecision_90 = precision_score(y_train_5, y_train_pred_90)\nprint(precision_90)\nrecall_90 = recall_score(y_train_5, y_train_pred_90)\nprint(recall_90)\n", "intent": "From the first plot, 90% precision is at about 120.000.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc = roc_auc_score(y_true=y_train_5, y_score=y_scores)\nprint(roc)\n", "intent": "A perfect classifier will have a ROC AUC = 1. Purely random classifier will have ROC AUC = 1\n"}
{"snippet": "nb_predictions_test = nb_model.predict(arr_test_feature)\n", "intent": "Use the dev model to predict the ddi_labels in the test set.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did.\n"}
{"snippet": "print(classification_report(y_test, knnpreds_test))\n", "intent": "__Classification report__\n"}
{"snippet": "def predict(x,W,b):\n    p = list()\n    for i in x:\n        y = None\n        p.append(y)\n    return p\n", "intent": "Now we can check if everything is working as expected!\nLets build out prediction function.\n"}
{"snippet": "def predict(x,W,b):\n    p = list()\n    for i in x:\n        y = W*i + b\n        p.append(y)\n    return p\n", "intent": "Matplotlib is a Python plotting library that is also useful for plotting.  You can install it with:\n'pip install matplotlib'\n"}
{"snippet": "Xception_predictions=[np.argmax(Xception_model.predict(np.expand_dims(feature,axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint'Test accuracy: %.4f%%' % test_accuracy\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = predict(parameters, X_train)\nprint ('Training Accuracy: %d' % float((np.dot(Y_train,predictions.T) + np.dot(1-Y_train,1-predictions.T))/float(Y_train.size)*100) + '%')\npredictions = predict(parameters, X_test)\nprint ('Test Accuracy: %d' % float((np.dot(Y_test,predictions.T) + np.dot(1-Y_test,1-predictions.T))/float(Y_test.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResNet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "some_data=housing.iloc[:5]\nsome_labels=housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint \"Predictions:\", lin_reg.predict(some_data_prepared)\nprint \"Labels:\", list(some_labels)\n", "intent": "Let's see how it worked on a few instances from the training set:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's measure this regression model's RMSE on the whole training set using Scikit-Learn's mean_squared_error function\n"}
{"snippet": "chs_raw = metrics.calinski_harabaz_score(features_sd, k5cls.labels_)\nchs_pca = metrics.calinski_harabaz_score(features_xl_sc, k_pca.labels_)\npd.Series({'0-CHS-Raw': chs_raw,\n           '1-CHS-PCA': chs_pca})\n", "intent": "Calinski-Harabaz score: ratio of between and within dispersion.\n[[`scikit-learn` Explanation]](http://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "r2 = pd.Series({'Baseline': metrics.r2_score(db['l_price'],\n                                              m1.fittedvalues),\n                'Augmented': metrics.r2_score(db['l_price'],\n                                              m3.fittedvalues)})\nr2\n", "intent": "To note:\n- Switch from inference to prediction\n- Overall idea of summarising model performance\n- $R^2$\n- Error-based measures\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test)\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n", "intent": "<b>Confusion Matrix</b>\n"}
{"snippet": "model.predict(normed_test_data)\n", "intent": "Finally, predict MPG values using data in the testing set:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test,clf.predict(X_test)))\n", "intent": "- Notice that all type 2,3,4,5,6,7,8,9 all have very low recall rate\n- Use confusion matrix to show which type is classied wrong\n"}
{"snippet": "score, acc = model.evaluate( x_test, y_test, batch_size = batch_size) \nprint('Test score:', score) \nprint('Test accuracy:', acc)\n", "intent": "Finally, we evaluate the model on samples stored in the test set which the model has never seen.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "Another useful function is the classification_report which provides precision, recall, fscore and support for all classes. \n"}
{"snippet": "from sklearn.metrics import log_loss\ny_true = [0, 0, 1, 1]\ny_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\nlog_loss(y_true, y_pred)\n", "intent": "The [log_loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n"}
{"snippet": "yhat_knn = neigh.predict(test_X)\njaccforknn = jaccard_similarity_score(test_y,yhat_knn)\nf1forknn = f1_score(test_y, yhat_knn, average='weighted')\nprint (\"Jaccard Score: %.2f\" % jaccforknn)\nprint (\"F1 Score: %.2f\"% f1forknn)\n", "intent": "<div class=\"\">\n    <h2>KNN</h2>\n</div>\n"}
{"snippet": "yhat_destree = loadTree.predict(test_X)\njaccfordst = jaccard_similarity_score(test_y, DecisionTree)\nf1fordst = f1_score(test_y, DecisionTree, average='weighted')\nprint(\"DecisionTree Jaccard index: %.2f\" % jaccfordst )\nprint(\"DecisionTree F1-score: %.2f\" % f1fordst )\n", "intent": "<div class=\"\">\n    <h2>Decision Tree</h2>\n</div>\n"}
{"snippet": "SVM = clf.predict(test_X)\njaccforsvm = jaccard_similarity_score(test_y, SVM)\nf1forsvm = f1_score(test_y, SVM, average='weighted')\nprint(\"SVM Jaccard index: %.2f\" % jaccforsvm)\nprint(\"SVM F1-score: %.2f\" % f1forsvm )\n", "intent": "<div class=\"\">\n    <h2>SVM</h2>\n</div>\n"}
{"snippet": "LR_yhat = LR.predict(test_X)\nLR_yhat_prob = LR.predict_proba(test_X)\njaccforlog = jaccard_similarity_score(test_y,LR_yhat)\nf1forlog = jaccard_similarity_score(test_y, LR_yhat)\nyhatlogproba = f1_score(test_y, LR_yhat, average='weighted')\nloglossforlog = log_loss(test_y, LR_yhat_prob)\nprint(\"LR Jaccard index: %.2f\" % f1forlog)\nprint(\"LR F1-score: %.2f\" % yhatlogproba)\nprint(\"LR LogLoss: %.2f\" % loglossforlog)\n", "intent": "<div class=\"\">\n    <h2>Logistic Regression</h2>\n</div>\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(dy_test,dX_test_pred)\n", "intent": "<div id=\"evaluation\">\n    <h3>5. Evaluation</h3>\n    Next, let's import <b>metrics</b> from sklearn and check the accuracy of our model.\n</div>\n"}
{"snippet": "from sklearn.metrics import jaccard_similarity_score, f1_score, log_loss\njaccard_knn = jaccard_similarity_score(y_test, neigh.predict(x_test))\nf1_knn = f1_score(y_test,neigh.predict(x_test), average=\"weighted\")\nprint(\"Evaluation of KNN Jaccard Score: %.4f\" % jaccard_knn)\nprint(\"Evaluation of KNN F1 Score: %.4f\" % f1_knn)\n", "intent": "<div class=\"text-info\"> \n    <h3> Model Evaluation using Jaccard, F1 and Logloss </h3>\n<div>\n"}
{"snippet": "from sklearn.metrics import jaccard_similarity_score, f1_score, log_loss\njaccard_dtc = jaccard_similarity_score(y_test, dtc.predict(x_test))\nf1_dtc = f1_score(y_test, dtc.predict(x_test), average=\"weighted\")\nprint(\"Evaluation of DTC Jaccard Score: %.4f\" % jaccard_dtc)\nprint(\"Evaluation of DTC F1 Score: %.4f\" % f1_dtc)\n", "intent": "<div class=\"text-info\"> \n    <h3> Model Evaluation using Jaccard, F1 and Logloss </h3>\n<div>\n"}
{"snippet": "y_hat= regr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])\nx = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])\ny = np.asanyarray(test[['CO2EMISSIONS']])\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((y_hat - y) ** 2))\nprint('Variance score: %.2f' % regr.score(x, y))\n", "intent": "<h2 id=\"prediction\">Prediction</h2>\n"}
{"snippet": "evals = model.evaluate([x_test, t_test])\n", "intent": "Then it's evaluated\n"}
{"snippet": "x_test = dataset_preprocessing_by_keras(x_test)\nevals = model.evaluate([x_test, t_test])\nprint(evals)\n", "intent": "Then it's evaluated.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "The average accuracy stays almost same as that of Logistic Regression model accuracy; hence, we can say that our model generalizes well.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred_nb)\n", "intent": "We can observe that there are 4366+1090 correct predictions and 6628+273 incorrect predictions\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred_bnb)\n", "intent": "We can observe that there are 10685+297 correct predictions and 263+1112 incorrect predictions\n"}
{"snippet": "def answer_eight():\n    X_train, X_test, y_train, y_test = answer_four()\n    knn = answer_five()\n    cv=knn.predict(X_test)\n    accuracy = knn.score(X_test, y_test)\n    return accuracy\n", "intent": "Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n*This function should return a float between 0 and 1*\n"}
{"snippet": "c = kmeans.predict(data)\n", "intent": "Given the training of the algorithm, we now assign each row (i.e., each sample) to a cluster.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We test our model on the test dataset of dog images and check the accuracy.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "To test how well our model identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "To test how well our model identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: {:.2f} %'.format(test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "start_time = time.monotonic()\nprint(cross_val_score(mlp_relu, X_test, y_test, cv=5))\nend_time = time.monotonic()\nprint('Time:', timedelta(seconds=end_time - start_time))\n", "intent": "Our data is now fit to our model. Look at the time though with 67% of the data it took over 8 minutes to fit the data. \n"}
{"snippet": "start_time = time.monotonic()\nprint(cross_val_score(gbr, X_test, y_test, cv=5))\nend_time = time.monotonic()\nprint('Time:', timedelta(seconds=end_time - start_time))\n", "intent": "Our boosted trees model performed extremely quickly only taking nine seconds to run, but how well did it do? \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for st_layer in range(len(style_layers)):\n        Gl = gram_matrix(feats[style_layers[st_layer]])\n        Al = style_targets[st_layer]\n        loss += style_weights[st_layer]*tf.reduce_sum((Gl-Al)**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "lr.predict(X_test.reshape(-1, 1)) \n", "intent": "The `sklearn` built-in methods can make this `predict` process faster:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_logreg = cross_val_score(logreg, X_train, y_train, cv=10, scoring='accuracy')\n", "intent": "Check with cross validation.\n"}
{"snippet": "print (\"Validation result for Bernoulli_NB Classifier\")\nfor label in load_labels('labels.txt'):\n    nb_classifier = CS5304NBClassifier()\n    nb_classifier.train(train_data, train_target[:, label])\n    validation_score = nb_classifier.get_score(eval_data, eval_target[:, label])\n    print (\"For label\", label,\n           \", the accuracy on validation set is: \", validation_score)\n", "intent": "Below shows the accuracy score for the validation set with label and k indicated on Bernoulli_NB Classifier.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('\\nTest Model \\t\\t Loss: %.6f\\tAccuracy: %.6f' % (test_loss, test_acc))\n", "intent": "Again to evaluate the model we need to check the accuracy on unseen or test data:\n"}
{"snippet": "estimator.evaluate(lambda:input_fn(test_images, \n                                   test_labels,\n                                   epochs=1,\n                                   batch_size=BATCH_SIZE))\n", "intent": "In order to check the performance of our model we then call the `evaluate` method on our Estimator:\n"}
{"snippet": "estimator.evaluate(input_fn=lambda:input_fn(test_folder,\n                                            labels, \n                                            shuffle=True,\n                                            batch_size=BATCH_SIZE,\n                                            buffer_size=2048,\n                                            num_epochs=1))\n", "intent": "Once trained we can evaluate the accuracy on the test set, which should be around 95% (not bad for an initial baseline!):\n"}
{"snippet": "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test)\nnp.mean(predicted_mnb_stemmed == y_test)\n", "intent": "Here we evaluate by predicting on the test set, and comparing to the actual labels.\n"}
{"snippet": "preds = classifier.predict(X_test)\nprint(list(preds[:10]))\nprint(y_test[:10])\n", "intent": "Lets sample our results and see how we do...\nWe take 10 samples, make a prediction using our model and compare the results to the actual labels.\n"}
{"snippet": "X1k=X_test.iloc[1000,:]\nX1k=X1k.reshape(-1,64)\ntrue_1000th_test_value=model.predict(X1k)\nprint(y_test[1000])\nprint(\"1000th test label: \", true_1000th_test_value)\n", "intent": "Print out the TRUE value of the 1000th digit in the test set. By TRUE value, we mean, the actual provided, ground-truth label for that sample:\n"}
{"snippet": "yprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n", "intent": "The predict_proba method gives the posterior probabilities of each label.\n"}
{"snippet": "scores = cross_validation.cross_val_score(hipotese, data, target, cv=5, scoring='f1_weighted')\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\nscores\n", "intent": "USING CROSS-VALIDATION DEFAULT - Stratified k-fold\n---------\n"}
{"snippet": "cv = cross_validation.ShuffleSplit(data.shape[0], n_iter=3, test_size=0.3, random_state=0)\nscores =  cross_validation.cross_val_score(hipotese, data, target, cv=cv)\nprint(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\nscores\n", "intent": "USING CUSTOM CROSS-VALIDATION: Shuffle Split\n---------\n"}
{"snippet": "cv = cross_validation.LeaveOneOut(10)\nscores =  cross_validation.cross_val_score(hipotese, data, target, cv=cv)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "USING CUSTOM CROSS-VALIDATION: LEAVE-ONE-OUT\n---------\n"}
{"snippet": "y_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy :', accuracy)\ndisplay(pd.crosstab(np.array(y_test), \n            y_pred,\n            rownames=['True'],\n            colnames=['Predicted'],\n            margins=True))\n", "intent": "Evaluate the trained classifier\n"}
{"snippet": "accuracys = cross_val_score(pruned_clf, X_train, y_train, cv=10)\nmean = format(np.mean(accuracys),'.4f')\nstd = format(accuracys.std(), '.4f')\nprint('10xCV Accuracy : ', mean, '+-', std)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_Model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def predError(x, y, b):\n    y1 = predict(x, b)\n    return np.sum(np.absolute(y1 - y))/len(y)\n", "intent": "This function computes error rate of our hypothesis.\n"}
{"snippet": "for model in models:\n    scores = cross_val_score(model['estimator'], X_best, cc_labels, cv=10, scoring='r2' )\n    print(\" %s Accuracy: %0.2f (+/- %0.2f)\" % (model['name'], scores.mean(), scores.std() * 2))\n", "intent": "Using 12 best features\n"}
{"snippet": "for model in models:\n    scores = cross_val_score(model['estimator'], cc_validation, cc_label_validation, cv=10, scoring='r2' )\n    print(\" %s Accuracy: %0.2f (+/- %0.2f)\" % (model['name'], scores.mean(), scores.std() * 2))\n", "intent": "Using all the features\n"}
{"snippet": "for model in models:\n    scores = cross_val_score(model['estimator'], cc_validation, cc_label_validation, cv=10, scoring='mean_squared_error' )\n    print(\" %s MSE: %0.2f (+/- %0.2f)\" % (model['name'], scores.mean(), scores.std() * 2))\n", "intent": "Evaluating different models based on MSE\n"}
{"snippet": "pred=model.predict(x_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "test_evaluation = nn.test()\nprint(confusion_matrix(*test_evaluation))\nprint()\nprint(classification_report(*test_evaluation))\n", "intent": "<h1>Testing Results</h1>\n"}
{"snippet": "plant_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(plant_predictions)==np.argmax(test_targets, axis=1))/len(plant_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of plant images. This will be our \"test accuracy\". \n"}
{"snippet": "plant_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(plant_predictions)==np.argmax(test_targets, axis=1))/len(plant_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "After we test the model again we see that we improved test accuracy up to 51%\n"}
{"snippet": "plant_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(plant_predictions)==np.argmax(test_targets, axis=1))/len(plant_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images. This will be our \"test accuracy\". \n"}
{"snippet": "plant_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(plant_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "After we test the model again we see that we improved test accuracy up to 51%\n"}
{"snippet": "test_record = [[0.20168,0.49748,1.37391,0.57140,-0.630657,-0.53965,-0.07627,-0.91710,0.27000,-0.48028,-0.51253,0.68090,0.08992,-0.157405,-0.58881,-0.47790,0.22538,-0.66868,-0.02359,-0.22230,0.71896,-0.17187,-0.16620,0.77647,0.81846,0.44254,0.14275,0.21923,0.1254368]]\ny_test_record_class = us_lrclf.predict(test_record)\ny_test_record_proba = us_lrclf.predict_proba(test_record)\nprint(y_test_record_class)\nprint(y_test_record_proba)\n", "intent": "Let's see whether the following transaction is fraud or not\n"}
{"snippet": "print 'The MSE for the linear regression is {0: ,}'.format(mean_squared_error(y_val, y_pred**2))\n", "intent": "The cone shape has been removed with the log transform! \nThe model also showed improvement.\n"}
{"snippet": "def get_top_n_rate(model, n, x_test, y_test):\n    predict_prob = model.predict(x_test)\n    top_n = np.argsort(predict_prob)[:, :-n-1:-1]\n    found = .0\n    for i in range(len(top_n)):\n        curIndex = np.argmax(y_test[i])\n        if curIndex in top_n[i]:\n            found += 1\n    return found/len(top_n)\n", "intent": "If considering the top 5 returned predictions, the accuracy reached 91.9%.\n"}
{"snippet": "def get_top_n_rate(model, n, x_test, y_test):\n    predict_prob = model.predict(x_test)\n    top_n = np.argsort(predict_prob)[:, :-n-1:-1]\n    found = .0\n    for i in range(len(top_n)):\n        curIndex = np.argmax(y_test[i])\n        if curIndex in top_n[i]:\n            found += 1\n    return found/len(top_n)\n", "intent": "If considering the top 5 returned predictions, the accuracy reached 90.0%.\n"}
{"snippet": "info_string, models = run_train()\nensemble_predict(info_string, models)\n", "intent": "Run the training and prediction code\n"}
{"snippet": "print(votingClassifier.predict(X_test[song_index].reshape(1,-1)))\n", "intent": "Let's look at the prediction for song at index 20 !\n"}
{"snippet": "mse = (np.mean((bos.PRICE - lm.predict(X)) ** 2))\nmse\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "x_test = np.array(['I am very happy']) \nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "num_samples = 64 \nnce = tf.nn.nce_loss(weights=nce_weights,\n                     biases=nce_biases,\n                     labels=train_labels,\n                     inputs=embed,\n                     num_sampled=num_samples,\n                     num_classes=vocabulary_size)\nloss = tf.reduce_mean(nce)\n", "intent": "Let's try to **predict** the **target word** using the **NCE** objective\n"}
{"snippet": "predictions = l[97].predict(x)\nX[:,30]=predictions\nprint(accuracy_score(y_train, l[97].predict(X_train)))\nprint(accuracy_score(y_cv, l[97].predict(X_cv)))\nprint(accuracy_score(y_test, l[97].predict(X_test)))\n", "intent": "Random state = 97 gives highest cross validation accuracy, so creating a new dataset with the predictions from this random forest model\n"}
{"snippet": "model1.evaluate(te_img, gpu=Gpu(devices=[0]))\n", "intent": "Now use the trained `model1` and the `evaluate()` function to score images from the test image partition `te_img`.\n"}
{"snippet": "model1.evaluate(te_img)\n", "intent": "Now use the trained model to score input images. Use the test data `te_img` that was partitioned earlier in the example.  \n"}
{"snippet": "model4.evaluate(data=te_img ,  gpu=Gpu(devices=[1]))\n", "intent": "Now use model4 to score input data.\n"}
{"snippet": "test_result = model1.predict(test_tbl)\ntest_result_table = model1.valid_res_tbl\n", "intent": "Note: for example simplicity, we just evaluate the testing results. Feel free to check the training result by changing `test_tbl` to `train_tbl`.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100 * np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis=1)) / len(Xception_predictions)\nprint(\"Xception Test accuracy: %.4f%%\" % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100 * np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis=1)) / len(Xception_predictions)\nprint(\"Xception 1 Test accuracy: %.4f%%\" % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\",lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\",list(some_labels))\n", "intent": "Done! Let's try it out on a few instances from the training set:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's measure this regression model's RMSE on the whole training set.\n"}
{"snippet": "some_datasome_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "Let's try the full pipeline on a few instances:\n"}
{"snippet": "best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)\nprint('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n", "intent": "- can be improved through the by increasing iteration and cross-validation\n"}
{"snippet": "def scoring_a(y_true, y_predict):\n    return metrics.accuracy_score(y_true, y_predict)\n", "intent": "**Note: ** We can also use __RandomizedSearchCV__ for performance improvements.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntrain_predict = clf.predict(train_features)\ntest_predict = clf.predict(test_features)\ntrain_accuracy = accuracy_score(train_target,train_predict)\ntest_accuracy = accuracy_score(test_target,test_predict)\nprint (\"train_accuracy: \",train_accuracy)\nprint (\"test_accuracy: \",test_accuracy)\n", "intent": "6\\. Using the classifier built in 2.2, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each?\n"}
{"snippet": "rsquared = metrics.explained_variance_score(y_test, predictions)\n", "intent": "Let's also calculate the explained variance score (R^2) for our model. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(lm, X_littletrain, y_littletrain, n_jobs=1, cv=5)\n", "intent": "This is an attempt to run K-Fold on the partial model. Unsure at this point how to implement it into better predictions. \n"}
{"snippet": "y_pred_test = np.expm1(regressional_model_best.predict(test_data))\n", "intent": "**Final prediction on the Testing set.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "** Use lm.predict() to predict off the X_test set of the data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, roc_auc_score\nprint(roc_auc_score(y_test, probs))\n", "intent": "Need to evaluate the tensor. Any tensor is also a numpy array.\n"}
{"snippet": "inception_predicted_train = fitted_inception.predict(features_inception_train)\ninception_predicted_test = fitted_inception.predict(features_inception_test)\nprint('Accuracy on the training set:', round(accuracy_score(inception_predicted_train, np.asarray(y_train)),4))\nprint('Accuracy on the test set:', round(accuracy_score(inception_predicted_test, np.asarray(y_test)),4))\n", "intent": "Checking basic stats regarding the model:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.0\n    for i in range(len(style_layers)):\n        gramMat = gram_matrix(feats[style_layers[i]])\n        loss = loss + style_weights[i]*tf.reduce_sum(tf.squared_difference(gramMat, style_targets[i]))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "price_predict = model.predict(tinput)\nrss = np.sum(price_predict-tdata[['price']].values)**2\nprint (rss)\n", "intent": "**Quiz Question: what is the RSS on TEST data for the model with the degree selected from Validation data?**\n"}
{"snippet": "one_unit_SRNN.predict(numpy.array([ [[1], [1], [0]] ]))\n", "intent": "This passes in a single sample that has three time steps.\n"}
{"snippet": "two_unit_SRNN.predict(numpy.array([ [[2], [3]] ]))\n", "intent": "This passes in a single sample with four time steps.\n"}
{"snippet": "predictions = (np.array(model1.predict(testX))[:,0] >= 0.5).astype(np.int_)\ntest_accuracy = np.mean(predictions == testY[:,0], axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "One hidden layer network\n"}
{"snippet": "predictions = (np.array(model2.predict(testX))[:,0] >= 0.5).astype(np.int_)\ntest_accuracy = np.mean(predictions == testY[:,0], axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "Two hidden layer network\n"}
{"snippet": "predictions = (np.array(model3.predict(testX))[:,0] >= 0.5).astype(np.int_)\ntest_accuracy = np.mean(predictions == testY[:,0], axis=0)\nprint(\"Test accuracy: \", test_accuracy)\n", "intent": "Three hidden layer network\n"}
{"snippet": "xte, tte, xtr, ttr = getBalencedDataSets(data,class_labels,300,verbose=False)\nxtr,xte = standardise(xtr,xte)\nresults = p.map(test_with_k_fold,hidden_units_array)\nbest_num_hidden = max(hidden_units_array,key = lambda x:results[x-1]-0.00000001*x)\nprint(\"Optimal Hidden units with standardisation :\",best_num_hidden)\nprint(\"The accuracy achieved from a layer with this many hidden units achieves an accuracy of : \",\n      train_and_evaluate(xtr,ttr,xte,tte,best_num_hidden))\n", "intent": "It is worth noting that the above is unstandardised data and repeatedly achieves a testing accuracy of 1.0 (100%)\n"}
{"snippet": "xte, tte, xtr, ttr = getBalencedDataSets(data,class_labels,300,verbose=False)\nxte,xtr = standardise(xte,xtr)\nresults = p.map(test_with_k_fold,hidden_units_array) \nbest_num_hidden = max(hidden_units_array,key = lambda x:results[x-1]-0.00000001*x)\nprint(\"Optimal Hidden units with standardisation :\",best_num_hidden)\nprint(\"The accuracy achieved from a layer with this many hidden units achieves an accuracy of : \",\n      train_and_evaluate(xtr,ttr,xte,tte,best_num_hidden))\n", "intent": "The results of this were quite poor - how about with the gausian normalisation?\n"}
{"snippet": "actress_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(actress_predictions)==np.argmax(test_targets, axis=1))/len(actress_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out the model on test data\n"}
{"snippet": "y_test = classifier.predict(pad_test)\n", "intent": "Predicting the test set\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\nprint(\"Root Mean Squared Error Linear Regression Model: {:.2f}\".format(\n    sqrt(mean_squared_error(y_test.values, regressor.predict(X_test_scaled)))\n))\n", "intent": "* Obtain Root Mean Squared Error\n* Smaller the Root Mean Squared Error the better.\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\nprint(\"Root Mean Squared Error Linear Regression Model: {:.2f}\".format(\n    sqrt(mean_squared_error(y_test.values, regressor.predict(X_test_scaled)))\n))\n", "intent": "* Obtain Root Mean Squared Error\n* Smaller the Root Mean Squared Error the better (Improve Accuracy by reducing your error less than 0.62)\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\nprint(\"Root Mean Squared Error Linear Regression Model: {:.2f}\".format(\n    sqrt(mean_squared_error(y_test.values, regressor.predict(X_test_scaled)))\n))\n", "intent": "* Obtain Root Mean Squared Error\n* Smaller the Root Mean Squared Error the better \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram_matrix(feats[style_layers[i]]) \n                                                                - style_targets[i] ))\n    return (style_loss)    \n", "intent": "Next, implement the style loss:\n"}
{"snippet": "final_Y_pred = np.exp((ridge_random.best_estimator_.predict(final_test_X) +\n                   elastic_net_random.best_estimator_.predict(final_test_X) +\n                   xgboost.predict(final_test_X)) / 3)\n", "intent": "The final predicted values - \n"}
{"snippet": "print \"Class predictions according to Scikit-Learn:\" \nprint sentiment_model.predict_proba(sample_test_matrix)[:,1]\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from Scikit-Learn.\n"}
{"snippet": "preds_simp_mod_train = simple_model.predict(train_matrix_word_subset)\nn_cor_preds_simp_mod_train = float(np.sum(preds_simp_mod_train == train_data['sentiment'].values))\nn_tol_preds_simp_mod_train = float(len(preds_simp_mod_train))\nacc_simp_mod_train = n_cor_preds_simp_mod_train/n_tol_preds_simp_mod_train\nprint acc_simp_mod_train\n", "intent": "Now, compute the classification accuracy of the **simple_model** on the **train_data**:\n"}
{"snippet": "preds_simp_mod_test = simple_model.predict(test_matrix_word_subset)\nn_cor_preds_simp_mod_test = float(np.sum(preds_simp_mod_test == test_data['sentiment'].values))\nn_tol_preds_simp_mod_test = float(len(preds_simp_mod_test))\nacc_simp_mod_test = n_cor_preds_simp_mod_test/n_tol_preds_simp_mod_test\nprint acc_simp_mod_test\n", "intent": "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:\n"}
{"snippet": "def get_pred_from_score(scores_array):\n    predictions = scores_array\n    scores_array[scores_array<=0] = -1\n    scores_array[scores_array>0] = 1\n    return predictions\n", "intent": "First, writing a helper function that will return an array with the predictions. \n"}
{"snippet": "pred_l2_pen_0_train = get_pred_from_score(scores_l2_pen_0_train)\npred_l2_pen_4_train = get_pred_from_score(scores_l2_pen_4_train)\npred_l2_pen_10_train = get_pred_from_score(scores_l2_pen_10_train)\npred_l2_pen_1e2_train = get_pred_from_score(scores_l2_pen_1e2_train)\npred_l2_pen_1e3_train = get_pred_from_score(scores_l2_pen_1e3_train)\npred_l2_pen_1e5_train = get_pred_from_score(scores_l2_pen_1e5_train)\n", "intent": "Now, getting the predictions for the training data and the validation data for the 6 L2 penalties we considered.\n"}
{"snippet": "samp_vald_data_prob = decision_tree_model.predict_proba(sample_validation_data.ix[:, sample_validation_data.columns != \"safe_loans\"])[:,1]\n", "intent": "For each row in the **sample_validation_data**, what is the probability (according **decision_tree_model**) of a loan being classified as **safe**? \n"}
{"snippet": "small_model.predict(sample_validation_data.ix[79, sample_validation_data.columns != \"safe_loans\"])[0]\n", "intent": "**Quiz Question:** Based on the ** small_model **, what prediction would you make for this data point?\n"}
{"snippet": "predic_valid_data = decision_tree_model.predict(validation_data.ix[:, validation_data.columns != \"safe_loans\"])\nlabels_valid_data = validation_data[\"safe_loans\"].values\n", "intent": "First, let us make predictions on `validation_data` using the `decision_tree_model`. Then, let's store the labels of `validation_data`.\n"}
{"snippet": "y_pred = dtree.predict(X_test)\n", "intent": "Predict the labels of the new data points with predict:\n"}
{"snippet": "ret, y_pred = lr.predict(X_train)\n", "intent": "Calculating the accuracy score on the training set:\n"}
{"snippet": "ret, y_pred = lr.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)\n", "intent": "Check on the test dataset\n"}
{"snippet": "test_preds = best_alpha*X_test_level2[:,0] + (1-best_alpha)*X_test_level2[:,1] \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "Y_predict_test = svc.predict(X_test)\nprint(Y_predict_test)\n", "intent": "Predict the testing data using the SVM model.\n"}
{"snippet": "y_predict_test_rbf = svc_rbf.predict(X_test)\nprint(y_predict_test_rbf)\n", "intent": "Predict the testing data using SVM model.\n"}
{"snippet": "y_predict_test_rbf = linear_svc.predict(X_test)\nprint(y_predict_test_rbf)\n", "intent": "Predict the testing data using SVM model.\n"}
{"snippet": "rf_predict_train = rf_model.predict(X_train)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, rf_predict_train)))\n", "intent": "Predict on Training Data\n"}
{"snippet": "rf_predict_test = rf_model.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, rf_predict_test)))\n", "intent": "Predicting on Testing Data\n"}
{"snippet": "lr_cv_predict_test = lr_cv_model.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_cv_predict_test)))\nprint(metrics.confusion_matrix(y_test, lr_cv_predict_test) )\nprint(\"\")\nprint(\"Classification Report\")\nprint(metrics.classification_report(y_test, lr_cv_predict_test))\n", "intent": "Predict on Test data\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gi = gram_matrix(feats[style_layers[i]])\n        Li = style_weights[i] * tf.reduce_sum((gi - style_targets[i]) ** 2)\n        style_loss += Li\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "cnnModel_predictions = [np.argmax(cnnModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(cnnModel_predictions)==np.argmax(test_targets, axis=1))/len(cnnModel_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "logreg.predict_proba(X_train[0])\n", "intent": "Predict the probability of survival for the first person in X_train using scikit-learn.\n"}
{"snippet": "y_pred = logreg.predict(X_test)\n", "intent": "Make predictions on the testing data and calculate the accuracy.\n"}
{"snippet": "print metrics.accuracy_score(y_test, [0]*len(y_test))\nprint metrics.accuracy_score(y_test, y_pred)\n", "intent": "Compare this to the null accuracy.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, predicted))\n", "intent": "Assess accuracy - about 15%, not great.\n"}
{"snippet": "def predict(reg, x):\n", "intent": "After training the regression decision tree model, given a new data, we can use the pre-trained model to predict to hourse price.\n"}
{"snippet": "score, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size)\nprint('Test loss:', score)\nprint('Test accuracy %:', acc)\n", "intent": "and evaluate its accuracy:\n"}
{"snippet": "def predict_xgb(gb,xgdtest):\n    y_pred = gb.predict(xgdtest)\n    return y_pred\n", "intent": "Using the following code to predict, examples are like:\n<pre><code>pred = bst.predict(yourtestDmatrix)</code></pre>\n"}
{"snippet": "predict_label1 = knnClassifier.predict(test)\nerror = 0\nfor i in range(len(test_label)):\n    if test_label[i] != predict_label1[i]:\n        error += 1\nerror = error * 1.0 / len(test)\nprint \"error rate: \", error\n", "intent": "Next step is to use the classifier to predict labels of the test dataset and calculate the error rate.\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)\n", "intent": "To evaluate our model, we can calculate accuracy, recall, and precision, f1-score\n"}
{"snippet": "def classify_movies(tfidf, classifier, test_data):\n    test_df = apply_tokenizer_to_df(test_data)\n    test_df['modified_plot'] = test_df['plot_tokens'].apply(lambda x: \" \".join(x))\n    smatrix = tfidf.transform(test_df['modified_plot'])\n    return classifier.predict(smatrix)\n", "intent": "Now let's classify our test data to predict whether or not the user will like the movie or not. \n"}
{"snippet": "preds = model.predict(x)\nresult = index_class.get(np.argmax(preds))\nprint(result)\n", "intent": "Predict the image above and print out the result of the predicted class\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])\n        loss += style_weights[i] * tf.reduce_sum(tf.squared_difference(gram, style_targets[i]))\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "rsofs =  np.sum((bos.PRICE - lm.predict(X)) ** 2)\nrsofs\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "reanalysis_composite = reanalysis.copy()\nmodel_clust = best_fit.fit_predict(pc_ts) \nweather_types = pd.Series(model_clust, index=model['time']).to_xarray() \nreanalysis_composite['WT'] = weather_types\nreanalysis_composite = reanalysis_composite.groupby('WT').mean(dim='time').unstack('grid')['gh']\nreanalysis_composite['M'] = 0\n", "intent": "Now, for each ensemble member, calculate the weather types and their anomalies.\n"}
{"snippet": "print  metrics.accuracy_score(y_test, dt.predict(X_test))\nprint  metrics.confusion_matrix(y_test, dt.predict(X_test))\nprint  metrics.classification_report(y_test, dt.predict(X_test))\n", "intent": ">Observation - By adjusting the max_depth to 6 and min samples to 45, improved accuracy of model\n"}
{"snippet": "print  metrics.accuracy_score(y_test, dt.predict(X_test))\nprint  metrics.confusion_matrix(y_test, dt.predict(X_test))\nprint  metrics.classification_report(y_test, dt.predict(X_test))\n", "intent": ">Observation - By adjusting the max_depth to 6 and min samples to 5, improved accuracy of model\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in\n                           test_InceptionV3]\ntest_accuracy = 100 * np.sum(np.array(InceptionV3_predictions) == np.argmax(test_targets, axis=1)) / len(\n    InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model = mh.get_module(iterator=test_iterator)\npredictions = np.argmax(model.predict(test_iterator), axis=1).asnumpy().astype(int)\nimages = get_images(test_iterator)\nshow_predictions(predictions, images, imagenet_class_to_human, None, (15, 1.5))\n", "intent": "Let's try to predict with the pre-trained network.\n"}
{"snippet": "scores = cross_val_score(clf, title_df, y, scoring='accuracy', cv=10)\nprint('Accuracy score for random forest is {:.4f} +/- {:.4f}'.format(np.mean(scores), np.std(scores)))\n", "intent": "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n"}
{"snippet": "labels = clf.predict(iris.data)\n", "intent": "important class and method:\n```\n - clf.predict(X)\n```\n"}
{"snippet": "k_folds = 5\nscores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=k_folds)\nscores\n", "intent": "important class and method:\n```\n - scores = cross_validation.cross_val_score(clf, X, y, cv=k_folds)\n```\n"}
{"snippet": "r2_score(dataoutput,datainput.pin)\n", "intent": "If just guessed the previous pressure.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    s_len = len(style_layers)\n    style_loss = 0\n    for l in range(s_len):\n        cl = style_layers[l]\n        style_loss += style_weights[l] * tf.reduce_sum((gram_matrix(feats[cl]) - style_targets[l])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "train_predict(clf_A, X_train, y_train, X_test, y_test)\nprint ''\ntrain_predict(clf_B, X_train, y_train, X_test, y_test)\nprint ''\ntrain_predict(clf_C, X_scaled_train, y_scaled_train, X_scaled_test, y_scaled_test)\n", "intent": "Here I repeat the test on the full training set using the balanced data. In the case of the SVM the scaled and balanced data is used.\n"}
{"snippet": "pred_labels = lr.predict(X)\nprint pred_labels\nprint len(lr.predict(X))\n", "intent": "** seems if we say all are 0s, we will have 93.2% accuracy **\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy2 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy2)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])   \n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0.0, name=\"loss\")\n    print(loss)\n    for i in range(len(style_layers)):\n        gram = gram_matrix(feats[style_layers[i]])   \n        loss += style_weights[i] * tf.reduce_sum((gram - style_targets[i])**2)\n    print(loss)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    print(correct)\n    print(student_output)\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "Res50_predictions = [np.argmax(Res50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(Res50_predictions)==np.argmax(test_targets, axis=1))/len(Res50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "(knn.predict(X_test) == y_test).sum() / len(X_test)\n", "intent": "The accuracy is calculated by the number of correct prediction over total number of data points\n"}
{"snippet": "np.random.seed(seed=898942)\nscores = cross_val_score(lin_reg,housing_prepared,housing_labels,\\\n                         scoring='neg_mean_squared_error', cv=10)\n", "intent": "We can get a better idea of whether our models are overfitting by making use of a cross validation methodology\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop('median_house_value',axis=1)\ny_test = strat_test_set['median_house_value'].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test,final_predictions)\nprint('Test RMSE: {}'.format(np.sqrt(final_mse)))\n", "intent": "Finally it's time to evaluate your model on the test set\n"}
{"snippet": "week_A_univ_input = np.array(week_A['Time (x1)']).reshape(-1 , 1)\nweek_A_multiv_input = week_A[['Time (x1)','C_eff (x2)']]\nweek_A_predictions_univ = clf_univ.predict(week_A_univ_input)\nweek_A_predictions_multiv = clf_multiv.predict(week_A_multiv_input)\nweek_B_univ_input = np.array(week_B['Time (x1)']).reshape(-1 , 1)\nweek_B_multiv_input = week_B[['Time (x1)', 'C_eff (x2)']]\nweek_B_predictions_univ = clf_univ.predict(week_B_univ_input)\nweek_B_predictions_multiv = clf_multiv.predict(week_B_multiv_input)\n", "intent": "After setting up the dataframe for 2016 and grabbing test observations from week A and week B, predictions about previous unseen data can be done.\n"}
{"snippet": "print (\"Confusion matrix\")\nprint(\"{0}\".format(metrics.confusion_matrix(Y_test, nb_predict_test, labels=[1,0])))\nprint(\"\")\nprint(\"Classification report\")\nprint(metrics.classification_report(Y_test, nb_predict_test, labels=[1,0]))\n", "intent": "how did we really get to these numbers\n"}
{"snippet": "rf_predict_test = rf_model.predict(X_test)\nprint(\"Accuracy {0:.4f}\".format(metrics.accuracy_score(Y_test, rf_predict_test )))\n", "intent": "With the test data we got very good accuracy. Let's see the test data\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = bag_clf.predict(X)\naccuracy_score(y, y_pred)\n", "intent": "According to this oob eval, this BaggingClassifier is likely to achieve 96% accuracy on the test set.\n"}
{"snippet": "def compute_error(estimator, X, y):\n    cv = model_selection.ShuffleSplit(n_splits=2, test_size=0.3, random_state=np.random.randint(1,1000))\n    cv_score = model_selection.cross_val_score(estimator, X, y, cv=cv, scoring='f1')\n    print 'cross validated f1 scores: ', cv_score\n    return cv_score.mean()\n", "intent": "What error should we minimize?\nf1 = 2 * ( precsion * recall )  / (precsion + recall)\np = tp / (fp + tp)\nr = tp / (fp + fn)\n"}
{"snippet": "XX = pca.transform(X_test)\nprediction_set = clf.predict_proba(XX) \ny_pred_test_with0 = np.zeros(XX.shape[0]) \ny_pred_test_with0[prediction_set[:,0] > 0.7] = -1\ny_pred_test_with0[prediction_set[:,1] > 0.7] = 1\nnp.savetxt('y_pred2.txt', y_pred_test_with0, fmt='%d')\n", "intent": "Trying this processus with the test data and sending it online\n"}
{"snippet": "predicted = gke_search.predict(mnist.data[60000:])\nprint(len([p for i, p in enumerate(predicted) if p == mnist.target[60000:][i]]))\n", "intent": "You can also call `predict()`, which deligates the call to the `best_estimator_`.\nBelow we calculate the accuracy on the 10000 test images.\n"}
{"snippet": "dummies_n = dummyVars(Metadata_Treatment ~ ., data=df_n_num)\ndf_n_num_ = predict(dummies_n, newdata = df_n_num)\n", "intent": "Handling categorical variables:   \n- using only relevant ones   \n- using one-hot encoding\n"}
{"snippet": "library(caret)\nparams.pca=preProcess(df_n_num_nzv_no_meta_trans_noNA, method='pca')\ndf_n_num_nzv_no_meta_trans_noNA_PCA=predict(params.pca, df_n_num_nzv_no_meta_trans_noNA)\nprint(params.pca)\nprint(dim(df_n_num_nzv_no_meta_trans_noNA_PCA))\n", "intent": "Analysis with caret package.\n"}
{"snippet": "for epoch in range(n_epochs):\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n", "intent": "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the split.\n"}
{"snippet": "test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n", "intent": "Finally, the metric you actually care about, the test loss and accuracy.\n"}
{"snippet": "test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n", "intent": "Finally, the metric you actually care about, the test loss, accuracy and AUC.\n"}
{"snippet": "for epoch in range(n_epochs):\n    train_loss, train_acc, train_auc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc, valid_auc = evaluate(model, valid_iterator, criterion)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Train AUC: {train_auc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%, Val. AUC: {valid_auc*100:.2f}%')\n", "intent": "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the split.\n"}
{"snippet": "test_loss, test_acc, test_auc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%, Test AUC: {test_acc*100:.2f}%')\n", "intent": "Finally, the metric you actually care about, the test loss, accuracy and AUC.\n"}
{"snippet": "def createFeatures(path,name):\n    rgb_hires = load_array(path + name + '_hires.dat')\n    conv_feat = model.predict(rgb_hires, batch_size=64, verbose=0)\n    save_array('/home/ubuntu/features/'+name+'_sqznt.dat',conv_feat)\n    del(conv_feat,rgb_hires)\n    gc.collect()\n", "intent": "Precompute the features and save to disk to save training time\n"}
{"snippet": "batches = get_batches(path+'test', shuffle=False, batch_size=1, class_mode=None, target_size=(360,640))\ntest = np.concatenate([batches.next() for i in range(batches.nb_sample)])\ntest_features = vgg640.predict(test, batch_size = 32, verbose =1)\ndel test\n", "intent": "Repeat for test data\n"}
{"snippet": "model.predict(test)\n", "intent": "WITH THE SET OF DATA from the example, works perfectly...\n"}
{"snippet": "model_16.predict(test_nn)\n", "intent": "Run predictions + accuracy\n"}
{"snippet": "p = model.predict(X_test)\np1 = model1.predict(X_test1)\n", "intent": "Predicting the target value for both dataset\n"}
{"snippet": "def model_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n    return trainScore[0], testScore[0]\nprint(\"--- With Previous day open price ----\")\nmodel_score(model, X_train, y_train, X_test, y_test)\nprint(\"--- Without Previous day open price ----\")\nmodel_score(model1, X_train1, y_train1, X_test1, y_test1)\n", "intent": "Calculaying the RMSE value for both dataset.\n"}
{"snippet": "print \"accuracy (before training) = \", evaluate(net, dataset)\n", "intent": "Let us see the performance of our (un-trained) network on the test set by calling the `evaluate()` function.\n"}
{"snippet": "print \"accuracy (after training) = \", evaluate(net, dataset)\n", "intent": "Having trained the network, let us see if the performance on the test set has improved over what we got prior to training. \n"}
{"snippet": "scores, labels = evaluate(net, dataset)\nverifMetric = Metrics(scores, labels)\navgDistSim, avgDistDiss = verifMetric.getAvgDist()\nprint \"avgDistSim = \", avgDistSim, \", avgDistDiss = \", avgDistDiss\neer, auc, fpr, tpr = verifMetric.getROC()\nprint \"EER = \", eer, \", AUC = \", auc\n", "intent": "After we are done training, let us take a look at the same metrics once again.\n"}
{"snippet": "pred = Optimized_Model.predict(X_validation)\nprint('the accuracy is : ', metrics.accuracy_score(Y_validation, pred, normalize=True, sample_weight=None))\nprint('the roc-auc score is :', metrics.roc_auc_score(Y_validation, pred))\n", "intent": "Results on the validation set :\n"}
{"snippet": "actual_sm = pd.Series(Ytest, name = 'Actual')\npredicted_sm = pd.Series(clf_sm.predict(Xtest), name = 'Predicted')\nct_sm = pd.crosstab(actual_sm, predicted_sm, margins = True)\nprint(ct_sm)\n", "intent": "Overall accuracy has improved notably. Let's check the confusion matrix. \n"}
{"snippet": "pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\n", "intent": "Iterate through the classifier items to train, predict and append results to a display dataframe.\n"}
{"snippet": "my_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test]\nmy_test_accuracy = 100*np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1))/len(my_predictions)\nprint('Test accuracy: %.4f%%' % my_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nquestions, gold = read_toefl()\npredictions = []\nfor question in questions:\n    prediction = predict_synonym(question)\n    predictions.append(prediction)\nprint(\"Accuracy for TOEFL test: \", accuracy_score(predictions, gold))\n", "intent": "Finally, compute the accuracy score between our predictions and the correct answers.\n"}
{"snippet": "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)\nscores = cross_val_score(gs, X, y, scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n", "intent": "In scikit-learn, we can perform nested cross-validation as follows:\n"}
{"snippet": "my_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\nclf_labels += ['Majority Voting']\nall_clf = [pipe1, clf2, pipe3, my_clf]\nfor clf, label in zip(all_clf, clf_labels):\n    scores = cross_val_score(estimator = clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n    print('ROC AUC: %0.2f (+/- %0.2f) [%s]' %(scores.mean(), scores.std(), label))\n", "intent": "Now let's move on to the more exciting part and combine the individual classifiers\nfor majority rule voting in our MajorityVoteClassifier :\n"}
{"snippet": "from sklearn.metrics import r2_score\nprint('R^2 train: %.3f test: %.3f'%(r2_score(y_train,y_train_pred), r2_score(y_test, y_test_pred)))\n", "intent": "Sometimes it may be useful to report the coefficient of determination (R squared), which can be uderstood as a standardized version of the MSE.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(test_labels, preds))\n", "intent": "Now we evalute the model's accuracy by comparing to the pre-labelled values, using ```accuracy()```\n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n    return f1_score(target.values, y_pred, pos_label='yes')\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "We initialize three helper functions for training and testing the three supervised learning models:\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\nresnet_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % resnet_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "sns.jointplot(\n    y_test, model.predict(X_test),\n    xlim=(0,20), ylim=(0,20)\n)\n", "intent": "Looks like it's creating the right overall trends, but I wouldn't publish it...\n"}
{"snippet": "accuracy_score(y_test, predictions)\n", "intent": "$$ACC = \\frac{TP + TN}{P + N}$$\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_pred_class))\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 -accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test, y_pred_prob))\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve:**\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true,pred))\n", "intent": "**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "Classification accuracy:\n<ul>\n<li>Proportion of correct predictions</li>\n<li>Common evaluation metric for classification problems</li>\n</ul>\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "<ul>\n   <li>Returns a NumPy array</li>\n   <li>Can predict for multiple observations at once</li>\n</ul>\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nlabels = k_means.labels_\nmetrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "applied to our model before, we can use \n"}
{"snippet": "def evaluate(model, y):\n    correct_prediction = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    return accuracy\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy on test set: \",score[1])\n", "intent": "_Model accuracy improved from 90.04% in epoch 1 to 98.95% in epoch 15_\n"}
{"snippet": "preds_on_test = searcher.predict(X = X_test)\n", "intent": "It looks like our accuracy on our training dataset is about 90%.  Finally, output the predictions to a file.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):  \n        gram_i = gram_matrix(feats[style_layers[i]], normalize=True)\n        style_loss += style_weights[i] * tf.reduce_sum((gram_i - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "val_preds = search.predict(X_val)\nprint('Validation accuracy: %.5f' % h2o_accuracy_score(actual, val_preds['predict']))\nval_preds.head()\n", "intent": "So our best estimator achieves a mean cross validation accuracy of 93%! We can predict on our best estimator as follows:\n"}
{"snippet": "predict_train = rf_model.predict(x_train)\npredict_validation = rf_model.predict(x_validation)\n", "intent": "Getting my predictions...\n"}
{"snippet": "print('Training Accuracy:', metrics.accuracy_score(y_train, predict_train))\nprint('Validation Accuracy:', metrics.accuracy_score(y_validation, predict_validation))\n", "intent": "Now I'll see how accurate the predictions are for the train and test data.\n"}
{"snippet": "validation_size = 1200\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))\n", "intent": "Extracting a validation set, and measuring score and accuracy.\n"}
{"snippet": "scores = model.evaluate(X_train, y_train)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same dataset.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: {:.4f}' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "inception_predictions = [np.argmax(incept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: ',test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "metrics.accuracy_score(y_test, predicted)\n", "intent": "Let's calculate our validation metric:\n"}
{"snippet": "a = list(classifier.predict(test_data))[0]\nprint ( a, test_labels[0])\ndisplay(0)\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "test_pred = log_model.predict(X_test)\n", "intent": "And finally, we use this classifier to label the evaluation set we created earlier:\n"}
{"snippet": "iris_sample_probs = log_reg_classifier.predict_proba(iris_sample)\niris_sample_probs\n", "intent": "Having the regression classifier as well as the the Iris Sample to be examined instantiated, we can now calculate the probabilities for this sample:\n"}
{"snippet": "sample_predicition = log_reg_classifier.predict(iris_sample)\nsample_predicition\n", "intent": "... and can retrieve a prediction from the Logistic Regression classifier what Iris class the sample might be:\n"}
{"snippet": "fitted_val = final_model_2nd.predict(X_2nd)\nresiduals = y - fitted_val\nfit_vs_resid = pd.concat([fitted_val.rename('fitted'), residuals.rename('residuals')], axis = 1)\nggplot(aes(x = 'fitted'), data = fit_vs_resid) + \\\n    geom_point(aes(y = 'residuals'), color = 'blue', alpha = .4, size = 80) + \\\n    geom_hline(y = 0, color = 'red', alpha = .6, size = 2) + \\\n    theme_bw()\n", "intent": "The fitted values vs. residuals plot reveals that our residuals are generally homoscedastic.\n"}
{"snippet": "x_new = [[5.0, 3.4, 2.2, 0.6]]\nclf.predict(x_new)\n", "intent": "Now the model is trained. We can use it to predict the outcome of some unseen data.\n"}
{"snippet": "ResNet50_breed_predictions = [np.argmax(ResNet50_model_breed.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_breed_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "decoded_lattice = model.predict(STEM_real)\n", "intent": "Applying trained model to real STEM data\n"}
{"snippet": "X_test_d = model.predict(X_test)\n", "intent": "Let's see how well the model performed on the validation set.\n"}
{"snippet": "from pandas import DataFrame\nfrom sklearn.metrics import accuracy_score, confusion_matrix\ndef performance(y_prediction, name = ''):\n    print(' - '+name+' - \\n '+'-'*(len(name)+4))\n    print(\" Accuracy : \\n\", accuracy_score(y_prediction, y_test), \"\\n\")\n    print(\" Confusion Matrix :  \\n\", confusion_matrix(y_prediction, y_test), \"\\n\")\n", "intent": "The next function will be used to evaluate the performance of a prediction : \n"}
{"snippet": "M = np.array([[0,1,2],\n              [1,0,1],\n              [2,1,0]]) \nclf = predictor(loss = 'wasserstein', method = 'sgd', eta = 0.001, n_iter=1000, n_sinkhorn=20, ground_metric=M)\nclf.train(X,y_score)\ny_score_pred = clf.predict(X)\ny_pred = y_score_pred.argmax(axis = 1)\nprint(confusion_matrix(y_pred, y))\n", "intent": "If I try to modify results with a different metrics $M$ on $\\{0,1,2\\}$ :\n"}
{"snippet": "M = np.array([[0,1,1],\n              [1,0,2],\n              [1,2,0]])\nclf = predictor(loss = 'wasserstein', method = 'sgd', eta = 0.001, n_iter=1000, n_sinkhorn=20, ground_metric=M)\nclf.train(X,y_score)\ny_score_pred = clf.predict(X)\ny_pred = y_score_pred.argmax(axis = 1)\nprint(confusion_matrix(y_pred, y))\n", "intent": "It does not change anything... and this is not suprising ...\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for l, t, w in zip(style_layers, style_targets, style_weights):\n        gram_loss = tf.reduce_sum((gram_matrix(feats[l]) - t)**2)\n        style_loss += w * gram_loss\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "accuracy_score(test_target, clf_predictions)\n", "intent": "After the model has been created it's time to evaluate it's performance on new data that it hasn't been trained on.\n"}
{"snippet": "metrics.accuracy_score(test_target, clf_predictions)\n", "intent": "After the model has been created it's time to evaluate it's performance on new data that it hasn't been trained on.\n"}
{"snippet": "print 'Recall: %.4g' % metrics.recall_score(clf_y_test, gbm_clf.predict(clf_X_test))\nprint 'Precision: %.4g' % metrics.precision_score(clf_y_test, gbm_clf.predict(clf_X_test))\nprint 'F1 Score: %.4g' % metrics.f1_score(clf_y_test, gbm_clf.predict(clf_X_test))\n", "intent": "In this step I will be comparing the results of my regression path to the results of my classification path using recall.\n"}
{"snippet": "x_test = np.array(['You only live once'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "class CustomMetric(object):\n    def get_final_error(self, error, weight):\n        return 0.0\n    def is_max_optimal(self):\n        return True\n    def evaluate(self, approxes, target, weight):\n        return 0.0, 0.0\n", "intent": "Create an object that implements the following interface:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nmodel = CatBoostClassifier(iterations=400, loss_function='MultiClass', custom_loss='Accuracy')\nscores = cross_val_score(model, x_tr, y_tr, scoring='accuracy', n_jobs=-1, fit_params=None)\n", "intent": "each of array contains loss in each iteration (like staged_predict)\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once we have trained the model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "valid_predictions = predict(images_valid[:4])\nvalid_predictions += coords_mean\nvalid_predictions *= coords_max\ntrue_locations = coords_valid[:4]\ntrue_locations += coords_mean\ntrue_locations *= coords_max\nvalid_images_display = images_valid[:4] + image_mean\nvalid_images_display *= image_max\n", "intent": "So how does a model perform?  Let's visualise the predictions on the images against the ground truths  \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:\n"}
{"snippet": "from evaluate import *\nevaluate(prediction)\n", "intent": "For automating the evaluation of your predictions, call the evaluate function that is defined in the `evaluate.py` script:\n"}
{"snippet": "a = []\nfor prediction in ols_model.predict(finaltrain_df):\n    if prediction <( days_list.mean() - days_list.std()):\n        a.append(True)\n    else:\n        a.append(False)\nfinaltrain_df.loc[:,'ols_pred'] = (a)\nfinaltrain_df[['ols_pred']] = finaltrain_df[['ols_pred']].astype(float)\nfinaltrain_df.head()\n", "intent": "We then get the predictions on the entire dataset made by the ols regression and append to our dataframe\n"}
{"snippet": "cod_multi = r2_score(y_test_multi, y_pred_multi, multioutput='variance_weighted')\nrms_multi = mean_squared_error(y_test_multi, y_pred_multi) ** 0.5\nmae_multi = mean_absolute_error(y_test_multi, y_pred_multi)\ncc_multi = np.corrcoef(y_test_multi, y_pred_multi)[1][0] \nprint(\"Metric of the multivariate linear regression\\n\")\nprint(\"Coefficient of Determination:\\t \" ,cod_multi) \nprint(\"Root Mean Square Error:\\t\\t \" ,rms_multi)\nprint(\"Median Absolute error:\\t\\t \" ,mae_multi)\nprint(\"Correlation Coefficient:\\t \" ,cc_multi)\nprint(\"---------------------------------------------------------\")\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "Resnet50_Predictions = [np.argmax(dog_classifier.predict(np.expand_dims(features, axis=0))) for features in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_Predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_Predictions)\nprint('\\nTest accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "The mean error is worse than linear-regression. Cross-validation had proven that the initial estimate is overfitting.\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n", "intent": "Here's a visualization of our new results:\n"}
{"snippet": "y_pred_full = lr_under.predict(X_test)\nprint(recall_score(y_test,y_pred_full))\nprint(accuracy_score(y_test,y_pred_full))\n", "intent": "It also generalises good enough for full data.\n"}
{"snippet": "validation_prediction = GBC.predict(second_level_input)\nprint('Blended Model Validation: ',round(accuracy_score(validation_prediction, y_validation) * 100, 2))\n", "intent": "scikit-learn Gradient Boosted Classifier\n"}
{"snippet": "X_test = generate_a_disk()\nX_test = X_test.reshape(1, X_test.shape[0])\nprint(model.predict(X_test))\nprint(model_2.predict(X_test))\n", "intent": "We arrive to a 1 accuracy even faster. We can check our classifiers with a newly generated image:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "MSE_mult = mean_squared_error(Y_mult, predict_fit_mult)\nRMSE_mult = math.sqrt(MSE_mult)\nprint('RMSE: ', RMSE_mult)\nMAE_mult = mean_absolute_error(Y_mult, predict_fit_mult)\nprint('MAE: ', MAE_mult)\nCC_mult = np.corrcoef(Y_mult, predict_fit_mult)\nprint('CC: ', CC_mult)\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "clf.predict([[ 5.0,  3.6,  1.3,  0.25]])\n", "intent": "Once we have learned from the data, we can use our model to predict the most likely outcome on unseen data:\n"}
{"snippet": "y_training_pred = clf.predict(x_training)\naccuracy = accuracy_score(y_training, y_training_pred)\nmacro_f1 = f1_score(y_training, y_training_pred, average='macro')\nprint('accuracy = %s' % (accuracy))\nprint('macro f1 score = %s' % (macro_f1))\n", "intent": "What if we had been foolish and evaluated our performance on the training data?\n"}
{"snippet": "predict('images/photo1.jpg')\n", "intent": "Lets give it a try :\n"}
{"snippet": "predictions = logreg.predict(x_test)\n", "intent": "We now predict the test data with our trained model.\n"}
{"snippet": "print(\"Best Parameters Selected: \", logreg.best_params_)\nprint(\"Accuracy score for the prediction: \", accuracy_score(y_test, predictions))\nprint(\"Confusion Matrix:\") \nprint(confusion_matrix(y_test, predictions))\n", "intent": "The best parameters selected by GridSearchCV and the prediction scores for Logistic Regression are as follows:\n"}
{"snippet": "print(\"Accuracy score for the prediction: \", accuracy_score(y_test, svm_soft_predictions))\nprint(\"Confusion Matrix:\") \nprint(confusion_matrix(y_test, svm_soft_predictions))\n", "intent": "Some prediction scores are then obtained.\n"}
{"snippet": "svm_hard_predictions = svm_hard.predict(x_test)\n", "intent": "The value of C is selected as 100 and the kernel is selected as polynomial by GridSearchCV. We then make predictions based on the test data.\n"}
{"snippet": "print(\"Accuracy score for the prediction: \", accuracy_score(y_test, svm_hard_predictions))\nprint(\"Confusion Matrix:\") \nprint(confusion_matrix(y_test, svm_hard_predictions))\n", "intent": "The accuracy score for the prediction and the confusion matrix is then obtained.\n"}
{"snippet": "print(\"Accuracy score for the prediction: \", accuracy_score(y_test, ridge_predictions))\nprint(\"Confusion Matrix:\") \nprint(confusion_matrix(y_test, ridge_predictions))\n", "intent": "The following scores are obtained:\n"}
{"snippet": "predictions = logreg.predict(x_test)\n", "intent": "The test data is then used to predict the class with the trained model.\n"}
{"snippet": "print(\"Best Parameters Selected: \", logreg.best_params_)\nprint(\"Accuracy score for the prediction: \", accuracy_score(y_test, predictions))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, predictions))\n", "intent": "The best parameters selected by GridSearchCV, the accuracy score of the model and the confusion matrix are then obtained as follows.\n"}
{"snippet": "print(\"Accuracy score for the prediction: \", accuracy_score(y_test, svm_predictions))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, svm_predictions))\n", "intent": "The accuracy score and the confusion matrix for the model are then obtained.\n"}
{"snippet": "ridge_predictions = ridge.predict(x_test_normalized)\nridge_predictions = ridge_predictions.tolist()\nfor i in range(0, len(ridge_predictions), 1):\n\tif ridge_predictions[i] < 1 or ridge_predictions[i] > 1.5 or ridge_predictions[i] > 2.5:\n\t\tridge_predictions[i] = ceil(ridge_predictions[i])\n\telse:\n\t\tridge_predictions[i] = floor(ridge_predictions[i])\nridge_predictions = np.asarray(ridge_predictions)\n", "intent": "We then predict the class and normalize the results to obtain the accuracy score and confusion matrix.\n"}
{"snippet": "print(\"Accuracy score for the prediction: \", accuracy_score(y_test, ridge_predictions))\nprint(\"Confusion Matrix:\") \nprint(confusion_matrix(y_test, ridge_predictions))\n", "intent": "The accuracy score and confusion matrix for the prediction are then obtained.\n"}
{"snippet": "ols_predictions = ols.predict(x_test)\n", "intent": "We then use the trained model to predict the test data.\n"}
{"snippet": "print(\"Mean squared error: \", mean_squared_error(y_test, ols_predictions))\nprint('r^2 score: ', r2_score(y_test, ols_predictions))\n", "intent": "The mean square error and r^2 score are obtained for the predictions.\n"}
{"snippet": "lasso_predictions = lasso.predict(x_test)\n", "intent": "As we can see, the best value of alpha was selected to be 2. We now use the trained model to predict our test data.\n"}
{"snippet": "print(\"Mean squared error: \", mean_squared_error(y_test, lasso_predictions))\nprint('r^2 score: ', r2_score(y_test, lasso_predictions))\n", "intent": "The mean square error and r^2 score are then obtained as follows.\n"}
{"snippet": "ridge_predictions = ridge.predict(x_test_normalized)\n", "intent": "The best parameters selected by GridSearchCV can be observed above. The trained model is then used to predict the test values.\n"}
{"snippet": "print(\"Mean squared error: \", mean_squared_error(y_test, ridge_predictions))\nprint('r^2 score: ', r2_score(y_test, ridge_predictions))\n", "intent": "The mean square error and r^2 scores are then obtained.\n"}
{"snippet": "def predict_groups(models, X_train, y_train):\n    for model in models:\n        model_score = cross_val_score(models[model], X_train, y_train, cv=kfold, scoring='accuracy')\n        print(\"\\n{}:\\n\\tAccuracy: {} ({})\".format(model, model_score.mean(), model_score.std()))\n", "intent": "They reported only accuracy scores so I will do the same.\n"}
{"snippet": "cross_val_score(estimator=pipe, X=X, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)\n", "intent": "And this is where it breaks:\n"}
{"snippet": "def huber_loss(labels, predictions, delta=1.0):\n    residual = tf.abs(predictions - labels)\n    condition = tf.to_float(tf.less(residual, delta))\n    small_res = 0.5 * tf.square(residual)\n    large_res = delta * residual - 0.5 * tf.square(delta)\n    return condition * small_res + (1 - condition) * large_res\n", "intent": "Step 5a: implement Huber loss function from lecture and try it out\n"}
{"snippet": "def content_loss_func(sess, model):\n    def _content_loss(p, x):\n        N = p.shape[3]\n        M = p.shape[1] * p.shape[2]\n        return (1.0 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2))\n    return _content_loss(sess.run(model['conv4_2']), model['conv4_2'])\n", "intent": "Define the equation (1) from the paper to model the content loss. We are only concerned with the \"conv4_2\" layer of the model.\n"}
{"snippet": "def classify(tfidf, classifier, test_data, english_only=True):\n    text = test_data[\"text\"]\n    if english_only:\n        def filter_text(L): return list(filter(lambda s: s in english_words_no_bad, L))\n        text = text.apply(filter_text)\n    strings = text.apply((\" \").join)\n    X = tfidf.transform(strings)\n    return classifier.predict(X.toarray())\n", "intent": "It looks like we will want to use a linear kernel in our model.\n"}
{"snippet": "predictions_new_season_noluck = model_noluck.predict_proba(X_test)\nprint '\\n'\nperformance_new_season_noluck_df = pd.concat([\n        feats_import.iloc[season15_end+1:,7].apply(min, args=(cutoff_GD,)).apply(max, args=(-cutoff_GD,)),\n        pd.Series(data=map(np.round, map(exp_score, predictions_new_season_noluck)), name='EXP', index=feats_import.iloc[season15_end+1:].index)], axis=1)\nsuccess_new_season_noluck_res_df = pd.crosstab(performance_new_season_noluck_df.loc[:, \"FTGD\"].apply(np.sign), performance_new_season_noluck_df.loc[:, \"EXP\"].apply(np.sign))\nprint \"Success identifying H, D, A is \" + str(round(100.* np.trace(success_new_season_noluck_res_df)/len(X_test),2)) + \" percent\"\nsuccess_new_season_noluck_res_df\n", "intent": "Let us now check by how much these percentages improve, if we use the more sophisticated deep neural network:\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\n", "intent": "Evaluate model on test data\n"}
{"snippet": "def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content))\nlayer_features = layers['block2_conv2']\ncontent_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(content_image_features,\n                                      combination_features)\n", "intent": "The content loss is the (scaled, squared) Euclidean distance between feature representations of the content and combination images.\n"}
{"snippet": "ResNet50_local_predictions = [np.argmax(ResNet50_model_local.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50_local]\ntest_accuracy = 100*np.sum(np.array(ResNet50_local_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_local_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "knn.predict_proba(x_test)\n", "intent": "3) The KNeighborsClassifier also has a [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "accuracy_score(y_test,y_pred)\n", "intent": "- Evaluate the accuracy of the model. \n"}
{"snippet": "for i,j in zip(X_test,Y_test):\n    if (str(j) != str(svm.predict(i)[0])):\n        print(\"\\033[1;31;47m Actual value is: {:15} <---> Predicted value is: {:20} \\n\".format(str(j), str(svm.predict(i)[0])))\n    else:\n        print('\\033[1;32;47m Actual value is: {:15} <---> Predicted value is: {:20} \\n'.format(str(j), str(svm.predict(i)[0])))\n", "intent": "Value by value comparision. For easier interpretation.\n"}
{"snippet": "df= test[['Age', 'Pclass', 'SibSp', 'Parch', 'Fare']]\nunknown_age = df[df.Age.isnull()].as_matrix()\nestimated_age = regr_age.predict(unknown_age[:, 1:])\ntest.loc[test.Age.isnull(), 'Age'] = estimated_age\ntest.head(5)\n", "intent": "We use the regression model obtained from the training set to predict the missing age in the test set.\n"}
{"snippet": "recall_score(acum_true_classes_int, acum_predicted_classes_int, average='macro')\n", "intent": "The proportion of positives that are correctly identified as such.\n$$RECALL=\\frac{TP}{P}$$\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_predict = clf.predict(X_train)                   \nprint ('Training accuracy = ', accuracy_score(y_train, y_predict))\nclass_labels = clf.get_classes()\nprint ('Class labels=', class_labels)\nprint ('Confusion matrix [known in lines, predicted in columns]=',confusion_matrix(y_train, y_predict, class_labels))\n", "intent": "Compute the training accuracy.\n"}
{"snippet": "print('MSE for training response data: {:.2f}'.format(np.mean((svc_clf.predict(x_train) - y_train) **2)))\nprint('MSE for test response data: {:.2f}'.format(np.mean((svc_clf.predict(x_test) - y_test) ** 2)))\nscore = cross_val_score(svc_clf, x_data, y, cv = 5, scoring = 'neg_mean_squared_error')\nprint('Cross-validation score = {:.2f}'.format(np.mean(np.abs(score))))\n", "intent": "RBF kernel results in much better accuracy.\n"}
{"snippet": "def print_rmse(model, name, input_fn):\n  metrics = model.evaluate(input_fn=input_fn, step = 1)\n  print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, 'validation', get_valid)\n", "intent": "<h2> Evaluate model </h2>\n"}
{"snippet": "yhat = clf.predict(X_test)\nyhat [0:10]\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the mean squared error (MSE):\n$$ MSE = \\frac{1}{N}\\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Finding the accuracy of the model on the test-dataset.\n"}
{"snippet": "dense_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_features]\ntest_accuracy = 100 * np.sum(np.array(dense_predictions)==np.argmax(test_targets, axis=1)) / len(dense_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Getting the test accuracy for Model Architecture 1.\n"}
{"snippet": "resnet_gap.load_weights('saved_models/weights.best.tl.hdf5')\ngap_predictions = [np.argmax(resnet_gap.predict(np.expand_dims(feature, axis=0))) for feature in test_features]\ntest_accuracy = 100 * np.sum(np.array(gap_predictions)==np.argmax(test_targets, axis=1)) / len(gap_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Getting the test accuracy for Model Architecture 2.\n"}
{"snippet": "predictions = np.array(model.predict(testX))[:,0]\nmse = ((testY[:,0] - predictions) ** 2).mean(axis=0)\nprint(\"MSE: \", mse)\n", "intent": "Run the network on the test set to measure its performance. \n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Mean Absolute Error, Mean Squared Error, and Root Mean Squared Error (RMSE) are options.\n"}
{"snippet": "preds = model.predict(data[nrows:])\nprint(preds.shape)\n", "intent": "Make predictions on the validation sample. These predictions will be submitted to Kaggle for scoring.\n"}
{"snippet": "preds_dev = model.predict(data[:nrows])\nprint(preds_dev.shape)\n", "intent": "Finally, let's create predictions on our development sample. We could use these predictions in an ensemble method down the road.\n"}
{"snippet": "features = pd.Series(xg_toxic.get_booster().get_score(importance_type='gain')).sort_values(ascending=False).to_frame().reset_index()\nfeatures.columns = ['Feature','Importance']\nfeatures['ColInd'] = features['Feature'].str[1:].astype(int)\nf_ind = features['ColInd'].values\nprint(f_ind.shape)\n", "intent": "Trim the dataset down to only the important predictors.\n"}
{"snippet": "model_error = ps.spreg.GM_Error(y=transformed_y.values, \\\n                                x=transformed_x.values, \\\n                                w=rW, \\\n                                name_y='transformed_average_price', \\\n                                name_x=name_x)\nprint(model_error.summary)\n", "intent": "**There are several over spatial regression methods built into PySAL**\n**Spatial Error is when the error values in the models are spatially lagged**\n"}
{"snippet": "y_pred = logreg.predict(X_test)\n", "intent": "**Step 4 :** Make predictions on the testing set\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred)\n", "intent": "Print classification accuracy\n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision_score(target_test, target_predicted)\n", "intent": "We can use a different score:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nX_all, y_all = getTrainData()\ncv = ShuffleSplit(n_splits=2, test_size=0.1, random_state=37)\nensembler = HandGestureEnsembler(all_pipelines)\nscores = cross_val_score(ensembler,X_all, y_all, cv=cv)\nscores\n", "intent": "We will try to get some cross validation scores\n"}
{"snippet": "x_test = np.array(['No mamma'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0, dtype=tf.float32)\n    for idx, layer in enumerate(style_layers):\n        gram = gram_matrix(feats[layer])\n        style_loss += style_weights[idx] * tf.reduce_sum(tf.square(gram - style_targets[idx]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test_preds = X_test_level2[:,0]*best_alpha + X_test_level2[:,1]*(1-best_alpha)\nr2_test_simple_mix = r2_score(y_test,test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = cls_lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2,train_preds)\ntest_preds = cls_lr.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test,test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "with model:\n    X_mask = pm.Normal('X_mask', mu=Xmu, sd=1., shape=(n1, 2))\n    mu2 = alpha + tt.dot(X_mask, beta)\n    y2 = pm.Normal('y2', mu=mu2, sd=sd, shape=(n1,))\n    ppc2 = pm.sample_ppc(trace, vars=[y2], samples=200)\nax0, ax1 = plot_predict(ppc2['y2'], y_test)\nax1.set_ylim(0, .8);\n", "intent": "When there is no missing information in $X_{new}$, we can see the prediction uncertainty is small.\n"}
{"snippet": "print(np.sum((bos.PRICE - lm.predict(X)) ** 2)/len(bos.PRICE))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = clf.predict(X)\nr2_score(y,clf.predict(X))\n", "intent": "Make prediction on the sample data and calculate the R Squared value\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "Thats a pretty good accuracy score\n"}
{"snippet": "prediction = regr.predict(x_test)\n", "intent": "Get prediction for the testing set:\n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "**Using the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_new_pp = y_new - y_tr_avg  \nX_new_pp = X_new - X_tr_avg  \navg_pred_err2 = np.mean(np.abs(lr_no_b2.predict(X_new_pp) - y_new_pp))\nprint(\"Prediction error on a new set of data after legit preprocessing: {:2.3f}\".format(avg_pred_err2))\n", "intent": "As we can see, the test error in this case is higher than before on the few data provided. Let's try again the out-of-sample test.\n"}
{"snippet": "train_new = np.where(extra_optim.predict_proba(X_train)[:, 1] > 0.1, 1, 0)\n", "intent": "Seems that around 0.3 is the sweet spot\n"}
{"snippet": "def loss_poisson(target, predictions):\n    return tf.reduce_mean(tf.nn.log_poisson_loss(target, predictions))\n", "intent": "We can create our own loss function and use it to train the model\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Getting predictions..\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = nb.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names = iris.target_names))\n", "intent": "---\n- Classification reports\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_test_predicted = rf_model.predict(X_test)\nprint accuracy_score(y_test, y_test_predicted)\n", "intent": "Now we'll compute accuracy of prediction of test set.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test1.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "pred=model.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))\nprint('\\n')\nprint('Accuracy score is: ', accuracy_score(y_test, pred))\n", "intent": "**Part 6: Model Evaluation**\n"}
{"snippet": "def evaluate(model, y):\n    correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    return tf.reduce_mean(tf.cast(correct, tf.float32))\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "XtestNN = nnTrain(XtestScaled, look_back)\npredNN = model.predict(XtestNN)\nyHatNN = np.array([])\nfor pred in predNN[:-1]:\n    yHatNN = np.append(yHatNN, pred[0])\nyHatNN = np.append(yHatNN, predNN[-1])\nyHatNN = yHatNN.reshape(len(yHatNN), 1)\nplotFc(yHatNN, \"LTSM\")\n", "intent": "Finally forcast the test data.\n"}
{"snippet": "slr.predict(5)\n", "intent": "y = 215.1933 + 2349.729 * x\n"}
{"snippet": "scores = cross_val_score(knn, X, y, scoring='accuracy', cv=10)\nprint scores\nprint scores.mean()\n", "intent": "The overall Decision Tree algorithm accuracy is **55.5%**\n"}
{"snippet": "scores = cross_val_score(lr, X, y, scoring='accuracy', cv=10)\nprint scores\nprint scores.mean()\n", "intent": "The overall K-Nearest Neighbor accuracy is **53.02%**\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"0015.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "x_test = np.array(['What a nice gift '])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(train_set)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "note_predictions = classifier.predict(X_test)\n", "intent": "** Using the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p5(x))\nprint('r2 = {:.3f}'.format(r2))\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\nResNet50_test_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % ResNet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred_train = model.predict_proba(df_train_final2_train.iloc[:, 2:])\nscore = log_loss(df_train_final2_train[\"TripType\"], pred_train)\nprint 'Logloss for the training set: ' + str(score)\n", "intent": "Figure out the log loss value for the training set.\n"}
{"snippet": "pred_train = model.predict_proba(df_train_final2_test.iloc[:, 2:])\nscore = log_loss(df_train_final2_test[\"TripType\"], pred_train)\nprint 'Logloss for the validation set: ' + str(score)\n", "intent": "Figure out the log loss value for the held out test set.\n"}
{"snippet": "Y_predicted = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "Y_predicted = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print classification_report(Y_test, Y_predicted)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "df_predictions_deep = df_train.copy()\ndf_predictions_deep['y'] = clf_deep.predict(df_train[['X1','X2']])\n", "intent": "This is a model with a higher degree of complexity. Once again, create predictions for the deep tree (and put them inside a data frame).\n"}
{"snippet": "df_predictions_superdeep = df_train.copy()\ndf_predictions_superdeep['y'] = clf_superdeep.predict(df_train[['X1','X2']])\n", "intent": "Evaluate predictions.\n"}
{"snippet": "df_predictions_crazy = df_train.copy()\ndf_predictions_crazy['y'] = clf_crazy.predict(df_train[['X1','X2']])\n", "intent": "And now evaluate this tree.\n"}
{"snippet": "t_predictions = titanic_train.copy()\nt_predictions['Survived'] = clf_t.predict(titanic_train.drop('Survived',axis=1))\nh_predictions = titanic_holdout.copy()\nh_predictions['Survived'] = clf_t.predict(titanic_holdout.drop('Survived',axis=1))\n", "intent": "- Using your trained model, make two sets of predictions, one for the training set, the other for the holdout set.\n"}
{"snippet": "np.exp(lasso_log.predict(train_num_df))\n", "intent": "**Sample Output for Lasso Model Prediction after reversing natural log of price**\n"}
{"snippet": "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))\n", "intent": "Let's focus on logistic regression, and vizualise the accuracy, recall and confusion matrix of this model:\n"}
{"snippet": "trainer.predict([tensorl([3]), tensorl([6])])\n", "intent": "We can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user \n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "** Precision    Recall  F1-score   Support **\n"}
{"snippet": "print('accuracy %s' % accuracy_score(y_test, predictions))\ncm = confusion_matrix(y_test, predictions)\nprint('confusion matrix\\n %s' % cm)\n", "intent": "** Accuracy Confusion metrics **\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake).data.cpu().numpy()\n    g_loss = ls_generator_loss(score_fake).data.cpu().numpy()\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * torch.sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        style_loss += style_weights[i] * tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i]) ** 2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "sz = X_train.shape[0]//200\nx1 = np.concatenate([np.random.permutation(X_train)[:sz], CNN_G.predict(noise(sz, x_len))])\nx1.shape\n", "intent": "We train the discriminator so it can at least tell real images from random noise\n"}
{"snippet": "def plot_gen(G, n_ex=16):\n    plot_multi(G.predict(noise(n_ex)).reshape(n_ex, 28, 28), cmap='gray')\n", "intent": "Plot a bunch of generated images\n"}
{"snippet": "def data_D(sz, G):\n    real_img = X_train[np.random.randint(0, n_train, size=sz)]\n    fake_img = G.predict(noise(sz))\n    X = np.concatenate((real_img, fake_img))\n    labels = [0]*sz + [1]*sz\n    return X, labels\n", "intent": "Create a batch of 50% real and 50% fake/ generated data with appropriate labels(0=real/1=fake)\n"}
{"snippet": "input = pd.DataFrame.from_dict(data =\n                              {'dayofweek' : [4,5,6],\n                               'mintemp' : [60,15,60],\n                              'maxtemp' : [80,80,65],\n                               'rain' : [0,0.8,0]})\nestimator = tf.contrib.learn.DNNRegressor(model_dir='/tmp/trained_model',\n                                         hidden_units=[5])\npred = estimator.predict(input.values)\nprint pred\n", "intent": "- Use the model on new data and create dictionary with predictor variables:\n"}
{"snippet": "def crossValidation(model, X, y, cv=5):\n    scores = cross_val_score(model, X, y, cv=cv)\n    print(\"Average CrossValidation Score of %0.2f runs: %0.5f\\n\" %(cv, scores.mean()*100))\n", "intent": "***crossValidation()*** function takes a trained *model, X, y values* and prints the cross validation score\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(clf, X_train_all_scale[:,indices], y_train_all, scoring='neg_log_loss', cv = 10) \ncv_scores\n", "intent": "Outputs the log loss on a test test set k times. \n"}
{"snippet": "Resnet50_predictions = [np.argmax(rn_dog.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred=dtc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(km.labels_,CD['Cluster']))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred=logm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred=nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred))\nprint('/n')\nprint(confusion_matrix(y_test,y_pred))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "y_pred=pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y_pred=list(classifier.predict(X_test))\ny_pred\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "cv_scores=[]\nfor l in lr:\n    XGB=XGBClassifier(learning_rate=l)\n    scores=cross_val_score(XGB,X_train_tfidf,y_train,cv=3,scoring='accuracy')\n    cv_scores.append(scores.mean())\n", "intent": "<h3> 4.5.3 XGB Classifier </h3>\n"}
{"snippet": "test_len=len(y_test)\npredicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)\n", "intent": "<h2> 5.1 Random Model </h2>\n"}
{"snippet": "sumbit = best_model.predict(test)\n", "intent": "<h3>Make the Predictions</h3>\n"}
{"snippet": "pred_r = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,pred_r))\nprint(confusion_matrix(y_test,pred_r))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print(confusion_matrix(colleges['Cluster'],kmeans.labels_))\nprint(classification_report(colleges['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "print(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "print((TP) / (TP + FN))\nprint(metrics.recall_score(train_data.Positive_Result, train_data.predicted))\n", "intent": " * How 'sensitive' or selective is the model with detecting a positive instance?\n"}
{"snippet": "print(TP / (TP + FP))\nprint(metrics.precision_score(train_data.Positive_Result, train_data.predicted))\n", "intent": " * How precise the classifier is when selecting a positive instance?\n"}
{"snippet": "y_unknown = clf.predict(well_features)\nwell_data['Facies'] = y_unknown\nwell_data\n", "intent": "Finally we predict facies labels for the unknown data, and store the results in a `Facies` column of the `test_data` dataframe.\n"}
{"snippet": "acc = accuracy_score(labels,preds)\nprint(acc)\n", "intent": "<p>Let us now evaluate our accuracy:</p>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(ytest, y_model)\nacc = acc*100\nprint(\"%.2f\" % acc, \"%\")\n", "intent": "We can use ``accuracy_score`` utility to see the fraction of predicted labels that match their true value\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(ytest, y_model)\nprint(\"%.2f\" %acc, \"%\")\n", "intent": "We predicted our model. Let's check the accurancy\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n", "intent": "Finally, we computer the fraction of correctly labeled points\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [.6, -14] + [14, 18] * rng.rand(2000,2)\nynew = model.predict(Xnew)\n", "intent": "We generate some data and try to predict the labels\n"}
{"snippet": "yprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n", "intent": "Naturally, the Bayesian formalism allows probabilistic classification, which we can computer with using the ``preict_proba`` method:\n"}
{"snippet": "from spotlight.evaluation import rmse_score\ntrain_rmse = rmse_score(model, train)\ntest_rmse = rmse_score(model, test)\nprint('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))\n", "intent": "Now that the model is estimated, how good are its predictions?\n"}
{"snippet": "def RSS(weights, feature_matrix, target):\n    prediction = predict(weights, feature_matrix)\n    return ((prediction - target) ** 2).sum()\n", "intent": "$J(W) = \\sum (WX - Y)^2$\n"}
{"snippet": "def gradient_descent(feature_matrix, target, initialize_weights=lambda x: (np.zeros(x.shape[1])),\n                     alpha=1e-12, iterations=1000):\n    current_weights = initialize_weights(feature_matrix)\n    for i in range(1, iterations):\n        prediction = predict(current_weights, feature_matrix)\n        error = prediction - target\n        gradient = (error @ feature_matrix)\n        current_weights -= alpha * gradient\n    return current_weights\n", "intent": "$\\frac{\\partial J}{\\partial W} = (WX - Y) X$\n"}
{"snippet": "ytest_pred = svr_linear.predict(Xs_test)\nRSStest = np.mean(((ytest_pred - ys_test)**2) / (np.std(ys_test)**2))\nprint(\"Normalized RSS by linear kernel is {0:f}\".format(RSStest))\n", "intent": "Calculating the RSS for test data\n"}
{"snippet": "ytest_pred = svr_rbf.predict(Xs_test)\nRSStest = np.mean(((ytest_pred - ys_test)**2) / (np.std(ys_test)**2))\nprint(\"Normalized RSS by rbf kernel is {0:f}\".format(RSStest))\n", "intent": "Calculating the RSS for test data\n"}
{"snippet": "lasso_rmse = np.mean(np.sqrt(-cross_val_score(lasso, X_train, y, scoring='neg_mean_squared_error', cv=10)))\nlasso_rmse\n", "intent": "We can check the RMSE of our lasso model on the training data.\n"}
{"snippet": "TL_predictions = [np.argmax(TL_model.predict(np.expand_dims(feature, axis=0))) for feature in test_TL_model]\ntest_accuracy = 100*np.sum(np.array(TL_predictions)==np.argmax(test_targets, axis=1))/len(TL_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "confMatrix = metrics.confusion_matrix(validLabels, clf.predict(validImages))\nprint confMatrix\n", "intent": "A confusion matrix is a handy table that helps visualize the accuracy of a classification algorithm. \n"}
{"snippet": "def log_likelihood(clf, x, y):\n    prob = clf.predict_log_proba(x)\n    negatives = y == False\n    positives = ~negatives\n    return prob[negatives, False].sum() + prob[positives, True].sum()\n", "intent": "We plan on using log-likelyhood as a scoring metric and write a support function to be able to do so.\n"}
{"snippet": "def log_likelihood(clf, x, y):\n    prob = clf.predict_log_proba(x)\n    negatives = y == False\n    positives = ~negatives\n    return prob[negatives, False].sum() + prob[positives, True].sum()\n", "intent": "We plan on using log-likelyhood as a scoring metric and write a support function to be able to do so\n"}
{"snippet": "from sklearn.metrics import roc_curve,roc_auc_score\ndef get_roc_curve(Model, features, labels):\n    probas=Model.predict_proba(features)[:,1]\n    fpr,tpr,thresholds=roc_curve(labels,probas)\n    return (fpr,tpr,thresholds)\ndef get_auc_score(Model, features, labels):\n    probas=Model.predict_proba(features)[:,1]\n    return roc_auc_score(labels,probas)\n", "intent": "Let us now define a couple of functions to get back the ROC Curve and the AUC score for any classifier\n"}
{"snippet": "Val_Rf_pred= best_Rf.predict(Val_X)\n", "intent": "Let us put use our best selected model to predict labels on our validation set.\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = clf.predict(x_test)\nprint (pd.crosstab(y_test.greater_than_50k\n                  ,y_pred\n                  ,rownames = ['Actual']\n                  ,colnames = ['Predicted']))\nprint ('\\n \\n')\nprint (classification_report(y_test.greater_than_50k,y_pred))\n", "intent": "Let's see how the model is performing\n"}
{"snippet": "ypred = result.predict(add_constant(X_valid))\nprint (mse(ypred,y_valid))\nypred_alternate = result_alternate.predict(add_constant(X_valid[:, 2]))\nprint (mse(ypred_alternate,y_valid))\n", "intent": "Usemos la data de test para probar el modelo.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n", "intent": "Evaluating the algorithm\n"}
{"snippet": "test_image = pics[22515]\ntest_image_batch = np.expand_dims(test_image, axis=0)\nresult = model.predict(test_image_batch, batch_size=1, verbose=1)\nlabelToText(result)\n", "intent": "Now let's test it again, with the weights we've trained over time on a faster system.\n"}
{"snippet": "feature_layers = ['block1_conv2', 'block2_conv2',\n                  'block3_conv3', 'block4_conv3',\n                  'block5_conv3']\nfor layer_name in feature_layers:\n    layer_features = layers[layer_name]\n    style_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_features, combination_features)\n    loss_chain += (style_weight / len(feature_layers)) * sl\n", "intent": "Now that we have our helper functions processed, let's continue adding to our loss_chain by adding the layers that will define our style loss.\n"}
{"snippet": "loss_chain += total_variation_weight * total_variation_loss(combination_image)\n", "intent": "and we finish off the loss_chain by adding a \"noisy\" loss helper to smooth out the results a bit...\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_]) \npreds[:,0], np.mean(preds[:,0]), y_valid[0]\n", "intent": "We'll grab the predictions for each individual tree, and look at one example.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.\n    for i in range(len(style_layers)):\n        loss += style_weights[i] * ((gram_matrix(feats[style_layers[i]]) - style_targets[i])**2).sum()\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0.\n    for i in range(len(style_layers)):\n        loss += style_weights[i] *\\\n                    tf.reduce_sum((gram_matrix(feats[style_layers[i]]) - style_targets[i])**2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "test['e_rec'] = RFreg_Energy.predict(test[features])                                                      \ntest['disp_rec'] = RFreg_Disp.predict(test[features])                                                     \ntest['src_x_rec'],test['src_y_rec'] = utils.disp_to_pos(test['disp_rec'],                                 \n                                                       test['x'],                                              \n                                                       test['y'],                                              \n                                                       test['psi'])\ntest.keys()\n", "intent": "<font size=\"4\">\nWe can now predict the **energy** and **disp** of the test events, and from **disp**, calculate the reconstructed direction.\n"}
{"snippet": "test['hadro_rec'] = RFcls_GH.predict(test[features_sep])\n", "intent": "<font size=\"4\">\nPredict the hadroness of the test events:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(model_res.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "mean_squared_error(predictions, test[target])\n", "intent": "    > compare predictions made from model to actual target column (average rating) in test data \n"}
{"snippet": "import numexpr as ne\ndef mandelbrot_numpy(c, maxiter):\n    output = np.zeros(c.shape)\n    z = np.zeros(c.shape, np.complex64)\n    for it in range(maxiter):\n        notdone = ne.evaluate('z.real*z.real + z.imag*z.imag < 4.0')\n        output[notdone] = it\n        z = ne.evaluate('where(notdone,z**2+c,z)')\n    output[output == maxiter-1] = 0    \n    return output\n", "intent": "We can avoid temporary array creation with NumExpr.\n"}
{"snippet": "y2 = regr.predict(X_test)\nSSRes = sum((y2-y_test)**2)\nSSTot = sum((y_test-np.mean(y_test))**2)\nr2 = 1 - SSRes/SSTot\nprint(r2)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "print('Sensitivity (TP / (TP + FN): ', round(TP / float(TP + FN), 2))\nprint('Sensitivity (scikit recall_score calculation) True positive, True Negative: ', recall_score(target, y_pred, average=None))\n", "intent": "198/747 or 27%\nSensitivity measures how good we are at catching positives, or how sensitive our model is to identifying positives.\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint('Precision (manual): ', round(TP / float(TP + FP), 2))\nprint('Precision (scikit precision_score): ', precision_score(target, y_pred, average=None))\n", "intent": "This measures how precise the classifier is when predicting positive instances.\n"}
{"snippet": "parameters = \nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))\n", "intent": "Build the model with no dropout and regualrization,   \nassign **learning_rate = 0.01** and **num_iter = 10000**\n"}
{"snippet": "predictions = linmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "* Evaluate on the test set\n* model.evaluate API returns the loss and an array of metrics defined while compiling the model\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y, y_pred_rf_whole)\n", "intent": "First; this is a regression problem.\nSecond; since our data is not linear we better use nonlinear regression models.\n"}
{"snippet": "accu = metrics.accuracy_score(ytest, ypred)\nconf_mat = metrics.confusion_matrix(ytest, ypred, labels=list(range(10)))\nprint \"accuracy =\", 100*accu, \"%\"\nprint conf_mat\nvisualize_confusion_matrix(conf_mat)\n", "intent": "__Question__ What is the accuracy of this classifier? Display its confusion matrix.\n"}
{"snippet": "print \"Values of C tested:\"\nfor c in clf.Cs_:\n    print \"%.2g\"  % c\nprint \"Accuracy =\", \"%.2g\"  % metrics.accuracy_score(ytest, ypred)\n", "intent": "__Question__ What values of C were tested? What is the accuracy now?\n"}
{"snippet": "accu = metrics.accuracy_score(ytest, ypred)\nconf_mat = metrics.confusion_matrix(ytest, ypred, labels=list(range(10)))\nprint \"accuracy =\", 100*accu, \"%\"\nprint conf_mat\nvisualize_confusion_matrix(conf_mat)\n", "intent": "__Question__ What is the accuracy of this classifier? Display its confusion matrix. How does it compare to the logistic regression?\n"}
{"snippet": "accu = metrics.accuracy_score(ytest, ypred)\nconf_mat = metrics.confusion_matrix(ytest, ypred, labels=list(range(10)))\nprint \"accuracy =\", 100*accu, \"%\"\nprint conf_mat\nvisualize_confusion_matrix(conf_mat)\n", "intent": "C's value is 1.0 (default)\n"}
{"snippet": "accu = metrics.accuracy_score(ytest, ypred)\nconf_mat = metrics.confusion_matrix(ytest, ypred, labels=list(range(10)))\nprint \"accuracy =\", 100*accu, \"%\"\nprint conf_mat\nvisualize_confusion_matrix(conf_mat)\n", "intent": "it is better than svm but worse than knn\n"}
{"snippet": "def mean_square_error(X, y, beta):\n    return \nmse = mean_square_error(X_bt, y, beta)\nprint('MSE: %.02f RMSE: %.02f' % (mse, np.sqrt(mse)))\n", "intent": "**Question:** To quantify the error, we can use the mean square error (MSE). Implement this the quadratic loss function given above:\n"}
{"snippet": "from numpy import log, exp\ndef log_loss(X, y, beta):\n    return \n", "intent": "**Question:** Implement the loss function of a logistic regression:\n"}
{"snippet": "mseFull = np.mean((bos.PRICE - lm.predict(X))**2)\nprint mseFull\n", "intent": "You can notice that there is some error in  the prediction as the housing prices increase.\nLets calculate the mean squared error.\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train),y_train),\n           rmse(m.predict(X_valid),y_valid),\n           m.score(X_train,y_train),m.score(X_valid,y_valid)]\n    if hasattr(m,'oob_score_'): res.append(m.oob_score_)\n    print(\"Training RMSE:%f\\n Validation RMSE:%f\\nTraining Accuracy:%f\\nValidation Accuracy:%f\\n\" %(res[0],res[1],res[2],res[3]))\n", "intent": "This function is required to calculate the performance between training and validating sets.\n"}
{"snippet": "quare_meansquare_error(y_train, y_test, X_train, X_test, \"Random Forest Regression tree\", rfr_best)\n", "intent": "* MSE in train and test data, R2 in train and test data\n"}
{"snippet": "predictions = lm.predict(x)\n", "intent": "PREDICT y BASED ON x.\n"}
{"snippet": "ridge_resid = y_test - ridge.predict(X_test)\n", "intent": "Residuals: we are predicting y by using X-test sample and deducting the predicted y from actual Y values in \ntest sample to get residuals\n"}
{"snippet": "preds = lin_reg.predict(train_X_prepared)\nmse = mean_squared_error(train_y, preds)\nrmse = np.sqrt(mse)\nprint('Prediction error of ${0:.{1}f}'.format(rmse, 0))\n", "intent": "Measure the accuracy with the root mean squared error (RMSE). For now test it on the training set.\n"}
{"snippet": "recall_score(y_train_5, y_train_pred_90)\n", "intent": "However, this is at the cost of recall. For example, we can get x% precision classifier by simply adjusting the threshold but the recall will suffer!\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Similar to before you want to use cross-validation to evaluate classifier:\n"}
{"snippet": "model.load_weights('model/weights/twitter_pre_trained_glove_model.h5')\nmodel.evaluate(x_test, y_test)\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "lr_params = {\"C\": 4, \"dual\": True}\npredictor = LogisticPredictor(**lr_params)\nstratified_cv_loss = predictor.evaluate(train_x, train_ys, method='stratified_CV')\ncv_loss = predictor.evaluate(train_x, train_ys, method='CV')\nsplit_loss = predictor.evaluate(train_x, train_ys, method='split')\nprint(\"CV Stratified CV log loss: {}\\nCV log loss: {}\\nSplit CV log loss: {}\".format(stratified_cv_loss, cv_loss, split_loss))\n", "intent": "Let's how our reduced input does using an (untuned) classifier from `sklearn`.\n"}
{"snippet": "print \"Prediction at x = 15 is \" + \"{0:.2e}\".format(ridgepoly.predict([[15]])[0]) + \" when the true value is 15.\"\n", "intent": "Be careful if the test set is quite different from the training set.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n", "intent": "5) Evaluate your system on the test set\n"}
{"snippet": "content_cost = content_loss(content_features,gen_image_features[content_layer])\nstyle_cost = style_loss(style_features,gen_image_features,Style_layer)\ntotal_cost =  total_loss(content_cost,style_cost,alpha,beta)\ntrain = tf.train.AdamOptimizer(learning_rate=5).minimize(total_cost,var_list = [gen_image])\n", "intent": "Now that we have all the required feautures, we add nodes to compute all the losses and the  Adam optimizer.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('\\n', classification_report(np.argmax(test_targets, axis=1), diagnosis_predictions, target_names=['CNV','DME','DRUSEN','NORMAL']))\n", "intent": "- precision    \n- recall  \n- f1-score   \n- support\n"}
{"snippet": "testPredict = []\nfor xt in X_train:\n    testPredict.append(decisionTreeClassifier.predict([xt]))\nprint(metrics.accuracy_score(Y_train , testPredict) * 100 , \"%\") \n", "intent": "Decision Tree Training Set accuracy\n-----------\n"}
{"snippet": "testPredict = []\nfor xt in X_test:\n    testPredict.append(decisionTreeClassifier.predict([xt]))\nprint(metrics.accuracy_score(Y_test , testPredict) * 100 , \"%\") \n", "intent": "Decision Tree Test Set accuracy\n-----------\n"}
{"snippet": "testPredict = []\nfor xt in X_validation:\n    testPredict.append(decisionTreeClassifier.predict([xt]))\nprint(metrics.accuracy_score(Y_validation , testPredict) * 100 , \"%\") \n", "intent": "Decision Tree Validation Set accuracy\n-----------\n"}
{"snippet": "scores = cross_val_score(decisionTreeClassifier, X_test, Y_test.ravel(), cv=10, scoring='f1_macro')\nprint(scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Decision tree 10 fold cross validation. \n-------------\ncv parameter controls number of folds.\n"}
{"snippet": "testPredict = []\nfor xt in X_train:\n    testPredict.append(logistic.predict([xt]))\nprint(metrics.accuracy_score(Y_train , testPredict) * 100 , \"%\") \n", "intent": "Logistic Regression Train Set accuracy\n-----------\n"}
{"snippet": "testPredict = []\nfor xt in X_test:\n    testPredict.append(logistic.predict([xt]))\nprint(metrics.accuracy_score(Y_test , testPredict) * 100 , \"%\") \n", "intent": "Logistic Regression Test Set accuracy\n-----------\n"}
{"snippet": "testPredict = []\nfor xt in X_validation:\n    testPredict.append(logistic.predict([xt]))\nprint(metrics.accuracy_score(Y_validation , testPredict) * 100 , \"%\") \n", "intent": "Logistic Regression Validation Set accuracy\n-----------\n"}
{"snippet": "scores = cross_val_score(logistic, X_test, Y_test.ravel(), cv=10, scoring='f1_macro')\nprint(scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Logistic Regression 10 fold cross validation. \n-----------\ncv parameter in function cross_val_score controls number of folds.\n"}
{"snippet": "pred=mod_loaded.predict(X_test[set_vars1])\nd=np.column_stack([y_test.values,pred])\neval_functions.eval_model_results_binary(d,5)\n", "intent": "<h3>Each - Full Gains</h3>\n"}
{"snippet": "pred=mod_loaded.predict(X_test[set_vars1])\nd=np.column_stack([y_test.values,pred])\neval_functions.eval_model_results_binary(d,10)\n", "intent": "<h3>Each - Full Gains</h3>\n"}
{"snippet": "predictions = logisticRegr.predict(test)\n", "intent": "Now, let's measue model performance\nFirst, we are going to make predictions using our new model and the test data\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint('MSE: {:.2f}'.format(mean_squared_error(bos.PRICE, lm.predict(X))))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictions = mlp.predict(X_test)\n", "intent": "Now that the classifier has been trained on the training set we'll use it to make predictions for the test data set:\n"}
{"snippet": "y_preds = nb.predict(X_test)\n", "intent": "Use the trained `nb` classifier to classify the texts in the test data.\n"}
{"snippet": "from sklearn.metrics.classification import classification_report, accuracy_score\ndef report_performance(y_preds, y_test):\n    acc = accuracy_score(y_preds, y_test)\n    print(f'accuracy: {acc}')    \n", "intent": "Write a function to evaluate your predictions against the truth. This is a classification problem, so we need to use classification metrics here.\n"}
{"snippet": "y_preds = lr.predict(X_test)\nreport_performance(y_preds, y_test)\n", "intent": "Use the trained `lr` classifier to classify the texts in the test data. Report the performance of this classifier.\n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", acc) \n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "def mean_squared_error(y_gold, y_pred):\n    return ((y_gold - y_pred) ** 2).mean()\n", "intent": "Let us check whether the output of our theano function is the same as our numpy implementation:\n"}
{"snippet": "preds = model.predict(X_test)\nprint(preds.shape)\n", "intent": "Using our reloaded model again, to get predictions for our test data can be done as follows:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Test accuracy:', accuracy_score(predicted_classes, correct_classes))\n", "intent": "How well are we doing so far? Let's find out:\n"}
{"snippet": "output = NN(X, num_of_hidden_layers)\ncost = tf.reduce_mean(tf.pow(output - Y, 2))\noptimizer = tf.train.AdamOptimizer().minimize(tf.nn.l2_loss(output - Y))\n", "intent": "Build Neural Network using `NN()` function. Create cost function, and link optimizer.\n"}
{"snippet": "X_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n       iris_dataset['target_names'][prediction]))\n", "intent": "Let's create a new example and ask the kNN model to classify it\n``` python\nX_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\n```\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\n", "intent": "Feeding all test examples to the model yields all predictions\n``` python\ny_pred = knn.predict(X_test)\n```\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipe, cancer.data, cancer.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n", "intent": "* Now with cross-validation:\n``` python\nscores = cross_val_score(pipe, cancer.data, cancer.target)\n```\n"}
{"snippet": "print(classification_report(y_test, pred))\n", "intent": "Precision, recall, F1-score now yield 10 per-class scores\n"}
{"snippet": "predictions = model.predict(x_test)\nprint(\"Review 0: \", ' '.join([reverse_word_index.get(i - 3, '?') for i in test_data[0]]))\nprint(\"Predicted positiveness: \", predictions[0])\nprint(\"\\nReview 16: \", ' '.join([reverse_word_index.get(i - 3, '?') for i in test_data[16]]))\nprint(\"Predicted positiveness: \", predictions[16])\n", "intent": "Out of curiosity, let's look at a few predictions:\n"}
{"snippet": "preds = model.predict(x)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n", "intent": "* Sanity test: do we get the right prediction?\n``` python\npreds = model.predict(x)\n```\n"}
{"snippet": "def predict_images(images):\n    resized_images = []\n    for image in images:\n        resized_images.append(numpy.resize(image, (image_size, image_size, 1)))\n    return model2.predict(numpy.array(resized_images))\n", "intent": "Again, run our test images through the model to see what the filters output.\n"}
{"snippet": "one_unit_SRNN.predict(numpy.array([ [[-1], [3], [7]] ]))\n", "intent": "**This passes in a single sample that has three time steps.**\n"}
{"snippet": "two_unit_SRNN.predict(numpy.array([ [[1], [2], [3], [4]] ]))\n", "intent": "**This passes in a single sample with four time steps.**\n"}
{"snippet": "predictions = res.predict()\nprint(predictions[0:10])\n", "intent": "The predict()  function can be used to predict the probability that the market will go down, given values of the predictors.\n"}
{"snippet": "pred_p = lda.predict_proba(x_test)\n", "intent": "Applying a 50% threshold to the posterior probabilities allows us to recreate\nthe predictions\n"}
{"snippet": "classificationReport=classification_report(y_test, y_pred, digits=3)\n", "intent": "Comparing with 4.6.2 The LDA and logistic regression predictions are almost identical.\nThe classification report is shown in the following table:\n"}
{"snippet": "pred_p = lda.predict_proba(x_test)\n", "intent": "Applying a 50% threshold to the posterior probabilities allows us to recreate the predictions\n"}
{"snippet": "predict = optimal_lasso.predict(XTest_fitss)\n", "intent": "* Predict saleprice using lasso model\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(Y_test, ypred))\n", "intent": "<li> Print out a classification report for the support vector machine classifier (displaying precision, recall and f1-score)</li>\n"}
{"snippet": "from sklearn.metrics import roc_curve\nfrom sklearn import metrics\nyppb = lgst.predict_proba(X_test_scaled)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(Y_test, yppb)\npyl.xlabel('False Positive')\npyl.ylabel('True Positive')\npyl.plot(fpr, tpr)\npyl.grid()\npyl.show()\n", "intent": "<li> Plot an ROC curve for the logistic regression model classifier</li>\n</ol>\n"}
{"snippet": "print(linreg.predict(5.2))  \n", "intent": "We can also use our linear regression model to make this same prediction:\n"}
{"snippet": "y_mult_pred = multiple_linreg.predict(X_mult)\nprint(\"R^2 value: \", metrics.r2_score(y_mult, y_mult_pred))\nprint(\"RMSE for multiple linear regression model (6 features): \",np.sqrt(metrics.mean_squared_error(y_mult, y_mult_pred)))\n", "intent": "We check to see if this model has a better R<sup>2</sup> and RMSE value than the previous simple linear regression model.\n"}
{"snippet": "vect_test = vectorizer.transform(X_test.description_alt)\nnum_feats = X_test.drop(\"description_alt\",axis=1).astype(float).values\ntesting_data = sparse.hstack((num_feats,vect_test))\npred_test = gbm.predict(testing_data)\n", "intent": "The presence of a balcony and mentionning \"salle bains\" (bathroom) seems to increase the cost. \n"}
{"snippet": "ypred = pl.predict(Xval)\nprint(\"R squared score is:\", r2_score(yval,ypred).round(3))\n", "intent": "This seems good with the residual running around 0 and does not following any trend. Now I will check with validation set.\n"}
{"snippet": "classification_report(naive_bayes_model, feature_test_fixed_data, predicted_test)\n", "intent": "| True Negative | False Positive |\n| - | - | \n| False Negative | True Positive | \n"}
{"snippet": "from xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nmodel = XGBClassifier(random_state=seed)\nevaluate(model)\nshow_decision_borders(model, X, Y)\n", "intent": "<h2>XGBoost classifier</h2>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = model.predict(X)\npredictions = [round(x[0]) for x in predictions]\naccuracy = accuracy_score(Y, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n", "intent": "<h2>Predictions</h2>\n"}
{"snippet": "from xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nmodel = XGBClassifier(random_state=seed)\nevaluate(model)\n", "intent": "<h2>XGBoost classifier</h2>\n"}
{"snippet": "from xgboost import XGBClassifier\nmodel = XGBClassifier(random_state=seed)\nevaluate(model)\n", "intent": "<h2>XGBoost classifier</h2>\n"}
{"snippet": "from xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=Warning)\nmodel = XGBClassifier(random_state=seed)\nevaluate(model)\n", "intent": "<h2>XGBoost classifier</h2>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom keras.wrappers.scikit_learn import KerasClassifier\nestimator = KerasClassifier(build_fn=build_model, epochs=400, batch_size=20, verbose=0)\nkfold = KFold(n_splits=10, shuffle=True, random_state=42)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Average accuracy: %.2f%% (stdev = %.2f%%)\" % (results.mean() * 100, results.std() * 100))\n", "intent": "<h2>10-fold Cross Validation</h2>\n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=3,model_dir=\"/tmp/iris-linear_model\")\nevaluate(classifier)\n", "intent": "<h2>Linear classifier</h2>\n"}
{"snippet": "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,hidden_units=[10, 20, 10],n_classes=3,model_dir=\"/tmp/iris-dnn_model\")\nevaluate(classifier)\n", "intent": "<h2>Deep Neural Network</h2>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = model.predict(X)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n", "intent": "<h2>Evaluate model</h2>\nEvaluate the model on the training dataset.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_cv_pred = cross_val_predict(model, X, y, cv=10)\npredictions_cv = [round(value) for value in y_cv_pred]\naccuracy = accuracy_score(y, predictions_cv)\nprint(\"Average accuracy: %.2f%%\" % (accuracy * 100.0))\n", "intent": "<h2>Cross-validation</h2>\nEvaluate the model using cross-validation.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom keras.wrappers.scikit_learn import KerasClassifier\nestimator = KerasClassifier(build_fn=build_model, epochs=400, batch_size=20, verbose=0)\nkfold = KFold(n_splits=3, shuffle=True, random_state=42)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Average accuracy: %.2f%% (stdev = %.2f%%)\" % (results.mean() * 100, results.std() * 100))\n", "intent": "<h2>3-fold Cross Validation</h2>\n"}
{"snippet": "from xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nmodel = XGBClassifier(random_state=seed, max_depth=15, n_estimators=25)\nevaluate(model)\nshow_decision_borders(model, X, Y)\n", "intent": "<h2>XGBoost classifier</h2>\n"}
{"snippet": "def chi_square_error(observed_y, predicted_y):\n    error = 0\n    observed_y_list = observed_y.tolist()\n    predicted_y_list = predicted_y.tolist()\n    for i in predicted_y_list:\n        error += ((observed_y_list[predicted_y_list.index(i)] - i)**2)/numpy.var(predicted_y)**2\n    print(error)\n", "intent": "Defines a chi square error function.\n"}
{"snippet": "chi_square_error(y, dumb_linreg(x_oops, reg.coef_[i], reg.intercept_))\n", "intent": "Calculates chi square error for this linear regression. The error is very large!\n"}
{"snippet": "from sklearn.cross_validation import KFold, cross_val_score\nkf = KFold(len(train), n_folds=10, shuffle=False)\nprint('mean auc from Cv folds:', \n      cross_val_score(lm, \n                      X,\n                      train['responded'],\n                      scoring = 'roc_auc',\n                      cv=kf).mean())\n", "intent": "** Cross validation ** \n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint(all_predictions)\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset. Let's begin by getting all the predictions:\n"}
{"snippet": "y_pred=clf.predict(X_test[X_test.columns[1:]])\n", "intent": "predicted for test dataset\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n", "intent": "sklearn also has many native performance metrics we can take a look at.\n"}
{"snippet": "lr.predict_proba(X_test_std[0,:].reshape(1,-1))\n", "intent": "Let's briefly look at a prediction metric within the Logistic regression class:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=pipe_lr,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=-1)\nprint('CV accuracy scores: %s' % scores)\n", "intent": "As it happens, scikit-learn has a helpful cross validation scorer that can make the process even easier.\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision: %.3f' % precision_score(\n            y_true=y_test, y_pred=y_pred))\n", "intent": "Often these scores are combined into the so-called **F1-score**.\n$$F1 = 2\\frac{PRE \\times REC}{PRE + REC}$$\n"}
{"snippet": "mv_clf = MajorityVoteClassifier(classifiers=[pipe1,clf2,pipe3])\nclf_labels += ['Majority Voting']\nall_clf = [pipe1, clf2, pipe3, mv_clf]\nfor clf, label in zip(all_clf, clf_labels):\n    scores = cross_val_score(estimator=clf,\n                             X=X_train,\n                             y=y_train,\n                             cv=10,\n                             scoring='roc_auc')\n    print(\"Accuracy: %0.2f, (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n", "intent": "Note that the DecisionTree is not part of a pipeline because it is scale invariant. Now let's incorporate these into a MajorityVoteClassifier.\n"}
{"snippet": "def abline(ax, fit, xname, **kwargs):\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    x = scipy.linspace(*xlim)\n    y = fit.predict({xname: x})\n    ax.plot(x, y, **kwargs)\n    ax.set_xlim(*xlim)\n    ax.set_ylim(*ylim)\n", "intent": "You can do this manually:\n"}
{"snippet": "scores = cross_val_score(lr, X, y, cv=10, scoring='neg_mean_squared_error')\nnp.mean(np.sqrt(-scores))\n", "intent": "*Ensuring the cross_val_score method produces similar results for the test RMSE, which it does*\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    ls = 0\n    for i in range(len(style_layers)):\n        gl = gram_matrix(feats[style_layers[i]], normalize=True)\n        al = style_targets[i]\n        ll = torch.sum((gl - al) ** 2) * style_weights[i]\n        ls += ll\n    return ls\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint (confusion_matrix(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "scores = cross_val_score(log_model, xvalid, yvalid, cv=5, scoring='accuracy')\nscores\n", "intent": "*Cross Validation: Check for Overfitting*\n"}
{"snippet": "c = 0\ns = lrn.predict(X_val)\nfor i in range(len(y_val)):\n    if np.absolute(y_val[i] - s[i]) > 1:\n        c += 1       \nprint(\"Predictions more than 1 day off: {}%\".format(((c*1000)//len(y_val))/10))\nprint(\"Tested on {} samples\".format(len(y_val)))\n", "intent": "Now we test it on the validation set X_val (and accept deviations of +-1 days):\n"}
{"snippet": "c = 0\ns = lrn.predict(X_val)\nfor i in range(len(y_val)):\n    if np.absolute(y_val[i] - s[i]) > 1:\n        c += 1       \nprint(\"Predictions more than 1 day off: {}%\".format((c*100)//len(y_val)))\nprint(\"Tested on {} samples\".format(len(y_val)))\n", "intent": "Now we test it on the validation set X_val (and accept deviations of +-1 days):\n"}
{"snippet": "test_preds =  (X_test_level2 * [best_alpha, 1 - best_alpha]).sum(axis=1)\nr2_test_simple_mix = r2_score(y_test,test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr2.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2,train_preds)\ntest_preds = lr2.predict(X_test_level2)\nr2_test_stacking =  r2_score(y_test,test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from basic_model_tf import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\nclass supervised_training:\n    input_sequence = tf.placeholder('int32',[None,None])\n    reference_answers = tf.placeholder('int32',[None,None])\n    logprobs_seq =  model.symbolic_score(input_sequence, reference_answers)\n    crossentropy = - select_values_over_last_axis(logprobs_seq,reference_answers)\n    mask = infer_mask(reference_answers, out_voc.eos_ix)\n    loss = tf.reduce_sum(crossentropy * mask)/tf.reduce_sum(mask)\n    train_step = tf.train.AdamOptimizer().minimize(loss, var_list=model.weights)\ninitialize_uninitialized(s)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "test_iter = mx.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size)\nstart = time.time()\nprob = lenet_model.predict(test_iter)\nduration = time.time() - start\nprint(len(prob)/duration)\nprint('images per second')\n", "intent": "Finally, we'll use the trained LeNet model to generate predictions for the test data.\n"}
{"snippet": "cvscore = cross_val_score(cv=5,estimator=xgb1,X = X_train,y = y_train, n_jobs = -1, verbose=3)\n", "intent": "Running the CV for the ensemble regressor crashes it; get an indication on one regressor.\n"}
{"snippet": "y_pred = grid.predict(X)\nacc_score (y,y_pred)\n", "intent": "Gridsearch tests all possible combinations of parameters and automatically saves the best model. So we can use it to make predictions.\n"}
{"snippet": "X_test = df_test.drop(['PassengerId'], axis = 1).as_matrix()\nX_test = scaler.transform(X_test);\ny_pred = grid.predict(X_test)\n", "intent": "Let's build __X_test__ and normalize it with the training values. Then we can creat __y_test__ using the predict() method.\n"}
{"snippet": "predict_fn = lambda x: xgb.predict_proba(encoder.transform(x)).astype(float)\n", "intent": "Create a scoring function. Note that on the fly, the input data is encoded to dummies.\n"}
{"snippet": "bagging_pred = bagging.predict(dummy_left_out_X)\n", "intent": "As we did before compute the accuracy score on the `left_out` dataset\n"}
{"snippet": "random_forest_pred = random_forest.predict(dummy_left_out_X)\n", "intent": "As we did before compute the accuracy score on the `left_out` dataset\n"}
{"snippet": "logit_train_pred = logit.predict_proba(dummy_X_train)[:, 0]\n", "intent": "2) Predict using each base layer model for training data and test data.\n"}
{"snippet": "gbm_pred = gbm.predict(boosting_test)\n", "intent": "4) Finally, predict using the top layer model with the predictions of bottom layer models that has been made for testing data.\n"}
{"snippet": "nb_test = np.asarray( [ nb_featurize( s ) for s in test['strings'] ] )\nnb_pred = mnb.predict( nb_test ) \nprint( 'Naive Bayes testing accuracy: ' + str(sk.metrics.accuracy_score( test_labels , nb_pred )))\nprint( 'Naive Bayes testing precision, ham: ' + str(sk.metrics.precision_score( test_labels , nb_pred , pos_label=0 )))\nprint( 'Naive Bayes testing precision, spam: ' + str(sk.metrics.precision_score( test_labels , nb_pred , pos_label=1 )))\nprint( 'Naive Bayes testing recall, ham: ' + str(sk.metrics.recall_score( test_labels , nb_pred , pos_label=0 )))\nprint( 'Naive Bayes testing recall, spam: ' + str(sk.metrics.recall_score( test_labels , nb_pred , pos_label=1 )))\n", "intent": "A testing data set will be used to evaluate the model. This work will focus on accuracy.\n"}
{"snippet": "test_num = 1000\ntesting_inputs_1 = [sentenceTrain[:test_num], positionTrain1[:test_num], positionTrain2[:test_num]]\nmodel.predict(testing_inputs_1)\ninputs = testing_inputs_1\n", "intent": "Load a small sample set for visualization, and run the model to get the predictions\n"}
{"snippet": "ios_pt = datasets[1][0]\nios_img, _ = unpack_point(ios_pt)\nsim, close_ios = find_most_similar_image(ios_pt, datasets[1][1:], encoder)\nclose_img, _ = unpack_point(close_ios)\nimg2add, _ = sample_point(datasets[1])\ndef encode(img):\n    return encoder.predict(np.expand_dims(img, 0))[0]\n", "intent": "The following code reproduces the steps taken to reproduce a specific user interface algebra example given in the original blog post.\n"}
{"snippet": "test_loss, test_acc = randomNormalModel.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "predictionsRandomNormal = randomNormalModel.predict(test_images)\n", "intent": "With the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "predictions_single_random_normal = randomNormalModel.predict(img)\nprint(predictions_single_random_normal)\n", "intent": "Now predict the image:\n"}
{"snippet": "Y_predict = clf.predict(X_test)\nprint(Y_predict)\n", "intent": "Now let's see how well our trained tree can generalize by letting it predict the loans in the test data set.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in train_tensors]\ntrain_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(train_targets, axis=1))/len(dog_breed_predictions)\nprint('Training accuracy: %.4f%%' % train_accuracy)\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in valid_tensors]\nvalid_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(valid_targets, axis=1))/len(dog_breed_predictions)\nprint('Validation accuracy: %.4f%%' % valid_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We'll test the model on the test dataset of dog images.\n"}
{"snippet": "from sklearn import metrics\nprint(\"MAE:\", metrics.mean_absolute_error(ytest,y_predicted))\nprint(\"MSE:\", metrics.mean_squared_error(ytest,y_predicted))\nprint(\"RMSE:\", np.sqrt(metrics.mean_squared_error(ytest,y_predicted)))\nprint(metrics.explained_variance_score(ytest,y_predicted))\n", "intent": "*** PERFORMANCE EVALUATION USING MEAN AVERAGE ERROR, MEAN SQUARE ERROR AND ROOT MEAN SQUARE ERROR ***\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(\"CONFUSION MATRIX:\\n\\n \",confusion_matrix(house_data['Cluster'],km.labels_))\nprint(\"\\nCLASSIFICATION REPORT:\\n\\n\",classification_report(house_data['Cluster'],km.labels_))\n", "intent": "*** COMPARISON OF CLUSTER CLASSIFIER LABEL WITH GENERATED K-MEANS LABELS FOR PERFORMANCE ANALYSIS ***\n"}
{"snippet": "print(\"\\nCLASSIFICATION REPORT:\\n\\n\",classification_report(y_test,predictions))\nprint(\"\\nCONFUSION MATRIX: \\n\\n\",confusion_matrix(y_test,predictions))\n", "intent": "*** PERFORMANCE ANALYSIS ***\n"}
{"snippet": "print(\"\\nCLASSIFICATION REPORT:\\n\\n\",classification_report(y_test,preds))\nprint(\"\\nCONFUSION MATRIX: \\n\\n\",confusion_matrix(y_test,preds))\n", "intent": "*** PERFORMANCE ANALYSIS ***\n"}
{"snippet": "results = classifier.evaluate(input_fn=numpy_input_fn({\"x\": mnist.test.images}, mnist.test.labels, shuffle=False))\nprint('-' * 30)\nprint('Results out-of-sample')\nfor key in sorted(results):\n    print('%s: %s' % (key, results[key]))\n", "intent": "After the training we can evaluate the performance of our classifier on the test dataset using the `evaluate` method.\n"}
{"snippet": "y_pred = regr.predict(X_test)\n", "intent": "Now that the model has been fit, it can be used to predict the y values, given the X values.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint('Mean squared error: %.2f'\n     % mean_squared_error(y_test, y_pred))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\n", "intent": "The predictions can be evaluated by looking at the coefficients, the mean squared error, and the variance score.\n"}
{"snippet": "scores = model2.evaluate(new_test_images_X, new_test_images_Y, batch_size=64, verbose=1)\nprint('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))\n", "intent": "the model predicted 10 out of 10 signs correctly, it gives 100% accurate on these new images.\n"}
{"snippet": "to_predict = np.array(img.getdata())\npredicted = kmeans.predict(to_predict)\no = [tuple(map(int, kmeans.cluster_centers_[c])) for c in predicted]\nout = Image.new('RGB', (width, height))\nout.putdata(o)\nout\n", "intent": "Now use the 7 colors to repaint the image:\n"}
{"snippet": "mseX = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mseX)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "RMSE = (mean_squared_error(obs_pred['Observation'], obs_pred['Predicted']))**0.5\nRMSE\n", "intent": "The model overestimates the GVA.\n"}
{"snippet": "y_pred_prob = modelGrid.predict_proba(X_test) \ny_pred_class = modelGrid.predict(X_test) \n", "intent": "Now lets test the model with an independent test set (that was not used in the training process):\n"}
{"snippet": "test_gen = d.test_generator()\nvalid_preds, valid_loss = p.predict(test_gen)\n", "intent": "Here FP samples are showed and FP rate is computed.\n"}
{"snippet": "pred_data = Data(path_anomaly_data, predict=True)\npred_gen = pred_data.predict_generator()\nanomaly_preds, anomaly_loss = p.predict(pred_gen)\n", "intent": "Here TP samples are showed and TP rate is computed.\n"}
{"snippet": "def do_evaluation():\n    dmm.rnn.eval()\n    val_nll = svi.evaluate_loss(val_batch, val_batch_reversed, val_batch_mask,\n                                 val_seq_lengths) / np.sum(val_seq_lengths)\n    test_nll = svi.evaluate_loss(test_batch, test_batch_reversed, test_batch_mask,\n                                  test_seq_lengths) / np.sum(test_seq_lengths)\n    dmm.rnn.train()\n    return val_nll, test_nll\n", "intent": "With the test and validation data now fully prepped, we define the helper function that does the evaluation: \n"}
{"snippet": "score = model1.evaluate(x_test, y_test, verbose=0)\nprint('Mean squared error on the test data:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Check the score on test data:\n"}
{"snippet": "score = model4.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Check the performance on the test dataset:\n"}
{"snippet": "print ('STATUS:: Begin of dog breed predictions')\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\nprint ('STATUS:: End of dog breed predictions')\nprint ('STATUS:: Going to compute test accuracy')\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "print ('STATUS:: Begin VGG16 predictions')\nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\nprint ('STATUS:: End VGG16 predictions')\nprint ('STATUS:: Going to compute test accuracy')\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "print ('STATUS:: Begin Resnet50 predictions')\nResnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nprint ('STATUS:: End Resnet50 predictions')\nprint ('STATUS:: Going to compute test accuracy')\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "X_new = np.linspace(0, 3, 1000).reshape(-1,1)\ny_proba = log_reg.predict_proba(X_new)\n", "intent": "Decision boundaries\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = \nfor i in range(len(predictions)):\n    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\nwith open(\"output.txt\", \"w\") as text_file:\n    text_file.write(\"error = %f\\n\" % error)\n", "intent": "- using **model_fit** forecast the values by using start and end index of test data (follow the code snippet provided in the course)\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nerror = mean_squared_error(test, predictions)\nprint(\"MSE = \", error)\nwith open(\"output.txt\", \"w\") as text_file:\n    text_file.write(\"error = %f\\n\" % error)\n", "intent": "- Run the below cell to output model error\n"}
{"snippet": "pred_probs = model.predict(test_x)\npred_labels = np.argmax(pred_probs, axis=1)\n", "intent": "Finally, we can use the trained model to do predictions.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\npredicted = model.predict(X)\nmse = mean_squared_error(y, predicted)\nr2 = r2_score(y, predicted)\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-squared (R2 ): {r2}\")\n", "intent": "Mean Squared Error (MSE)\nR2 Score\n"}
{"snippet": "X_test = X[threefourths:,:]\ny_test = y[threefourths:]\ny_hat = glm_model.predict(X_test)\n", "intent": "Let's predict the neural response on the training set.\n"}
{"snippet": "final_model = grid_search.best_estimator_\nfinal_predictions = final_model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)   \nfinal_rmse\n", "intent": "We now have our final model. In order to see how it will do with unseen data we are going to use the test set\n"}
{"snippet": "reload(extr)\nF1 = metrics.f1_score(SP.vstack(labels).ravel().astype('int'),pred.ravel()>.5, average=\"macro\")\nprint('Macro averaged F1 score %1.2f:' % F1)\nextr.plotPerfromanceCurve(pred.ravel(),  SP.vstack(labels).ravel())\nextr.plotAUC(pred.ravel(),  SP.vstack(labels).ravel(), is_annotated=True)\n", "intent": "Finally, we can evaluate the model perfromacne on the test set by calculating the macro-averaged F1 score and plot an ROC curve\n"}
{"snippet": "def f1_score(actual, predicted, pos_label):\n    return 0\nprint \"Balanced: {}\".format(f1_score(actual, predicted, 1))\nprint \"Unbalanced: {}\".format(f1_score(actual, predicted, 1))\n", "intent": "$F1$ score is the harmonic mean of Precision & Recall\n$F1 = 2\\frac{P . R}{P+R} $\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Yes, the classifier is better because the training accuracy score is close to the test accuracy score.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "** Now from that grid model, creating some predictions using the test set**\n"}
{"snippet": "print(metrics.classification_report(y_test, y_test_pred, target_names=str(digits.target)))\nprint(y_test)\n", "intent": "Now, we calculate the metrics of the model based on test data\n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n    return (\"Mean score: {0:.3f} (+/- {1:.3f})\").format(np.mean(scores), sem(scores))\nprint(mean_score(scores))\n", "intent": "We have the accuracy of each combination in the 10 fold validation and we are going to calculate the mean of all of them\n"}
{"snippet": "print(\"Predicted probabilities\", model.predict_proba(x_train[:10]))\n", "intent": "In the predicted and expected digits we can see that the model is wrong with two of the train data predictions that the list shows\n"}
{"snippet": "from sklearn import metrics\ny_train_pred = model.predict(x_train)\nprint(\"Accuracy in training\", metrics.accuracy_score(y_train, y_train_pred))\naccuracy_tree_training1 = metrics.accuracy_score(y_train, y_train_pred)\ny_test_pred = model.predict(x_test)\nprint(\"Accuracy in testing \", metrics.accuracy_score(y_test, y_test_pred))\naccuracy_tree_testing1 = metrics.accuracy_score(y_test, y_test_pred)\n", "intent": "Now we calculate the accuracy of training and testing\n"}
{"snippet": "print(metrics.classification_report(y_test, y_test_pred,target_names=str(digits.target_names)))\nprint(metrics.confusion_matrix(y_test, y_test_pred))\n", "intent": "Now I'm going to evaluate the algorithm\n"}
{"snippet": "Y_predicted = linreg.predict(X_new)\n", "intent": "Predict the output values for the new samples using the trained linear model.\n"}
{"snippet": "compute_accuracy(Y_class_train, logistic_regression.predict(X_class_train.reshape(-1,1)))\n", "intent": "Evaluating the model's accuracy on the training data.\n"}
{"snippet": "classifier_mybag_final = train_classifier_improvement(X_train_mybag, y_train, 1.0, 'l1')\ny_val_predicted_labels_mybag_improvement = classifier_mybag_improvement.predict(X_val_mybag)\n", "intent": "When you are happy with the quality, create predictions for *test* set, which you will submit to Coursera.\n"}
{"snippet": "x, y = next(batches)\npreds = model.predict(x, x.shape[0])\ndecode_predictions(preds, 1)[0]\npreds = [yi[0][1] for yi in decode_predictions(preds, 1)[0:10]]\ntruth = ['dog' if yi==1 else 'cat' for yi in np.argmax(y[0:10], axis=1)]\nprint(\"          Predictions           VS    Ground truth\")\nfor (p, t) in zip(preds, truth):\n    print('%30s' %p, '%10s' % t)\n", "intent": "Use ResNet directly for prediction\n"}
{"snippet": "x, y = next(val_batches)\npreds = model.predict(x, x.shape[0])\npreds = [yi[0][1] for yi in decode_predictions(preds, 1)[0:10]]\ntruth = ['dog' if yi==1 else 'cat' for yi in np.argmax(y[0:10], axis=1)]\nprint(\"          Predictions           VS    Ground truth\")\nfor (p, t) in zip(preds, truth):\n    print('%30s' %p, '%10s' % t)\n", "intent": "Use ResNet directly for prediction\n"}
{"snippet": "def get_score(prediction, lables):    \n    print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))\ndef train_test(estimator, x_trn, x_tst, y_trn, y_tst):\n    prediction_train = estimator.predict(x_trn)\n    get_score(prediction_train, y_trn)\n    prediction_test = estimator.predict(x_tst)\n    get_score(prediction_test, y_tst)\n", "intent": "**Methods to compute score**\n"}
{"snippet": "def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n", "intent": "**Function to calculate accuracy of the model**\n"}
{"snippet": "compute_score(model, train, targets, scoring='accuracy')\n", "intent": "**Calculating score of our predictive model**\n"}
{"snippet": "reddit_cv=tcv.transform(all_data.proc_text) \nreddict_ch2=ch2.transform(reddit_cv) \nreddit_sent=nb_class.predict(reddict_ch2)\n", "intent": "Now that we've trained our multinomial Naive Bayes classifier on our labeled Twitter data, let's apply our sentiment classifier to the reddit posts\n"}
{"snippet": "reddit_cv=tcv.transform(all_data.proc_text) \nreddict_ch2=ch2.transform(reddit_cv) \nreddit_sent=nb_class.predict(reddict_ch2)\n", "intent": "Let's apply our sentiment classifier to our reddit posts\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\nprint(test_VGG16.shape)\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "pred = classifier.predict(FEAT_test)\nprint pred[:10]\n", "intent": "Evaluate performance\n"}
{"snippet": "print('Coefficients: \\n', model.coef_)\nprint(\"Mean squared error: %.2f\" % np.mean((model.predict(xfit) - yfit) ** 2))\nprint('Variance score: %.2f' % model.score(xfit, yfit))\n", "intent": "* get model results\n"}
{"snippet": "result=model.predict(test)\n", "intent": "* use our model to do prediction to test data\n"}
{"snippet": "ResNet_predictions = [np.argmax( ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nprint(len(ResNet_predictions))\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy : %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lasso_stock_predictions_std = rnd_search_lasso_cv.best_estimator_.predict(X_stock_test)\nlasso_mse_std = mean_squared_error(y_stock_test, lasso_stock_predictions_std)\nlasso_rmse_std = np.sqrt(lasso_mse_std)\nprint('Lasso Regression RMSE with Standardization', lasso_rmse_std)\nridge_stock_predictions_std = rnd_search_ridge_cv.best_estimator_.predict(X_stock_test)\nridge_mse_std = mean_squared_error(y_stock_test, ridge_stock_predictions_std)\nridge_rmse_std = np.sqrt(ridge_mse_std)\nprint('Ridge Regression RMSE with Standardization', ridge_rmse_std)\n", "intent": "Calculated RMSE for Lasso and Ridge separately as it was ran thorugh the Cross Validation. The RMSE didn't outperform Linear Regression\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nrnd_search_rmse = np.sqrt(mse)\n", "intent": "Looks much better than the linear model. Let's select this model and evaluate it on the test set:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nfor name, mdl in models.items():\n    scores = cross_val_score(mdl, \n                             X_train, \n                             y_train, \n                             cv=5, scoring='accuracy')\n    print('Accuracy score on training set for {}: {}'.format(name, scores.mean()))\n", "intent": "First, let's see how the models perform predicting chemicals in the training set.  That is, how they perform using 5-fold cross validation.  \n"}
{"snippet": "y_pred_train = NaiveB_classifier.predict(X_train)\ny_pred_test = NaiveB_classifier.predict(X_test)\ny_pred_test\n", "intent": "All models are wrong but some are useful...\n"}
{"snippet": "y_pred_train = SVC_classifier.predict(X_train)\ny_pred_test = SVC_classifier.predict(X_test)\ny_pred_test\n", "intent": "All models are wrong but some are useful...\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(bos.PRICE, lm3.predict(X.PTRATIO.to_frame()))\n", "intent": "The greater the pupil-teacher ratio the lower the price (by a factor of 2 as seen by the coefficient).\n"}
{"snippet": "scores = []\nfor alpha in np.arange(0.1, 0.9, 0.01):\n    pr = nx.pagerank(g_test, alpha=0.9)\n    y_pred_test_pagerank = predict_initial_problem(initial_ds_test, pr)\n    scores += [metrics.accuracy_score(y_true=y_test, y_pred=y_pred_test_pagerank)]\nmax(scores)\n", "intent": "Stil we have very bad accuracy.\nWe now try tuning the pagerank algorithm using different $\\alpha$ values.\n"}
{"snippet": "resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ypred1 = regressor1.predict(xtest)\n", "intent": "The target unit sales are find out as follows:\n"}
{"snippet": "ypred2 = regressor2.predict(xtest)\n", "intent": "The unit sales are predicted as follows:\n"}
{"snippet": "yhat = knn.predict(x)\nTP = np.sum(np.logical_and(yhat==-1,y==-1))\nTN = np.sum(np.logical_and(yhat==1,y==1))\nFP = np.sum(np.logical_and(yhat==-1,y==1))\nFN = np.sum(np.logical_and(yhat==1,y==-1))\nprint 'TP: '+ str(TP), ', FP: '+ str(FP)\nprint 'FN: '+ str(FN), ', TN: '+ str(TN)\n", "intent": "Let us compute the elements of a confusion matrix.\n"}
{"snippet": "X = [2025]\ny_hat = model.predict(X)\nj = 1 \ny_hat = (y_hat*month_means.mean()/100) + month_means[j]\nprint \"Prediction of extent for January 2025 (in millions of square km):\", y_hat\n", "intent": "5) We can also estimate the extent value for 2025. For that we use the function predict of the model.\n"}
{"snippet": "def perfMetric(groundTruth,predicted):\n    perfScore = r2_score(groundTruth,predicted)  \n    return perfScore\n", "intent": "Defining the R^2 score as the performance measure. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L=tf.constant(0.0)\n    print(style_weights)\n    for i in np.arange(len(style_layers)):\n        L += style_weights[i]*tf.reduce_sum((gram_matrix(feats[style_layers[i]])-style_targets[i])**2)\n    return(L)\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "x = ['This movie is not remarkable, touching, or superb in any way']\nx_vector = vectorizer.transform(x)\nresult = clf.predict(x_vector)[0]\nif result == 1:\n    print('The result from the classifer is: fresh')\nelse:\n    print('The result from the classifer is: rotten')\n", "intent": "**Answer** The result is not expected. The \"not\" in the sentence changed the result. But the classifier can not check the \"not\".\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n", "intent": "**accuracy_score()** from **sklearn.metrics**\n"}
{"snippet": "print('Precision: {:.3}'.format(sklearn.metrics.precision_score(y_true=y_test, y_pred=y_pred)))\n", "intent": "We can get these metrics directly from `sklearn.metrics`.\n"}
{"snippet": "Dunkirk = np.array([106,3.98,8.4,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0])\nEmoji_Movie = np.array([86,1.08,2.2,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0])\nLa_La_Land = np.array([128,3.80,8.2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1])\nMad_Max_Fury_Road = np.array([120,3.83,8.1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0])\nprint('Dunkirk:', model_fit.predict(Dunkirk)[0])\nprint('The Emoji Movie:', model_fit.predict(Emoji_Movie)[0])\nprint('La La Land:', model_fit.predict(La_La_Land)[0])\nprint('Mad Max Fury Road:', model_fit.predict(Mad_Max_Fury_Road)[0])\n", "intent": "Now that the model is finished, I get some recent movies and plug their values into the model to get a predicted rating out of four stars.\n"}
{"snippet": "model.predict(Xprime)\n", "intent": "Predicting the class is simple.  We invoke our model with the predict function and pass as an argument the transformed data.\n"}
{"snippet": "metrics.accuracy_score(y_test, y_predicted)\n", "intent": "We can now compute the accuracy and F1-Score.\n"}
{"snippet": "newdf['sentiment'] = model.predict(newX)\n", "intent": "We can create a new variable in our dataset with the predictions from the trained model.\n"}
{"snippet": "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\nprint(sess.run(loss))\n", "intent": "Define mean square error as the loss function\n"}
{"snippet": "predictions= nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions=pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Excellent! Let's take one more look at that accuracy...\n"}
{"snippet": "print('Using Cross-Validation, our model produces the following accuracy for all data: ' + str(cross_val_score(decision_tree, pokemon_features, pokemon['Legendary'], cv=5)))\n", "intent": "Validate Model - first by using cross validation, then by the train_test_split.\n"}
{"snippet": "a=confusion_matrix(list(set2['ytest']), rf.predict(x1_pca))\nplot_confusion_matrix(a,['CN','EMCI','LMCI','AD'])\n", "intent": "The accuracy is boosted to 58%, which is sigficant higher than the previous data.\n"}
{"snippet": "from concise.eval_metrics import auc, auprc\ndef evaluate(model, x, y, add_metrics={}):\n    keras_metrics = {name: metric \n                     for name, metric in zip(model.metrics_names, \n                                             model.evaluate(x, y, verbose=0))}\n    y_pred = model.predict(x)\n    add_metrics= {k: v(y, y_pred) for k,v in add_metrics.items()}\n    return {**keras_metrics, **add_metrics}\n", "intent": "Once the model has been trained, one should evaluate the model on the held-out test set using different metrics.\n"}
{"snippet": "valid_labels_ = to_categorical(valid_labels, 2)\nvalid_ = average(valid)\nvalid_ = np.resize(valid_, (len(valid_), ROWS, COLS, 1))\nprint(\"valid set :\", model.evaluate(valid_, valid_labels_, verbose=False)[1]*100, \"%\")\nprint(\"--------------------\")\nprint(\"train set :\", model.evaluate(train_, labels_, verbose=False)[1]*100, \"%\")\n", "intent": "Evaluate on validation set\n"}
{"snippet": "genieclust.compare_partitions.adjusted_rand_score(labels_true[labels_true>=0], labels_genie[labels_true>=0])\n", "intent": "The above confusion matrix can be summarized using some cluster similarity measures, like the Adjusted Rand Index (`ar`).\n"}
{"snippet": "def createSubmission(model):\n    pred = np.squeeze(model.predict(test))\n    ids = testData['Unnamed: 0'].as_matrix()\n    return np.stack([ids, pred], axis=1)\ndef outputSubmission(filename, subm):\n    np.savetxt(filename, subm, fmt='%d,%.5f', header=',Made Donation in March 2007', comments='')\nsubmissionFile = dataDir + 'submissionLinear.csv'\noutputSubmission(submissionFile, createSubmission(linearModel) )\n", "intent": ",Made Donation in March 2007\n659,0.5\n276,0.5\n263,0.5\n303,0.5\n83,0.5\n500,0.5\n530,0.5\n244,0.5\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "This time, it seems that the classifier predicts more reviews with star rating 1.\n"}
{"snippet": "print classification_report(y_test, best_rfc.predict(X_test))\n", "intent": "<h3> Precision and Recall </h3>\n"}
{"snippet": "from sklearn import cross_validation\nscores = cross_validation.cross_val_score(best_rfc, data, y, cv=10)\n", "intent": "<h3> K-Fold Cross Validation </h3>\n"}
{"snippet": "def evaluate(articles, session):\n  for article in articles:\n    session.run(reset_sample_state)\n    predicted = []\n    for word in article:\n      prediction = session.run(sample_prediction, { sample_input: [dictionary[word]] })\n      predicted.append(prediction[0])\n    print_md(represent(article, predicted))\n", "intent": "Let us now see the world with neural network's eyes.\nFirst recollect what the model has learned:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ndt_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                            scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\ntree_rmse_scores = np.sqrt(-dt_scores)\n", "intent": "+ **10-fold cross-validation for the Decisin Tree model:**\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10,\n                             n_jobs=-1)\nlin_mse_scores = np.sqrt(-lin_scores)\nprint(\"Scores: \", lin_mse_scores)\nprint(\"\\nMean: \", lin_mse_scores.mean())\nprint(\"\\nStandard deviation: \", lin_mse_scores.std())\n", "intent": "+ **10-fold cross-validation for the Linear Regression model:**\n"}
{"snippet": "precision_score(y_train_5, y_train_pred_90)\n", "intent": "Check these predictions' precision and recall:\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Evaluate these classifiers**:\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\ny_train_pred\n", "intent": "**Look at the confusion matrix:**\n"}
{"snippet": "X_new = np.linspace(0, 3, 100).reshape(-1,1)\ny_proba = log_reg.predict_proba(X_new)\ny_proba\n", "intent": "Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_.\n"}
{"snippet": "y_pred_rf = rf_clf.predict(X_test)\n", "intent": "roughly equivalent to the previous `RandomForestClassifier`:\n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\ny_pred\n", "intent": "Now, we have an ensemble containing three trees. It can make predictions on a new instance simply adding up the predictions of all the trees:\n"}
{"snippet": "X_test_reduced = pca.transform(X_test)\ny_pred = rf_clf_2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n", "intent": "**Exericse 9-5:** Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n"}
{"snippet": "y_pred = log_clf_2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n", "intent": "> Reducing dimensionality led to speedup.\n"}
{"snippet": "result = m.predict(test_df)\npredictions = [row for row in result]\nwriteTOfile(pId_list, predictions, \"submission.csv\")\n", "intent": "Make predictions on preprocessed testing data.\n"}
{"snippet": "print(metrics.confusion_matrix(y_test, y_pred020))\nprint(metrics.classification_report(y_test, y_pred020))\n", "intent": "Let us compare this to the situation where we set the posterior probability threshold for 'default' at 20%.\n"}
{"snippet": "scores = cross_val_score(logReg, X_train, y_train, cv=kFold)\nprint('CV accuracy on train set: {0: .3f} +/- {1: .3f}'.format(np.mean(scores), np.std(scores)))\n", "intent": "As a comparison we can also run a test on the unscaled data set `X_train`. The results are indeed worse as literature suggests.\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet_model]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "(model.predict_proba(X[:100]) - predict_proba(model, X[:100], n_jobs=4)).sum()\n", "intent": "We can also confirm that this is producing the correct result.\n"}
{"snippet": "def content_loss(target_img, combination_img):\n    return (K.sum(K.square(combination_img - target_img)))\n", "intent": "Loss function for content -> sum of the square difference between the combination image and the target image\n"}
{"snippet": "def total_variation_loss(x):\n    img_height = x.get_shape()[1].value\n    img_width = x.get_shape()[2].value\n    a = K.square(\n        x[:, :-1, :-1, :] - x[:, 1:, :-1, :])\n    b = K.square(\n        x[:, :1, :-1, :] - x[:, :-1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n", "intent": "The further spaced out the combo image moves from the content image, the greater the loss becomes\n"}
{"snippet": "kf = StratifiedKFold(n_splits = 10)\nnp.mean(cross_val_score(pipeline1, mails, y_encoded, scoring=\"accuracy\", cv=kf))\n", "intent": "<b>Accuracy:</b> around 97-98%, depends on the shuffle\n"}
{"snippet": "y_predicted_1 = cross_val_predict(pipeline1, mails, y_encoded, cv=10)\nconfusion_matrix(y_encoded, y_predicted_1)\n", "intent": "<b>Confusion Matrix:</b> It seems that the number of false positives and false negatives are balanced.\n"}
{"snippet": "kf = StratifiedKFold(n_splits = 10)\nnp.mean(cross_val_score(pipeline2, mails, y_encoded, scoring=\"accuracy\", cv=kf))\n", "intent": "<b>Accuracy:</b> around 95%, depends on the shuffle, it seems that the pipeline with CountVectorizer performs slightly better.\n"}
{"snippet": "kf = StratifiedKFold(n_splits = 10)\nnp.mean(cross_val_score(pipeline3, mails, y_encoded, scoring=\"accuracy\", cv=kf))\n", "intent": "<b>Accuracy - in case of nGramAnalyzer</b>\n"}
{"snippet": "kf = StratifiedKFold(n_splits = 10)\nnp.mean(cross_val_score(pipeline3_only_bigrams, mails, y_encoded, scoring=\"accuracy\", cv=kf))\n", "intent": "<b>Accuracy - in case of nGramAnalyzer_only_words</b>\n"}
{"snippet": "lr.predict_proba(np.array([50]).reshape(-1, 1))\n", "intent": "This is the multiplier for how the odds change per unit change of Age. We can see the effect of changing age:\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    score = r2_score(y_true,y_predict)\n    return score\n", "intent": "Using $R^2$ as a measurement index  \nWith `r2_score` in `sklearn.metrics` to caculate\n"}
{"snippet": "baseline = np.repeat(np.mean(y_test), len(y_test))\nprint(f'Root Mean Square Error of baseline model is: \\\n${int(mean_squared_error(y_test, baseline)**0.5)}.')\n", "intent": "The RMSE is quite good, and similar for the training and test data\n"}
{"snippet": "model = clf\ny_train_preds = model.predict_proba(X_train_tf)[:,1]\ny_valid_preds = model.predict_proba(X_valid_tf)[:,1]\n", "intent": "Calculate probability of re-admission\n"}
{"snippet": "print ('Intercept:')\nprint regr.intercept_\nprint ('Coefficients:')\nprint regr.coef_\nprint ('Mean squared error:')\nprint \"%.3f\" % np.mean((regr.predict(churn_X_test) - churn_y_test) ** 2)\nprint ('Variance score:')\nprint \"%.3f\" % regr.score(churn_X_test, churn_y_test)\n", "intent": "The scatter plots show positive linear trend. TotalCharges increase as the tenure increase. Both associate to each other.\n"}
{"snippet": "from sklearn import metrics\nprint((\"Neigh's Accuracy: \"), metrics.accuracy_score(y_testset, pred))\n", "intent": "Awesome! Now let's compute neigh's <b>prediction accuracy</b>. We can do this by using the <b>metrics.accuracy_score</b> function\n"}
{"snippet": "print((\"Neigh23's Accuracy: \"), metrics.accuracy_score(y_testset, pred23))\nprint((\"Neigh90's Accuracy: \"), metrics.accuracy_score(y_testset, pred90))\n", "intent": "Interesting! Let's do the same for the other instances of KNeighborsClassifier.\n"}
{"snippet": "print((\"RandomForests's Accuracy: \"), metrics.accuracy_score(y_testset, predForest))\n", "intent": "Let's check the accuracy of our model. <br>\nNote: Make sure you have metrics imported from sklearn\n"}
{"snippet": "preds = model.predict(X_test)\nprint(classification_report(teY, np.argmax(preds, axis=1)))\n", "intent": "Then evaluate the the quality of the predictions for each digit:\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nexplained_variance_score(ytest, ypredselfNW)  \n", "intent": "Can also compute [sklearn metrics](http://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "from dask import compute, delayed\ndef process(Xin):\n    return model.predict(Xin)\ndobjs = [delayed(process)(x.reshape(1,-1)) for x in Xtest]\n", "intent": "Only need this if Xtest is too big\n"}
{"snippet": "ypred = clf.predict(Xtest)\n", "intent": "Finally, do the classification and output the test file, including the predicted labels.\n"}
{"snippet": "from dask import compute, delayed\ndef processSVM(Xin):\n    return svm.predict(Xin)\ndobjsSVM = [delayed(processSVM)(x.reshape(1,-1)) for x in XStest]\n", "intent": "Finally, do the classification and output the test file, including the predicted labels.\n"}
{"snippet": "clf.evaluate(get_input_fn_from_pandas(test,10))\n", "intent": "Predict on the 20% unseen dataset\n"}
{"snippet": "index_database = encoder.predict(batch)\n", "intent": "Let's prepare our index\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,1]\nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)\nprobs0 = probs[:, 0]\nprobs1 = probs[:, 1]\nprint('preds=%s' % preds[:8])\nprint('probs=%s' % describe(probs))\nprint('probs0=%s' % probs0[:8])\nprint('probs1=%s' % probs1[:8])\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "xgb_preds = np.expm1(model_xgb.predict(X_test))\n", "intent": "(colsample_bylevel has no effect if num_row > 2**15 and tree_method=approx )\n"}
{"snippet": "def log_reg_predict(coeff, x):\n    pass\ndef log_reg_coef_sgd(features, labels, l_rate, num_epochs):\n    pass\ndef DP_log_reg_coef_sgd(features, labels, l_rate, num_epochs, eps):\n    pass\nprint \"Utility functions loaded\"\n", "intent": "The following provides a guide of the structure to utilize for implementing Logistic Regression with DP-updates\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We try out our model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We try out our model on the test dataset of dog images. We ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    i = 0\n    for layer in style_layers:\n        gram = gram_matrix(feats[layer])\n        style_loss += style_weights[i] * (tf.reduce_sum((gram - style_targets[i])**2))\n        i+=1\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.3f' %accuracy_score(y_true=y_test,y_pred=y_pred))\nnb_clf_acc = accuracy_score(y_true=y_test,y_pred=y_pred)\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test,y_pred))\n", "intent": "Our Naive Bayes classifier has accuracy of 85.5% with an AUC of .929.  Recall is noticebly higher for the positive class (MBA).\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.3f' %accuracy_score(y_true=y_test,y_pred=y_pred))\nsvm_clf_acc = accuracy_score(y_true=y_test,y_pred=y_pred)\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test,y_pred))\n", "intent": "Our SVM model has an accuracy of 88.1% and an AUC of .944.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.3f' %accuracy_score(y_true=y_test,y_pred=y_pred))\nsgd_clf_acc = accuracy_score(y_true=y_test,y_pred=y_pred)\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test,y_pred))\n", "intent": "Our Stochastic Gradient Descent Classifier has an accuracy of 86.3% and an AUC of 0.932.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: %.3f' %accuracy_score(y_true=y_test,y_pred=y_pred))\nrf_clf_acc = accuracy_score(y_true=y_test,y_pred=y_pred)\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test,y_pred))\n", "intent": "Our Random Forest model has an accuracy of 86% and an AUC of .938.  Precision and recall are very close for both classes.\n"}
{"snippet": "bronx_rat_forecast = bronx_rat_prophet.predict(test_data.query(\"Borough == 'Bronx'\"))\nbk_rat_forecast = bk_rat_prophet.predict(test_data.query(\"Borough == 'Brooklyn'\"))\nmnhtn_rat_forecast = mnhtn_rat_prophet.predict(test_data.query(\"Borough == 'Manhattan'\"))\nqns_rat_forecast = qns_rat_prophet.predict(test_data.query(\"Borough == 'Queens'\"))\nsi_rat_forecast = si_rat_prophet.predict(test_data.query(\"Borough == 'Staten Island'\"))\nbk_rat_forecast.head()\n", "intent": "---\nMaking predictions\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npreds = forecast_data[\"yhat\"]\nactuals = forecast_data[\"y\"]\nmean_absolute_error(preds, actuals)\n", "intent": "---\nHow Accurate is the forecasting?\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L = 0\n    for style_layer, style_target, style_weight in zip(style_layers, style_targets, style_weights):\n        G = gram_matrix(feats[style_layer])\n        L += style_weight * torch.pow(G - style_target, 2).sum()\n    return L\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint(\"Accuracy is {:.2f}\".format(np.mean(y_test == y_pred)))\nprint(\"Accuracy is {:.2f}\".format(knn.score(X_test, y_test)))\n", "intent": "Estimation of accuracy\n"}
{"snippet": "print(classification_report(y_test, rf.predict(X_test)))\n", "intent": "Classification report for RF\n"}
{"snippet": "results = modelf.predict(x_test_np)\nresults\n", "intent": "And now for the application of our uploaded model to the test data. Cross your fingers!\n"}
{"snippet": "model.predict_log_proba(X)\n", "intent": "***model.predict_log_proba(X)***\nLike predict_proba except that it returns the log probabilities instead of the probabilities.\n"}
{"snippet": "y_hat = BayesianNetwork.from_samples(X_missing, max_parents=1).predict(X_missing)\nnumpy.abs(numpy.array(y_hat)[i, j] - X[i, j]).mean()\n", "intent": "Now, we can try building a Bayesian network using the Chow-Liu algorithm and then use the resulting network to fill in the matrix.\n"}
{"snippet": "model.predict_proba([[False, False, False, False, None, None, False, None, False, None, \n                      True, None, None, True, False]])\n", "intent": "The predict method will simply return the argmax of all the distributions after running the `predict_proba` method. \n"}
{"snippet": "y_pred = numpy.concatenate([model.predict(seq) for seq in X_test_seqs])\n", "intent": "Now we can make predictions using the maximum a posteriori (MAP) algorithm.\n"}
{"snippet": "print \"Complete Match: \", (model.predict(X) == model.predict(X, n_jobs=4)).all()\nprint model.predict(X[:20])\nprint model.predict(X[:20], n_jobs=4)\n", "intent": "Now let's make sure that we are getting the same results for both of them.\n"}
{"snippet": "(model.predict_proba(X[:100]) - model.predict_proba(X[:100], n_jobs=4)).sum()\n", "intent": "We can also confirm that this is producing the correct result.\n"}
{"snippet": "from sklearn import metrics\ny_hat = clf.predict(X)\nprint('confusion matrix:')\nmetrics.confusion_matrix(y_true=y, y_pred=y_hat)\n", "intent": "Get confusion matrix\n"}
{"snippet": "kfold = KFold(n =X.shape[0] ,n_folds=5, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Results mean: %.2f\" % results.mean())\nprint(\"Results std: %.3f\" % results.std())\n", "intent": "The final step is to evaluate this baseline model. We will use **5-fold** cross validation to evaluate the model.\n"}
{"snippet": "print(classification_report(y_test, pred))\n", "intent": "Let's take a look at our classification report to examine accuracy of our predictions.\n"}
{"snippet": "result = opt.fmin_bfgs(cost_fn, theta, args=(X, y, lamb), maxiter=500, disp=False)  \npredictions = predict(theta_min, X)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]  \naccuracy = (sum(map(int, correct)) % len(correct))  \nprint ('BFGS accuracy = {0}%'.format(accuracy)  )\nprint(f'result object: \\n {result}')\n", "intent": "Try using BFGS, without f-prime/grad_fn\n"}
{"snippet": "result = opt.minimize(cost_fn, theta, args=(X, y, lamb))  \npredictions = predict(theta_min, X)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]  \naccuracy = (sum(map(int, correct)) % len(correct))  \nprint ('Minimize accuracy = {0}%'.format(accuracy)  )\nprint(f'result object: \\n {result}')\n", "intent": "Try using with generic minimize function that defaults to BFGS, L-BFGS-B, SLSQP, depending if the problem has constraints or bounds.\n"}
{"snippet": "client_data = [[5, 17, 15],\n               [4, 32, 22],\n               [8, 3, 12]]\nfor i, price in enumerate(reg.predict(client_data)):\n    print \"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price)\n", "intent": "What price would you recommend each client sell his/her home at? Do these prices seem reasonable given the values for the respective features?\n"}
{"snippet": "probDist=model2_lr_std.predict_proba(X_test)\nprint(\"Probability distribution is-\", probDist)\npredProb=model2_lr_std.predict(X_test_std)\nprint('')\nprint(\"Predicted probabilty is-\",predProb)\nintercept_weight=model2_lr_std.intercept_[0]\nprint(\"The bias weight is-\",intercept_weight)\nprint('')\npridprob=model2_lr_std.predict_proba(X_test_std)\nprint(pridprob)\n", "intent": "In this section, we deal with calculating evidence for the different types of objects-\n"}
{"snippet": "nb.predict_proba(X_test_dtm)\n", "intent": "btw, naive bayes not precise with probabilities. Good classifier, not as good at predictive probabilities\n"}
{"snippet": "def predict(dataset = \"./test\"):\n\tcmd = \"{0} '{1}' ./train.model ./test.out\".format(svmpredict_exe, dataset)\n\texecute = os.system(cmd)\n", "intent": "Use LIBSVM utiliteis to predict.\n"}
{"snippet": "local_results = classifier.predict(test_features)\nlocal = pd.Series(local_results, name='local')\npd.crosstab(actual,local)\n", "intent": "Use a confusion matrix create a visualization of the predicted results from the local model. These results should be identical to the results above.\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.ERROR)\nsample_flower = np.array( [[-1.501490,0.097889,-1.279104,-1.311052]], dtype=float)\npred = list(classifier.predict(sample_flower, as_iterable=True))\nprint(\"Predict that {} is: {}\".format(sample_flower,species[pred]))\nsample_flower = np.array( [[1.0, 2., 3., 1.],[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]], dtype=float)\npred = list(classifier.predict(sample_flower, as_iterable=True))\nprint(\"Predict that {} is: {}\".format(sample_flower,species[pred]))\n", "intent": "Now that you have a neural network trained, we would like to be able to use it.  The following code makes use of our neural network.\n"}
{"snippet": "results_train = metrics.classification_report(y_true=(y_train<=2.5).astype(int), y_pred=y_pred_train)\nprint(results_train)\nmetrics.accuracy_score(y_true=(y_train<=2.5).astype(int), y_pred=y_pred_train)\n", "intent": "This model simply predicts if the given recipe will be rated high or low. Let's see how it performs on the test set.  \n"}
{"snippet": "x_test = np.array(['I am not '])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\npred = model.predict(X_test)\npred = np.argmax(pred,axis = 1) \ny_true = np.argmax(y_test,axis = 1)\n", "intent": "**For better performance use exponential decaying learning rate and specify steps_per_epoch**\n"}
{"snippet": "scores = cross_val_score(lr, X, boston_y, cv=5)\nprint(scores.mean(),scores.std())\n", "intent": "We may also want to see an overall score for these folds. Typically, we will take the average $R^2$ across each fold:\n"}
{"snippet": "new_sample = [[1, 1]]\nprint(mnb.predict_proba(new))\nprint(bnb.predict_proba(new))\n", "intent": "The models are fit now. Let's apply it to a sample, where the features `x1` and `x2` are both 1.\n"}
{"snippet": "Y_pred = vote_clf.predict(X)\nID = testset['PassengerId']\nfilename = 'result.csv'\nmyfile = open(filename,'w')\ntitleRow = 'PassengerID,Survived\\n'\nmyfile.write(titleRow)\nfor i in range(len(Y_pred)):\n    row = str(ID[i]) + ',' + str(Y_pred[i]) + '\\n'\n    myfile.write(row)\nmyfile.close()\n", "intent": "We then apply Random Forest Classification to the processed data and publish the result into a .csv file\n"}
{"snippet": "X = test_data.drop('Result',axis=1)\nX = X.reindex_axis(sorted(X.columns), axis=1).values\np=logistic_reg.predict_proba(X)\ngt = np.zeros_like(p)\ngt[:,1]=test_data['Result'].values\ngt[:,0] = 1 - gt[:,1]\nlogloss = sklearn.metrics.log_loss(gt,p)\nprint('All Features Log Loss is %1.4f'%logloss)\n", "intent": "Let's check our resulting log loss.\n"}
{"snippet": "X = df_sample.drop(['Pred','Team','Opponent'],axis=1)\nX = X.reindex_axis(sorted(X.columns), axis=1).values \npreds=logistic_reg.predict_proba(X)[:,1]\nclipped_preds = np.clip(preds, 0.01, 0.99)\ndf_sample['Pred'] = clipped_preds\ndf_sample.head()\n", "intent": "Now that we have our table of stats, we can use our model and save our predictions.\n"}
{"snippet": "gt = np.zeros_like(logreg_preds)\ngt[:,1]=y_test\ngt[:,0] = 1 - gt[:,1]\nlogloss = sklearn.metrics.log_loss(gt,logreg_preds)\nprint(logloss)\n", "intent": "Ok, not great but probably about as expected.  Let's look at the log loss (ie the cross entropy).  \n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n", "intent": "Predicting the Test Results\n"}
{"snippet": "import math\nfrom sklearn.metrics import mean_squared_error\nrmse = math.sqrt(mean_squared_error(real_stock_price, predicted_stock_price))\nrmse\n", "intent": "Calculating the Root Mean Square Error as the metric\n"}
{"snippet": "import sklearn.metrics as skm\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\ntraining_pred = adb.predict(feature_train)\nprint (skm.confusion_matrix(list(training_pred), list(results_train)))\n", "intent": "We can now see how well the trained algorithm fits the training data.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(trf_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "copy = img[y:y+h, x:x+w, :3]\ncopy = pre_process(copy, dims)\nfeat = hog.compute(copy)\nprob = lsvm.predict_proba(feat.reshape(1,-1))[0]\nprint(\"Probability of this being a coffee cup is %0.2f percent\" % (prob[1] * 100))\n", "intent": " * We then compute HOG only for what's inside the bounding box\n * Also showing whether our trained SVM \"thinks\" it is a coffee cup or not\n"}
{"snippet": " xgb_clf.booster().get_score()\n", "intent": "However, it's not yet in the pip repository:\n"}
{"snippet": "predictions = model.predict(X_test_arr)\nrounded = [round(x[0]) for x in predictions]\na = np.dot(rounded-Y_test_arr, rounded-Y_test_arr)\nprint(1.0-a/len(rounded))\n", "intent": "The **acc** is the accuracy defined by the ratio we have right prediction. This is the same as doing:\n"}
{"snippet": "print (model.predict_proba(X_test_arr)[:10])\n", "intent": "We can further compute the probability of each Pima Indian patient having diabetes as\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nprint(\"MAE San Juan:\", mean_absolute_error(sj['total_cases'], sj['fitted']))\nprint(\"MSE San Juan:\", mean_squared_error(sj['total_cases'], sj['fitted']))\nprint(\"MAE Iquitos:\", mean_absolute_error(iq['total_cases'], iq['fitted']))\nprint(\"MSE Iquitios:\", mean_squared_error(iq['total_cases'], iq['fitted']))\n", "intent": "Performed on subset of the training data\n"}
{"snippet": "res50_predictions = [np.argmax(roh_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res50]\ntest_accuracy = 100*np.sum(np.array(res50_predictions)==np.argmax(test_targets, axis=1))/len(res50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_prob = clf.predict(X_test)\ny_pred = binarize_prediction(y_prob, threshold=0.5)\n", "intent": "Once we have the trained model, we can obtain some metrics.\n"}
{"snippet": "ridge_predict = pd.Series(np.exp(model.predict(X_predict)))\n", "intent": "The Ridge model showed slightly better training and testing error compared to the XGBoost model, but overall gave comparable results.\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Detect images of the number 5:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "Evaluate the model using cross-validation:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_5, y_train_pred)\n", "intent": "Another interesting (and more concise) metric is the accuracy of the positive predictions, called the *precision* of the classifier:\n"}
{"snippet": "recall_score(y_train_5, y_train_pred)\n", "intent": "Precision is typically used with another metric named *recall*, also called *sensitivity* or *true positive rate (TPR)*:\n"}
{"snippet": "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method=\"decision_function\")\n", "intent": "Decide which threshold to use:\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n", "intent": "Compute the *area under the curve* (AUC):\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Evaluate the classifier's accuracy:\n"}
{"snippet": "knn_clf.predict([some_digit])\n", "intent": "Is it large? Is it odd?\n"}
{"snippet": "from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, log_loss, make_scorer\nfrom sklearn.model_selection import GridSearchCV\ndef calculate_metrics(model):\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy: {:.2f}%\".format(accuracy * 100.0))\n    y_pred_prob = model.predict_proba(X_test)\n    loss = log_loss(y_test, y_pred_prob)\n    print(\"Log Loss: {:.4f}\".format(loss))\n", "intent": "The classifer will first be run with the default parameters then using GridSearch a few parameters will be refined to arrive at an optimized model.\n"}
{"snippet": "x_test = np.array(['I want a cup of tea'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "boost_test_pred_probs = \\\n    boost_model.predict_proba(\n        X=cs_test[X_var_names])\nboost_test_oos_performance = \\\n    bin_classif_eval(\n        boost_test_pred_probs[:, 1], cs_test.SeriousDlqin2yrs,\n        pos_cat=1, thresholds=selected_prob_threshold)\nboost_test_oos_performance\n", "intent": "Let's then evaluate the performance of the selected Boosted Trees model at the decision threshold determined above:\n"}
{"snippet": "print ('R^2 score for the model: ', model.score(X_train,y_train))\ny_pred = model.predict(X_test)\nprint('Root mean squared error between y_pred and y_test: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Let's get the R^2 coefficient for our model and see how well it performs on our test data\n"}
{"snippet": "eda.gender_vs_score(df)\n", "intent": "We wanted to see if male directors are better, or how does the gender of the main actor influences the movie score.\n"}
{"snippet": "print(\"Mean squared error: %.2f\" % np.mean((lm.predict(X) - bos.PRICE) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "pd.crosstab(train_y, knn.predict(train_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])\n", "intent": "<p>Now we'll see how accurate the model is on the training dataset and then the test dataset. </p>\n"}
{"snippet": "loss = tf.losses.log_loss(y, y_proba)  \n", "intent": "The same with TensorFlow's ``log_loss`` function:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "RDGpredictions=RDGmdl.predict(Xte)\nRDGpredictions=pd.Series(RDGpredictions)\nRDGpredictions.describe()\n", "intent": "Predicting with ridge classifier\n"}
{"snippet": "RFpredictions=RFmdl.predict(Xte)\nRFpredictions=pd.Series(rftemp)\nRFpredictions.describe()\n", "intent": "Predicting with Random Forest classifier.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(DogeModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = log_model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)\n", "intent": "And finally, we use this classifier to label the evaluation set we created earlier:\n"}
{"snippet": "score = metrics.accuracy_score(y_test,y_pred)\nprint(score)\nprint(\"accuracy: %0.3f\" % score)\nprint(metrics.classification_report(y_test,y_pred))\n", "intent": "The following cell evaluates your result by comparing it with the test labels.\n"}
{"snippet": "accuracy = model.evaluate(x=train_x,y=train_y,batch_size=32)\n", "intent": "Now we can evaluate if the trained model is working well by feeding in the test_x data and comparing the results with the train_y \n"}
{"snippet": "np.mean(cross_val_score(lr, transformed_sd_like[['PC1']], reading, cv=5))\n", "intent": "- What is the mean cross-validation score?\n"}
{"snippet": "np.mean(cross_val_score(lr, transformed_sd_like[['PC1', 'PC2', 'PC3']], reading, cv=5))\n", "intent": "- What is the mean cross-validation score?\n"}
{"snippet": "P_test = np.dot(X_test-mu,W);\ny_predict = clf.predict(P_test)\n", "intent": "Next, project test data into 2D subspace and test our trained model\n"}
{"snippet": "nnproba_s = clf.predict_proba(nn_test.drop('Survived', axis=1))\nlog_model.predict_proba(trainFeaturesFor(test))\nprint('I think I need to do something with these')\n", "intent": "1. Set up a voting classifier that uses the logistic nn model.\n2. Is a way to do the pd.get_dummies where you can pass in all of the valid values. \n"}
{"snippet": "ytr_pred = regr.predict(Xtr)\nRSS_tr = np.mean((ytr_pred-ytr)**2)/(np.std(ytr)**2)\nprint(\"Normalized training RSS=%f\" % RSS_tr)\nyts_pred = regr.predict(Xts)\nRSS_rel_ts = np.mean((yts_pred-yts)**2)/(np.std(yts)**2)\nprint(\"Normalized test RSS = {0:f}\".format(RSS_rel_ts))\n", "intent": "Measure and print the normalized RSS on the test data.\n"}
{"snippet": "scores = cross_val_score(linreg, X, y, cv=10, scoring='mean_squared_error')\nnp.mean(np.sqrt(-scores))\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    return sum([w * sum_square(gram_matrix(feats[l]) - t)\n                for l, t, w in zip(style_layers, style_targets, style_weights)])\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "for name, val in zip(model.metrics_names, model.evaluate(x_test3, y_test3)):\n    print(\"%s: %f\" % (name, val))\n", "intent": "We evaluate the trained model on our `x_test` dataset. Simply calling `model.evaluate` does the trick.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ndef rmse(target, predicted):\n    return sqrt(mean_squared_error(target, predicted))\nrmse(df.taste, predictions)\n", "intent": "$$\nRMSE = \\sqrt{\\frac{1}{n}\\sum_i\\left(y_i - f(x_i)\\right)}\n$$\n"}
{"snippet": "predictions = rf1.predict(test)\n", "intent": "Predicting the test dataset and saving it to csv file\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_lg, logreg.predict(new_data)))\n", "intent": "The predictions consist of two values. The second value is the probability and the observation is a target.\n"}
{"snippet": "print(confusion_matrix(y_lg, logreg_new.predict(new_data1)))\n", "intent": "In sample prediction accuracy using confusion matrix and accuracy scores. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracyFull= accuracy_score(y_lg, logreg_new.predict(new_data1))\naccuracyFull\n", "intent": "The result is telling us that we have 13472+980 correct predictions and 3170+980 incorrect predictions.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg, new_data1, y_lg)\nscores.mean()\n", "intent": "Cross validation is a clever method to avoid overfitting. \n"}
{"snippet": "n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "We use the cross_val_score function of Sklearn\n"}
{"snippet": "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average=\" \"))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average=\" \"))\nprint('Recall: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average=\" \"))\n", "intent": "How do these results differ from the previous results and why?\n"}
{"snippet": "example = [\"Free Viagra call today\"]\nexample_counts = count_vectorizer.transform(example)\nprediction = nb_clf.predict(example_counts);\nprediction[0]\n", "intent": "Now that we have seen the score, let's test it on an example. <br/>\nIs the prediction correct?\n"}
{"snippet": "examples = [\"Free Viagra call today!\", \n            \"I'm going to attend the Python Learning group tomorrow.\",\n            \"Free Viagra Free Viagra Free Viagra\",\n            \"Today we will learn about Machine Learning\"]\nexample_counts = count_vectorizer.transform(examples)\npredictions = nb_clf.predict(example_counts)\nfor i in range(len(predictions)):\n    print(\"E-Mail %i: %s\"%(i, predictions[i]))\n", "intent": "A few more examples. Are the predictions also correct\n"}
{"snippet": "your_test_mail = [\" ... \"]\ntest_mail_counts = count_vectorizer.transform(your_test_mail)\nprint(nb_clf.predict(test_mail_counts))\n", "intent": "Now try your own example. Type in some E-Mail text inside the quotes and see if the prediction is correct\n"}
{"snippet": "y_pred = svm_clf.predict(X_test)\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\nprint('F1-Score: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\n", "intent": "Don't forget to look at the other scores as well\n"}
{"snippet": "example = [\"Free Viagra call today\"]\nexample_counts = count_vectorizer.transform(example)\nprediction = svm_clf.predict(example_counts);\nprediction[0]\n", "intent": "And test it with an example.\n"}
{"snippet": "X = one_hot_encode([7, 3, 2, 3, 7, 4, 6, 4, 6, -9999])\nyhat = model.predict(X, verbose=0)\nprint(\"*\"*70)\nprint(\"Test case \nprint(\"Input :                \", one_hot_decode(X[0]))\nprint(\"Expected Output:       \", one_hot_decode(y[0]))\nprint(\"Model Predicted Output:\", one_hot_decode(yhat[0]))\nprint(\"*\"*70)\nprint(\"\\n\")\n", "intent": "Enter in your own sequence to test the model.\n"}
{"snippet": "ypred = model.predict(Xtrain)\nypred_test = model.predict(Xtest)\npredround = [int(round(x)) for x in ypred]\nprint (\"The accuracy score of the linear model on the train set is {}\"\n       .format(metrics.accuracy_score(ytrain, predround)))\npredround_test = [int(round(x)) for x in ypred_test]\nprint (\"The accuracy score of the linear model on the test set is {}\"\n       .format(metrics.accuracy_score(ytest, predround_test)))\n", "intent": "We used a linear model rather than a logistic model as although the y-varaible was categorical (1, 2, 3, 4, or 5), it was ordinal.\n"}
{"snippet": "from sklearn import metrics\nprint('Random Forest')\nprint('Accuracy on training data:', clf_forest.score(X_train, y_train))\nprint(metrics.classification_report(y_test, predicted_forest,\n    target_names=['<50k', '>=50k']))\nprint('Gaussian Naive Bayes')\nprint('Accuracy on training data:', clf_gnb.score(X_train, y_train))\nprint(metrics.classification_report(y_test, predicted_gnb,\n    target_names=['<50k', '>=50k']))\n", "intent": "2 pts Compute the accuracy of predictions on the test set.\n"}
{"snippet": "from sklearn import metrics\nprint((metrics.accuracy_score(y_test, y_pred_class))*100)\n", "intent": "- Classification Accuracy : percentage of correct prediction\n"}
{"snippet": "from sklearn import metrics\nprint((metrics.accuracy_score(y_train, y_pred_class_train))*100)\n", "intent": "- Classification Accuracy : percentage of correct prediction\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Original'],kmeans.labels_))\nprint(classification_report(df['Original'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "prediction = nb.predict(X_test)\n", "intent": "_________\nTime to see how our model did!\n"}
{"snippet": "grid_predictions = gridsearch.predict(X_test)\n", "intent": "After running Gridsearch, we've found out the optimal value for C and gamma through rounds of testing. Let's try using these parameters now.\n"}
{"snippet": "xg_test_preds = xg.predict(X_test)\nX_test_with_xg = X_test\nX_test_with_xg['xg_pred'] = xg_test_preds\nX_test_with_xg.head()\n", "intent": "Seems to be really good! Maybe overfitting? Let's try to submit and see.\n"}
{"snippet": "elastic_preds = elastic_model.predict(X_train)\nX_train_with_elastic = X_train\nX_train_with_elastic['elastic_pred'] = elastic_preds\nX_train_with_elastic.head()\n", "intent": "Pretty good! This is our best yet, but let's try to stack one more time.\n"}
{"snippet": "LGBM = LGBMRegressor(learning_rate = 0.2, n_estimators =100)\nnum['y_predict'] = CV_LGBM.predict(num)\n", "intent": "Using the tuned hyperparameters\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We try out our model on the test dataset of dog images.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "We try out our model on the test dataset of dog images.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "- call some sample prediction based on test dataset. evaluate the model\n"}
{"snippet": "score = [0]*nn\nfor j in range(nn):\n    score[j] = model[j].evaluate(x_test,y_test)\n", "intent": "Now we will calculate the test loss and accuracy. We can calculate the avg loss and accuracy\n"}
{"snippet": "ResNet50bottleneck_predictions = [np.argmax(ResNet50bottleneck_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50bottleneck]\ntest_accuracy_ResNet50bottleneck = 100*np.sum(np.array(ResNet50bottleneck_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50bottleneck_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_ResNet50bottleneck)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_final = logRegGrid.predict(final_test_s)\n", "intent": "Next we predict whether the passengers in the final test, <code>final_test_s</code>, would have survived the titanic disaster.\n"}
{"snippet": "probabilities = model.predict(X_holdout, batch_size = 32)\n", "intent": "And now calling predictions on the holdout set, as shown below. MAKE SURE to clear the memory before this step!\n"}
{"snippet": "first100_linpredict = lm.predict(first100_df_train)\nfirst100_linpredict\n", "intent": "_(2.3) Print the predicted target feature value for the first 100 training examples_\n"}
{"snippet": "print(\"Accuracy: \", metrics.accuracy_score(x, df_ChurnClass3))\nprint(\"Confusion matrix: \\n\", metrics.confusion_matrix(x, df_ChurnClass3))\nprint(\"Classification report:\\n \", metrics.classification_report(x, df_ChurnClass3))\nmse = ((df_test.churn - lm.predict(df_test))** 2).mean()\nprint(\"\\nMean Squared Error:\\n\", mse)\nmae = abs(df_test.churn - lm.predict(df_test)).mean()\nprint(\"\\nMean Absolute Error:\\n\", mae)\n", "intent": "_(2.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set_\n"}
{"snippet": "df_log100 = X_train[:100]\ntrain_logreg.predict_proba(df_log100)\n", "intent": "_(3.3) Print the predicted target feature value for the first 100 training examples_\n"}
{"snippet": "logpredictions_train = train_logreg.predict(X_train)\nlogpredictions_100 = logpredictions_train[:100]\nprint(\"Predictions for first 100 rows: \\n\", logpredictions_100)\n", "intent": "_(3.3) Print the predicted class for the first 100 examples_\n"}
{"snippet": "def mean_error(vector):\n    return sum((vector - vector.mean())**2)\ndef eval_split(df, split):\n    return (mean_error(df[df.ix[:,0] >= split]['target']) \n            + mean_error(df[df.ix[:,0] < split]['target']))\n", "intent": "Define method that enables to evaluate a split given the value\n"}
{"snippet": "for leaf_size in [200, 100, 50, 20, 10]:\n    rules = train(train_data, leaf_size)\n    predict(test_data, rules)\n    print('Leaf size {0} RMSE {1}'.format(leaf_size, np.mean((test_data['target'] - test_data['prediction'])**2)))\n", "intent": "Can we improve this score by expanding the structure?\n"}
{"snippet": "predictions = model_survival.predict(X_test)\n", "intent": "model_survival = DecisionTreeClassifier(max_leaf_nodes=12, random_state=0)\nmodel_survival.fit(X, y)\n"}
{"snippet": "def content_loss(content, combination):\n    return backend.sum(backend.square(combination - content))\nlayer_features = layers['block2_conv2']\ncontent_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(content_image_features,\n                                      combination_features)\n", "intent": "Now the content loss:\n"}
{"snippet": "def total_variation_loss(x):\n    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n    return backend.sum(backend.pow(a + b, 1.25))\nloss += total_variation_weight * total_variation_loss(combination_image)\n", "intent": "Finally the total variation loss:\n"}
{"snippet": "import datetime\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\ndatetime.datetime.now().strftime(\"%d.%b %Y %H:%M:%S\")\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "clf.predict(digits.data[-1:])\n", "intent": "We used all the digits aside from the last one to train the classifier. Lets see how well it predicts the last\ndigit\n"}
{"snippet": "minimize = tf.train.GradientDescentOptimizer(0.5).minimize(tf.losses.get_total_loss())\n", "intent": "The convenience function `tf.losses.get_total_loss` returns the total loss in your graph, including potential regularization terms.\n"}
{"snippet": "print(np.mean(bos.PRICE - lm.predict(X) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "> The F1 score is the harmonic mean of\nprecision and recall\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):    \n    def fit(self, x, y=None):        \n        pass    \n    def predict(self, x):        \n        return np.zeros((len(x), 1), dtype=bool) \n", "intent": "On the other hand, this will show the accurancy of not having a five in our image\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_5,y_pred)\n", "intent": "This function helps us to compare 2 classifiers.\n"}
{"snippet": "y_pred_90 = (y_score > 0)\nprecision_score(y_5, y_pred_90)\n", "intent": "Precision may go down as we raise our threshold\n"}
{"snippet": "[ \n    parameter_,\n    data_generator_,\n] = evaluate([ \n    parameter, \n    data_generator,\n])\nprint(\"'parameter_' evaluated Tensor :\", parameter_)\nprint(\"'data_generator_' sample evaluated Tensor :\", data_generator_)\n", "intent": "More generally, we can use our `evaluate()` function to convert between the Tensorflow `tensor` data type and one that we can run operations on:\n"}
{"snippet": "n_submissions_ = len(votes)\nsubmissions = tfd.Uniform(low=float(0.), high=float(n_submissions_)).sample(sample_shape=(4))\nsubmissions_ = evaluate(tf.to_int32(submissions))\nprint(\"Some Submissions (out of %d total) \\n-----------\"%n_submissions_)\nfor i in submissions_:\n    print('\"' + contents[i] + '\"')\n    print(\"upvotes/downvotes: \",votes[i,:], \"\\n\")\n", "intent": "Above is the top post as well as some other sample posts:\n"}
{"snippet": "expert_prior_mu = tf.constant([x[0] for x in expert_prior_params_.values()], dtype=tf.float32)\nexpert_prior_std = tf.constant([x[1] for x in expert_prior_params_.values()], dtype=tf.float32)\ntrue_mean = tf.to_float(stock_returns.mean())\nprint(\"Observed Mean Stock Returns: \\n\", evaluate(true_mean),\"\\n\")\ntrue_covariance = tf.to_float(stock_returns.cov().values)\nprint(\"\\n Observed Stock Returns Covariance matrix: \\n\", evaluate(true_covariance))\n", "intent": "And here let's form our basic model:\n"}
{"snippet": "from keras import backend as K\ndef max_margin_loss(y_true, y_pred):\n    print(y_pred.shape)\n    return K.sum(K.maximum(0.0, 1.0 - y_pred[:,0] + y_pred[:,1]))\n", "intent": "\\begin{equation*}\nloss = \\sum_i{max(0, 1 -p_i + n_i)}\n\\end{equation*}\n"}
{"snippet": "def content_loss(Ac, Ag):\n    N = Ac.shape[3]\n    M = Ac.shape[1] * Ac.shape[2]\n    return tf.reduce_sum(tf.square(Ac - Ag)) / (4.0 * M * N)\n", "intent": "Here we calculate the Content Loss, Style Loss and Total Loss.\n"}
{"snippet": "print(\"Residual sum of squares:\", ((lm.predict(X) - y) ** 2).sum())\n", "intent": "We can measure the overall error of the fit by calculating the **Residual Sum of Squares**:\n$$RSS = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(\"Testing:\")\nprint('MSE:', metrics.mean_squared_error(y_test, gen_y_test))\nprint('R_squared:', metrics.r2_score(y_test, gen_y_test))\n", "intent": "And now let's assess the fit of the line on the unsees testing data.\n"}
{"snippet": "predictions = rf.predict(X_test)\nerrors_reg = abs(predictions - y_test)\nprint('Mean Absolute Error:', round(np.mean(errors_reg), 2))\n", "intent": "Now we'll use the trained model to make predictions from the test set.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "N = 315\nshow_digit(test_values[N])\nprediction_onehot = lr.predict(test_values[N:N+1])\nprediction = np.argmax(prediction_onehot)\ntarget = test_labels[N].item()\nprint(f'prediction={prediction}\\ttarget={target}')\n", "intent": "Visualize an example\n"}
{"snippet": "N = 315\nshow_digit(test_values[N])\nprediction = np.argmax(model.predict(test_values[N:N+1]))\nprint(f'prediction={prediction}\\ttarget={test_labels[N]}')\n", "intent": "Visualize an example\n"}
{"snippet": "xgb.set_params(n_estimators=1000)\nxgb = modelfit(xgb, train_txf_df, train_txf_target)\nprint('Accuracy:')\ncompute_cv_score(xgb, train_txf_df, train_txf_target, scoring='accuracy')\nprint('F1:')\ncompute_cv_score(xgb, train_txf_df, train_txf_target, scoring='f1')\n", "intent": "- The submission score is 0.76077 (even its local score is quite good)\n"}
{"snippet": "xgb.set_params(n_estimators=1000)\nxgb = modelfit(xgb, train_txf_df, train_txf_target)\nprint('Accuracy:')\ncompute_cv_score(xgb, train_txf_df, train_txf_target, scoring='accuracy')\nprint('F1:')\ncompute_cv_score(xgb, train_txf_df, train_txf_target, scoring='f1')\nprint('AUC:')\ncompute_cv_score(xgb, train_txf_df, train_txf_target, scoring='roc_auc')\n", "intent": "- The submission score is 0.76077 (7/23) (even its local score is quite good)\n"}
{"snippet": "scores = cross_val_score(clf_dt, features_train, target_train, cv=5)\nprint(\"Cross Validation Score for each K\",scores)\nscores.mean()          \n", "intent": "Cross Validation - Decision Tree\n"}
{"snippet": "scores = cross_val_score(neigh_knn, features_train, target_train, cv=5)\nprint(\"Cross Validation Score for each K\",scores)\nscores.mean()          \n", "intent": "Cross Validation - KNN\n"}
{"snippet": "l1_dist_mat = euclidean_distances(X,X)\nresult = {}\nfor K in range(10, 100, 5):\n    labels = EKNNclus.EKNNclus_Th(X, K=K, D = l1_dist_mat , \n                            ntrials=10, y0 = list(range(X.shape[0])),verbose = False,tr = False)\n    result[K] = (labels, sklearn.metrics.silhouette_score(X, labels))\n", "intent": "We use EKNNclus for clustering with K varing from 10 to 95 with step=5\n"}
{"snippet": "def accuracy(actual, predicted):\n    return sum(actual == predicted) / (len(actual) * 1.)\neval_acc = accuracy(actual, predicted)\neval_acc_sk = accuracy_score(actual, predicted)\nassert eval_acc == eval_acc_sk\nprint 'Accuracy:', eval_acc\n", "intent": "$$Accuracy = \\frac{\\text{\n"}
{"snippet": "def evaluate(logits, labels):\n    y_pred_classes=tf.argmax(logits,axis=1)\n    y_true_classes=tf.argmax(labels,axis=1)\n    correct_prediction = tf.equal(y_pred_classes, y_true_classes)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    return accuracy\n", "intent": "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation \n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]\n", "intent": "Return prediction of each validation data for each decision tree\n"}
{"snippet": "alphabet_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\nfrom sklearn import metrics\ny_true = [np.argmax(y_test) for y_test in test_targets]\nf1_accuracy = 100* metrics.f1_score(y_true,alphabet_predictions, average = 'micro')\nprint('Test F1 accuracy: %.4f%%' % f1_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "y_test_pred = gs.best_estimator_.predict(x_test)\ny_test_probas = gs.best_estimator_.predict_proba(x_test)[:,1]\nprint('Test accuracy score: {}'.format(accuracy_score(y_test, y_test_pred)), \n      '\\n' + 'Test roc_auc score: {}'.format(roc_auc_score(y_test, y_test_probas)),\n      '\\n' + 'Test precision score (positive predictive value): {}'.format(precision_score(y_test, y_test_pred)),\n      '\\n' + 'Test recall score (true positive rate): {}'.format(recall_score(y_test, y_test_pred)))\nprint('Confusion matrix:')\npd.crosstab(y_test, y_test_pred, rownames = ['True'], colnames = ['Predicted'], margins = True)\n", "intent": "**Use best param searched for prediction**\n"}
{"snippet": "y_test_pred = gs.best_estimator_.predict(x_test)\ny_test_probas = gs.best_estimator_.predict_proba(x_test)[:,1]\nprint('Test accuracy score: {}'.format(accuracy_score(y_test, y_test_pred)), \n      '\\n' + 'Test roc_auc score: {}'.format(roc_auc_score(y_test, y_test_probas)),\n      '\\n' + 'Test precision score (positive predictive value): {}'.format(precision_score(y_test, y_test_pred)),\n      '\\n' + 'Test recall score (true positive rate): {}'.format(recall_score(y_test, y_test_pred))) \nprint('Confusion matrix:')\npd.crosstab(y_test, y_test_pred, rownames = ['True'], colnames = ['Predicted'], margins = True)\n", "intent": "**Use best param searched for prediction**\n"}
{"snippet": "print(\"Boosting MSE: %s\" % mean_squared_error(y_test, gbr.predict(X_test)))\n", "intent": "e) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen before.\n"}
{"snippet": "print('Accuracy for MNIST: %.4f' % accuracy_score(mnist_label_test, discretizeLabels(mnist_pred_mu)))\nprint('Accuracy for CIFAR: %.4f' % accuracy_score(cifar10_label_test, discretizeLabels(cifar_pred_mu)))\n", "intent": "As figure of merit, we can also use the accuracy ratio.\n"}
{"snippet": "print('Accuracy score for MNIST........:  %.4f' % accuracy_score(mnist_label_test, preds))\nprint('Log-Likelihood on test for MNIST: %.4f' % -log_loss(mnist_label_test, np.nan_to_num(probs)))\n", "intent": "Let's see how it performs using the `Accuracy score`, the `Loglikelihood` and the `Confusion Matrix`.\n"}
{"snippet": "print('Accuracy score for CIFAR........:  %.4f' % accuracy_score(cifar10_label_test[:,0], preds))\nprint('Log-Likelihood on test for CIFAR: %.4f' % -log_loss(cifar10_label_test[:,0], np.nan_to_num(probs)))\n", "intent": "And let's measure the performances.\n"}
{"snippet": "clean_train = [sent for sents in train.values() for sent in sents]\nclean_test = [sent for sents in test.values() for sent in sents]\ntagger = SimpleUnigramTagger(train=clean_train)\nnltk_tagger = nltk.UnigramTagger(train=clean_train)\nprint(\"SimpleUnigramTagger evaluation: \\n{0}\\n\".format(tagger.evaluate(clean_test)))\nprint(\"nltk.UnigramTagger evaluation: \\n{0}\".format(nltk_tagger.evaluate(clean_test)))\n", "intent": "3.1.2 SimpleUnigramTagger and nltk.UnigramTagger evalution comparison\n"}
{"snippet": "eval = tagger3.microEvaluate(test)\nRecall = eval.getRecall()\nPrecision = eval.getPrecision()\nF_Measure = eval.getF_Measure()\nRecallS = sorted(Recall, key=Recall.get)\nPrecisionS = sorted(Precision, key=Precision.get)\nF_MeasureS = sorted(F_Measure, key=F_Measure.get)\nprint(\"Recall lowest universal tags \"+str(RecallS[0])+\" \"+str(Recall[RecallS[0]])+\" \"+str(RecallS[1])+\" \"+str(Recall[RecallS[1]])+\" \"+str(RecallS[2])+\" \"+str(Recall[RecallS[2]]))\nprint(\"Precision lowest universal tags \"+str(PrecisionS[0])+\" \"+str(Precision[PrecisionS[0]])+\" \"+str(PrecisionS[1])+\" \"+str(Precision[PrecisionS[1]])+\" \"+str(PrecisionS[2])+\" \"+str(Precision[PrecisionS[2]]))\nprint(\"F-Measure lowest universal tags \"+str(F_MeasureS[0])+\" \"+str(F_Measure[F_MeasureS[0]])+\" \"+str(F_MeasureS[1])+\" \"+str(F_Measure[F_MeasureS[1]])+\" \"+str(F_MeasureS[2])+\" \"+str(F_Measure[F_MeasureS[2]]))\n", "intent": "Evaluate Tagger: Recall, Precision, F_Measure per tag\n"}
{"snippet": "perc_tagger.evaluate(clean_test)\n", "intent": "Average Perceptron Tagger evaluation using brown tagset:\n"}
{"snippet": "perc_tagger_uni.evaluate(test)\n", "intent": "Average Perceptron Tagger evaluation using universal tagset\n"}
{"snippet": "no_dict_perc_tagger.evaluate(clean_test)\n", "intent": "Evaluating NoDictPerceptronTagger's accuracy compared to nltk.PerceptronTagger\n"}
{"snippet": "import numpy as np\nbaseline = np.zeros(len(y_test))\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, baseline))\nprint(\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test, baseline))\nprint(\"F1 Score (Test): %f\" % metrics.f1_score(y_test, baseline))\n", "intent": "**Bsaseline: Constant prediction**\n"}
{"snippet": "y_pred = clf.predict(X_test)\ny_pred = (-y_pred+1)/2\nprint(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, y_pred))\nprint(\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test, y_pred))\nprint(\"F1 Score (Test): %f\" % metrics.f1_score(y_test, y_pred))\n", "intent": "**For IsolationForest**\n"}
{"snippet": "def get_error(m, b, points):\n    squaredErrorSum = 0.0\n    for x, y in points:\n        prediction = m * x + b\n        squaredErrorSum += (y - prediction) ** 2\n    return squaredErrorSum / len(points)\n", "intent": "Refered later as $E(m, b)$\n"}
{"snippet": "from sklearn.metrics import coverage_error, label_ranking_average_precision_score\nprint('Coverage:', coverage_error(y_val, predictions))\nprint('LRAP:', label_ranking_average_precision_score(y_val, predictions))\n", "intent": "Scikit-learn has some applicable [multilabel ranking metrics](http://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "predictions = model.predict(x_test)\n", "intent": "To further analyze the results, we can produce the actual predictions for the test data.\n"}
{"snippet": "with torch.no_grad():\n    predictions = evaluate(validation_loader)\n", "intent": "To further analyze the results, we can produce the actual predictions for the validation data.\n"}
{"snippet": "pred_dt = clf_dt.predict(X_test)\nprint('Predicted', len(pred_dt), 'digits with accuracy:', accuracy_score(y_test, pred_dt))\n", "intent": "Classifying a new sample with a decision tree is fast, as it consists of following a single path in the tree until a leaf node is found.\n"}
{"snippet": "print(classification_report(y_test, pred_rf, labels=labels))\n", "intent": "Precision and recall for each class:\n"}
{"snippet": "pred_xgb = clf_xgb.predict(X_test)\nprint('Predicted', len(pred_xgb), 'digits with accuracy:', accuracy_score(y_test, pred_xgb))\n", "intent": "At least with only a subset of training data and default hyperparameters values, XGBoost does not reach the performance of random forest.\n"}
{"snippet": "t0 = time()\npredictions = clf.predict(X_test[:200,:])\nprint('Time elapsed: %.2fs' % (time()-t0))\n", "intent": "And try to classify some test samples with it.\n"}
{"snippet": "t0 = time()\npredictions_reduced = clf_reduced.predict(X_test)\nprint('Time elapsed: %.2fs' % (time()-t0))\n", "intent": "Now we can use the classifier created with reduced data to classify our whole test set in a reasonable amount of time.\n"}
{"snippet": "from lightgbm import LGBMRegressor\nscores = cross_val_score(LGBMRegressor(n_estimators=128, num_leaves=128, min_data_in_leaf=25),\n                         X, y, cv=ShuffleSplit(n_splits=5, test_size=0.2))\nprint(\"Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n", "intent": "So now we have tuned parameters, so we can evaluate how well the model does now:\n"}
{"snippet": "print(classification_report(yv, ypred, target_names=['Rejected', 'Accepted']))\n", "intent": "We can look at the \"classification report\"\n"}
{"snippet": "model = BigramTagger(train_sentences, backoff=UnigramTagger(train_sentences, backoff=DefaultTagger('NN')))\nprint(\"Model score = \", model.evaluate(test_sentences))\n", "intent": "One step better is a Bigram Tagger, this takes the previous words Part of Speech tag into account.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss=0.0\n    for i in range(len(style_targets)):\n        style_origin=gram_matrix(feats[style_layers[i]])\n        style_loss+=style_weights[i]*tf.reduce_sum(tf.squared_difference(style_origin,style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "preds = net1.predict(X_test)\n", "intent": "Now we can use the model to predict the entire testing dataset:\n"}
{"snippet": "preds = best_net.predict(X_test)\n", "intent": "Now we can use the model to predict the entire testing dataset:\n"}
{"snippet": "print('Number of coefficents is', len(lm.coef_))\nprint('Estimated intercept coefficeint is', lm.intercept_)\nprint('Root mean squared error on the training data:', np.sqrt(mean_squared_error(y_train, lm.predict(X_train))))\nprint('Root mean squared error on the test data:', np.sqrt(mean_squared_error(y_test, lm.predict(X_test))))\n", "intent": "We will now get a measure on how accurate our model is using the root mean squared error which will be in units of gbp.\n"}
{"snippet": "print(classification_report(y_test,y_pred))\n", "intent": "**Yikes 26% accuracy. Let's have a look at the classification report and confusion matrix **\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in features['test']]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy of {}: {:.4f}'.format(model_name, test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = gs.best_estimator_.predict(X_test)\n", "intent": "The prediction of the classifier on the test data:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Recall (R)   : {:.2%}'.format(recall_score(y_test, y_pred)))\nprint('Precision (P): {:.2%}'.format(precision_score(y_test, y_pred)))\nprint('F1-score (F1): {:.2%}'.format(f1_score(y_test, y_pred)))\n", "intent": "The comparison of the predictive and reference labels can be done in terms of _recall_, _precision_, and _F1-score_.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(best_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "for data_set in datasets:\n    bottleneck_features = np.load(bottleneck_features_files[data_set][0], mmap_mode='r+')\n    test_set = bottleneck_features['test']    \n    preds = [np.argmax(all_models[data_set].predict(np.expand_dims(feature, axis=0))) for feature in test_set]\n    test_accuracy = 100*np.sum(np.array(preds)==np.argmax(test_targets, axis=1))/len(preds)\n    print('Model: {:s}, test accuracy: {:.3f}'.format(data_set, test_accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(labelled_classification_report(y_validation, gnb_pred))\n", "intent": "This is better than flipping a coin, but we can definitely improve on this.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint('Test accuracy:', scores[1])\n", "intent": "scores[1] will correspond to accuracy if we pass metrics=['accuracy']\n"}
{"snippet": "def nll_true(theta, X):\n    g.set_value(theta[0])\n    return (p0.nll(X) - p1.nll(X)).sum()\ndef nll_approx(theta, X):\n    g.set_value(theta[0])\n    return -np.sum(cc_decomposed.predict(X, log=True))\n", "intent": "Next let us construct the log-likelihood curve for the artificial dataset. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n    style_loss = 0\n    for i in range(len(style_weights)):\n        G = gram_matrix(model.extract_features()[style_layers[i]])\n        A = style_targets[i]\n        loss_i = style_weights[i] * tf.reduce_sum(tf.pow(G - A, 2))\n        style_loss += loss_i\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "lm.predict(new)\n", "intent": "Now we just call `.predict()` method:\n"}
{"snippet": "y1=dtc.predict(X_train)\n", "intent": "predicting on X_train\n"}
{"snippet": "y =rm.predict(X_train)\nm=sum((y-y_train)**2)/len(y_test)\nprint(\"MSE on the training set:  \",m)\n", "intent": "> Same problem I described above.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccuracy =  cross_val_score(dtc, X_train, y_train, cv=10)\nprecision =  cross_val_score(dtc, X_train, y_train, cv=10 , scoring = 'precision')\nrecall =  cross_val_score(dtc, X_train, y_train, cv=10 , scoring = 'recall')\nprint (\"Mean Accuracy = \", accuracy.mean())\nprint (\"Mean Recall = \", recall.mean())\nprint (\"Mean Precision = \", precision.mean())\n", "intent": ">Now it ran this time, and I am not sure why it worked here, but it did not work previously.\n"}
{"snippet": "model.evaluate(x_test,y_test,verbose=1,batch_size=128)\n", "intent": "<b>Evaluating the model with test data</b>\n"}
{"snippet": "y_pred = classifier.predict(X_test)\ny_pred\n", "intent": "Predicting the Test Set Results\n"}
{"snippet": "rNet50_pred = [np.argmax(rNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_acc = 100 * np.sum(np.array(rNet50_pred)==np.argmax(test_targets, axis=1))/len(rNet50_pred)\nprint('Test accuracy: {:.2f}'.format(test_acc))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "for m in models:\n    print(\"Model {}, test accuracy: {:.3f}\".format(m, models[m].score(X_test, y_test)))\n    y_true = y_test\n    y_pred = models[m].predict(X_test)\n    target_names = [\"class 0\", \"class 1\", \"class 2\"]\n    print(skl.metrics.classification_report(y_true, y_pred, target_names=target_names))\n", "intent": "The held-out test set has similar results:\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Evaluate the model on the test dataset of dog images.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Evaluate the model on the test dataset of dog images.\n"}
{"snippet": "def extract_InceptionV3(tensor):\n    from keras.applications.inception_v3 import InceptionV3, preprocess_input\n    return InceptionV3(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n", "intent": "Now, we'll write an algorithm to accept an image path as input, obtain the bottleneck features, and classify the dog breed using our CNN. \n"}
{"snippet": "print(metrics.classification_report(y_test, y_pred_voting,  target_names=['FL', 'DR+SF', 'FD', 'IM', 'MM', 'IF', 'OB', 'RH', 'DOC', 'INFO+RE' ]))\n", "intent": "**Performance metrics:**\n"}
{"snippet": "print(metrics.classification_report(y_test_2, y_pred_voting_2, target_names = ['other', 'FL+DR+SF']))\n", "intent": "**Performance metrics:**\n"}
{"snippet": "def avg_log_loss(y_true, y_pred):\n    log_loss = - (y_true * np.log(y_pred)\n            + (1 - y_true) * np.log(1 - y_pred))\n    return log_loss.mean()\n", "intent": "Compute the average log-loss\n"}
{"snippet": "predicted = cross_validation.cross_val_predict(knn, iris.data,iris.target, cv=10)\n", "intent": "[cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_predict.html\n"}
{"snippet": "predicted = model.predict(X_test)\nprint metrics.accuracy_score(y_test, predicted)\n", "intent": "An accuracy of 72% is obtained which is very good\n"}
{"snippet": "idx = 23\nexp = explainer.explain_instance(valid_token[idx], c.predict_proba, num_features = 6)\nprint (\" The text is: \", valid_token[idx])\nprint(\"Prob for MWS: \", c.predict_proba([valid_token[idx]])[0,1])\nprint (\"Prob for EAP: \", c.predict_proba([valid_token[idx]])[0,0])\nprint(\"Prob for HPL: \", c.predict_proba([valid_token[idx]])[0,2])\nprint(\"Actual: \", list(valid.author)[idx])\n", "intent": "Let's pick a random example and see how the prediction differs from the actual class.\n"}
{"snippet": "x_test = np.array(['done'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "ridge_test_preds = ridge_scaled.predict(X_test_scaled)\nlasso_test_preds = lasso_scaled.predict(X_test_scaled)\nenet_test_preds = enet_scaled.predict(X_test_scaled)\nridge_test_preds_cv = ridge_scaled_cv.predict(X_test_scaled)\nlasso_test_preds_cv = lasso_scaled_cv.predict(X_test_scaled)\nenet_test_preds_cv = enet_scaled_cv.predict(X_test_scaled)\n", "intent": "Interpret Regression Metrics for each of your models. Choose one of the following:\n* R2\n* MSE / RMSE\n* MAE\nWhat are your top 3 performing models? \n"}
{"snippet": "def evaluate_on_test_data(model=None):\n    predictions = model.predict(X_test)\n    sum_of_squared_error = 0\n    for i in range(len(y_test)):\n        err = (predictions[i]-y_test[i]) **2\n        sum_of_squared_error += err\n    mean_squared_error = sum_of_squared_error/len(y_test)\n    RMSE = np.sqrt(mean_squared_error) \n    return RMSE\n", "intent": "2.2 Use Support Vector Machine with different kinds of kernels and evaluate performance\n----------------------------------\n"}
{"snippet": "probs = np.array([1,0,1,0,1,0,1,0,0])\ntarg = np.array([1,1,1,0,0,0,1,1,1])\nscipy_metrics.roc_auc_score(targ, probs)\n", "intent": "* http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n* http://binf.gmu.edu/mmasso/ROC101.pdf\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    losses = 0\n    N = len(style_layers)\n    for n in range(N):\n        gram = gram_matrix(feats[style_layers[n]])\n        gram_target = style_targets[n]\n        loss = (gram - gram_target) ** 2\n        losses += tf.reduce_sum(loss) * style_weights[n]\n    return losses\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "cross_val_score(subset_regressor, feature_subset.values, endogenous, cv=5)\n", "intent": "This all seems pretty plausible; let's try actually doing a train/test with this:\n"}
{"snippet": "preds = model.predict(X_test)\n", "intent": "We can get the predictions on the test data.\n"}
{"snippet": "accuracy_score(y_test,preds)\n", "intent": "Now, let's check the accuracy\n"}
{"snippet": "print (LinearR_model.predict(house2.sqft_living.values.reshape(1,-1)))\n", "intent": "<img src=\"https://ssl.cdn-redfin.com/photo/1/bigphoto/302/734302_0.jpg\">\n"}
{"snippet": "model_prediction = [np.argmax(choosing_model.predict(np.expand_dims(feature, axis=0))) for feature in test_model]\ntest_accuracy = 100*np.sum(np.array(model_prediction)==np.argmax(test_targets, axis=1))/len(model_prediction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(\n    np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==\n                           np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Xception model test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, clf.predict(X_test), target_names=iris.target_names))\n", "intent": "Measuring accuracy alone ignores other important features of a classifier:\nhttp://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "x_test = np.array(['not bad'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "plot(model.predict(x)[0])\nplot(x[0]);\n", "intent": "The model fits the data quite nicely.\n"}
{"snippet": "x_noised = x + 0.2 * np.random.random(len(x[0]))\nplot(x_noised[0], label='input')\nplot(model.predict(x_noised)[0], label='predicted')\nlegend();\n", "intent": "The model is able to predict on noise-corrupted data.\n"}
{"snippet": "x_shifted = np.cos(2*np.pi/50*t)\nplot(x_shifted[0], label='input')\nplot(model.predict(x_shifted)[0], label='predicted')\nlegend();\n", "intent": "However the model does is not able to predict a sinusoid with different phase.\n"}
{"snippet": "x_scaled = 0.2 * x\nplot(x_scaled[0], label='input')\nplot(model.predict(x_scaled)[0], label='predicted')\nlegend();\n", "intent": "The model is able to deal with scaled sinuoid, but the farther it is from the original amplitude, the more noise.\n"}
{"snippet": "y_pred_lib_test=reg.predict(X_test)\n", "intent": "For our Machine learning API, We predict the values on the test data set and calculate the accuracy for test data.\n"}
{"snippet": "accuracy = cross_val_score(logreg, X, Y, cv=10,scoring='accuracy')\nprint (cross_val_score(logreg, X, Y, cv=10,scoring='accuracy').max())\n", "intent": "<p align> We are displaying the maximum accuracy \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(df.actual_label.values, df.predicted_RF.values)\n", "intent": "We can obtain the accuracy score from scikit learn, which takes as inputs the actual labels and the predicted labels\n"}
{"snippet": "def my_accuracy_score(y_true, y_pred):\n    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n    return \n", "intent": "Define your own function that duplicates `accuracy_score`, using the formula above. \n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall_score(df.actual_label.values, df.predicted_RF.values)\n", "intent": "We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labels\n"}
{"snippet": "def my_recall_score(y_true, y_pred):\n    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n    return \n", "intent": "Define your own function that duplicates `recall_score`, using the formula above. \n"}
{"snippet": "from sklearn.metrics import precision_score\nprecision_score(df.actual_label.values, df.predicted_RF.values)\n", "intent": "We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labels\n"}
{"snippet": "def my_precision_score(y_true, y_pred):\n    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n    return \nassert my_precision_score(df.actual_label.values, df.predicted_RF.values) == precision_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\nassert my_precision_score(df.actual_label.values, df.predicted_LR.values) == precision_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\nprint('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_RF.values)))\nprint('Precision LR: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_LR.values)))\n", "intent": "Define your own function that duplicates `precision_score`, using the formula above. \n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(df.actual_label.values, df.predicted_RF.values)\n", "intent": "We can obtain the f1 score from scikit-learn, which takes as inputs the actual labels and the predicted labels\n"}
{"snippet": "def my_f1_score(y_true, y_pred):\n    recall = my_recall_score(y_true,y_pred)  \n    precision = my_precision_score(y_true,y_pred)  \n    return \nassert my_f1_score(df.actual_label.values, df.predicted_RF.values) == f1_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\nassert my_f1_score(df.actual_label.values, df.predicted_LR.values) == f1_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\nprint('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_RF.values)))\nprint('F1 LR: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_LR.values)))\n", "intent": "Define your own function that duplicates `f1_score`, using the formula above. \n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nauc_RF = roc_auc_score(df.actual_label.values, df.model_RF.values)\nauc_LR = roc_auc_score(df.actual_label.values, df.model_LR.values)\nprint('AUC RF:%.3f'% auc_RF)\nprint('AUC LR:%.3f'% auc_LR)\n", "intent": "To analyze the performance, we will use the area-under-curve metric.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    L = []\n    for i, l in enumerate(style_layers):\n        A = style_targets[i];\n        G = gram_matrix(feats[l], normalize=True);\n        L.append(style_weights[i]*tf.square(tf.norm(A-G)));\n    loss = tf.add_n(L);\n    return loss\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "tuned_results = best_model.predict(X_test)\ntuned_recall = recall_score(y_test, tuned_results)\nprint('Tuned logistic model recall:', round(tuned_recall, 5))\n", "intent": "With the best hyperparameters, we can now run the model on the holdout set to retrieve test predictions. \n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = tf.constant(0, dtype=tf.float32)\n    for ii, idx in enumerate(style_layers):\n        cur = gram_matrix(feats[idx])\n        diff = cur - style_targets[ii]\n        loss += style_weights[ii] * tf.reduce_sum(diff * diff)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "print( metrics.mean_squared_error(regr.predict(diabetes_X_test), diabetes_y_test))\n", "intent": "However, you can also use the built-in methods provided by `sklearn` to compute the mean squared error. \n"}
{"snippet": "print(genius_regression_model.predict([8,4]))\n", "intent": "And finally, we predict the corresponding value of Y for X = [8,4]\n"}
{"snippet": "predicted_prices = results.predict(10, 364, exog=np.transpose(exog_data), dynamic=True)\n", "intent": "Let's look at the predicted prices:\n"}
{"snippet": "model.evaluate(test_data, test_labels)\n", "intent": "Do a few examples in an explict way, and compute the accuracy and the confusion matrix.\n"}
{"snippet": "MSE_train = numpy.mean((regr.predict(X_train) - y_train)** 2)\nvar_score_train = regr.score(X_train, y_train)\nprint(\"Residual sum of squares: %.2f\" % MSE_train)\nprint('Variance score: %.2f' % var_score_train)\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "encoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)\n", "intent": "After 100 epochs, it reaches a train and test loss of ~0.097, a bit better than our previous models. Our reconstructed digits look a bit better too:\n"}
{"snippet": "score = model.evaluate(test_tensors, test_targets, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\nscore = model.evaluate(valid_tensors, valid_targets, verbose=0)\nprint('Valid loss:', score[0])\nprint('Valid accuracy:', score[1])\nscore = model.evaluate(train_tensors, train_targets, verbose=0)\nprint('Train loss:', score[0])\nprint('Train accuracy:', score[1])\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "eval = top_model.evaluate(test_tensors, test_targets, verbose=0)\nprint(\"loss: \" + str(eval[0]))\nprint(\"acc: \" + str(eval[1]*100) + \"%\")\neval = top_model.evaluate(valid_tensors, valid_targets, verbose=0)\nprint(\"loss: \" + str(eval[0]))\nprint(\"acc: \" + str(eval[1]*100) + \"%\")\neval = top_model.evaluate(train_tensors, train_targets, verbose=0)\nprint(\"loss: \" + str(eval[0]))\nprint(\"acc: \" + str(eval[1]*100) + \"%\")\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "yPredTrain = predict()   \nyPredTest = predict()    \n", "intent": "Predict the output of test and train data using X_trainT and X_testT using predict() method> Use the parametes returned from the trained model\n"}
{"snippet": "plot_decision_boundary(X.T,y,lambda x: lr_model.predict(x))\n", "intent": "- Run the cell below cell to plot the decision boundary perdicted by logistic regression model\n"}
{"snippet": "plot_decision_boundary(X_data,y,lambda x: predict(x.T,parameters))\n", "intent": "Run the below cell to plot the decision boundary predicted by the deep nural network\n"}
{"snippet": "for i in range(1, 11):\n    print(\"School Enrollment: {}\".format(i*10), regr3.predict([i * 10, 100, 100]))\n    print(\"Sanitation Access: {}\".format(i*10), regr3.predict([100, i * 10, 100]))\n    print(\"Water Access: {}\".format(i*10), regr3.predict([100, 100, i * 10]))\n    print(\"-----\\n\")\n", "intent": "Now that we have an even stronger model what can we predict.  Let's try walking through the values to find out.\n"}
{"snippet": "message = [\"Hi, Bekk how are you?  I am great did you get that message I sent you?\"]\ntransormed_message = vectorizer.transform(message)\nclassifier.predict(transormed_message)\n", "intent": "Even when we pass messages in to our prediction function it will give us an answer - and remember, with 97% accuracy.\n"}
{"snippet": "dnn_predictions = [np.argmax(dnn_model.predict(np.expand_dims(feature, axis=0))) for feature in test_features]\ntest_accuracy = 100*np.sum(np.array(dnn_predictions)==np.argmax(test_targets, axis=1))/len(dnn_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "final_loss, final_accuracy = model.evaluate(X_test, Y_test)\nprint(\"The final loss on the test set is:\", final_loss)\nprint(\"The final accuracy on the test set is:\", final_accuracy)\n", "intent": "Here we want to perform our final evaluation of our model on the test-set. \n"}
{"snippet": "print(\"Elementary Logistic Regression Cross-Validated Score\")\ncross_val_score(elem_log_mod,elem_x,elem_y,cv=10).mean()\n", "intent": "Above both Random Forest and Logistic regression models are fit on both the highschool and elementary school data.\n"}
{"snippet": "print(\"High School Logistic Regression Cross-Validated Score\")\ncross_val_score(high_log_mod,high_x,high_y,cv=10).mean()\n", "intent": "Cross validated Logistic regression score for the Elementary school data.\n"}
{"snippet": "elem_score = cross_val_score(elem_mod,elem_x,elem_y, cv=10).mean()\nprint(\"Elementary Random Forest Cross-Validated Score\")\nelem_score\n", "intent": "Cross validated Logistic regression score for the Highschool data.\n"}
{"snippet": "high_score = cross_val_score(high_mod,high_x,high_y, cv= 10).mean()\nprint(\"Highschool Random Forest Cross-Validated Score\")\nhigh_score\n", "intent": "Cross validated Random Forest score for the Elementary school data.\n"}
{"snippet": "yhat = logreg.predict(Xs)\nacc = np.mean(yhat == y)\nprint('Accuracy on the training data is {0:f}'.format(acc))\n", "intent": "Measure the accuracy on the training data.\n"}
{"snippet": "y_tr_pred = regr.predict(X_tr)\nrsq_tr = 1-np.mean((y_tr_pred-y_tr)**2)/(np.std(y_tr)**2)\nprint(\"R^2 training = %f\" % rsq_tr)\n", "intent": "Now, we can measure the normalized RSS on the training data.  \n"}
{"snippet": "y_ts_pred = regr.predict(X_ts)\nrss_ts = np.mean((y_ts_pred-y_ts)**2)/(np.std(y_ts)**2)\nrsq_ts = 1-rss_ts\nprint(\"Normalized test RSS = %f\" % rss_ts)\nprint(\"Normalized test R^2 = %f\" % rsq_ts)\n", "intent": "Ths `R^2` value is about `0.68`.  However, we need to evaluate the model on the test data.  \n"}
{"snippet": "Yhat = regr.predict(Xts)\nrsq = r2_score(Yts, Yhat)\nprint(rsq)\n", "intent": "Now, test the model on the test data and measure the `R^2` value.  You should get a much better fit than with the Ridge regression solution.  \n"}
{"snippet": "grid_x_points = 1000\nx1_test_min, x1_test_max, x2_test_min, x2_test_max = np.amin(x_train[:, 0]), np.amax(x_train[:, 0]), np.amin(x_train[:, 1]), np.amax(x_train[:, 1])\nx_test = np.dstack(np.mgrid[x1_test_min : x1_test_max : (x1_test_max - x1_test_min)/grid_x_points, x2_test_min : x2_test_max : (x2_test_max - x2_test_min)/grid_x_points]).reshape(-1, 2)\ny_test = model.predict(X = x_test)\ntolerance = 0.0001\nx_boundary = x_test[np.abs(y_test - 0.5) <= tolerance]\nax = pickle.loads(pickle.dumps(ax))\nax.plot(x_boundary[:, 0], x_boundary[:, 1], linestyle = '-', marker = '', color = 'black')\n", "intent": "We discretize the space surrounding the inputs and test the model on this space. The decision boundary is the subspace where y ~= 0.5.\n"}
{"snippet": "for i in xrange(1, 100):\n    array = np.mat(data.pixels[i]).reshape(48, 48)\n    image = scipy.misc.toimage(array, cmin=0.0)\n    display(image)\n    print(emotion_labels[data.emotion[i]])\n    input_img = np.array(array).reshape(1,48,48,1)\n    prediction = model.predict(input_img)\n    print(emotion_labels[prediction.argmax()])\n", "intent": "Let's classify first 100 faces.\n"}
{"snippet": "yPred = regressor.predict(xTest)\n", "intent": "<h3>Predicting the Test set results</h3>\n"}
{"snippet": "print('Predicted salary using Random Forest Regression: {}'.format(regressor.predict(6.5)[0]))\n", "intent": "<h3>Predicting the previous salary of the new employee</h3>\n"}
{"snippet": "print('Predicted salary using Regression Decision Tree: {}'.format(regressor.predict(6.5)[0]))\n", "intent": "<h3>Predicting the previous salary of the new employee</h3>\n"}
{"snippet": "yPred = regressor.predict(xTest)\n", "intent": "<h3>Predicting the Test set Results</h3>\n"}
{"snippet": "yPred = classifier.predict(xTest)\n", "intent": "<h3>Predicting the Test set results</h3>\n"}
{"snippet": "print(regr.predict(X_test))\n", "intent": "The following is previewing a sample output for Y using the prediction on X_test\n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. We should notice strange results.**\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = clf_rf.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "Evaluating the model accuracy on the validation set.\n"}
{"snippet": "test_y = clf_rf.predict(df_test)\n", "intent": "The model is now run on the test dataset and the class labels are predicted.\n"}
{"snippet": "Ypred = clfo.predict(trainX)\ntrainacc_svm = metrics.accuracy_score(Ypred, trainY)\nYpred = clfo.predict(testX)\ntestacc_svm = metrics.accuracy_score(Ypred, testY)\nprint \"SVM accuracies:\", trainacc_svm, testacc_svm\n", "intent": "Here are the training and test errors.\n"}
{"snippet": "predYtrain = clf.predict(trainX)\npredYtest = clf.predict(testX)\nacc      = metrics.accuracy_score(trainY, predYtrain)\nprint \"train accuracy =\", acc\nacc      = metrics.accuracy_score(testY, predYtest)\nprint \"test accuracy  =\", acc\n", "intent": "Calculate the training and test accuracy for the SVM classifier.\n"}
{"snippet": "newXfn  = scalerf.transform(newXf)        \nprednewY = bestclf.predict(newXfn)\n", "intent": "Now predict using your classifier.  The extracted features are in `newXf`.\n"}
{"snippet": "scores = model.evaluate(X, Y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1],scores[1]*100))\n", "intent": "1. Only training accuracy for now\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit\nn_samples = iris.data.shape[0]\ncv = ShuffleSplit(n_splits = 4, test_size = 0.3, random_state = 0)\ncross_val_score(clf,iris.data,iris.target,cv = cv)\n", "intent": "Instead of passing in cv = 5, eg. we can use a cross validation iterator\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n", "intent": "Step 3: evaluate the performance in terms of accuracy. The accuracy is simply the proportion of labels that are correctly predicted by your model. \n"}
{"snippet": "accuracy_score = classifier.evaluate(x=test_set.data,y=test_set.target)['accuracy']\nprint('Accuracy: {0:f}'.format(accuracy_score))\n", "intent": "**Evaluate Model Accuracy**\n"}
{"snippet": "predictions = {}\ntest_accuracy = {}\nfor mod in models:\n    predictions[mod] = [np.argmax(model[mod].predict(np.expand_dims(feature, axis=0))) for feature in test[mod]]\n    test_accuracy[mod] = 100*np.sum(np.array(predictions[mod])==np.argmax(test_targets, axis=1))/len(predictions[mod])\n    print('Test accuracy for the ' + mod + ' model is: %.4f%%' % test_accuracy[mod])\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lr.predict(cars[[\"weight\"]])\npredictions[:5]\ncars[\"mpg\"][:5]\n", "intent": "With this trained model, we can start making predictions.\n"}
{"snippet": "mse = mean_squared_error(cars[[\"mpg\"]], predictions)\nmse\n", "intent": "As can be seen from the plot, our model performs well linearly. At the same time, we need to calculate model's **error**.\n"}
{"snippet": "predictions = clf.predict(X)\nprint predictions[0:11]\n", "intent": "**The best model fit with the best hyper-parameters and the whole training set can be used to make predictions:**\n"}
{"snippet": "print(\"Let's see the predictions if we use the training instances as test instances\")\nprint(clf.predict([[0, 0],\n                   [0, 1],\n                   [1, 0],\n                   [1, 1]]))\nprint(\"And now, let's try some actually new instances (test instances)\")\nprint(clf.predict([[0.5, 0],\n                   [0, 0.2],\n                   [0.1, 0],\n                   [0.9, 0.9]]))\n", "intent": "After being fitted, the model can then be used to predict test instances\n"}
{"snippet": "print(\"The output gives the probability of 'o' and the probability of '1'\")\nclf.predict_proba([[0.9, 0.8]])\n", "intent": "Probabilities of each class can also be predicted (the fraction of training samples of the same class in a leaf)\n"}
{"snippet": "print(\"Predicion for this instance: {0}\".format(iris.data[:1, :]))\nprint(\"Class is: {0}\".format(clf.predict(iris.data[:1, :])))\nprint(\"Probabilities are (for class1, class2, class3): {0}\".format(clf.predict_proba(iris.data[:1, :])))\n", "intent": "After being fitted, the model can then be used to predict the class (and the probability) of samples:\n"}
{"snippet": "evaluator = RegressionEvaluator(\nlabelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\nmae = evaluator.evaluate(predictions_sd)\nprint(mae)\n", "intent": "**Finally, MAE is computed on the _label_ and _prediction_ columns.**\n"}
{"snippet": "pred_y_test2 = modelZ.predict(X_test)\n", "intent": "I used U-net CNN model to predict the testing set and visualized the predicted masks.\n"}
{"snippet": "iris_results = classifier.evaluate(input_fn=test_input_fn, steps=1)\n", "intent": "Evaluate the accuracy of the function with test input\n"}
{"snippet": "print(classification_report(test_labels, y_pred))\n", "intent": "* First off is the 1D-convolutional model\n"}
{"snippet": "print(classification_report(test_labels, ly_pred))\n", "intent": "* second is the LSTM model\n"}
{"snippet": "print(classification_report(test_labels, by_pred))\n", "intent": "* Third model is with 1D-convolutional layers and LSTM RNN layer\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.cross_validation import cross_val_predict\ny_pred = cross_val_predict(lr, X, y, cv=10)\nprint(classification_report(y, y_pred))\n", "intent": "Get classification report\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.cross_validation import cross_val_predict\ny_pred = cross_val_predict(mlp, X, y, cv=10)\nprint(classification_report(y, y_pred))\n", "intent": "- Classification Report\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.cross_validation import cross_val_predict\ny_pred = cross_val_predict(forest, X, y, cv=10)\nprint(classification_report(y, y_pred))\n", "intent": "Get classification report\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for idx in range(len(style_layers)): \n        features = feats[style_layers[idx]]\n        gram_current = gram_matrix(features)\n        gram_target = style_targets[idx]\n        style_loss += style_weights[idx]*tf.reduce_sum((gram_current-gram_target)**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "scores = anomaly_detector.anomaly_score(data, y=4)\n", "intent": "Compute the anomaly scores for each data point\n"}
{"snippet": "preds = cnn.predict(X_test, batch_size=64).ravel()\n", "intent": "Predict the years for the test set and check the results.\n"}
{"snippet": "vgg_preds = vgg_cnn.predict(X_test, batch_size=64).ravel()\n", "intent": "Validation MAE of 11.4643 is what we are able to squeeze out from this. Next, predictions on the test set are made.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.6f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    loss = 0\n    for idx, i in enumerate(style_layers):\n        style_image = gram_matrix(feats[i])\n        loss += style_weights[idx] * tf.reduce_sum((style_image - style_targets[idx]) ** 2)\n    return loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "forest100_pred = forest100.predict(test_data)\nnp.save('forest100pred', forest100_pred)\n", "intent": "Make predictions with random forest set at 100 learners\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_pred,y_test)\n", "intent": "*** Finally, we find out the accuracy of the model.***\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\na=mean_squared_error(x, norm.cdf(x))\naccuracy_normal = 100-a\naccuracy_normal\n", "intent": "*** Calculating accuracy for Normal distribution. ***\n"}
{"snippet": "from sklearn.metrics import accuracy_score\na=mean_squared_error(x, expon.cdf(x))\naccuracy = 100-a\naccuracy\n", "intent": "*** Calculating accuracy for Exponential distribution. ***\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\na=mean_squared_error(x, bernoulli.cdf(x,p))\naccuracy_bernoulli = 100-a\naccuracy_bernoulli\n", "intent": "*** Calculating accuracy for Bernoulli distribution. ***\n"}
{"snippet": "Incept_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Incept_predictions)==np.argmax(test_targets, axis=1))/len(Incept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def t_repredict(est,t, xtest):\n    probs=est.predict_proba(xtest)\n    p0 = probs[:,0]\n    p1 = probs[:,1]\n    ypred = (p1 > t)*1\n    return ypred\n", "intent": "Start with an arbitrary threshold t, and see how we fare at different thresholds for logistic regression\n"}
{"snippet": "test_predict = model.predict(test_resnet50)\ntest_predict = np.argmax(test_predict, axis=1)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.\n    for l, i in zip(style_layers, range(len(style_layers))):\n        gram = gram_matrix(feats[l])\n        style_loss += style_weights[i] * tf.reduce_sum(tf.square(gram - style_targets[i]))\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "result = classifier.predict(sc_test)\n", "intent": "Here I've scaled the 'test' data to 'sc_test'\n"}
{"snippet": "print (classification_report(y_test, best_rfc.predict(X_test)))\n", "intent": "Precision and Recall\n"}
{"snippet": "from sklearn.metrics import classification_report, accuracy_score\nprint '\\nThe overall accuracy of the model is: ' + str(accuracy_score(y_test, predictions)) + \"\\n\"\nreport = classification_report(y_test, predictions, target_names = iris.target_names)\nprint 'A detailed classification report: \\n\\n' + report\n", "intent": "And see a classification report. That computes precision, recall, f1-score and shows how many examples from the test were in each class.\n"}
{"snippet": "sgd_clf.predict([some_digit])\n", "intent": "Predicting the digit for random number.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=5, scoring=\"accuracy\")\n", "intent": "Cross Value Score will evaluate the model for each fold and gives us an array representing the accuracy obtained in each fold.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,method=\"decision_function\")\n", "intent": "<img src=\"tradeoff.png\" height=200px width=800px></img>\n"}
{"snippet": "print('Training error',model.evaluate(x=x_test, y=y_test))\nprint('Test error',SK_AdaBoost.score(Xval, Yval))\n", "intent": "Plot the training loss and validation accuracy vs the number of weak decision trees\n"}
{"snippet": "precisionscore=precision_score(y_test,y_pred)\nprecisionscore\n", "intent": "Confusion Matrix show high rate of True Positives\n"}
{"snippet": "clustered_kp = [clusterer2.predict(img) for img in X_test_desc_corr]\nimg_hist = np.array([np.bincount(kp_words,minlength=K2) for kp_words in clustered_kp])\n", "intent": "We select the K2=3 with the highest silhouette_score.\n"}
{"snippet": "y2_pred_class = knn.predict(X2_test)\nprint((metrics.accuracy_score(y2_test, y2_pred_class)))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "y_pred_xgb = model.predict(x_test)\n", "intent": "Predicting the Y value from the above obtained model\n"}
{"snippet": "print(classification_report(y_test,y_predict))\n", "intent": "1. In classification report, we can see that there is a 0.83 f1 score.\n"}
{"snippet": "ResNet50plus_predictions = [np.argmax(ResNet50plus_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50plus_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50plus_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores1= cross_val_score(modelo1, X, y, cv=k_fold, n_jobs=-1)\nnp.mean(scores1)\n", "intent": "Ahora evaluemos performance (accuracy)\n"}
{"snippet": "pred1 = modelo1.predict(X)\npred1\n", "intent": "Clasifiquemos las observaciones y despues obtengamos probabilidades de exito y fracaso \n"}
{"snippet": "predict = model.predict_proba(Xte)[:,1]\npredict = np.where(predict >= optimal_threshold, 1, 0)\nfig, axes = mp.subplots(figsize=(8,6))\ncm = confusion_matrix(yte, predict).T\ncm = cm.astype('float')/cm.sum(axis=0)\nax = sns.heatmap(cm, annot=True, cmap='Blues');\nax.set_xlabel('True Label')\nax.set_ylabel('Predicted Label')\nax.axis('equal')\n", "intent": "We were close! The actual optimal threshold value is about 0.12. What happens now if we use this value to predict the test dataset classes?\n"}
{"snippet": "predict = model.predict(Xte)\nfig, axes = mp.subplots(figsize=(8,6))\ncm = confusion_matrix(yte, predict).T\ncm = cm.astype('float')/cm.sum(axis=0)\nax = sns.heatmap(cm, annot=True, cmap='Blues');\nax.set_xlabel('True Label')\nax.set_ylabel('Predicted Label')\nax.axis('equal')\n", "intent": "We can look at the model's classification accuracy.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint (confusion_matrix(y_test,predictions))\nprint ('\\n')\nprint (classification_report(y_test, predictions))\n", "intent": "Accuracy of the model is not that good\n"}
{"snippet": "print(confusion_matrix(y_test, nb_predictions))\nprint('\\n')\nprint(classification_report(y_test, nb_predictions))\n", "intent": "** It seems removing TF-IDF produced much better results **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\nAccuracy on training data: 0.925252\nAccuracy on test data:     0.728516\n", "intent": "This classifier does better on the training data but worse on the test data.\n"}
{"snippet": "print(\"the number predicted is {} with the projected principal components values {}\".format(\n    lda.predict(m[42].reshape(1,2)),m[42]))\nprint(\"the number predicted is {} with the projected principal components values {}\".format(\n    lda.predict(m[28].reshape(1,2)),m[28]))\n", "intent": "Prediction maximized over the \"m\" specificly that satisfies an upper prediction over **ziptest**\n"}
{"snippet": "y_pred = (lr.predict_proba(X)[:,1]>0.03)*1\nconfusion_matrix(y, y_pred)\n", "intent": "Logistic regression tends to favour over-represented class\nCut the probability at the right point\n"}
{"snippet": "y_pred = (lda.predict_proba(X)[:,1]>0.03)*1\nconfusion_matrix(y, y_pred)\n", "intent": "Linear discriminant is a bit better in the case of unbalanced data. \n"}
{"snippet": "rgc_pred = rgc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n"}
{"snippet": "pred = lm.predict(X_test)\n", "intent": "Let's evaluate the model by predicting offf the test values. The x test values will be used to make the prediction.\n"}
{"snippet": "from sklearn import metrics\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,pred))\nprint(\"MSE:\",metrics.mean_squared_error(y_test,pred))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,pred)))\n", "intent": "We can evaluate the performance of our model by calculating the residual sum of squares.\n"}
{"snippet": "pred = lm.predict(X_test)\n", "intent": "Now to predict values with the testing data.\n"}
{"snippet": "pred = nb.predict(X_test)\n", "intent": "Time to see how our model did.\n"}
{"snippet": "pred_pl = pipeline.predict(X_test)\n", "intent": "Now use the pipeline to predict from the X_test and create a classification report and confusion matrix.\n"}
{"snippet": "pred_train = lm.predict(X_train)\npred_test = lm.predict(X_test)\nprint('Root mean Score Training: {}'.format(r2_score(y_train, pred_train)))\nprint('Root mean Score Testing: {}'.format(r2_score(y_test, pred_test)))\n", "intent": "**The coefficient suggests that it has a poor coorelation with the target, hence a poor model is expected**\n"}
{"snippet": "scores = cross_val_score(model,x,y,cv=KFold(n_splits=10,shuffle=True))\nscores.mean()\nprint('Score: {0:.2f}'.format(model.score(xTest,yTest)))\n", "intent": "<br>\nEvaluate the model using cross-validation\n"}
{"snippet": "my_model_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(my_model_predictions)==np.argmax(test_targets, axis=1))/len(my_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "x, y = make_xy(critics, vectorizer)\nprob = clf.predict_proba(x)[:, 0]\npredict = clf.predict(x)\n", "intent": "We can see mis-predictions as well.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "We might be less accurate bit we are certainly not overfit.\n"}
{"snippet": "all_predictions = nb.predict(X_test)\nall_predictions\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ntrain_error = mean_squared_error(train_data['price'], sqft_model.predict(train_data['sqft_living_scaled'].values.reshape(-1, 1)))\ntest_error = mean_squared_error(test_data['price'], sqft_model.predict(test_data['sqft_living_scaled'].values.reshape(-1, 1)))\nprint (train_error, test_error)\n", "intent": "Evaluate the model using mean_squared_error\n"}
{"snippet": "x_test_predict = clf.predict(X_test)\n", "intent": "Copying x_test predicted values into x_test_predit\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint( classification_report( y_test, best_evol_classifier.predict(X_test) ) )\n", "intent": "I suppose that's not terrible.\n"}
{"snippet": "evo_predictions_fishpoo_sample    = [ evol_catcodes[i] for i in best_evol_classifier.predict( e_embedded_fishpoo ) ]\nswitch_predictions_fishpoo_sample = [ switch_catcodes[i] for i in best_switching_classifier.predict( e_embedded_fishpoo ) ]\n", "intent": "After all that work, actually running the classifiers on the microbiome data is\nridiculously fast.\n"}
{"snippet": "seaborn.kdeplot( best_evol_classifier.predict_proba( e_embedded_fishpoo )[:,0] )\n", "intent": "Each classification takes about 4 *microseconds*.\n"}
{"snippet": "X_test = X_train \ny_test = y_train\ny_pred_proba = model.predict_proba(X_test) \ny_pred = model.predict(X_test)\n", "intent": "X_test = X_train\ny_test = y_train\n"}
{"snippet": "def new_points():\n  return np.array([[1.0, 1.0],\n                 [-1.5, -1.0]], dtype = np.int32)\npredictions = list(dnn_spiral_classifier.predict(input_fn=new_points))\nprint(\n    \"New Samples, Class Predictions:    {}\\n\"\n    .format(predictions))\n", "intent": "Let's classify a new point\n"}
{"snippet": "import numpy as np\nfrom sklearn import metrics\ny = np.array(msft['close'][5:])\ny_pred = np.array(ma['close'])\nsst = np.sum((y - np.mean(y)) ** 2)\nsse = np.sum((y_pred - y) ** 2)\nmetrics.r2_score(y, y_pred)\n", "intent": "What does lambda function do?\n"}
{"snippet": "class CapsNet(nn.Module):\n    def __init__(self):\n        super(CapsNet, self).__init__()\n        self.conv = PrimaryCapsIn()\n        self.primary = PrimaryCaps()\n        self.digits = DigitCaps()\n    def forward(self, inputs):\n        return self.digits(self.primary(self.conv(inputs)))\n    def loss(self, inputs, labels):\n        return self.margin_loss(inputs, labels)\n", "intent": "Now let's combine everything into one network.\n"}
{"snippet": "Res_predictions = [np.argmax(Res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Res]\ntest_accuracy = 100*np.sum(np.array(Res_predictions)==np.argmax(test_targets, axis=1))/len(Res_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "x_web_images = dataset.get_pre_processed_additional_images('./web_images')\ny_web_images = [12,3,25,34,1,18,11,30]\nmodel.evaluate(x_web_images, y_web_images)\n", "intent": "**Answer:**\nThe model prediction of 62.5% is pretty bad on the captured images compared to the test accuracy of 95%. \n"}
{"snippet": "def rmse_cv(model, x_train, y):\n    rmse = np.sqrt(-cross_val_score(model, x_train, y, scoring=\"neg_mean_squared_error\", cv = 10))\n    return rmse\n", "intent": "There are 288 features, let's see if we can do PCA to reduce the number of features\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mse)\n", "intent": "***\nThis is simply the mean of the squared errors.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "N = 10000\ny_test = np.random.randn(N,1) \ny_pred_test = np.random.randn(N,1) \nrms_test = sqrt(mean_squared_error(y_test, y_pred_test))\nprint(\"root mean square error of test:\", rms_test)\n", "intent": "Generate example data\n"}
{"snippet": "predicted_ratings = predict(stacker, proposed_games)\n", "intent": "Now we can use this set of 100000 prospective game designs, and see if any will likely receive high scores.\n"}
{"snippet": "nts1 = 5000\nIperm_ts = np.random.permutation(nts) \nXts1 = Xts[Iperm_ts[:nts1],:]\nyts1 = yts[Iperm_ts[:nts1]]\nyhat = logreg.predict(Xts1)\nacc = np.mean(yhat == yts1)\nprint('Accuaracy = {0:f}'.format(acc))\n", "intent": "Now, we can measure the accuracy on the test data.  Again, we will test on a small sub-sample.\n"}
{"snippet": "yhat_ts = svc.predict(Xts1)\n", "intent": "Measure the accuracy on the test data.  The prediction can take several minutes too -- SVMs are *very* slow!\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(classification_report(yts, yhat, target_names=target_names))\nprint(\"Confusion matrix on the test data\")\nprint(confusion_matrix(yts, yhat, labels=range(n_classes)))\n", "intent": "We can use following tools to print classification performance.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_tl.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy_ResNet50 = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_ResNet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = tf.constant(0.0)\n    for i in range(len(style_layers)):\n        gram_i      = gram_matrix(feats[style_layers[i]])\n        style_loss += style_weights[i]*tf.reduce_sum(tf.squared_difference(style_targets[i], gram_i))\n    pass\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "header_dict['exercise'] = 10\nheader_dict['age'] = 23\nheader_dict['movies'] = 10\nheader_dict['reading'] = 10\nheader_dict['music'] = 10\nheader_dict['samerace'] = 1\nextract_values = list(header_dict.values())\npredictions = rnd_forest.predict([extract_values])\nprint(predictions)\n", "intent": "As you can see above, the random forest predicted a match rate of 0. We can alter some features in order to try and achieve a match rate of 1.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_dog.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions=RForestC.predict(X_test)\nprint classification_report(y_test,predictions)\n", "intent": "**Predictions and Evaluation**\n"}
{"snippet": "predictions = clf.predict(x_test)\ni=0\nfor row in y_test.iterrows():\n    prediction = predictions[i]\n    i+=1\n    actual = row[1]['usage']\n    print(prediction,actual)\n", "intent": "<h4>Check predictions</h4>\n"}
{"snippet": "from keras.utils.np_utils import to_categorical\ny_test_binary = to_categorical(y_test)\nscores = model.evaluate(X_test_pad, y_test_binary, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "def rms_error(y_pred,y_true):\n    se = (y_pred - y_true)**2\n    mse = np.mean(se)\n    error =  math.sqrt(mse)\n    print 'RMSE:', error\n    return (se,error)\nse_arr = {}\nrmse = {}\n", "intent": "Let's also define an error used to evaluate a model. Write a function to calculate the root-mean-square error.  \n"}
{"snippet": "se_arr['House_LR'], rmse['House_LR'] = rms_error(clf.predict(X_test),y_test)\n", "intent": "Using housing features gives up a model that explains about 57% of the variance in the energy consumption data. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n", "intent": "Let's measure RMSE score:\n"}
{"snippet": "final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n", "intent": "* After tweaking for a while, its time to evaluate on test set.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n", "intent": "Before we get excited with accuracy, let's look at a very dumb classifier that just classifies every single image in \"not-5\" class:\n"}
{"snippet": "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\nprecision_score(y_train_5, y_train_pred_forest)\n", "intent": "*ROC* of Random Forest looks better than *ROC* of SGD\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Let's evaluate `SGDClassifier`'s accuracy using `cross_val_score()` function:\n"}
{"snippet": "X_new = np.expand_dims(np.arange(-3, 3, 0.1), 1)\nX_poly_new = poly_features.transform(X_new)\ny_predict = lin_reg.predict(X_poly_new)\n", "intent": "True Equation: $y = 0.5x^2 + x + 2 + Gaussian\\hspace{0.1cm}noise$\nModel: $y = 0.486x^2 + 1.000x + 1.94$\n"}
{"snippet": "X_new = [[3,5,4,2],[5,4,3,2]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y,y_pred))\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "metrics.mean_squared_error(true,pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "preds = lr.predict(X_test)\n", "intent": "By default, if this probability is more than 0.5, then the observation is classified as a positive outcome, otherwise a negative.\n"}
{"snippet": "sales_pred =model2.predict(advert[['TV','Newspaper']])\nsales_pred[1:10]\n", "intent": "The above result gives the model:   Sales =5.77 + 0.046 * TV +0.04 * Newspaper\n"}
{"snippet": "deep_pred = deep_features_model.predict(test_deep_features)\ntrue_label =le.transform(image_test['label'])\naccuracy_score(true_label,deep_pred)\n", "intent": "As we can see, deep features provide us with significantly better accuracy (about 78%)\n"}
{"snippet": "print \"Class predictions according to scikit learn:\" \nprint sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(data['Cluster'],kmean.labels_))\nprint(classification_report(data['Cluster'],kmean.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predications=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predications=nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predications=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = list(classifier.predict(x_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "class_names = ['Young', 'Middle Age', 'Old']\nscore = model.score(x_test[feature_names], Y_test['AgeClassMulti'])\nY_pred = model.predict(x_test[feature_names])\neval_classifier(Y_pred, Y_test['AgeClassMulti'], score, class_names)\n", "intent": "As we've done before, we can evaluate our classification model by looking at the percentage of correct classifications and at the confusion matrix.\n"}
{"snippet": "import random\nidx = random.randint(0, len(issues))\ny = vectorizer.transform([issues[idx]['description']])\nprediction = feature_model.predict(y)\nprint(\"The following command belongs in cluster {}:\".format(prediction[0]))\nprint(issues[idx]['description'])\nprint(\"\")\nprint(\"Feature cluster %d:\" % prediction[0]),\nfor ind in order_centroids[prediction[0], :10]:\n    print(' %s' % terms[ind])\n", "intent": "Now I can make predictions of which cluster a string belongs to...\n"}
{"snippet": "def predict(X, coeffs, thres=0.5):\n    g = predict_proba(X, coeffs)\n    return (g > thres).astype(int)   \n", "intent": "If predicted probability > thres, we classify the points as blue; if predicted probability < thres, we classfiy the points\nas red.\n"}
{"snippet": "def log_likelihood_gradient(X, y, coeffs):\n    f = predict_proba(X, coeffs)\n    return X.T.dot(y - f)\n", "intent": "This is what we'll use to update the coefficients in each iteration of gradient descent.\n"}
{"snippet": "X_test_scaled = scaler.transform(X_test_new)\ny_test = clf.predict(X_test_scaled).astype(int)\ndf_test= df_test.assign(salary = y_test)\n", "intent": "* Scale the test data ndarray\n* Predict the salary using the trained regressor\n* Print out the output in csv file.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "<h3>Predicting the test Set</h3>\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "<h3>Predicting the Test set results</h3>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,classifier.predict(X_test))\ncm\n", "intent": "<h2>Evaluating the classifier Model, using confusion matrix.</h2>\n"}
{"snippet": "regressor.predict(6.5)\n", "intent": "<h2>Predicting the dependent variable using the Model</h2>\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,classifier.predict(X_test))\n", "intent": "Evaluating the model, by <b>Confusion Matrix</b>\n"}
{"snippet": "def performance_metric(label, prediction):\n    evs = explained_variance_score(label, prediction)\n    mea = mean_absolute_error(label, prediction)\n    mse = mean_squared_error(label, prediction)\n    mae = median_absolute_error(label, prediction)\n    r2  = r2_score(label, prediction)\n    return evs, mea, mse, mae, r2\n", "intent": "def performance_metric(label, prediction):\nCalculate and return the appropriate error performance metric.\n"}
{"snippet": " def softmax_loss(x, y):\n    probabilities = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n    loss = -np.sum(np.log(probabilities[np.arange(x.shape[0]), y]+1e-8)) / x.shape[0]\n    dx = probabilities.copy()\n    dx[np.arange(x.shape[0]), y] -= 1\n    dx /= x.shape[0]\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "df = appendActualPredict(df)\ndf = maeAppend(df)\ndf = indicatorAppend(df)\ndf = plAppend(df)\ndf = otherMeasures(df)\n", "intent": "Here we will be trying out our data to see if it works.\n"}
{"snippet": "acc_knn_test = accuracy_score(test_y, pred_knn_test)\nacc_knn_train = accuracy_score(train_y, pred_knn_train)\nprint(\"Accuracy KNN test\", acc_knn_test, \"; Accuracy KNN train \", acc_knn_train)\nindex += [\"knn\"]\nresults_dict[\"train\"] += [acc_knn_train]\nresults_dict[\"test\"] += [acc_knn_test]\n", "intent": "Let's calculate the accuracy.\n"}
