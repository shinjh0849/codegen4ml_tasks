{"snippet": "roc_auc_score(y_test, test_probs)\n", "intent": "<b> What does this tell us about the model? Is it a good or bad model?<b>\n"}
{"snippet": "mean_cv_score = cross_val_score()\nprint (\"The cross validated accuracy score is {:.2f} percent\").format(mean_cv_score*100)\n", "intent": "How does the testing accuracy compare to the first one?\n<br><br><br><br>\nUse cross validation to derive a truer testing accuracy score\n"}
{"snippet": "train_y_hat = model.predict(train_X)\nprint np.sqrt(metrics.mean_squared_error(train_y, train_y_hat))\ntest_y_hat = model.predict(test_X)\nprint np.sqrt(metrics.mean_squared_error(test_y, test_y_hat))\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error)\n"}
{"snippet": "def predict(clf, timeline):\n    messages = []\n    for tweet in timeline:\n        messages.append(tweet.text)\n    new_tweets = np.array(messages)\n    y_pred = clf.predict(new_tweets)\n    return y_pred\n", "intent": "- Use the SVM classifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "print(\"Light Cars: \", linreg.predict([[0,0]])) \nprint(\"Heavy Cars: \", linreg.predict([[1,0]])) \nprint(\"Medium Cars: \", linreg.predict([[0,1]])) \n", "intent": "** Now let's use predict function to predict mpg for light, heavy and medium cars **\n"}
{"snippet": "RSS = sum((y-linreg.predict(X))**2) \nTSS = sum((y-y.mean())**2)\nR_Squared = 1 - float(RSS)/TSS\nprint(R_Squared)\n", "intent": "This is the definition of **R_Squared**. Let's first calculate it = this is the hard way!\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc,roc_auc_score\ny_hat_probability = lm.predict_proba(X).T[1]  \nprint(y_hat_probability)\nprint(\"AUC is %f \" %roc_auc_score(y, y_hat_probability)) \nvals = roc_curve(y, y_hat_probability) \n", "intent": "Read more about ROC curves in puthon [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n"}
{"snippet": "member_data['Predicted'] = model1_fit.predict(member_data[train_cols])\n", "intent": "Using the .predict function I created a Predicted column to compare the original Renewed value to the model's prediction.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "print(accuracy_score(Y_test, Y_pred))\nprint(precision_score(Y_test, Y_pred))\nprint(recall_score(Y_test, Y_pred))\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_test, Y_pred)\nprint(acc)\n", "intent": "Calculate the accuracy with the accuracy_score() function from sklearn\n"}
{"snippet": "from sklearn.metrics import classification_report\ncls_rep = classification_report(Y_test, Y_pred)\nprint(cls_rep)\n", "intent": "Create the classification report with the classification_report() function\n"}
{"snippet": "def find_mse(df, degree):\n    model, prediction = fit_reg_poly(df, degree)\n    prediction = np.ravel(prediction)\n    result = mean_squared_error(df['DepDelay'], model.predict(df['CRSDepTime'][:, np.newaxis]))\n    return result\n", "intent": "Write a function named `find_mse()` that returns the mean squared error of ridge regresion model, given the degree. Assume that alpha is always 1.0.\n"}
{"snippet": "print classification_report(y_test, y_pred)\n", "intent": "Very similar to training data.\n"}
{"snippet": "batch_size = 2\ninput_width = 4\ninput_height = 4\ninput_depth = 3\nfilter_size = 4\noutput_depth = 3\nstride = 2\npadding = 1\ndef relative_error(x, y):\n", "intent": "Let's test our layer on some dummy data:\n"}
{"snippet": "dout = np.random.randn(batch_size, pool_size, pool_size, input_depth)\ndx_num = eval_numerical_gradient_array(lambda x: max_pool_forward(x, pool_size, stride), X, dout)\nout = max_pool_forward(X, pool_size, stride)\ndx = max_pool_backward(dout, X, out, pool_size, stride)\nprint('Testing max_pool_backward function:')\ndiff = relative_error(dx, dx_num)\nif diff < 1e-12:\n    print('PASSED')\nelse:\n    print('The diff of a is too large, try again! a: ' + str(diff))\n", "intent": "And we again use numerical gradient checking to ensure that the backward function is correct: \n"}
{"snippet": "def relative_error(x, y):\n", "intent": "Let's test this on some dummy data.\n"}
{"snippet": "zip(temps,pfail, clf.predict(temps.reshape(-1,1)))\n", "intent": "The failures in prediction are, exactly where you might have expected them to be, as before.\n"}
{"snippet": "logreg = cv_and_fit(train_x, train_y, np.logspace(-4, 3, num=100))\npd.crosstab(test_y, logreg.predict(test_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])\n", "intent": "We then do a cross-validated logistic regression. Note the large amount of the regularization. Why do you think this is the case?\n"}
{"snippet": "criteria = [part5_to_int_mapper['06200'], 2000, 0, 5, symptom_to_int_mapper['WARNING LIGHT ON']]\nint_to_symp_class_mapper[clf.predict([criteria])[0]]\n", "intent": "**Predict just one record.**  Here I will feed the prediction model a transmission part \n"}
{"snippet": "logreg.predict_proba([1, 0, 29, 0])[:, 1]\n", "intent": "Predict probability of survival for **Adam**: first class, no parents or kids, 29 years old, male.\n"}
{"snippet": "logreg.predict_proba([2, 0, 29, 0])[:, 1]\n", "intent": "Predict probability of survival for **Bill**: same as Adam, except second class.\n"}
{"snippet": "clf, ts_preds = classify_topics(nmf, td_norm, train['target'], test_data, check_random_state(0))\nprint(classification_report(test['target'], ts_preds, target_names=test['target_names']))\n", "intent": "The resulting classification report and confusion matrix are shown to demonstrate the quality of this classification method.\n"}
{"snippet": "new_observation = [[3, 5, 4, 2]]\nknn.predict(new_observation)\n", "intent": "- New observations are called \"out-of-sample\" data\n- Uses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "print (10 + 0 + 20 + 10)/4.\nfrom sklearn import metrics\nprint metrics.mean_absolute_error(true, pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print (10**2 + 0**2 + 20**2 + 10**2)/4.\nprint metrics.mean_squared_error(true, pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint np.sqrt(metrics.mean_squared_error(true, pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "print(model.predict(vect.transform(['having bad issue with gadget ',\n                                    'everything working great'])))\n", "intent": "1 is positive review and 0 is negative review\n"}
{"snippet": "L2_LOSS = 0.0015\nl2 = L2_LOSS * \\\n    sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_Y, labels = Y)) + l2\n", "intent": "Again, we must properly name the tensor from which we will obtain predictions. We will use L2 regularization and that must be noted in our loss op:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint 'classification report results:\\r\\n' + classification_report(y_pred=knn4_pred,y_true=y_val)\n", "intent": "as we can see our assamption was indeed correct - categories 6,8 and 2 are those easiest to predict\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('classification report results:\\r\\n' + classification_report(y_pred=knn4_pred,y_true=y_val))\n", "intent": "as we can see our assamption was indeed correct - categories 6,8 and 2 are those easiest to predict\n"}
{"snippet": "print(\"F1: %4f\" % metrics.f1_score(bank_labels_test, y_pred))\n", "intent": " F1 score conveys the balance between the precision and the recall.\n"}
{"snippet": "X_enc = encode(X_train[0:5])\nprint(\"X_enc.shape : \", X_enc.shape)\nX_rec = decode(np.random.random((2, 10)).astype(np.float32))\nprint(\"X_rec.shape : \", X_rec.shape)\nX_pred = model.predict(X_train[0:5])\npl.imshow(X_pred[0].squeeze(), interpolation='nearest', cmap=cm.binary)\npl.colorbar()\n", "intent": "A theano function to get the representation of a given input (without reconstructing it)\n"}
{"snippet": "y_3l = (y_enA * 4./9.) + (y_ccA * 2./9.) + (y_enB * 2./9.) + (y_ccB * 1./9.)\nprint('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n", "intent": "Simple weighted average of the previous 4 predictions.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint \"\\ntest loss: \", test_loss\nprint \"test accuracy: \", test_acc\n", "intent": "Let's evaluate our model on the test set:\n"}
{"snippet": "print \"predicting on test data...\"\ny_pred = model.predict(X_test)\nmu_pred = y_pred[:,:num_clusters*data_dim]\nmu_pred = np.reshape(mu_pred, [-1, num_clusters, data_dim])\nsigma_pred = y_pred[:,num_clusters*data_dim : num_clusters*(data_dim+1)] \npi_pred = y_pred[:,num_clusters*(data_dim+1):]\nz_pred = np.argmax(pi_pred, axis=-1)\n", "intent": "We can now make predictions on test data:\n"}
{"snippet": "def ndcg_score(preds, dtrain):\n    labels = dtrain.get_label()\n    top = []\n    for i in range(preds.shape[0]):\n        top.append(np.argsort(preds[i])[::-1][:5])\n    mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int)\n    score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1))\n    return 'ndcg', score\n", "intent": "The evaluation metric is NDCG @k where k=5. So we select top 5 then get the average.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nregressor_mse = mean_squared_error(y_pred, y_test)\nimport math\nmath.sqrt(regressor_mse)\n", "intent": "So in our model, 60.7% of the variability in Y can be explained using X.\n"}
{"snippet": "y_prediction = regressor.predict(X_test)\ny_prediction\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Linear Regression Model\n<br><br></p>\n"}
{"snippet": "y_prediction = regressor.predict(X_test)\ny_prediction\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Decision Tree Regressor\n<br><br></p>\n"}
{"snippet": "y_pred = regressor.predict(X_test)\nprint('Liner Regression R squared: %.4f' % regressor.score(X_test, y_test))\n", "intent": "Done! We now have a working Linear Regression model.\n"}
{"snippet": "print(\"Average Precision Score: %4f\" % metrics.average_precision_score(bank_labels_test, y_pred))\n", "intent": "The Average Precision Score is quite low:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nlin_mae = mean_absolute_error(y_pred, y_test)\nprint('Liner Regression MAE: %.4f' % lin_mae)\n", "intent": "Calculate mean absolute error (MAE)\n"}
{"snippet": "predictions = humidity_classifier.predict(X_test)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPredict on Test Set \n<br><br></p>\n"}
{"snippet": "accuracy_score(y_true = y_test, y_pred = predictions)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nMeasure Accuracy of the Classifier\n<br><br></p>\n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = clf.predict(test_x)\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_y, \n                            predict_y, \n                            target_names=['Not Survived', 'Survived']))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$ \n$$Recall = \\frac{TP}{TP + FN}$$ \n$$F1 = \\frac{2TP}{2TP + FP + FN}$$ \n"}
{"snippet": "y_pred = knn.predict(X)\n", "intent": "Now we'll use this classifier to *predict* labels for the data\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\n", "intent": "Let's evaluate the model on the test data:\n"}
{"snippet": "model.load_weights('pre_trained_glove_model.h5')\nmodel.evaluate(x_test, y_test)\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "some_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "Let's try the full pipeline on a few instances:\n"}
{"snippet": "baseline_score = metrics.f1_score(bank_labels_test, y_pred)\nbaseline_score\n", "intent": "We have 1172 + 48 correct predictions and 29 + 108 false predictions\n"}
{"snippet": "X_test = preprocess_pipeline.transform(test_data)\ny_pred = svm_clf.predict(X_test)\n", "intent": "Great, our model is trained, let's use it to make predictions on the test set:\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\naccuracy_score(y_test, y_pred)\n", "intent": "Ah, this looks good! Let's select this model. Now we can test it on the test set:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred = lin_svr.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nmse\n", "intent": "Let's see how it performs on the training set:\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nnp.sqrt(mse)\n", "intent": "Now let's measure the RMSE on the training set:\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nnp.sqrt(mse)\n", "intent": "Looks much better than the linear model. Let's select this model and evaluate it on the test set:\n"}
{"snippet": "X_test_reduced = pca.transform(X_test)\ny_pred = rnd_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n", "intent": "*Exercise: Next evaluate the classifier on the test set: how does it compare to the previous classifier?*\n"}
{"snippet": "loss = tf.losses.log_loss(y, y_proba)  \n", "intent": "But we might as well use TensorFlow's `tf.losses.log_loss()` function:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_test, y_pred)\n", "intent": "Let's compute the model's precision and recall:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = dnn_clf.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The model is trained, let's see if it gets the same accuracy as earlier:\n"}
{"snippet": "accuracy_score(ytest, ytest_model)\n", "intent": "Compute the accuracy of the model:\n"}
{"snippet": "y_pred = dnn_clf_bn.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The best params are reached during epoch 48, that's actually a slower convergence than earlier. Let's check the accuracy:\n"}
{"snippet": "y_pred = dnn_clf.predict(X_train1)\naccuracy_score(y_train1, y_pred)\n", "intent": "Let's go back to the best model we trained earlier and see how it performs on the training set:\n"}
{"snippet": "y_pred = dnn_clf_dropout.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "Let's check the accuracy:\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_lr))\ncm = confusion_matrix(y_test, y_pred_lr)\nprint(cm)\n", "intent": "http://scikit-learn.org/stable/modules/model_evaluation.html\n- classification_report\n- confusion_matrix\n- accuracy_score\n- f1_score\n- ...\n"}
{"snippet": "labels_true = df_labels\nlabels_predict = db.labels_\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels_predict))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels_predict))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels_predict))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(labels_true, labels_predict))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(df_data_scaled, labels_predict))\n", "intent": "Compute the Evaluation Metrics:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\npred_scaled = linear_model.predict(X_test_scaled)\nprint('Truth:', y_test_scaled)\nprint('Predictions:', pred_scaled)\nprint('MSE:', mean_squared_error(y_test_scaled, pred_scaled))\nprint('R2:', r2_score(y_test_scaled, pred_scaled))\n", "intent": "Let's validate our model's performance by computing metrics and plotting the linear model.\n"}
{"snippet": "print('Logistic Regresion:')\nprint(classification_report(y_test, pred_logistic))\nprint('Logistic Regresion (balanced):')\nprint(classification_report(y_test, pred_logistic_bal))\nprint('SGD:')\nprint(classification_report(y_test, pred_sgd))\nprint('SGD (balanced):')\nprint(classification_report(y_test, pred_sgd_bal))\nprint('Baseline:')\nprint(classification_report(y_test, pred_baseline))\n", "intent": "8. metrics\n9. learning curve\n10. prediction\n"}
{"snippet": "y_pred_train = gs2.predict(Z_train)\nZ_train_pca = pca.transform(Z_train) \ny_pred_test = gs2.predict(Z_test)\nZ_test_pca = pca.transform(Z_test) \n", "intent": "This can be a way to determine why our errors are high\n"}
{"snippet": "y_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\nprint(classification_report(y_true, y_pred))\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n"}
{"snippet": "cvs = cross_val_score(rfc, Xtest, ytest, cv=10)\ncvs\n", "intent": "Use `cross_val_score` to perform k-fold cross validation (`k=10`) with this model:\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntrain_and_evaluate(OUTDIR, num_train_steps = 2000)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntrain_and_evaluate(OUTDIR, num_train_steps = 500)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "def print_rmse(model, df):\n  metrics = model.evaluate(input_fn = make_eval_input_fn(df))\n  print('RMSE on dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "def train_and_evaluate(output_dir, num_train_steps):\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n", "intent": "<h2> tf.estimator.train_and_evaluate </h2>\n"}
{"snippet": "OUTDIR='mnist/learned'\nshutil.rmtree(OUTDIR, ignore_errors = True) \nhparams = {'train_steps': 1000, 'learning_rate': 0.01}\ntrain_and_evaluate(OUTDIR, hparams)\n", "intent": "This is the main() function\n"}
{"snippet": "def print_rmse(model, name, df):\n  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n  print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, 'validation', df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "train_preds = meta_model.predict(X_train_level2).clip(0.,20.) \nrmse_train_stacking = math.sqrt(mean_squared_error(y_train_level2, train_preds))\nstack_val_preds = meta_model.predict(X_val_level2).clip(0.,20.) \nstack_test_preds = meta_model.predict(X_test_level2).clip(0.,20.) \nrmse_val_stacking= math.sqrt(mean_squared_error(y_val, stack_val_preds))\nprint('Train  RMSE for stacking is %f' % rmse_train_stacking)\nprint('-------------')\nprint('Validation set RMSE for stacking is %f' % rmse_val_stacking)\n", "intent": "Compute R-squared on the train and test sets and also the test set predictions\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nimport numpy as np\nprint np.sqrt(mean_squared_error(y_test, dtr.predict(X_test)))\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "print(classification_report(ytest, ytest_model))\n", "intent": "Compute the average accuracy and its standard deviation:\n"}
{"snippet": "y = km.predict(data)\n", "intent": "Predict the clusters for each data point, store this as `y`\n"}
{"snippet": "y = km.predict(data)\nprint y\n", "intent": "Predict the clusters for each data point, store this as `y`\n"}
{"snippet": "MIN_RATING = 0.0\nMAX_RATING = 5.0\nITEMID = 1\nUSERID = 1\nsvd.predict(ITEMID, USERID, MIN_RATING, MAX_RATING)\n", "intent": "Try using `svd.predict()` to predict ratings for a given user and movie, $\\hat{r}_{ui}$\n"}
{"snippet": "predict = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint 'CONFUSION MATRIX'\nprint confusion_matrix(y_test, predict)\nprint '\\n\\nCLASSIFICATION REPROT'\nprint classification_report(y_test, predict)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "C = 5\ny_test_oh = np.eye(C)[Y_test.reshape(-1)]\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\npred = model.predict(X_test_indices)\nfor i in range(len(X_test)):\n    x = X_test_indices\n    num = np.argmax(pred[i])\n    if(num != Y_test[i]):\n        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())\n", "intent": "You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. \n"}
{"snippet": "x_test = np.array(['not feeling happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "best_model = best_linear_model\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...?\nIt seems that Linear SVM has larger auc-score.\n"}
{"snippet": "best_model = best_linear_model_tf\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...? Still Linear SVM, but the Kernel version is very close to it.\n"}
{"snippet": "def predict(clf, timeline):\n    tweets = []\n    for i in timeline:\n        tweets.append(i.text)\n    y_pred = clf.predict(np.array(tweets))\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "def sum_squared_error(y_hat, y):\n    return np.sum(0.5*(y_hat - y)**2)\n", "intent": "A helper function for the Sum Squared Error\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xnew = np.linspace(m.X.mean.min(), m.X.mean.max())[:, None]\nYnew = m.predict(Xnew)[0]\n", "intent": "What does the underlying trend explaining the data actually look like according to the model?\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "What does the score indicate?\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(X, labels)\n", "intent": "_(pairplot with hue)_\n"}
{"snippet": "print(('Estimated number of clusters: %d' % n_clusters_))\nprint((\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels)))\nprint((\"Completeness: %0.3f\" % metrics.completeness_score(y, labels)))\nprint((\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels)))\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`**\n"}
{"snippet": "print((\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels)))\n", "intent": "**9.3 Evaluate DBSCAN visually, with silhouette, and with the metrics against the true `y`.**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat.naive))\nprint(rms)\n", "intent": "Let's use RMSE to check the accuracy of our model on the test data set.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.avg_forecast))\nprint(rms)\n", "intent": "Now, let's calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "def find_mse(df, degree):\n    mod, pred = fit_reg_poly(df, degree)\n    result = mean_squared_error(df['DepDelay'], pred)\n    return result\n", "intent": "Write a function named `find_mse()` that returns the mean squared error of ridge regresion model, given the degree. Assume that alpha is always 1.0.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.SES))\nprint(rms)\n", "intent": "Now, let's use RMSE to check to accuracy of our model.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.Holt_Winter))\nprint(rms)\n", "intent": "Let's look at the RMSE to check accuracy.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.SARIMA))\nprint(rms)\n", "intent": "Let's check the RMSE for accuracy of the model.\n"}
{"snippet": "y_pred_class = knn.predict(X)\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred_class))\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, Xs, y, cv=10)\nprint(lasso_scores)\nprint(np.mean(lasso_scores))\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nenet_scores = cross_val_score(enet, Xs, y, cv=10)\nprint(enet_scores)\nprint(np.mean(enet_scores))\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "xlist = [20,21]\nXlist = sm.add_constant(xlist)\nresults.predict(Xlist)\n", "intent": "Hint: You'll need to use a list - don't forget your intercept!\n"}
{"snippet": "linreg.predict(2)\n", "intent": "**Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object.**\n"}
{"snippet": "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1]\n", "intent": "**Create a confusion matrix of predictions on our test set using `metrics.confusion_matrix`**.\n"}
{"snippet": "CR_pred_train_y=CR_clf.predict(CR_train_X)\n", "intent": "Predict labels on the training set, and name the returned variable as `CR_pred_train_y`\n"}
{"snippet": "metrics.accuracy_score(y_test2,y_pred3)\n", "intent": "**Evaluate the model metrics now**\n"}
{"snippet": "y_pred = linreg.predict(X)\nglass['y_pred'] = y_pred\n", "intent": "**Using the `LinearRegression` object we have fit, create a variable that are our predictions for `ri` for each row's `al` in the data set.**\n"}
{"snippet": "print(metrics.mean_squared_error(y_test, y_test.apply(np.mean, broadcast=True)))\n", "intent": "How does this compare to what we achieved with linear regression. Is our model making an actual improvement?\n"}
{"snippet": "pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "CR_pred_test_y=CR_clf.predict(CR_test_X)\n", "intent": "Predict the labels on the test set. Name the returned variable as `CR_pred_test_y`\n"}
{"snippet": "print(clf.predict([[3., +2.5]]))\n", "intent": "What about at `[3., +2.5]`?\n"}
{"snippet": "from scipy.stats import pearsonr\npearsonr(gender_test, D.predict(X_test).ravel())\n", "intent": "The pearson correlation coefficient between gender and the classifier output also clearly highlights this dependency.\n"}
{"snippet": "D.predict(G.predict(z_test))\n", "intent": "What about unseen data?\n"}
{"snippet": "def print_rmse(model, name, df):\n  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n  print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss']))\nprint_rmse(model, 'validation', df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n", "intent": "$$\\text{accuracy} = \\frac{TP + TN}{N}$$\n"}
{"snippet": "from scipy.stats import gaussian_kde\ndef make_kde(sample):\n    xs = np.linspace(-4, 4, 101)\n    kde = gaussian_kde(sample)\n    ys = kde.evaluate(xs)\n    return pd.Series(ys, index=xs)\n", "intent": "To see what these overlapping distributions look like, I'll plot a kernel density estimate (KDE).\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_pred_prob = model.predict_proba(X)[:,1]\nauc = roc_auc_score(y, y_pred_prob)\n", "intent": "And compute the AUC.\n"}
{"snippet": "trainer.load('/home/cy/chainer_best_model_converted_to_pytorch_0.7053.pth')\nopt.caffe_pretrain=True \n_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\nvis_bbox(at.tonumpy(img[0]),\n         at.tonumpy(_bboxes[0]),\n         at.tonumpy(_labels[0]).reshape(-1),\n         at.tonumpy(_scores[0]).reshape(-1))\n", "intent": "You'll need to download pretrained model from [google dirve](https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY) \n"}
{"snippet": "def get_accuracy(ypred, ytest):\n    return accuracy_score(y_pred=ypred, y_true=ytest)*100\n", "intent": "Get the performance (accuracy) of your algorithm given ytest\n"}
{"snippet": "bn_model.evaluate(samp_conv_val_feat, samp_val_labels)\n", "intent": "We should find that the new model gives identical results to those provided by the original VGG model.\n"}
{"snippet": "conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\nconv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)\n", "intent": "We can now pre-compute the output of the convolutional part of VGG.\n"}
{"snippet": "inp = np.expand_dims(conv_val_feat[0], 0)\nnp.round(lrg_model.predict(inp)[0],2)\n", "intent": "We have to add an extra dimension to our input since the CNN expects a 'batch' (even if it's just a batch of one).\n"}
{"snippet": "p = top_model.predict(arr_hr[:20])\n", "intent": "Now we can pass any image through this CNN and it will produce it in the style desired!\n"}
{"snippet": "def eval_keras(input):\n    preds = model.predict(input, batch_size=128)\n    predict = np.argmax(preds, axis = 2)\n    return (np.mean([all(real==p) for real, p in zip(labels_test, predict)]), predict)\n", "intent": "To evaluate, we don't want to know what percentage of letters are correct but what percentage of words are.\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint(\"Loss = \" + str(preds[0]))\nprint(\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "predictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]\n", "intent": "We'll grab the predictions for each individual tree, and look at one example.\n"}
{"snippet": "print(accuracy_score(ytest, gaussian_ypredict))\n", "intent": "Compute the accuracy of the model:\n"}
{"snippet": "preds = predict(net, md.val_dl)\n", "intent": "Now that we have the parameters for our model, we can make predictions on our validation set.\n"}
{"snippet": "preds = predict(net2, md.val_dl).max(1)[1]\nplots(x_imgs[:8], titles=preds[:8])\n", "intent": "Now we can check our predictions:\n"}
{"snippet": "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n", "intent": "Now let's try out our agent to see if it raises any errors.\n"}
{"snippet": "def evaluate(agent, env, n_games=1):\n", "intent": "Let's build a function that measures agent's average reward.\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "As before, we need to coerce these *x* values into a ``[n_samples, n_features]`` features matrix, after which we can feed it to the model:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Finally, we can use the ``accuracy_score`` utility to see the fraction of predicted labels that match their true value:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Now that we have predicted our model, we can gauge its accuracy by comparing the true values of the test set to the predictions:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n", "intent": "Finally, we compute the fraction of correctly labeled points:\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "Now let's generate some new data and predict the label:\n"}
{"snippet": "youtput = rforest_model.predict(Xtest)\nprint(accuracy_score(ytest, youtput))\n", "intent": "Compute the accuracy of the model:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:\n"}
{"snippet": "labels = model.predict(patches_hog)\nlabels.sum()\n", "intent": "Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains a face:\n"}
{"snippet": "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\nf1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n", "intent": "**Warning**: the following cell may take a very long time (possibly hours depending on your hardware).\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint 'accuracy = {0}%'.format(accuracy)\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "x = preprocess_input(image_224_batch.copy())\npreds = model.predict(x)\n", "intent": "`image_224_batch` is now compatible with the input shape of the neural network, let's make a prediction.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\ntest_preds = model.predict([user_id_test, item_id_test])\nprint(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\nprint(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))\n", "intent": "Now that the model is trained, the model MSE and MAE look nicer:\n"}
{"snippet": "labels_to_encode = np.array([[3]])\nmodel.predict(labels_to_encode)\n", "intent": "We can use the `predict` method of the Keras embedding model to project a single integer label into the matching embedding vector:\n"}
{"snippet": "labels_to_encode = np.array([[3], [3], [0], [9]])\nmodel.predict(labels_to_encode)\n", "intent": "Let's do the same for a batch of integers:\n"}
{"snippet": "representation = base_model.predict(img_batch)\nprint(\"Shape of representation:\", representation.shape)\n", "intent": "The base model can transform any image into a flat, high dimensional, semantic feature vector:\n"}
{"snippet": "val_scores = np.array(cross_val_score(rforest_model, X, y, cv=10))\n", "intent": "Use `cross_val_score` to perform k-fold cross validation (`k=10`) with this model:\n"}
{"snippet": "def model_perplexity(model, X, y, verbose=1):\n    predictions = model.predict(X, verbose=verbose)\n    return perplexity(y, predictions)\n", "intent": "Let's measure the perplexity of the randomly initialized model:\n"}
{"snippet": "def model_perplexity(model, X, y, verbose=0):\n    predictions = model.predict(X, verbose=verbose)\n    return perplexity(y, predictions)\n", "intent": "Let's measure the perplexity of the randomly initialized model:\n"}
{"snippet": "output_test = model.predict(x_test)\ntest_casses = np.argmax(output_test, axis=-1)\nprint(\"test accuracy:\", np.mean(test_casses == y_test))\n", "intent": "**Exercice**\n - compute model accuracy on test set\n"}
{"snippet": "encoder_a.training = False\nencoder_c.training = False\ndecoder.training = False\nsamples = data.sample(n=100)\nfor (i, row) in samples.iterrows():\n    evaluate((row.french, row.english), encoder_a, encoder_c, decoder, french_vocab, english_vocab)\n", "intent": "To evaluate the learned model, simply execute the following\n"}
{"snippet": "def regr_metrics(act, pred):\n    return (math.sqrt(metrics.mean_squared_error(act, pred)), \n     metrics.mean_absolute_error(act, pred))\n", "intent": "It will be helpful to have some metrics on how good our prediciton is.  We will look at the mean squared norm (L2) and mean absolute error (L1).\n"}
{"snippet": "pred_test = lda_clf.predict(X_test)\nprint('Prediction accuracy for the test dataset')\nprint('{:.2%}'.format(metrics.accuracy_score(y_test, pred_test)))\n", "intent": "To verify that over model was not overfitted to the training dataset, let us evaluate the classifier's accuracy on the test dataset:\n"}
{"snippet": "print(evaluate('Th', 200, temperature=0.2))\n", "intent": "Lower temperatures are less varied, choosing only the more probable outputs:\n"}
{"snippet": "print(evaluate('Th', 200, temperature=1.4))\n", "intent": "Higher temperatures more varied, choosing less probable outputs:\n"}
{"snippet": "def evaluate_randomly():\n    pair = random.choice(pairs)\n    output_words, decoder_attn = evaluate(pair[0])\n    output_sentence = ' '.join(output_words)\n    print('>', pair[0])\n    print('=', pair[1])\n    print('<', output_sentence)\n    print('')\n", "intent": "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n"}
{"snippet": "def predict(clf, timeline):\n    X_test = np.array([tweet.text for tweet in timeline])\n    y_pred = clf.predict(X_test)\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "def get_rmse_log(net, X_train, y_train):\n    num_train = X_train.shape[0]\n    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n    return np.sqrt(2 * nd.sum(square_loss(\n        nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)\n", "intent": "Below defines the root mean square loss between the logarithm of the predicted values and the true values used in the competition.\n"}
{"snippet": "output_test = model.predict(x_test)\ntest_casses = np.argmax(output_test, axis=-1)\nprint(\"Test accuracy:\", np.mean(test_casses == target_test))\n", "intent": "**Exercices**\n- Compute model accuracy on test set\n"}
{"snippet": "gb.predict(ds[pd.isnull(ds.Biscuits)].drop(\"Biscuits\",axis=1))\n", "intent": "Looks like only biscuit can be approximated based on the other data, so let's predict the missing value from original dataset\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "def train_predict(learner, training_data, y):\n    cv = 10\n    learner = learner\n    scores = cross_val_score(learner, training_data, y, cv = cv, scoring = \"accuracy\")\n    print(\"Model's mean accuracy using %s is: %0.2f%s (+/- %0.2f)\" %(learner.__class__.__name__, scores.mean() * 100, \"%\", scores.std() * 2))\n    return (scores.mean())\n", "intent": "* DeciisionTreeClassifer\n* Multinomial Naive Bayes\n* Support Vector Machine\n* AdaBoost\n* RandomForest Classifier\n* Neural Network\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_run.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lasso_test_pred = np.expm1(LassoMd.predict(final_test_df.values))\nENet_test_pred = np.expm1(ENetMd.predict(final_test_df.values))\nKRR_test_pred = np.expm1(KRRMd.predict(final_test_df.values))\nGBoost_test_pred = np.expm1(GBoostMd.predict(final_test_df.values))\nfinalMd = (lasso_test_pred + ENet_test_pred + KRR_test_pred + GBoost_test_pred) / 4\nfinalMd\n", "intent": "np.expm1 ( ) is used to calculate exp(x) - 1 for all elements in the array. \n"}
{"snippet": "print('Predicting ri for al=2 through linear equation: ', linreg.intercept_ + linreg.coef_ * 2)\nprint('Predicting ri for al=2 through model: ', linreg.predict(2))\nmodel_coef = zip(feature_cols, linreg.coef_)\nprint('Model Coefficients:')\nfor values in model_coef:\n    print(values)  \n", "intent": "Linear regression equation: $y = \\beta_0 + \\beta_1x$\n"}
{"snippet": "print('Predicting ri for al=3 through linear equation: ', 1.51699012 - 0.0024776063874696235)\nprint('Predicting ri for al=3 through model: ', linreg.predict(3))\n", "intent": "<b>Interpretation:</b> A 1 unit increase in 'al' is associated with a 0.0025 unit decrease in 'ri'.\n"}
{"snippet": "def find_mse(df, degree):\n    est, pred = fit_reg_poly(df, degree, alpha=1.0)\n    np.ravel(pred)\n    result = mean_squared_error(np.array(df['DepDelay']), pred)\n    return result\n", "intent": "Write a function named `find_mse()` that returns the mean squared error of ridge regresion model, given the degree. Assume that alpha is always 1.0.\n"}
{"snippet": "X_test = test[feature_cols]\ny_test = test.price\ny_pred = treereg.predict(X_test)\nprint('Predicted Values: ', y_pred)\nprint('RMSE of the regressor tree model: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Question: Using the tree diagram above, what predictions will the model make for each observation?\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn, X, y, cv=5, scoring='accuracy').mean()\n", "intent": "- Train/test split is **faster and more flexible**\n- Cross-validation provides a **more accurate** estimate of out-of-sample performance\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "While the model may not be as accurate, we have avoided the problem of overfitting. \n"}
{"snippet": "from sklearn.metrics import r2_score\nwr_height_r2 = r2_score(height, wr_height_hat)\nprint 'regression R^2:', wr_height_r2\n", "intent": "---\nRecall that the $R^2$ metric calculates the variance explained by your model over the baseline model.\nThe formula, to refresh your memory, is:\n"}
{"snippet": "wr_height_no_r2 = r2_score(height, wr_height_hat_no)\nprint 'regression R^2:', wr_height_r2\nprint 'regression outliers removed R^2:', wr_height_no_r2\n", "intent": "---\nWhich performs better? Why do you think that is?\n"}
{"snippet": "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nenet_scores = cross_val_score(enet, Xn, y, cv=10)\nprint enet_scores\nprint np.mean(enet_scores)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "def calc_confusion_matrix(mod, X, y, margins=False):\n    pred = pd.Series(mod.predict(X), name='Predicted')\n    true = pd.Series(y, name='True')\n    confusion = pd.crosstab(true, pred, margins=margins)\n    return confusion\n", "intent": "---\nWhat do they tell you about the models?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_squared_error(y_true, y_pred)\n", "intent": "$$ MSE = \\frac{1}{n}\\sum (Y_{i}-\\hat{Y_{i}})^{2} $$\n"}
{"snippet": "score = model.evaluate(x_test1, y_test1, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "def predict(clf, timeline):\n    messages = []\n    for tweet in timeline:\n        messages.append(tweet.text)\n    new_tweets = np.array(messages)\n    y_pred = clf.predict(new_tweets)\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "SVC_score_on_training = metrics.accuracy_score(y_2_train, SVC_prediction_on_training)\nSVC_precision_on_training = metrics.precision_score(y_2_train, SVC_prediction_on_training, pos_label=\"REAL\")\nSVC_recall_on_training = metrics.recall_score(y_2_train, SVC_prediction_on_training, pos_label=\"REAL\")\nSVC_f1_on_training = metrics.f1_score(y_2_train, SVC_prediction_on_training, pos_label=\"REAL\")\nprint(\"accuracy for SVC on training dataset2:   %0.3f\" % SVC_score_on_training)\nprint(\"precision for SVC on training dataset2:   %0.3f\" % SVC_precision_on_training)\nprint(\"recall for SVC on training dataset2:   %0.3f\" % SVC_recall_on_training)\nprint(\"f1 for SVC on training dataset2:   %0.3f\" % SVC_f1_on_training)\n", "intent": "**Prediction of Labels(Fake/Real) on TRAIN - VALIDATION - TEST data**\n"}
{"snippet": "X_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (accuracy))\n", "intent": "**Testing the model 1**\n"}
{"snippet": "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n        else: pos_ok += 1\n    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n    else: pos_cnt += 1\nprint(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n", "intent": "**Printing results for model 1**\n"}
{"snippet": "X_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (accuracy))\n", "intent": "**Testing the model 2**\n"}
{"snippet": "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n        else: pos_ok += 1\n    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n    else: pos_cnt += 1\nprint(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n", "intent": "**Printing results for model 2**\n"}
{"snippet": "X_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (accuracy))\n", "intent": "**Testing the model 3**\n"}
{"snippet": "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n        else: pos_ok += 1\n    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n    else: pos_cnt += 1\nprint(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n", "intent": "**Printing results for model 3**\n"}
{"snippet": "X, Y, categories = dataset_2()\ncount = count_V(X, Y)\npred = model_b(count, Y)\nconf.append(precision_recall_fscore_support(Y[0], pred[0], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[0], pred[0], normalize=True, sample_weight=None))\nconf.append(precision_recall_fscore_support(Y[1], pred[1], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[1], pred[1], normalize=True, sample_weight=None))\nconf.append(precision_recall_fscore_support(Y[2], pred[2], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[2], pred[2], normalize=True, sample_weight=None))\n", "intent": "model b + dataset 2 + train + validation + test\n"}
{"snippet": "X, Y, categories = dataset_2()\ntfidf = tfidf_V(X, Y)\npred = model_a(tfidf, Y)\nconf.append(precision_recall_fscore_support(Y[1], pred[1], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[1], pred[1], normalize=True, sample_weight=None))\nX, Y, categories = dataset_1()\ncount = count_V(X, Y)\npred = model_b(count, Y)\nconf.append(precision_recall_fscore_support(Y[1], pred[1], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[1], pred[1], normalize=True, sample_weight=None))\n", "intent": "model a + dataset 2 + test \n<br>\nmodel b + dataset 1 + test\n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "- Back to [Table of Contents](\nNow let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:\n"}
{"snippet": "pretrained_tagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\npenn_taggs = nltk.corpus.treebank.tagged_sents()\nbrwn_taggs = nltk.corpus.brown.tagged_sents()\npenn_pt_score = pretrained_tagger.evaluate(penn_taggs[:1000])\nscores_list.append((\"Task 1.2\", penn_pt_score))\nbrwn_pt_score = pretrained_tagger.evaluate(brwn_taggs[:1000])\nscores_list.append((\"Task 1.5\", brwn_pt_score))\n", "intent": "    MaxEntropy Classifier: An NLKT pre-trained POS tagger\n"}
{"snippet": "print(linreg.predict([0,0])) \nprint(linreg.predict([1,0])) \nprint(linreg.predict([0,1])) \n", "intent": "** Now let's use predict function to predict mpg for light, heavy and medium cars **\n"}
{"snippet": "RSS = sum((y-linreg.predict(X))**2) \nTSS = sum((y-y.mean())**2)\nR_Squared = 1 - float(RSS)/TSS\nprint(R_Squared)\n", "intent": "This is the definition of R_Squared. Let's first calculate it = this is the hard way!\n"}
{"snippet": "metrics.accuracy_score(ypred, ytest)\n", "intent": "We can check the accuracy of this classifier:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Just for kicks, let's see how accurate our K-Means classifier is **with no label information:**\n"}
{"snippet": "model_labels16 = np.zeros_like(clusters16)\nfor i in range(10):\n    mask = (clusters16 == i)\n    model_labels16[mask] = mode(digits.target[mask])[0]    \naccuracy_score(digits.target, model_labels16)   \n", "intent": "And let's check the score:\n"}
{"snippet": "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\nD_train_step = D_solver.minimize(D_loss, var_list=D_vars)\nG_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n", "intent": "Create new training steps using the LSGAN loss:\n"}
{"snippet": "print \"Accuracy is \", accuracy_score(y_test,y_pred)*100\n", "intent": "**Is there an improvement in the Accuracy of your modified model? **\n"}
{"snippet": "logit_1.predict([[3], [4]])\n", "intent": "- Prediction can be made with the `predict` method:\n"}
{"snippet": "def report_results(model, X, y):\n    pred = model.predict(X)        \n    acc = accuracy_score(y, pred)\n    f1 = f1_score(y, pred)\n    prec = precision_score(y, pred)\n    rec = recall_score(y, pred)\n    result = {'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n    return result\n", "intent": "best params:\n{'svc__C': 0.03125, 'svc__degree': 1.0, 'svc__gamma': 0.03125, 'svc__kernel': 'linear'}\nbest cv score:\n0.87025\n"}
{"snippet": "LDA.predict(x_tm)\n", "intent": "- Predict with the LDA model:\n"}
{"snippet": "x_new = np.array([2.8, 5.6, 3.2, 6.7]).reshape(1,-1)\nprint gnb.predict(x_new)\nprint gnb.predict_proba(x_new)\n", "intent": "- Use `predict()` and `predict_proba()` methods to predict the new data point `x_new = [2.8, 5.6, 3.2, 6.7]`.\n"}
{"snippet": "random_divide = cv.KFold(150, 5, random_state=1)\nscores = cv.cross_val_score(logit, iris.data, iris.target, \n                            cv=random_divide)\nscores\n", "intent": "Here is a case that we randomly divide the data set by **KFold**:\n"}
{"snippet": "stratify_divide = cv.StratifiedKFold(iris.target, 5, \n                                     random_state=0)\nscores = cv.cross_val_score(logit, iris.data, \n                            iris.target, \n                            cv=stratify_divide)\nscores\n", "intent": "We can also use **StratifiedKFold** to split the dataset more evenly among different classes\n"}
{"snippet": "scores = cv.cross_val_score(model, X, y ,cv=5)\n1 - scores\n", "intent": "- Use the function **cross_val_score** to implement 5-fold cross validation with a **Linear Discriminant Analysis** model. Report the test errors.\n"}
{"snippet": "svm_model.predict(svm_model.support_vectors_)\n", "intent": "- Which classes do they belong to?\n"}
{"snippet": "test = data.loc[np.random.choice(data.index,size = 100000,replace=False)]\nypred = predict_tip(test)\nprint \"final mean_squared_error:\", metrics.mean_squared_error(ypred,test.Tip_percentage)\nprint \"final r2_score:\", metrics.r2_score(ypred,test.Tip_percentage)\n", "intent": "Make predictions on a sample of 100000 transactions\n"}
{"snippet": "forest_predictions = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, forest_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "accuracy_score(predictions, y_test)\n", "intent": "Let's see how accurate the model is.\n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import recall_score\nprint 'Recall = ', recall_score(t_test, y_test, average=None)\n", "intent": "The \"recall\" score measures the percentage of correctly classified cases for each class, and can be used to diagnose this kind of cases.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint classification_report(t_test, y_test)\n", "intent": "We can also request a summary table with `classification_report`.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.adjusted_rand_score(Y,Y_est)\n", "intent": "Estimate the error with sklearn.metrics.adjusted_rand_score\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred_train = model.predict(X_train)\nmean_squared_error(y_train, y_pred_train)\n", "intent": "That was easy. What's the training MSE?\n"}
{"snippet": "y_pred_test = model.predict(X_test)\n", "intent": "First, make predictions.\n"}
{"snippet": "mean_squared_error(y_test, y_pred_test)\n", "intent": "Now see how good they are.\n"}
{"snippet": "pred = p.predict(X_test)\nsklearn.metrics.accuracy_score(y_test, pred)\n", "intent": "Then we can evaluate the behavior of the classifier on the held-out set\n"}
{"snippet": "prepared_data = prepare_data(data)\nprint(\"This model predicted your input as\", first_model.predict(prepared_data).argmax())\n", "intent": "(tip: don't expect it to work)\n"}
{"snippet": "accuracy_score(y_test, prediction)\n", "intent": "Let us compute the accuracy of out classifier model.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits, True, skf_seed) \n    NNF = NearestNeighborsFeats(n_jobs=-1, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv = skf)\n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "predictions = []\nfor tree in rf.estimators_:\n    predictions.append(tree.predict_proba(X_val)[None, :])\n", "intent": "**Step 2:** Get predictions for each tree in Random Forest separately.\n"}
{"snippet": "scores = []\nfor pred in cum_mean:\n    scores.append(accuracy_score(y_val, np.argmax(pred, axis=1)))\n", "intent": "**Step 5:** Get accuracy scores for each `n_estimators` value\n"}
{"snippet": "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\n", "intent": "Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n"}
{"snippet": "MAX_TO_PRINT = 5\npredictions = estimator.predict(input_fn=test_input)\ni = 0\nfor p in predictions:\n    true_label = y_test[i]\n    predicted_label = p['class_ids'][0]\n    print(\"Example %d. True: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == MAX_TO_PRINT: break\n", "intent": "Here's how you would print individual predictions.\n"}
{"snippet": "test_input_fn = create_test_input_fn()\npredictions = estimator.predict(test_input_fn)\ni = 0\nfor prediction in predictions:\n    true_label = census_test_label[i]\n    predicted_label = prediction['class_ids'][0]\n    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == 5: break\n", "intent": "The Estimator returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples.\n"}
{"snippet": "result = model.evaluate(input_fn=test_input_fn)\n", "intent": "Once the model has been trained, we can evaluate its performance on the test-set.\n"}
{"snippet": "predictions = model.predict(input_fn=predict_input_fn)\n", "intent": "The model can also be used to make predictions on new data.\n"}
{"snippet": "predictions = model.predict(input_fn=test_input_fn)\n", "intent": "To get the predicted classes for the entire test-set, we just use its input-function:\n"}
{"snippet": "print(classification_report(test1['target'], pred1, target_names=test1['target_names']))\n", "intent": "Let us take a look at how well our model performs:\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def print_rmse(model, name, input_fn):\n  metrics = model.evaluate(input_fn=input_fn, steps=1)\n  print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss']))\nprint_rmse(model, 'validation', make_input_fn(df_valid))\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "tree_predictions = tree.predict(X_test)\n", "intent": "Make a prediction with the trained model on the test data.\n"}
{"snippet": "forest_predictions = rf.predict(X_test)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test)\naccuracy_score(y_test, tuned_forest_predictions)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "print(\"Normalized Accuracy Score for your model is {:.3f}\".format(accuracy_score(y_true, y_pred)))\nprint(\"Correctly classified samples {:d} out of {} instances\".format(accuracy_score(y_true, y_pred, normalize=False),\n                                                          len(Y_test)))\n", "intent": "___\n+ If {normalize == True}, return the correctly classified samples (float), else it returns the number of correctly classified samples (int).\n"}
{"snippet": "print(classification_report(y_pca_test, y_pred_dt_smote_pca))\n", "intent": "** Classification Report **\n___\n"}
{"snippet": "print(classification_report(y_pca_test, y_pred_rf_smote_pca))\n", "intent": "** Classification Report **\n___\n"}
{"snippet": "cls_p_score_rf_smote_pca = precision_score(y_pca_test, y_pred_rf_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Precision score is {:.2f}\".format(cls_p_score_rf_smote_pca))\ncls_r_score_rf_smote_pca = recall_score(y_pca_test, y_pred_rf_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Recall Score is {:.2f}\".format(cls_r_score_rf_smote_pca ))\n", "intent": "** Precision Score **\n___\n"}
{"snippet": "import torchbearer\nloss_function = nn.CrossEntropyLoss()\noptimiser = optim.Adam(model.parameters())\ntrial = torchbearer.Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'])\ntrial.with_generators(trainloader, test_generator=testloader)\ntrial.run(epochs=10)\nresults = trial.evaluate(data_key=torchbearer.TEST_DATA)\nprint(results)\n", "intent": "We can use the torchbearer `Trial` class to do all the hard work in training and evaluating for us:\n"}
{"snippet": "cls_p_score_et_smote_pca = precision_score(y_pca_test, y_pred_et_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Precision score is {:.2f}\".format(cls_p_score_et_smote_pca))\ncls_r_score_et_smote_pca = recall_score(y_pca_test, y_pred_et_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Recall Score is {:.2f}\".format(cls_r_score_et_smote_pca ))\n", "intent": "** Precision and Recall Score **\n___\n"}
{"snippet": "classifier_p_score_w = precision_score(y_true, y_pred, average='weighted')\nprint(\"Precision Value averaged by 'weighted' is {:.2f}\".format(classifier_p_score_w))\nclassifier_p_score_mi = precision_score(y_true, y_pred, average='micro')\nprint(\"Precision Value averaged by 'micro' is {:.2f}\".format(classifier_p_score_mi))\n", "intent": "+ The precision Value returned is the weighted average of the precision of each class for the multiclass task.\n"}
{"snippet": "img = prepare_image('imgs/dog2.jpg')\nout = model_vgg16_conv.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/dog2.jpg'/>\n"}
{"snippet": "img = prepare_image('imgs/sloth.jpg')\nout = model_vgg16_conv.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/sloth.jpg'/>\n"}
{"snippet": "knn.predict([[3, 5, 4, 2]])\n", "intent": "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "dout = np.random.randn(batch_size, pool_size, pool_size, input_depth)\ndx_num = eval_numerical_gradient_array(lambda x: max_pool_forward(x, pool_size, stride), X, dout)\nout = max_pool_forward(X, pool_size, stride)\ndx = max_pool_backward(dout, X, out, pool_size, stride)\nprint('Testing max_pool_backward function:')\ndiff = relative_error(dx, dx_num)\nif diff < 1e-12:\n    print 'PASSED'\nelse:\n    print 'The diff of %s is too large, try again!' % diff\n", "intent": "And we again use numerical gradient checking to ensure that the backward function is correct: \n"}
{"snippet": "predictions = model.predict(X) \n", "intent": "Make the predictions by the model:\n"}
{"snippet": "predictions = model.predict(X)\n", "intent": "Make the predictions by the model:\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint (predicted)\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "assert(theta.shape == (3,1))\nprint(\"Theta: \", theta.t())\nprint(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta, y_test))\n", "intent": "We can now print out the error achieved on the test set, as well as the parameter vector:\n"}
{"snippet": "predicted= cross_val_predict(clf,X,y,cv=10)\nmetrics.accuracy_score(y,predicted)\n", "intent": "Cross validation Predict: generate cross-validated estimates for each input data point\n"}
{"snippet": "from sklearn.metrics import adjusted_rand_score\nadjusted_rand_score(y, labels)\n", "intent": "sklearn also has a scoring for this: adjusted rand score: \n"}
{"snippet": "scores = cross_val_score(classifier, X, y)\nprint(scores)\nprint(np.mean(scores))\n", "intent": "Try it! How many folds did it use? You can control that. Try reproducing your own result.\n"}
{"snippet": "mols = list(dff.MurMol)\nsmarts,smi,img,mcsM = MCS_Report(mols,threshold=0.8,ringMatchesRingOnly=True)\nimg\n", "intent": "And now we'll extract the MCS core:\n"}
{"snippet": "model_1_predict = model_1.predict(test[feature_columns])\n", "intent": "**Let's use the model for prediction**\n"}
{"snippet": "y_pred = text_clf.predict(X_test)\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred))\n", "intent": "Now that the model is trained, let's evaluate it on the test set:\n"}
{"snippet": "test1 = [2.3, 2.5];\nsvc.predict([test1])\n", "intent": "Now test some points:\n"}
{"snippet": "gnb.predict([[15,30]])\n", "intent": "Predict some new points\n"}
{"snippet": "dtree_score = cross_val_score(dtree, features, labels, n_jobs=-1).mean()\nprint(\"{0} -> DTree: {1})\".format(columns, dtree_score))\n", "intent": "Estimate the accuracy of our DecisionTreeClassifier\n"}
{"snippet": "alpha = 0.00001\ntheta_gd = torch.rand((X_train.shape[1], 1))\nfor e in range(0, 10000):\n    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n    theta_gd -= alpha * gr\nprint(\"Gradient Descent Theta: \", theta_gd.t())\nprint(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))\n", "intent": "Now let's try using gradient descent:\n"}
{"snippet": "predictions = dtree.predict(test_data[columns].values)\n", "intent": "Try to predict the data using our Decision Tree:\n"}
{"snippet": "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\nY_test = svr.predict(X_test)\npl.plot(X_test,Y_test,c=\"g\", label=\"SVR\")\npl.scatter(X, y, c=\"k\", label=\"data\")\npl.legend()\npl.show()\n", "intent": "Plot the points and the SVR result\n"}
{"snippet": "dmtest = xgb.DMatrix(np.asarray(test)) \npred_prob2 = bst.predict(dmtest)\n", "intent": "Accuracy score : 73.53%\n"}
{"snippet": "cv.cross_val_score(grid.best_estimator_, X, y)\n", "intent": "Here is the performance of the best estimator.\n"}
{"snippet": "learn.sched.plot_loss()\n", "intent": "We can see the loss-spikes at the start of each SGDR cycle:\n"}
{"snippet": "model.predict([np.array([3]), np.array([6])])\n", "intent": "e can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user \n"}
{"snippet": "def get_next(inp):\n    idxs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n    p = model.predict(idxs)\n    return chars[np.argmax(p)]\n", "intent": "With 8 pieces of context instead of 3, we'd expect it to do better; and we see a loss of ~1.8 instead of ~2.0\n"}
{"snippet": "def get_nexts(inp):\n    idxs = [char_indices[c] for c in inp]\n    arrs = [np.array(i)[np.newaxis] for i in idxs]\n    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n    print(list(inp))\n    return [chars[np.argmax(o)] for o in p]\n", "intent": "This is what a sequence model looks like. We pass in a sequence and after every character, it returns a guess.\n"}
{"snippet": "model.evaluate(conv_val_feat, [val_bbox, val_labels])\n", "intent": "*NOTE: a powerful model will crop out these fish and run them through a second model*\n"}
{"snippet": "def mean_absolute_error(y, y_hat): \n    raise NotImplementedError()\n    raise NotImplementedError()\n    raise NotImplementedError()\n    return mae\n", "intent": "$$MAE = \\frac{1}{N} \\sum_{n=1}^N \\left| y_n - \\hat{y}_n \\right|$$\n"}
{"snippet": "preds = model.predict([conv_test_feat, test_sizes], batch_size=batch_size*2)\n", "intent": "---\nAll following code not run:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\n", "intent": "We can look at the earlier prediction examples visualizations by redefiing *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), \n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), \n           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Now we'll run our model again, but with the separate training and validation sets:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=14, scoring='accuracy')\nprint(scores)\n", "intent": "**Now we are going to implement a K-fold cross validation test to get a more generalized accuracy score.**\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom scipy.stats import sem\nscores = cross_val_score(pipeline, twenty_train_small.data,\n                         twenty_train_small.target, cv=3, n_jobs=-1)\nscores.mean(), sem(scores)\n", "intent": "Such a pipeline can then be cross validated or even grid searched:\n"}
{"snippet": "X_new = vectorizer.transform(['This movie is not remarkable, touching, or superb in any way'])\nclf.predict(X_new)\n", "intent": "Yes. Since this review contains mostly positive words and one negative, I would assume that this type of review will be classified incorrectly.\n"}
{"snippet": "labels =list(crf.classes_)\ny_pred = crf.predict(X_valid)\nmetrics.flat_f1_score(y_valid, y_pred,\n                      average='weighted', labels=labels)\n", "intent": "Let's now use the trained model to make predictions. \n"}
{"snippet": "y_pred_test = crf.predict(X_test)\nmetrics.flat_f1_score(y_test, y_pred_test,\n                      average='weighted', labels=labels)\n", "intent": "Let's also make predictions on the test set and see the results. \n"}
{"snippet": "import math\nassert math.isclose(0.33316349496726444, \n                    mean_absolute_error(pd.Series(np.random.RandomState(10).rand(10)), \n                                        pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.39070816989559587, \n                    mean_absolute_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                        pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.2567117528634928, \n                    mean_absolute_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                        pd.Series(np.random.RandomState(500).rand(10))))\n", "intent": "Expected output:\n```\nMAE: 3.2729446379969387\n```\n"}
{"snippet": "import numpy as np\nfrom scipy.optimize import check_grad\nfrom gradient_check import eval_numerical_gradient_array\ndef rel_error(x, y):\n      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n", "intent": "<h2 align=\"center\">BackProp and Optimizers</h2>\n"}
{"snippet": " def softmax_loss(x, y):\n    loss = 0.0\n    dx = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        loss += (np.log(np.exp(x[i]).sum()) - x[i, y[i]]) / x.shape[0]\n        dx[i] = 1.0 / (np.exp(x[i]).sum()) * np.exp(x[i])\n        dx[i, y[i]] -= 1\n    dx /= x.shape[0]\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "pred = X_test.dot(w_optimal)\nroc_auc_score(y_test, pred)\n", "intent": "Now predict output of logistic regression for the test sample and compute AUC\n"}
{"snippet": "test_pred = clf.predict_proba(test_at_all)\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "metrics.f1_score(y_train, clf.predict(X_train), average=\"macro\")\n", "intent": "The over-fitting we saw previously can be quantified by computing the\nf1-score on the training data itself:\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit\ncv = ShuffleSplit(n_splits=5)\ncross_val_score(clf, X, y, cv=cv)\n", "intent": "We can use different splitting strategies, such as random splitting\n"}
{"snippet": "def test_func(x, err=0.5):\n    return np.random.normal(10 - 1. / (x + 0.1), err)\ndef compute_error(x, y, p):\n    yfit = np.polyval(p, x)\n    return np.sqrt(np.mean((y - yfit) ** 2))\n", "intent": "We'll create a dataset like in the example above, and use this to test our\nvalidation scheme.  First we'll define some utility routines:\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc\nYpr = clf.predict_proba(X)\nfpr, tpr, thresholds = roc_curve(Y, Ypr[:,1]) \n", "intent": "[Courbe ROC](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/c_metric/roc.html)\n"}
{"snippet": "pmodel1 = rf.predict_proba(np_test)[:, 1]\npmodel2 = rfv.predict_proba(np_test_v)[:, 1]\n", "intent": "Avec une courbe [ROC](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) par exemple.\n"}
{"snippet": "def mean_squared_error(y, y_hat):\n    raise NotImplementedError()\n    raise NotImplementedError()\n    raise NotImplementedError()\n    return mse\n", "intent": "$$MSE = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$\n"}
{"snippet": "start = time.time()\ntest_data['label_pick'] = k_means.predict(test_data[['pickup_longitude','pickup_latitude']])\ntest_data['label_drop'] = k_means.predict(test_data[['dropoff_longitude','dropoff_latitude']])\ntest_cl = pd.merge(test_data, centroid_pickups, how='left', on=['label_pick'])\ntest_cl = pd.merge(test_cl, centroid_dropoff, how='left', on=['label_drop'])\nend = time.time()\nprint(\"Time Taken by above cell is {}.\".format(end-start))\n", "intent": "4.. Extracting cluster features - \n"}
{"snippet": "test_pred = rf_baseline.predict(X_test_real)\nsimulated_pred_rf = rf_baseline.predict(X_test_simulated)\nsimulated_pred_sample_mean = np.mean(y_train)\n", "intent": "See if the simulated data matter\n"}
{"snippet": "X_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test_counts)\ny_pred = y_pred = clf.predict(X_test_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "def precision(actual, preds):\n    return np.sum(actual * preds == 1) / np.sum(preds == 1)\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    return np.sum(actual * preds == 1) / np.sum(actual == 1)\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    pre = precision(preds, actual)\n    re = recall(preds, actual)\n    return 2 * (pre * re) / (pre + re)\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "preds_tree = tree_mod.predict(X_test) \npreds_rf = rf_mod.predict(X_test)\npreds_ada = ada_mod.predict(X_test)\npreds_reg = reg_mod.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.average((actual - preds) ** 2) \nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "y_pre1 = model1.predict(testing_data)\ny_pre2 = model2.predict(testing_data)\ny_pre3 = model3.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "import math\nassert math.isclose(0.16469788257519086, \n                    mean_squared_error(pd.Series(np.random.RandomState(10).rand(10)), \n                                       pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.22325626250313846, \n                    mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                       pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.13478449093337383, \n                    mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                       pd.Series(np.random.RandomState(500).rand(10))))\n", "intent": "Expected output: \n```\nMSE: 21.8977792176875\n```\n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = clf.predict(test_x)\ntest_y[0:100]\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "report = classification_report(\n    test.Play,\n    test_output,\n    target_names=[\"No Play\", \"Play\"]\n)\nprint(report)\n", "intent": "** Classifier Performance **\n"}
{"snippet": "examples = ['free viagra', \"Hi Bob, how about a game of golf tomorrow?\", \"Come to Sam GU's free lunch!\"]\nexample_counts = vectorizer.transform(examples)\npredictions = classifier.predict(example_counts)\npredictions\n", "intent": "Let's try out couple examples.\n"}
{"snippet": "print ('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint ('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "report = classification_report(\n    test['Species'],\n    test_output,\n    target_names=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n)\nprint(report)\n", "intent": "** Classifier Performance **\n"}
{"snippet": "knn.predict([[5.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0]])\n", "intent": "Let's make a prediction according to the column:\n"}
{"snippet": "x = km.fit_predict(df1)\nx\n", "intent": "Now, export the cluster identifiers to a list. Notice my values are 0 - 3. One value for each cluster.\n"}
{"snippet": "predicted = model2.predict(X_test)\npredicted\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "lm2.predict(50)\n", "intent": "**C. Scikit-learn Prediction**\n"}
{"snippet": "def root_mean_squared_error(y, y_hat): \n    raise NotImplementedError()\n    raise NotImplementedError()\n    return rmse\n", "intent": "$$RMSE = \\sqrt{MSE}$$\n"}
{"snippet": "adam = np.array([1, 0, 29, 0]).reshape(1,-1) \nlogreg.predict_proba(adam)[:, 1]\n", "intent": "Predict probability of survival for **Adam**: first class, no parents or kids, 29 years old, male.\n"}
{"snippet": "bill = np.array([2, 0, 29, 0]).reshape(1,-1)\nlogreg.predict_proba(bill)[:, 1]\n", "intent": "Predict probability of survival for **Bill**: same as Adam, except second class.\n"}
{"snippet": "susan = pd.Series([1, 0, 29, 1]).reshape(1,-1)\nlogreg.predict_proba(susan)[:, 1]\n", "intent": "Predict probability of survival for **Susan**: same as Adam, except female.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "linreg.predict(3)\n", "intent": "**Interpretation:** A 1 unit increase in 'al' is associated with a 0.0025 unit decrease in 'ri'.\n"}
{"snippet": "print metrics.classification_report(y, preds)\n", "intent": "**Exercise** Calculate:\nAccuracy\nSensitivity\nSpecificity\nPrecision by hand\n<br><br><br><br><br><br><br><br><br><br><br><br><br>\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint (confusion_matrix(y_test,predictions))\nprint (classification_report(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "def predict(i):\n    image = mnist.test.images[i]\n    actual_label = np.argmax(mnist.test.labels[i])\n    prediction = tf.argmax(y,1)\n    predicted_label = sess.run(prediction, feed_dict={images: [image]})\n    print (\"Predicted: %d, actual: %d\" % (predicted_label, actual_label))\n    pylab.imshow(mnist.test.images[i].reshape((28,28)), cmap=pylab.cm.gray_r) \n    return predicted_label, actual_label\npredict(5)\n", "intent": "A method to make predictions on a single image\n"}
{"snippet": "import math\nassert math.isclose(0.4058298690032448, \n                    root_mean_squared_error(pd.Series(np.random.RandomState(10).rand(10)), \n                                            pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.4725000132308342, \n                    root_mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                            pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.36713007358887645, \n                    root_mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                            pd.Series(np.random.RandomState(500).rand(10))))\n", "intent": "Expected output:\n```\nRMSE: 4.679506300635516\n```\n"}
{"snippet": "logreg.predict([[5.0]])[0]\n", "intent": "Predicting the state of the battery at 5.0 hours and at 5.2 hours gives:\n"}
{"snippet": "lgrg.predict([[650,20]])[0]\n", "intent": "So if, say, a customer had a credit score of 650 and was requesting a loan of \\$2000, what does our model predict?\n"}
{"snippet": "kumquat = [[1,0,0,0,1,0,0]]\nfnb.predict(kumquat)\n", "intent": "Now that we've created our model, let's see how well we can predict a few other fruit:<br>\nPredict a kumquat?  \n"}
{"snippet": "green_apple = [[1,1,0,1,0,0,0]]\nfnb.predict(green_apple)\n", "intent": "Predict a green apple?  \n"}
{"snippet": "naked_mole_rat = [[4,1,3,0,0,0]] \nanimals.predict(naked_mole_rat)\n", "intent": "Now let's see if it can predict some animals, like the naked mole rat, or the giant tortoise:\n"}
{"snippet": "platypus = [[4,1,1,1,0,1]]\nanimals.predict(platypus)\n", "intent": "Indeed it does!  What about a really weird choice, like a platypus?  They are warm-blooded, semi-aquatic, egg-laying mammals that have fur.\n"}
{"snippet": "kumquat = [[2,0,0]] \nftr.predict(kumquat)\n", "intent": "We can see if this model can accurately predict a kumquat.  (Orange, segmented, small) translates to `[2,0,0]` in this case:\n"}
{"snippet": "metrics.r2_score(train['Data'], train_regr)\n", "intent": "We compute the $R^2$ and MAE for the model on the *training set*:\n"}
{"snippet": "metrics.r2_score(test['Data'], intercept + slope*test.index)\n", "intent": "Then we compare to the measures for the model on the *test set*:\n"}
{"snippet": "y_hat = model.predict(x_test)\ncifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n", "intent": "This may give you some insight into why the network is misclassifying certain objects.\n"}
{"snippet": "metrics.r2_score(train1['Data'], train_regr)\n", "intent": "We compute the $R^2$ and MAE for the model on the *training set*:\n"}
{"snippet": "metrics.r2_score(test1['Data'], intercept + slope*test1.index)\n", "intent": "Then we compare to the measures for the model on the *test set*:\n"}
{"snippet": " metrics.r2_score((x_q,y_q), (x_q,intercept + slope*x_q))\n", "intent": "Inspection of the graph, combined with the very low $R^2$ score\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict_fn = lambda x: rf.predict_proba(encoder.transform(x))\n", "intent": "Note that our predict function first transforms the data into the one-hot representation\n"}
{"snippet": "sklearn.metrics.accuracy_score(labels_test, rf.predict(encoder.transform(test)))\n", "intent": "This classifier has perfect accuracy on the test set!\n"}
{"snippet": "print(neigh.predict([[1.1]]))\nprint(neigh.predict_proba([[0.9]]))\n", "intent": "Predict and Print Results\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "predict = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "print(classification_report(y_test,for_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'],km.labels_))\nprint(classification_report(df['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predict = KNN.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred = reg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y = km.predict(data)\nprint y\n", "intent": "Predict the clusters for each data point\n"}
{"snippet": "MIN_RATING = 0.0\nMAX_RATING = 5.0\nITEMID = 1\nUSERID = 1\nsvd.predict(ITEMID, USERID, MIN_RATING, MAX_RATING)\n", "intent": "Predict rating for a given user and movie, $\\hat{r}_{ui}$\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\ncv = KFold(len(df2),n_folds=5, shuffle=True)\nperf = cross_val_score(lm3, X, y, cv=cv)\nprint perf.mean(), perf.std()\n", "intent": "Try Mean of Cross Validation Score:\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. Can you get something over 85%?\n"}
{"snippet": "score = marvin_model.evaluate(marvin_dataset[\"X_train\"], marvin_dataset[\"y_train\"], verbose=1)\nprint(\"Accuracy is: {} \".format(score[1]))\nmarvin_metrics = {\"Accuracy\": score}\n", "intent": "Accuracy is used as Evaluation metric\n"}
{"snippet": "predicted = marvin_model.predict_classes(input_message)\nacc = marvin_model.predict(input_message)[0][predicted[0]]\nprint(\"The image has the number {} with {} accuracy\".format(predicted, acc))\nfinal_prediction = predicted[0]\n", "intent": "Do prediction and show accuracy.\n"}
{"snippet": "sorted_labels = sorted(\n    labels, \n    key=lambda name: (name[1:], name[0])\n)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "Inspect per-class results in more detail:\n"}
{"snippet": "crf = rs.best_estimator_\ny_pred = crf.predict(X_test)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "As you can see, quality is improved.\n"}
{"snippet": "x_test = np.array(['i like cheese'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", acc)\n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "y_test1 = model.predict(X_test1)\n", "intent": "Predizione sul dataset di test:\n"}
{"snippet": "y_test_5 = model_5.predict(X_test1)\ny_test_5_prob = model_5.predict_proba(X_test1)\nmetrics.roc_auc_score(db_result['FLG_DEF_6M'].tolist(), y_test_5_prob[:,1])\n", "intent": "Verifico la bonta' del modello:\n"}
{"snippet": "note_predictions = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))\n", "intent": "Scikit-learn provides implementation of all methods you need.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred))\n", "intent": "Implement all evaluation methods you have learned in the Scikit-learn tutorial. Decide which model performs best on the given problem.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf_pipeline.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred))\n", "intent": "Implement all evaluation methods you have learned in the Scikit-learn tutorial. Decide which model performs best on the given problem.\n"}
{"snippet": "y_pred = model.predict(X_test_flat)\nprint(y_pred.shape)\n", "intent": "First we need to convert probability vectors to class indices.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint (\"Test accuracy: {:.4f}\".format(accuracy_score(y_test_class, y_pred_class)))\nprint ()\nprint(metrics.classification_report(y_test_class, y_pred_class, digits=4))\n", "intent": "We can use the scikit-learn functions now.\n"}
{"snippet": "from numpy import int32\ny_pred = model.predict(X_test)\ny_pred = (y_pred >= 0.5).astype(int32)\n", "intent": "Predict target values and convert probabilities to binary values.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint (\"Test accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred, digits=4))\n", "intent": "Print evaluation metrics\n"}
{"snippet": "y_pred_cv = clf_cv.predict(Xtestlr)\nprint(accuracy_score(y_pred_cv, ytestlr))\n", "intent": "The GridSearch chose a different $C$ than we have origially chosen. \n"}
{"snippet": "print (np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "pred = rcf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "prediction = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions1 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = sv.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "import sklearn.metrics\ndef try_a_b_c(a,b,c):\n    this_try = [underlying_function(w, a, b, c) for w in wheel.seconds]\n    return sklearn.metrics.mean_squared_error(this_try, wheel.signal)\n", "intent": "We need a function that says how close a match a particular set of\nparameters are: let's use the mean squared error to make sense of it.\n"}
{"snippet": "metrics.silhouette_score(x, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "random_pred = random_for.predict_proba(test[rhs_columns]); random_pred\nfpr, tpr, _ = roc_curve(test['inclusion'], random_pred[:, 1])\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc) \n", "intent": "Print the AUC for Random Forest\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "sklearn.metrics.mean_squared_error(boston.target, lm.predict(X))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "- What types of data points do you think the model is having the most difficult time with?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Accuracy: \", accuracy_score(decision_tree_classifier.predict(test_features), test_labels))\n", "intent": "Let's look at the accuracy of our build model.\n"}
{"snippet": "print(\"Accuracy: \", accuracy_score(svm_classifier.predict(test_features), test_labels))\n", "intent": "Again, let's look at the accuracy of the machine learning model.\n"}
{"snippet": "print(\"Accuracy: \", accuracy_score(gaussian_nb_classifier.predict(test_features), test_labels))\n", "intent": "Again, let's look at the accuracy of the machine learning model.\n"}
{"snippet": "print(\"Accuracy: \", accuracy_rmse(svr_model.predict(test_features), test_labels))\n", "intent": "Let's quickly check the accuracy.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nfinal_scores = []\nfor class_name in class_names:\n    test_target = test_labels[class_name]\n    score = roc_auc_score(test_labels[class_name], predictions[class_name])\n    print('ROC AUC score for class {} is {}'.format(class_name, score))\n    final_scores.append(score)\nprint('Average ROC AUC score is {}'.format(np.mean(final_scores)))\n", "intent": "Finally, let's calculate the average ROC AUC score for the test set:\n"}
{"snippet": "roc_auc_score(y_test, prob)\n", "intent": "600 6 0.8224773007456205\n600 5 std 0.8404221862156795\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "clf.predict_proba([[1.0, 0.3, 1.4, 2.1]])\n", "intent": "If we have a flower that has:\n- Sepal.Length = 1.0\n- Sepal.Width = 0.3\n- Petal.Length = 1.4\n- Petal.Width = 2.1\n"}
{"snippet": "auc = sklearn.metrics.roc_auc_score(y_test, y_pred[:, 1])\nauc\n", "intent": "The first column in y_pred is the P(0), i.e. P(not fraud), and the second column is P(1/fraud).\n"}
{"snippet": "def compute_loss(HL, Y):\n    m = Y.shape[1]\n    loss = -(1./m) * np.sum(np.multiply(Y, np.log(HL)))\n    loss = np.squeeze(loss)      \n    assert(loss.shape == ())\n    return loss\n", "intent": " $Loss = -\\frac{1}{m}\\sum_{i=1}^{m}Y^{(i)}\\log(HL^{(i)})+(1-Y^{(i)})\\log(1-HL^{(i)})$\n"}
{"snippet": "pred_train = predict(train_set_x_new, train_set_y_new, parameters)\n", "intent": "Let's see the accuray we get on the training data.\n"}
{"snippet": "pred_test = predict(test_set_x, test_set_y, parameters)\n", "intent": "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data.\n"}
{"snippet": "predictions = nn_model.predict(test_set_x)\npredictions = np.argmax(predictions, axis = 1)\npredictions\n", "intent": "Now, let's make predictions on the test dataset.\n"}
{"snippet": "X_test_rfe_15 = X_test[col_15]\nX_test_rfe_15 = sm.add_constant(X_test_rfe_15, has_constant='add')\nX_test_rfe_15.info()\ny_pred = lm_15.predict(X_test_rfe_15)\n", "intent": "Note that the model with 15 variables gives about 93.9% r-squared, though that is on training data. The adjusted r-squared is 93.3.\n"}
{"snippet": "y_pred = mlr.predict(X_test)\ny_pred\n", "intent": "From the above result we may infern that if TV price increses by 1 unit it will affect sales by 0.045 units.\n"}
{"snippet": "X_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test_counts)\ny_pred = clf.predict(X_test_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "model_top.evaluate(validation_data, validation_labels)\n", "intent": "Loss and accuracy :\n"}
{"snippet": "y_pred = linreg.predict(X_test)\nfrom sklearn import metrics\nprint (\"MSE:\",metrics.mean_squared_error(y_test, y_pred))\nprint (\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "`PE = 453.694859258 - 1.97887108*AT - 0.23229086*V + 0.0628722*AP - 0.15832456*RH`\n"}
{"snippet": "all_preds_val = np.stack([m.predict(X_val, batch_size=256) for m in ensemble_model])\n", "intent": "Let us now see how the ensemble model does on our validation set\n"}
{"snippet": "predictions = model.predict(input_test, batch_size=1024)\n", "intent": "The above metric is actually calculating how many letters correct, but we want to know how many of the words we spell correctly.\n"}
{"snippet": "p = top_model.predict(arr_lr[0:100])\n", "intent": "Let us now get the first 100 predictions of the model\n"}
{"snippet": "all_preds_val = np.stack([m.predict(valid_features_to_dense, batch_size=256) for m in ensemble_model])\n", "intent": "Checking how the ensemble model does on our validation set ...\n"}
{"snippet": "style_losses = 0\nstyle_wgts = [0.05,0.05,0.2,0.3,0.4]\nfor act_layer, tar_layer, w in zip(output_layers, style_targets, style_wgts):\n    curr_loss = K.mean(style_loss(act_layer[0], tar_layer[0]))\n    style_losses = curr_loss*w + style_losses\n", "intent": "Let us select the output of a single content image conv activations\n"}
{"snippet": "loss = 0\nfor act_layer, tar_layer in zip(mul_conv_layer_outputs, style_targets):\n    curr_loss = K.mean(style_loss(act_layer[0], tar_layer[0]))\n    loss = curr_loss + loss\n", "intent": "Rest of the steps are similar to the content image steps.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Model Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "pred = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. \n"}
{"snippet": "print(type(model.evaluate(x_test, y_test, verbose=0)))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Model metric names: \", model.metrics_names)\nprint(score)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "y_pred = rf.predict(X_test)\nprint(\"ACCURACY ON TEST SET: {0:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"CLASSIFICATION REPORT:\\n\", \n      str(classification_report(y_test, y_pred, digits=3)))\nprint(\"\\n CLASSIFICATION REPORT: \\n\")\nprint(confusion_matrix(y_test, y_pred))\n", "intent": "Getting the prediction on the test set with the DTC and comparing to the truth.\n"}
{"snippet": "y_pred = dtc.predict(X_test)\nprint(\"ACCURACY ON TEST SET: {0:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"CLASSIFICATION REPORT:\\n\", \n      str(classification_report(y_test, y_pred, digits=3)))\n", "intent": "Getting the prediction on the test set with the DTC and comparing to the truth.\n"}
{"snippet": "y_hat = model.predict(X_poly)\nprint(y_hat)\n", "intent": "- Use function() predict of model to predict value for each row data of X_poly\n- Prediction is called y_hat which has the same shape to y\n"}
{"snippet": "print np.mean((bos.PRICE-lm.predict(X))**2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "split = 0.67\nX, XT, Z, ZT = loadDataset(split)\nprint 'Train set: ' + repr(len(X))\nprint 'Test set: ' + repr(len(XT))\nk = 3\nYT = predict(X, Z, XT, k)\naccuracy = getAccuracy(YT, ZT)\nprint('Accuracy: ' + repr(accuracy))\n", "intent": "Should output an accuracy of 0.95999999999999996.\n"}
{"snippet": "predictions = {}\ntest_accuracy = {}\nfor k in models.keys():\n    predictions[k] = [np.argmax(models[k].predict(np.expand_dims(feature, axis=0))) for feature in test[k]]\n    test_accuracy[k] = 100*np.sum(np.array(predictions[k])==np.argmax(test_targets, axis=1))/len(predictions[k])\n    print('%s test accuracy: %.4f%%' % (k, test_accuracy[k]))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is no longer overfitting, so thats better. But we could still improve the end accuracy result.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "y_mult_pred = multiple_linreg.predict(X_mult)\nmetrics.r2_score(y_mult, y_mult_pred)\n", "intent": "Does this model have a better R<sup>2</sup> value?\n"}
{"snippet": "print(\"MAE for fake data:\",metrics.mean_absolute_error(fake_y_true, fake_y_pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors/residuals:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(\"MSE for fake data:\",metrics.mean_squared_error(fake_y_true,fake_y_pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(\"RMSE for fake data:\",np.sqrt(metrics.mean_squared_error(fake_y_true, fake_y_pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "y_pred = linreg.predict(X)\nmetrics.r2_score(y, y_pred)\n", "intent": "Let's confirm the R-squared value for our simple linear model using `scikit-learn's` prebuilt R-squared scorer:\n"}
{"snippet": "y_pred = best_single_tree.predict(X_test)\ny_pred\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each test sample observation?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_test_predict=occupancy_tree.predict(X_occ_test)\naccuracy_score(y_occ_test,y_test_predict)     \n", "intent": "  * Evaluate the model using `accuracy_score` on the testing data.\n  * Is the accuracy score above chance? What is chance accuracy here?\n"}
{"snippet": "print(\"Prediction: \",knn.predict(test))   \n", "intent": "**what id \"should\" the classifier predict?**\nHopefully, you said id 2.\n"}
{"snippet": "print(\"Prediction: \",knn.predict(test))\n", "intent": "**what id \"should\" the classifier predict?**\nHopefully, you said id 2.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"mean_squared_error\"))\nrmse_cross_val_scores = np.sqrt(cross_val_scores)\nprint(\"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores))   \nprint(\"Std 10-fold rmse: \", np.std(rmse_cross_val_scores))   \n", "intent": "And now let's run the whole pipe through the `cross_val_score` object:\n"}
{"snippet": "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"mean_squared_error\"))\nrmse_cross_val_scores = np.sqrt(cross_val_scores)\nprint(\"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores))\nprint(\"Std 10-fold rmse: \", np.std(rmse_cross_val_scores))\n", "intent": "And now let's run the whole pipe through the `cross_val_score` object:\n"}
{"snippet": "predictions = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predictions = random_forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "classification_report(y_test,predictions)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "train_y_pred = \nmse_train = metrics.mean_squared_error(df_train_y, train_y_pred)\nvalid_y_pred = \nmse_valid = metrics.mean_squared_error(df_valid_y, valid_y_pred)\ntest_y_pred = \nmse_test = metrics.mean_squared_error(df_test_y, test_y_pred)\n", "intent": "Fit the model on the selected subset of features and calculate the performance on all datasets.\n"}
{"snippet": "test_y_pred_dt = dt.predict(df_test_X)\nmse_test_dt = metrics.mean_squared_error(df_test_y, test_y_pred_dt)\n", "intent": "Calculate the predictions and performance (MSE) on the test set for *dt*.\n"}
{"snippet": "def predict(models, weights, X):\n    n_data = len(X)\n    T = 0\n    y = np.zeros(n_data)\n    for i, h in enumerate(models):\n        y0 = weights[i] * h.predict(X)  \n        y += y0\n        T += np.sum(y0)\n    y = np.sign(y - T / (n_data*len(models)))\n    return y\n", "intent": "We define a prediction function to help with measuring accuracy:\n"}
{"snippet": "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\nprint('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))\n", "intent": "And the final accuracy:\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "print np.sqrt(metrics.mean_squared_error(y_true, y_pred))\nprint metrics.mean_squared_error(y_true, y_pred) ** 0.5\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "roc_auc_score(college['admit'], model.predict_proba(college[factors])[:,1])\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, scoring='roc_auc', cv=5)\nprint('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))\n", "intent": " http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, target_names = iris.target_names))\n", "intent": "* <b>Classification reports</b> - a text report with important classification metrics (e.g. precision, recall)\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\nfrom sklearn.cross_validation import cross_val_score, KFold\ncv = KFold(len(df),n_folds=5, shuffle=True)\nperf = cross_val_score(lg, X, y, cv=cv, scoring=\"accuracy\")\nprint perf.mean(), perf.std()\n", "intent": "I performed cross-validation on the data with 5 folds and obtained the mean accuracy and its standard deviation.\n"}
{"snippet": "test_predictions = model.predict_proba(X_test).argmax(axis=-1)\ntest_answers = y_test.argmax(axis=-1)\ntest_accuracy = np.mean(test_predictions==test_answers)\nprint(\"\\nTest accuracy: {} %\".format(test_accuracy*100))\nassert test_accuracy>=0.92,\"Logistic regression can do better!\"\nassert test_accuracy>=0.975,\"Your network can do better!\"\nprint(\"Great job!\")\n", "intent": "So far our model is staggeringly inefficient. There is something wring with it. Guess, what?\n"}
{"snippet": "denoising_mse = autoencoder.evaluate(apply_gaussian_noise(X_test),X_test,verbose=0)\nprint(\"Final MSE:\", denoising_mse)\nfor i in range(5):\n    img = X_test[i]\n    visualize(img,encoder,decoder)\n", "intent": "__Note:__ if it hasn't yet converged, increase the number of iterations.\n__Bonus:__ replace gaussian noise with masking random rectangles on image.\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1] \nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr_meta.predict(X_train_level2) \nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr_meta.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(messages['label'], all_predictions))\n", "intent": "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />\n"}
{"snippet": "train_eval_accuracy = estimator.evaluate(input_fn=predict_train_input_fn)\ntest_eval_accuracy  = estimator.evaluate(input_fn=predict_test_input_fn)\nprint(\"Training set accuracy: {accuracy}\".format(**train_eval_accuracy))\nprint(\"Test set accuracy: {accuracy}\".format(**test_eval_accuracy))\n", "intent": "Lets now run the predictions for both training and test sets.\n(Try increasing number of steps and/or num of hidden layers)\n"}
{"snippet": "print(\"Predicting people's names on the test set\")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n", "intent": "Quantitative evaluation of the model quality on the test set\n"}
{"snippet": "test = data[50:60]\npredict = neigh.predict(test) \nactual = list(target[50:60]) \nprint predict\nprint actual\n", "intent": "Prepare Test Data and Predict\n"}
{"snippet": "prediction_input_fn = learn_io.pandas_input_fn(\n    x=my_feature, y=targets, num_epochs=1, shuffle=False)\npredictions = list(linear_regressor.predict(input_fn=prediction_input_fn))\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nprint \"Mean Squared Error (on training data): %0.3f\" % mean_squared_error\nprint \"Root Mean Squared Error (on training data): %0.3f\" % math.sqrt(mean_squared_error)\n", "intent": "Let's make predictions on that training data, to see how well we fit the training data.\n"}
{"snippet": "y_probs_aa = gs_lrap.predict_proba(X_test)\nnew_test_thresh = [1 if x <= 0.10 else 0 for x in y_probs_aa.T[1]]\nprint \"50% thresh confusion matrix\"\nprint(confusion_matrix(y_test ,y_pred_aa))\nprint \"----------\"\nprint \"90% thresh confusion matrix\"\nprint(confusion_matrix(y_test ,new_test_thresh))\n", "intent": "This was bad for my overall model since it overly anticipated false positives and ruined my precision score. \n"}
{"snippet": "y_pred_knn = neighbs.predict(X_test)\nprint(confusion_matrix(y_test ,y_pred_knn))\n", "intent": "Very nearly the same, slightly more are accurately predicted to live. \n"}
{"snippet": "def predict(X, coeffs):\n    threshold = .5\n    bool_t = hypothesis( X, coeffs)>threshold \n    final=[]\n    for item in bool_t:\n        if item ==True:\n            final.append(1)\n        else:\n            final.append(0)\n    return np.array(final)\n", "intent": "Create a predict function which will predict any values strictly greater than 0.5 to be 1 when the hypothesis function is called.\n"}
{"snippet": "confusion_matrix(y_test_cv,multiNB.predict(X_test_cv))\n", "intent": ">  (0,0) = is True Negative \n> (1,0) = False Negative\n> (0,1) = False Positive\n> (1,1) = True Positives\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint('F1 score for Count Vectorizer with Multi Naive Bayes {:.2%}'.format(f1_score(y_test_cv,multiNB.predict(X_test_cv))))\n", "intent": "----\nCalcute the $F_1$ score \n<br>\n<details><summary>\nClick here for the $F_1$ score...\n</summary>\n<br>\n$F_1$ = 80.86%\n</details>\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "Yhat=lm.predict(X)\nYhat[0:5]   \n", "intent": " We can output a prediction \n"}
{"snippet": "Y_hat = lm.predict(Z)\n", "intent": " First lets make a prediction \n"}
{"snippet": "ypipe=pipe.predict(Z)\nypipe[0:4]\n", "intent": " Similarly,  we can normalize the data, perform a transform and produce a prediction  simultaneously\n"}
{"snippet": "Yhat=lm.predict(X)\nYhat[0:4]\n", "intent": "We can predict the output i.e., \"yhat\" using the predict method, where X is the input variable:\n"}
{"snippet": "mean_squared_error(df['price'], Yhat)\n", "intent": " we compare the predicted results with the actual results \n"}
{"snippet": "Y_predict_multifit = lm.predict(Z)\n", "intent": " we produce a prediction \n"}
{"snippet": "mean_squared_error(df['price'], Y_predict_multifit)\n", "intent": " we compare the predicted results with the actual results \n"}
{"snippet": "r_squared = r2_score(y, p(x))\nr_squared\n", "intent": "We apply the function to get the value of r^2\n"}
{"snippet": "mean_squared_error(df['price'], p(x))\n", "intent": " We can also calculate the MSE:  \n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "-1*cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n", "intent": " We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. \n"}
{"snippet": "yhat_train=lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]\n", "intent": "Prediction using training data:\n"}
{"snippet": "yhat_test=lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]\n", "intent": " Prediction using test data: \n"}
{"snippet": "yhat=poly.predict(x_test_pr )\nyhat[0:5]\n", "intent": " We can see the output of our model using the method  \"predict.\" then assign the values to \"yhat\".\n"}
{"snippet": "yhat=RigeModel.predict(x_test_pr)\n", "intent": " Similarly, you can obtain a prediction: \n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "The result is better than random guessing (0.1), but hopefully a CNN will do much better.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\naccuracy = 100*score[1]\nprint('Test accuracy: %.4f%%' % accuracy)\n", "intent": "We do not expect to perform better than random chance, which in this case is 10%.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100 * np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1)) / len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "predictions = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "pred_train = lreg.predict(X_train)\npred_test = lreg.predict(X_test)\n", "intent": "Now run a prediction on both the X training set and the testing set.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n0.692708333333\n", "intent": "Classification accuracy: percentage of correct predictions`\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmodel_output_0 = []\nmodel_output_1 = []\nfor row in input_data:\n    model_output_0.append(predict_with_network(row, weights_0))\n    model_output_1.append(predict_with_network(row, weights_1))\nmse_0 = mean_squared_error(model_output_0, target_actuals)\nmse_1 = mean_squared_error(model_output_1, target_actuals)\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)\n", "intent": "First we need to implement a loss function to measure the process at which we learn to approximate the real distribution. \n"}
{"snippet": "from sklearn.exceptions import NotFittedError\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))\n", "intent": "<br>\n**Finally, run this code to check that the models have been fitted correctly.**\n"}
{"snippet": "pred = fitted_models['rf'].predict(X_test)\n", "intent": "Predict the test set using the fitted random forest.\n"}
{"snippet": "print(r2_score(y_test, pred))\nprint(mean_absolute_error(y_test, pred))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "print(metrics.accuracy_score(y_test, predicted))\n", "intent": "<p>As we can see, the classifier is predicting a 1 (having an affair) any time the probability in the second column is greater than 0.5.</p>\n"}
{"snippet": "note_predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "y_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Now let's predict using the trained model.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint confusion_matrix(df['Cluster'], kmeans.labels_)\nprint \nprint classification_report(df['Cluster'], kmeans.labels_)\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = clf.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Agora preveja valores para os dados de teste. **\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Agora vamos prever o uso do modelo treinado.\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Vamos prever os valores do y_test e avaliar o nosso modelo.\n** Preveja a classe de not.fully.paid para os dados X_test. **\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Then you can re-run predictions on this grid object just like you would with a normal model.\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`**\n"}
{"snippet": "print(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n", "intent": "**9.3 Evaluate DBSCAN visually, with silhouette, and with the metrics against the true `y`.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "***\nFurther useful checks are the **classification report** and the **confusion matrix**,  \nthey give detailed Info on mis-classifications:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ytest, ypred))\n", "intent": "***\n**Detailed classification report**\n"}
{"snippet": "predictions = classifier.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = classifier.predict(X_test)\npredictions\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nypred = m.predict(Xtrain)\nconfusion_matrix(ytrain, ypred)\n", "intent": "With a *skewed dataset*, a confusion matrix is more robust:\n"}
{"snippet": "leo = np.array([[22, 3, 0]])\nkate = np.array([[25, 1, 1]])\nprint(m.predict(leo))\nprint(m.predict(kate))\n", "intent": "Create a data set for additional passengers and predict whether they will survive:\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake)\n    g_loss = ls_generator_loss(score_fake)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "scores = cross_val_score(logreg, features_array, target, cv=5,\n                         scoring='roc_auc')\nscores.min(), scores.mean(), scores.max()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "x_val, y_val = prepare_to_test(X, profiles, Y)\nlogging.info(\"- Predict using trained model\")\ny_pred = estimator.predict(x_val, verbose=0)    \ny_pred = pred_to_targets(y_pred)\nlogging.info(\"- Compute map7 score\")\nprint map7_score2(y_val, y_pred, clients_last_choice[TARGET_LABELS].values)\n", "intent": "Check score on the data 2016/05\n"}
{"snippet": "x_val, y_val = prepare_to_test(X, Y)\ny_pred = estimator.predict(x_val, verbose=0)\nprint \"Score on the whole dataset : \", map7_score2(y_val, y_pred, clients_last_choice[TARGET_LABELS].values)\n", "intent": "Check score on the same data\n"}
{"snippet": "logging.info(\"- Compute map7 score\")\nprint map7_score(y_val, y_preds, clients_last_choice[LC_TARGET_LABELS].values)\nlogging.info(\"- Compute max map7 score\")\nprint map7_score(y_val, y_val, clients_last_choice[LC_TARGET_LABELS].values)\n", "intent": "Check score on the data 2016/05\n"}
{"snippet": "y_val = targets_str_to_indices(Y[target_labels].values)\nlogging.info(\"- Compute map7 score\")\nprint map7_score(y_val, y_preds, Y[LAST_TARGET_LABELS].values)\nlogging.info(\"- Compute max map7 score\")\nprint map7_score(y_val, y_val, Y[LAST_TARGET_LABELS].values)\n", "intent": "Check score on the data 2016/05\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncvscores = cross_val_score(model, X, y, cv = 5, n_jobs=-1)\nprint \"CV score: {:.3} +/- {:.3}\".format(cvscores.mean(), cvscores.std())\n", "intent": "As usual we can calculate the cross validated score to judge the quality of the model.\n"}
{"snippet": "metrics.silhouette_score(dn1, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "metrics.accuracy_score(predY,y)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "metrics.silhouette_score(y,predY,metric=\"euclidean\")\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(\n            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_valid,y_pred)\n", "intent": "Check Confusion matrix\n"}
{"snippet": "y_hat = knn.predict(wine_df)\n(y_hat == y).mean()\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, Xs, y, cv=10)\nprint lasso_scores\nprint np.mean(lasso_scores)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nenet_scores = cross_val_score(enet, Xs, y, cv=10)\nprint enet_scores\nprint np.mean(enet_scores)\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "y_pred_dtree = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test, y_pred_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred= logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "**Classification report**\n"}
{"snippet": "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\nD_train_step = D_solver.minimize(D_loss, var_list=D_vars)\nG_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n", "intent": "Create new training steps so we instead minimize the LSGAN loss:\n"}
{"snippet": "pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "pred = clf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print classification_report(y_test, pred)\nprint\nprint confusion_matrix(y_test, pred)\nfp = confusion_matrix(y_test, pred)[0][1]\nprint \nprint fp\nexperiment.log_metric(\"confusion matrix\", fp)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy_resnet50 = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", (mean_squared_error(ys, predictions)**0.5)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "df['probability'] = model.predict_proba(df[features]).T[1]\n", "intent": "```model.predict_proba``` will give you the probability of an outcome, instead of just the outcome:\n"}
{"snippet": "silhouette_avg = silhouette_score(X1, labels)\nprint(\"The average silhouette_score is :\", silhouette_avg)\n", "intent": "Compute the Silhoutte Score, AMI and inertia to measure your analysis\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "metrics.silhouette_score(X_scaled, labels, metric='euclidean') \n", "intent": "And to compute the clusters' silhouette coefficient:\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, roc_auc_score, auc\nfrom sklearn.metrics import log_loss, hinge_loss, hamming_loss\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, precision_recall_curve\npred = logreg.predict(X_test)\nprint(classification_report(y_test, pred))\n", "intent": "<b>Reviewing classification scores to understand model fit</b>\n"}
{"snippet": "tree_pred=tree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rfc_pred=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(confusion_matrix(y_test,rfc_pred))\nprint('\\n')\nprint(classification_report(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "knn_pred=KNN.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "log_pred=logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "svm_pred=svm_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "labels = pipeline.predict(samples)\n", "intent": "**Step 4:** Obtain the cluster labels for `samples` by using the `.predict()` method of `pipeline`, assigning the result to `labels`.\n"}
{"snippet": "labels = pipeline.predict(movements)\n", "intent": "**Step 3:** Use the `.predict()` method of the pipeline to predict the labels for `movements`.\n"}
{"snippet": "accuracy_score(clf.predict(Xtestlr), ytestlr)\n", "intent": "Using grid search cross validation provided a different optimal value for C with a higher accuracy score than the previous method.\n"}
{"snippet": "y_pred = reg.predict(X_test)\n", "intent": "Predicting values on X_test\n"}
{"snippet": "print(r2_score(y_test, y_pred)*200)\n", "intent": "Checking accuracy of model\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "Predicting values on X_test\n"}
{"snippet": "skfold = StratifiedKFold(n_splits=5)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=skfold)\nscores, scores.mean()\n", "intent": "RepeatedStratifiedKFold\n"}
{"snippet": "predictions = list(classifier.predict(input_fn=pred_fn))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "note_predictions = list(classifier.predict(input_fn=pred_fn))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\ny_test = college_data['Cluster']\npredictions = kmeans.labels_\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = regressor.predict(X_test)  \n", "intent": "fit => our regressor has learnt the correlations of the training sets variables and has learnt to predict salary based on experience.\n"}
{"snippet": "gan_loss = tfgan.gan_loss(\n    conditional_gan_model, gradient_penalty_weight=1.0)\nevaluate_tfgan_loss(gan_loss)\n", "intent": "<a id='conditional_loss'></a>\n"}
{"snippet": "infogan_loss = tfgan.gan_loss(\n    infogan_model,\n    gradient_penalty_weight=1.0,\n    mutual_information_penalty_weight=1.0)\nevaluate_tfgan_loss(infogan_loss)\n", "intent": "<a id='infogan_loss'></a>\nThe loss will be the same as before, with the addition of the mutual information loss.\n"}
{"snippet": "crossed_lat_lon_fc = tf.feature_column.crossed_column(\n    [latitude_bucket_fc, longitude_bucket_fc], int(5e3))\nfc = [crossed_lat_lon_fc]\nest = tf.estimator.LinearRegressor(fc, model_dir=os.path.join(logdir, 'crossed'))\nest.train(in_fn(train_ds), steps = 5000)\nest.evaluate(in_fn(test_ds))\n", "intent": "The single-cell \"holes\" in the figure are caused by cells which do not contain examples.\n"}
{"snippet": "fc = [\n    latitude_bucket_fc,\n    longitude_bucket_fc,\n    crossed_lat_lon_fc]\nest = tf.estimator.LinearRegressor(fc, model_dir=os.path.join(logdir, 'both'))\nest.train(in_fn(train_ds), steps = 5000)\nest.evaluate(in_fn(test_ds))\n", "intent": "The model generalizes better if it also has access to the raw categories, outside of the cross. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\ncv = KFold(n_splits=5, random_state=42)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=cv)\nprint(\"Test  r2:%.2f\" % scores.mean())\n", "intent": "Scikit-learn provides user-friendly function to perform CV:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nscores.mean()\ndef balanced_acc(estimator, X, y, **kwargs):\n    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5, scoring=balanced_acc)\nprint(\"Test  ACC:%.2f\" % scores.mean())\n", "intent": "Scikit-learn provides user-friendly function to perform CV:\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "mean_absolute_error(xgb_test_fold, mlp_test_fold)\n", "intent": "And testing that these labels completely match (there's still a little rounding error due to log-exp conversion):\n"}
{"snippet": "def compute_score(clf, X, y,scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5,scoring=scoring)\n    return np.mean(xval)\n", "intent": "To evaluate our model we'll be using 5-fold cross validation with the Accuracy metric.\nTo do that, we'll define a small scoring function. \n"}
{"snippet": "class_predict = log_model2.predict(X_test)\nprint metrics.accuracy_score(Y_test,class_predict)\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "from sklearn import metrics\npredicted = model.predict(X_test)\nexpected = Y_test\nprint metrics.accuracy_score(expected,predicted)\n", "intent": "Now we'll go ahead and see how well our model did!\n"}
{"snippet": "predicted = model.predict(X_test)\nexpected = Y_test\n", "intent": "Now we predict the outcomes from the Testing Set:\n"}
{"snippet": "print metrics.accuracy_score(expected, predicted)\n", "intent": "Finally we can see the metrics for performance:\n"}
{"snippet": "print(svm.predict(X_train))\nprint(y_train)\n", "intent": "3) Apply / evaluate\n"}
{"snippet": "print(\"Loss: \", model.evaluate(x_test, y_test, verbose=0))\n", "intent": "Now that we have trained our model we can evaluate it on our testing set. It is also just one line of code.\n"}
{"snippet": "cifar_model.evaluate(cifar_x_test, y_test_cat)\n", "intent": "Now we can evaluate on our test set.\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "y_pred = imdb_model.predict(x_test_proc)\n", "intent": "Now we can look at our predictions and the sentences they correspond to.\n"}
{"snippet": "y_model = model.predict(...)\n", "intent": "Now let's use the model.predict method to see what values our model predict for the data. We'll use the X test data we set aside a few cells before. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model) \n", "intent": "Great, now lets use y_model and compare it to the test data we set aside and see how our model performed!\n"}
{"snippet": "validation_prediction_delta_lad = (model_delta_lad.predict(feature_numpy_validation) + data_training_validation['gap_t(j-1)'].to_numpy().astype(float)).tolist()\n", "intent": "* Predict gap_delta\n"}
{"snippet": "def seq2seq_loss(input, target):\n    sl,bs = target.size()\n    sl_in,bs_in,nc = input.size()\n    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n    input = input[:sl]\n    return F.cross_entropy(input.view(-1,nc), target.view(-1))\n", "intent": "The loss function used for this model is standard cross entropy\n"}
{"snippet": "pred = kmodel.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "metrics.mean_squared_error(y, model1.predict(X))\n", "intent": "Compare to the MSE for the model estimated on the entire dataset.\n"}
{"snippet": "metrics.confusion_matrix(y, gs.best_estimator_.predict(X))\n", "intent": "Confusion matrix of 'best' model:\n"}
{"snippet": "metrics.accuracy_score(y, model1.predict(X))\n", "intent": "Compute classification accuracy.\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(feature_set)\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "pred_probs = model1.predict_proba(X)[:,1]\n", "intent": "Compute predicted probabilities for GBM (`y` = 1).\n"}
{"snippet": "def specificity_score(y_true, y_pred):\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    return cm[0,0] / cm[0,:].sum()\n", "intent": "Define a function to compute specificity.\n"}
{"snippet": "sensitivities = np.zeros(cutoffs.size)\nspecificities = np.zeros(cutoffs.size)\nfor i, cutoff in enumerate(cutoffs):\n    sensitivities[i] = metrics.recall_score(y, pred_probs >= cutoff)\n    specificities[i] = specificity_score(y, pred_probs >= cutoff)\n", "intent": "Compute sensitivity and specificity at the cut-off values defined above.\n"}
{"snippet": "metrics.roc_auc_score(y, pred_probs)\n", "intent": "Compute area under the ROC curve (AUC).\n"}
{"snippet": "metrics.confusion_matrix(y, model2.predict(X))\n", "intent": "Compute confusion matrix for the 'best' model.\n"}
{"snippet": "labels = kmeans.fit_predict(crypto_close.transpose())\n", "intent": "Fit the model and retrieve cluster assignments.\n"}
{"snippet": "mses = ms.cross_val_score(spls, boroughs, feelings, scoring='neg_mean_squared_error', cv=three_fold_cv)\nnp.mean(-mses)\n", "intent": "Compute average MSE across folds.\n"}
{"snippet": "aucs = ms.cross_val_score(rf1, X, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Compute average AUC across folds.\n"}
{"snippet": "aucs = ms.cross_val_score(rf2, X_tfidf, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Compute average AUC across folds.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "metrics.f1_score(y_valid, lr_gs.best_estimator_.predict(X_valid))\n", "intent": "Check $F_{1}$ score and confusion matrix on the validation set.\n"}
{"snippet": "aucs = ms.cross_val_score(ssvc, X, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Compute average AUC across folds.\n"}
{"snippet": "pred_probs = nn.predict(X_scaled)[:,0]\n", "intent": "Use network to predict probabilities.\n"}
{"snippet": "-ms.cross_val_score(ssvc, X, y, scoring='neg_mean_squared_error', cv=split)\n", "intent": "Compute MSE for split.\n"}
{"snippet": "hires['Prediction'] = gs.best_estimator_.predict(X)\nhires['Hires'].plot()\nhires.loc[split.test_fold >= 0, 'Prediction'].plot(xlim=('2016-01-01', '2017-12-31'))\n", "intent": "Plot original time series and prediction from January 2016 onwards.\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint (\"RMSE:\", mean_squared_error(ys, predictions))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint (\"RMSE:\", mean_squared_error(ys, predictions))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math\nprint (\"RMSE:\", math.sqrt(mean_squared_error(ys, predictions)))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint(r2)\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "print(accuracy_score(y_test, predictions))\nprint(precision_score(y_test, predictions))\nprint(recall_score(y_test, predictions))\nplot_roc(y_test, predictions)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "X2 = [[5.4,  3.9,  1.7,  0.4], [6.3,  3.3,  6. ,  2.5]]\nknn.predict(X2)\n", "intent": "> __Q:__ *why does the X1 test data above have two brackets?*\n  _hint:_ remember the shape of our input data\nLet's try another test data\n"}
{"snippet": "y_pred = logreg.predict(X_test)\n", "intent": "Now that we have trained the model with the training data, let's see the prediction results from the test data\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred,\n                            target_names=['No', 'Yes']))\n", "intent": "True Negative = 1486\nTrue Positive = 325\nFalse Positive = 29\nFalse Negative = 23\n"}
{"snippet": "mse_scaled = metrics.mean_squared_error(usa_house_fitted_inv['Price'], \n                           usa_house_fitted_inv['Price predicted'])\nnumpy.sqrt(mse_scaled)\n", "intent": "RMSE root mean squared error. This is useful as it tell you in terms of the dependent variable, what the mean error in prediction is.\n"}
{"snippet": "y_train_predict = dtr.predict(X_train)\n", "intent": "Predict the training set.\n"}
{"snippet": "y_test_predict = dtr.predict(X_test)\n", "intent": "Predict the test set.\n"}
{"snippet": "r2_score(y_train, y_train_predict)\n", "intent": "Calculate the r2 score for the test set.\n"}
{"snippet": "print(svc.predict([[200000, 40]]))\n", "intent": "Or just use predict for a given point:\n"}
{"snippet": "y_test_pred = clf.predict(X_test)\n", "intent": "Predict the test set.\n"}
{"snippet": "r2_score(y_train, y_train_pred, multioutput='variance_weighted')\n", "intent": "Calculate the r2 score for the training set.\n"}
{"snippet": "r2_score(y_test, y_test_pred, multioutput='variance_weighted')\n", "intent": "Calculate the r2 score for the test set.\n"}
{"snippet": "from sklearn import metrics\ny_test = kmeans.predict(X_test)\nsilhouette_score = metrics.silhouette_score(X_test, y_test, metric=\"euclidean\")\nprint(silhouette_score)\n", "intent": "e.g., silhouette coefficient\n"}
{"snippet": "score = Resnet_model.evaluate(test_Resnet,test_targets,verbose=0)\nprint(\"Test Accuracy: %.3f%%\" %score[1])\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = classifier.predict(X_test)  \nprint(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\nprint(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))\n", "intent": "prediction on the test set with the accuracy and the confusion matrix\n"}
{"snippet": "print(\"test mean squared error: {:.3f}\".format(\n        ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test})))\n", "intent": "Evaluate predictions from the posterior predictive.\n"}
{"snippet": "y_pred = regressor.predict(X)\ny_pred[:10]\n", "intent": "As you can see the predicted value is not very far away from the actual value.\nNow let's try to predict the price for all the houses in the dataset.\n"}
{"snippet": "cross_val_score(classifier, X, y, cv=5)\n", "intent": "As you can see, the function uses three folds by default. You can change the number of folds using the cv argument:\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(np.array(trainY), p4(np.array(trainX)))\nprint(r2)\n", "intent": "...even though it fits the training data better:\n"}
{"snippet": "grid.predict(X)\n", "intent": "Then, as with all models, we can use ``predict`` or ``score``:\n"}
{"snippet": "xception_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import cross_validation\nX_train = X[:X.shape[0]]\ny_train = y[:X.shape[0]]\ny_pred = cross_validation.cross_val_predict(logres, X_train, y_train,cv=10)\nprint(metrics.classification_report(y_train, y_pred))\n", "intent": "Another method of evaluting this model is to use <b>cross validation</b>.\n"}
{"snippet": "y_predicted = text_clf.predict(X_test)\n", "intent": "Get the predicted classes for X_test\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test, y_predicted)\n", "intent": "Calculate accuracy of class predictions\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predicted, digits=4))\n", "intent": "Print the classification report\n"}
{"snippet": "documents_to_predict = [\"Home Runs Game\", \"Car Engine Noises\", \"Computer GIFs\"]\npredictions = text_clf.predict(documents_to_predict)\n", "intent": "Document contents to list form\n"}
{"snippet": "metric.accuracy_score(y_true, y_pred, normalize=False)\n", "intent": "The absolute accuracy is measured as follow.\n"}
{"snippet": "def train(self, training_data, epochs, mini_batch_size, learning_rate):\n    training_data = list(training_data)\n    for i in range(epochs):\n        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate), mini_batches))\n        acc = self.evaluate(training_data)\n        print(\"Epoch {} complete. Total cost: {}, Accuracy: {}\".format(i, cost, acc))\n", "intent": "We shall implement backpropagation with stochastic mini-batch gradient descent to optimize our network. \n"}
{"snippet": "from IPython.display import HTML\ndef classify(img):\n    img = img[len('data:image/png;base64,'):].decode('base64')\n    img = cv2.imdecode(np.fromstring(img, np.uint8), -1)\n    img = cv2.resize(img[:,:,3], (28,28))\n    img = img.astype(np.float32).reshape((1, 784))/255.0\n    return model.predict(img)[0].argmax()\nHTML(html+script)\n", "intent": "Now, try if your model recognizes your own hand writing.\nWrite a digit from 0 to 9 in the box below. Try to put your digit in the middle of the box.\n"}
{"snippet": "Res50_predictions = [np.argmax(Res50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Res50]\ntest_accuracy = 100*np.sum(np.array(Res50_predictions)==np.argmax(test_targets, axis=1))/len(Res50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "outer_cv = 5\ninner_cv = 2\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=inner_cv)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=outer_cv)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "How to code?\nUse grid search instead of classifier/pipeline with cross_val_score()\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n", "intent": "Previously we have been using accuracy as the score for optimization\n* e.g. grid search\nWe can easily plug in other performance metrics.\n"}
{"snippet": "from scipy.misc import comb\nimport math\nimport numpy as np\ndef ensemble_error(num_classifier, base_error):\n    k_start = math.ceil(num_classifier/2)\n    probs = [comb(num_classifier, k)*(base_error**k)*((1-base_error)**(num_classifier-k)) for k in range(k_start, num_classifier+1)]\n    return sum(probs)\n", "intent": "We have used the following code to compute and plot the ensemble error from individual classifiers for binary classification:\n"}
{"snippet": "y_lin_pred = lr.predict(X)\ny_quad_pred = pr.predict(X_quad)\n", "intent": "Quadratic polynomial fits this dataset better than linear polynomial\nNot always a good idea to use higher degree functions\n* cost\n* overfit\n"}
{"snippet": "y_train_pred = nn.predict(X_train)\nif sys.version_info < (3, 0):\n    acc = ((np.sum(y_train == y_train_pred, axis=0)).astype('float') /\n           X_train.shape[0])\nelse:\n    acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\nprint('Training accuracy: %.2f%%' % (acc * 100))\n", "intent": "The process converged around 800 epochs.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(knn_pipeline, x_train, y_train, cv=3)\n", "intent": "As promised, cross validation tools work directly with the pipeline object.\n"}
{"snippet": "meta_test_predictions = np.mean(predictions_test,axis=1) >.5 \nprint(\"Test accuracy (Classify by majority vote): \", accuracy_score(y_test, meta_test_predictions))\n", "intent": "Perhaps the simplest way of combining the models predictions (above) is a majority vote. Let's compute the test score under this rule\n"}
{"snippet": "tuning_scores = [x.score(x_tune,y_tune) for x in models]\nweights = np.array(tuning_scores)/np.sum(tuning_scores)\nprint(\"First five weights:\", weights[0:5])\nweighted_predictions = np.dot(predictions_test, weights) > .5\naccuracy_score(y_test,weighted_predictions)\n", "intent": "**Speaker note**: point out that we've got the old one-tuning-set problem, and CV can assist\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "linreg_test_prediction = X_test_level2[:, 0]\nlbg_test_prediction = X_test_level2[:, 1]\ntest_preds = best_alpha * linreg_test_prediction + (1 - best_alpha) * lbg_test_prediction \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=skf_seed)\n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, y=Y, cv=skf)\n    np.save(data_path.joinpath(f'knn_feats_{metric}_train.npy'), preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "y_pred = add_noise(clf.predict_proba(test_ada[variables])[:, 1])\n", "intent": "Compute prediction and add noise\n"}
{"snippet": "pred = km.predict(iris_test.iloc[:, :4])\ncenters = np.array([list(km.cluster_centers_[lab]) for lab in pred])\ndistances = np.sqrt(np.sum((iris_test.iloc[:, :4] - centers)**2, axis=1))\nis_out = [distances[i] > percentiles[lab] for i, lab in enumerate(pred)]\n", "intent": "3 - Use these percentiles to determine outliers in the test data and create a 3d plot using the first three attributes and highlight outliers\n"}
{"snippet": "print(confusion_matrix(y_true=ytest, y_pred=ypred))\nprint('TPR', recall_score(ytest, ypred))\nprint('PRE', precision_score(y_true=ytest, y_pred=ypred))\nprint('F1', f1_score(y_true=ytest, y_pred=ypred))\nprint('ACC', accuracy_score(y_true=ytest, y_pred=ypred))\n", "intent": "5 - Check your results in part (4) using `sklearn`.\n"}
{"snippet": "ypredprob = pipe.predict_proba(Xtest)\n", "intent": "6 - Plot the precision and recall curve for your fit.\n"}
{"snippet": "r2_score(y_1var, y_1var_pred)\n", "intent": "The slope of the equation represents the rate of change of `r` with respect to `double`.\n"}
{"snippet": "score = SegModel.evaluate(valX, valY, verbose=0)\nprint('Final Dice on validation set: {:.04f}'.format(1-score))\n", "intent": "After the training is complete, we evaluate the model again on our validation data to see the results.\n"}
{"snippet": "score = RegModel.evaluate(testX, testY, verbose=0)\nprint('Final loss on test set: {:.03e}'.format(score))\n", "intent": "After the training is complete, we evaluate the model on our test data to see the results.\n"}
{"snippet": "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "labels = kmeans.predict(X)\n", "intent": "Encontra os clusters para cada motivo de compra.\n"}
{"snippet": "y_pred = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "y_pred = rfmodel.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(data['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(data['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = logreg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = svclassifier.predict(X_test)  \n", "intent": "**Question 3.8:** Use the classifier to predict the outcome of our `X_test`. \n"}
{"snippet": "from theano import tensor as T\nraise NotImplementedError(\"TODO: add any other imports you need\")\ndef evaluate(x, y, expr, x_value, y_value):\n    raise NotImplementedError(\"TODO: implement this function.\")\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint(\"SUCCESS!\")\n", "intent": "This exercise requires you to compile a Theano function and call it to execute `\"x + y\"`.\n"}
{"snippet": "ypred = lm_fit.predict(xsample)\nTSS = np.sum((ysample - np.mean(ysample))**2)\nRSS = np.sum((ysample - ypred)**2)\nRsq_sample = (TSS - RSS)/TSS\nRsq_sample\n", "intent": "First let's compute the Rsquared for the last sample:\n"}
{"snippet": "X_pred = np.arange(0,100).reshape(-1,1)\nyhat = neigh.predict(X_pred)\n", "intent": "Let's create a vector of predictions:\n"}
{"snippet": "def print_rmse(model, name, df):\n  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n  print ('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, 'validation', df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\ndef evaluate_forecasts(test, forecasts, n_seq):\n    for i in range(n_seq):\n        actual = [row[i] for row in test]\n        predicted = [forecast[i] for forecast in forecasts]\n        rmse = sqrt(mean_squared_error(actual, predicted))\n        print('t+%d RMSE: %f' % ((i + 1), rmse))\nevaluate_forecasts(actual, forecasts, 30)\n", "intent": "Evaluate the RMSE for each forecast time step\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\ndef evaluate_forecasts(test, forecasts, n_lag, n_seq):\n    for i in range(n_seq):\n        actual = [row[i] for row in test]\n        predicted = [forecast[i] for forecast in forecasts]\n        rmse = sqrt(mean_squared_error(actual, predicted))\n        print('t+%d RMSE: %f' % ((i + 1), rmse))\nevaluate_forecasts(actual, forecasts, 1, 30)\n", "intent": "Evaluate the RMSE for each forecast time step\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy Xception: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_y = list(classifier.predict(input_fn=pred_fn))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predicted_y = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predicted_y = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "precision_score(y_true, y_pred, average=None)\n", "intent": "Precision, recall, and F1-scores for each class (without averaging)\n"}
{"snippet": "predicted_y = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predicted_Y = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predicted_y = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predicted_y = Pipeline_model.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predicted_y = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "bikes.loc[:,'predictions'] = lr_all.predict(X)\n", "intent": "- Store `lr_all`'s fitted values in a new `predictions` column of the `bikes` DataFrame.\n"}
{"snippet": "bikes.loc[:, 'predictions'] = lr_all.predict(X)\n", "intent": "- Store `lr_all`'s fitted values in a new `predictions` column of the `bikes` DataFrame.\n"}
{"snippet": "AdB_scores = cross_val_score(AdB, X, Y)\nAdB_scores.mean()\n", "intent": "Now compare CV scores\n"}
{"snippet": "tree_scores = cross_val_score(clf, X, Y)\ntree_scores.mean()\n", "intent": "What about a plain tree?\n"}
{"snippet": "precision_score(y_true, y_pred, average='micro')\n", "intent": "Micro-averaged precision, recall, and F1\n"}
{"snippet": "predicitons = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predict = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint('MSE - OLS train: %.3f, test: %.3f' % (\n        mean_squared_error(ytrain, ytrain_pred), \n        mean_squared_error(ytest, ytest_pred)))\nprint('R^2 0LS train: %.3f, test: %.3f' % (\n        r2_score(ytrain, ytrain_pred),\n        r2_score(ytest, ytest_pred)))\n", "intent": "Now since we have already trained and tested our model , we need to evaluate using MSE and R^2\n"}
{"snippet": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\ndef rsquare_meansquare_error(train_y, test_y, train_X, test_X, test, best_model):\n    print ('MSE ' + test + ' train data: %.2f, test data: %.2f' % (\n        mean_squared_error(train_y, y_train_pred),\n        mean_squared_error(test_y, y_test_pred)))\n", "intent": "You need to know if your model performed well on the test data. \nEvaluation using MSE and R^2\n"}
{"snippet": "rsquare_meansquare_error(y_train, y_test, X_train, X_test, \"Random Forest Regression tree\", rfr_best)\n", "intent": "You need to know if your model performed well on the test data. \nEvaluation using MSE and R^2.\nWe already created the function above\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(Universities['Cluster'],kmeans.labels_))\nprint(classification_report(Universities['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "Prediction=Logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "metrics.accuracy_score(df['target'], df['labels'])\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "precision_score(y_true, y_pred, average='macro')\n", "intent": "Macro-averaged precision, recall, and F1\n"}
{"snippet": "print metrics.classification_report(df['target'], df['labels'])\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "logodds = logreg.predict_proba(2)[:, 1] + 4.1804038614510901\nodds = np.exp(logodds)\nprob = odds/(1 + odds)\nprob\n", "intent": "**Interpretation:** A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'assorted'.\n"}
{"snippet": "random_pred = forest.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, random_pred[:, 1])\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc) \n", "intent": "Print the AUC for Random Forest\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "metrics.silhouette_score(new_X, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr_boh.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr_boh.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "pred_lr = lr.predict(test)\npred_lgb = model_lgb.predict(test)\nX_test_level2 = np.c_[pred_lr, pred_lgb]\n", "intent": "Now I predict on the test set & concatenate test predictions to get test meta-features.\n"}
{"snippet": "pred_lr = lr.predict(X_test.values)\npred_lgb = model_lgb.predict(X_test)\nX_test_level2 = np.c_[pred_lr, pred_lgb] \n", "intent": "Now I predict on the test set & concatenate test predictions to get test meta-features.\n"}
{"snippet": "y_est = model.predict(x_test)\nmse = np.mean(np.square(y_test_true-y_est))\nprint(mse)\n", "intent": "This model is slightly better than without any penalty on the weights.\n"}
{"snippet": "model.predict([\n        [+10.0, -10.0],   \n        [-10.0, +10.0],   \n])\n", "intent": "Which class is \"1\"?  Yellow or Magenta?  Let's classify some extreme points to find out:\n"}
{"snippet": "pointsRDD.map(lambda pt: (model.predict(pt.features), pt.label)).take(10)\n", "intent": "Let's compare the predictions to the real values for the first few points:\n"}
{"snippet": "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred) \nprint(loss)\n", "intent": "Define the loss function.\n"}
{"snippet": "def myGPpredict(x_new, x_data, y_data, K_inv, theta, sig):\n    K_et = cov_vect(x_new,x_data,theta,sig)\n    mu = K_et.dot(K_inv.dot(y_data))\n    k_xx = cov_function(x_new,x_new,theta,sig)\n    sigma = k_xx - K_et.dot(K_inv.dot(K_et.T))\n    return mu[0],sigma\n", "intent": "1.1 Code the function that predict the mean and the standard deviation of the Gaussian process using the previous defined function\n"}
{"snippet": "print y_test.shape\nprint X_test.shape\nprint X_test[:5,1]\nX_test_centered = X_test-np.mean(X_test,axis=0)\nkFold = sklearn.cross_validation.KFold(X_test.shape[0],n_folds=5)\nscore =[]\nfor _,idx in kFold:\n    score.append(sklearn.metrics.accuracy_score(y_test[idx],clf.predict(X_test[idx])))\nprint np.mean(score), np.std(score)\n", "intent": "Test the performance of our tuned KNN classifier on the test set.\n"}
{"snippet": "clflog.predict(Xtest)\n", "intent": "In `sklearn`, `clf.predict(test_data)` makes predictions on the assumption that a 0.5 probability threshold is the appropriate thing to do.\n"}
{"snippet": "confusion_matrix(ytest, t_repredict(clflog, 0.1, Xtest))\n", "intent": "See how the false negatives get suppressed?\n"}
{"snippet": "metrics.f1_score(y_train, clf.predict(X_train))\n", "intent": "The over-fitting we saw previously can be quantified by computing the\nf1-score on the training data itself:\n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "Let's see how we're doing in terms of precision, recall, and accuracy on our test set.\n"}
{"snippet": "y_est = model.predict(x_test)[:,None]\nmse = np.mean(np.square(y_test_true-y_est))\nprint(mse)\n", "intent": "The MSE is significantly better than both the above models.\n"}
{"snippet": "print(\"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2))\nprint(\"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2))\n", "intent": "Now, calculate the mean squared error using just the test data and compare to mean squared from using all the data to fit the model. \n"}
{"snippet": "print(np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "The residual sum of squares: \n"}
{"snippet": "print(np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "Mean squared error: \n"}
{"snippet": "y_model = model.predict(Xtest)\n", "intent": "Now let's use the model.predict method to see what values our model predict for the data. We'll use the X test data we set aside a few cells before. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Great, now lets use y_model and compare it to the test data we set aside and see how our model performed!\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=1)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predictions = list(classifier.predict(X_test, as_iterable = True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "prediction = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "model.evaluate(test_x, test_data['cnt'], batch_size=256)\n", "intent": "Try and beat my score of 0.36\n"}
{"snippet": "print(classification_report(y_test, prediction_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "pred = mnb.predict(msg_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(label_test, pred))\nprint(classification_report(label_test, pred))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "pred2 = pipeline.predict(msg2_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)\n", "intent": "<h2> 4.4 Building a random model (Finding worst-case log-loss) </h2>\n"}
{"snippet": "roc_values = []\nfor feature in ['Sex', 'Cabin', 'Embarked', 'Cabin']:\n    roc_values.append(roc_auc_score(y_test, X_test_enc[feature])) \n", "intent": "The strings were replaced by probabilities.\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "pred_y = grid.predict(test_x)\n", "intent": "Finally predict the values on the actual test set with given params:\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Now we make some predictions on our model\nWe give the model data it hasnt see yet\n"}
{"snippet": "vals = np.array([100,30,10000,7,3]).reshape(1,-1)\nmodel.predict(vals)\n", "intent": "Now we can use the model to predict avg grade for a given set of features\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "predict the test set \n"}
{"snippet": "lm.predict(np.array([80000, 5, 6, 4,20000]).reshape(1, -1))\n", "intent": "Estimate the value of the following house:<br> \n* 6 rooms \n* 4 bedrooms\n* Area income: 80000\n* 5 years old\n* Area population: 20000\n"}
{"snippet": "logmodel.predict(np.array([3, 59, 0, 14]).reshape(1, -1))\n", "intent": "Check if email with 3 rec, len=59, no attachment and subject len=14 is a spam\n"}
{"snippet": "logmodel.predict(np.array([12, 159, 0, 24]).reshape(1, -1))\n", "intent": "Check if email with 12 rec, len=159, no attachment and subject len=24 is a spam\n"}
{"snippet": "model.evaluate(test_x, test_data['cnt'], batch_size=256)\n", "intent": "It's important that you understand the difference between `model.predict` and `model.evaluate`.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nprint ('S Score with the Transformed Data', silhouette_score(seeds_new_transform, km_fit.labels_))\nprint ('S Score with the  Non Transformed Data', silhouette_score(seeds_new, km_fit.labels_))\n", "intent": "_(pairplot with hue)_\n"}
{"snippet": "y_test_predict_logreg = logregcv_fit.predict(X_test)\ny_logreg_pp = logregcv_fit.predict_proba(X_test)\n", "intent": "**9.B Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression.**\n"}
{"snippet": "yhat_ridge = lr_ridge.predict(X_test)\nyhat_ridge_pp = lr_ridge.predict_proba(X_test)\n", "intent": "**9.B Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression.**\n"}
{"snippet": "knn1_predictions = knn1_model.predict(X)\nacc = metrics.accuracy_score(y.values, knn1_predictions)\nprint(acc)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "logreg.predict_proba([[0]])\n", "intent": "The probability of admittance with an average gpa:\n"}
{"snippet": "ridge_pred = lr_ridge.predict(X_test)\nridge_pp = lr_ridge.predict_proba(X_test)\n", "intent": "**9.B Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression.**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, X, y, cv=10)\nprint accs\nprint np.mean(accs)\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "def run_gradient_descent(X, y, initial_beta_array, step_size, iterations=500):\n    beta_array = initial_beta_array\n    mses = []\n    mses.append(mean_squared_error(X, y, beta_array))\n    beta_arrays = []\n    for i in range(iterations):\n        beta_array = beta_update_function(X, y, beta_array, step_size)\n        mses.append(mean_squared_error(X, y, beta_array))\n        beta_arrays.append(beta_array)\n", "intent": "This is the function that wraps the gradient update with some number of iterations. It is the same, but takes an array of beta coefficients.\n"}
{"snippet": "logistic.predict_proba(x_test[:3])\n", "intent": "Predicting the probabilities for the first 3 images:\n"}
{"snippet": "elasticnet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nelasticnet_scores = cross_val_score(elasticnet, Xs, Y.values.ravel(), cv=10)\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "clusters = kmeans.fit_predict(iris_df)\ncentroids = kmeans.cluster_centers_\n", "intent": "**3.2 Compute the labels and centroids.**\n"}
{"snippet": "pred_prob=classifier.predict_proba(pred_test)\npred_prob[0,1]\n", "intent": "Instead of doing a Yes/No prediction, we can instead do a probability computation to show the probability for the prospect to buy the product\n"}
{"snippet": "browsing_data = np.array([1,0,1,0,0]).reshape(1, -1)\nprint(\"After checking reviews: propensity :\",classifier.predict_proba(browsing_data)[:,1] )\n", "intent": "It goes up. Next, he checks out reviews.\n"}
{"snippet": "new_data = np.array([100,0,50,0,0,0]).reshape(1, -1)\nnew_pred=model.predict(new_data) \nprint(\"The CLV for the new customer is : $\",new_pred[0])\n", "intent": "Let us say we have a new customer who in his first 3 months have spend 100,0,50 on your website. Let us use the model to predict his CLV.\n"}
{"snippet": "print (model.n_iter_)\nprint (silhouette_score(df[features], df['clusters']))\n", "intent": "<a id='scaling'></a>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, cross_val_predict\naccs = cross_val_score(knn,X,y, cv=10)\nprint (accs)\nprint (np.mean(accs))\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "y_pred  = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\nprint (y_pp)\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "y_pred  = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\nprint (y_pred)\nprint (y_pp)\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "logistic.predict_proba(test_x[:3].reshape(3,-1))\n", "intent": "Predicting the probabilities for the first 3 images:\n"}
{"snippet": "print('All features')\nscores = cross_val_score(lr, X, y)\nprint(scores, '\\n', 'Mean: ', scores.mean())\nselected_xs = df.iloc[:, rfe.support_].copy()\nprint('Feature Selection via RFE (3 columns)')\nscores = cross_val_score(lr, selected_xs, y)\nprint(scores, '\\n', 'Mean: ', scores.mean())\n", "intent": "We can then compare how well this performs compared to all columns:\n"}
{"snippet": "print(pipeline.score(X_train, y_train))\npredictions = pipeline.predict(X_train)\nprint(confusion_matrix(y_train, predictions))\nprint(classification_report(y_train, predictions))\n", "intent": "Then we'll score it and run the predictions from the training set.\n"}
{"snippet": "print(pipeline.score(X_test, y_test))\npredictions = pipeline.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n", "intent": "And finally, we'll score and predict with our test set. We do not need to refit this to the test set!\n"}
{"snippet": "print(model1.predict(X_test)[0:5])\n", "intent": "Or we can get predictions as well:\n"}
{"snippet": "scores = cross_val_score(lr, X, boston_y, cv=5)\nprint(scores.mean(), scores.std())\n", "intent": "We may also want to see an overall score for these folds. Typically, we will take the average $R^2$ across each fold:\n"}
{"snippet": "scores = cross_val_score(lr, X, boston_y, cv=5)\nprint(scores.mean())\n", "intent": "We may also want to see an overall score for these folds. Typically, we will take the average $R^2$ across each fold:\n"}
{"snippet": "confusion_matrix(set1['ytest'], digitstwo_log_set1.predict(set1['Xtest']))\n", "intent": "> YOUR TURN NOW: Calculate the confusion matrix for the regularized logistic regression\n"}
{"snippet": "confusion_matrix(set1['ytest'], digitstwo_log_set2.predict(set1['Xtest']))\n", "intent": "From the department of not-kosher things to do, (why?) we calculate the performance of this classifier on `set1`.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(setf['ytest'], digitspca_log2.predict(setf['Xtest']), )\n", "intent": "And a confusion matrix...\n"}
{"snippet": "logistic.predict_proba(x_test[:3].reshape(3,-1))\n", "intent": "Predicting the probabilities for the first 3 images:\n"}
{"snippet": "evaluate(predicted_knn, iris_test)\n", "intent": "Then let's apply this function to our predictions:\n"}
{"snippet": "np.mean((regr.predict(x_test)-y_test)**2)\n", "intent": "Now, we will compute metrics that can be used to assess fit:\n"}
{"snippet": "poly.predicted = lg.predict(x_test_poly)\npoly.predicted\n", "intent": "Now we can also do prediction:\n"}
{"snippet": "y_hat_train_0_1 = y_hat_train >.5\ny_hat_test_0_1 = y_hat_test >.5\nprint(\"Training Set Accuracy for Linear Regression: \", accuracy_score(y_train, y_hat_train_0_1))\nprint(\"Test Set Accuracy for Linear Regression: \", accuracy_score(y_test, y_hat_test_0_1))\n", "intent": "Notice it is possible for us to convert the above values into a binary classification by the following code.\n"}
{"snippet": "predicted = logit.predict(test[['female','First','Second','age']])\nthreshold = .5\nexpected = test['survived']\nprint(metrics.classification_report(expected, predicted))\n", "intent": "What does the blue line actually mean in this scenario? How might our model be different if we were at a different position on the line? \n"}
{"snippet": "predicted = logit.predict(test[['female','First','Second','age']])\nthreshold = .5\nexpected = test['survived']\nprint(metrics.classification_report(expected, predicted))\n", "intent": "What does the red line actually mean in this scenario? How might our model be different if we were at a different position on the line? \n"}
{"snippet": "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n                                                 model.score(x_test[y_test==1], y_test[y_test==1]), \n                                                 model.score(x_test[y_test==2], y_test[y_test==2]), \n                                                 cost(y_test, model.predict(x_test))],\n                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1', 'accuracy on class 2', 'total cost'])\n", "intent": "We'll define a function for computing the accuracy rates so that we can conveniently call it later.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\nscores\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"foto7.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "letter = [char2int[letter] for letter in \"white supremacists are \"]\nsentence = [int2char[l] for l in letter]\nfor i in range(150):\n    if sentence[-1]=='<END>':\n        break\n    p = model.predict(np.array(letter)[None,:])\n    letter.append(np.random.choice(len(char2int),1,p=p[0][-1])[0])\n    sentence.append(int2char[letter[-1]])\nprint(''.join(sentence))\n", "intent": "Feel free to change the starting sentence as you please. But remember simple letters.\n"}
{"snippet": "predictions = lm.predict(X_test)\nprint (\"Type of the predicted object:\", type(predictions))\nprint (\"Size of the predicted object:\", predictions.shape)\n", "intent": "**Prediction using the lm model**\n"}
{"snippet": "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_train,train_pred))\nprint(\"Mean square error (MSE):\", metrics.mean_squared_error(y_train,train_pred))\nprint(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_train,train_pred)))\n", "intent": "**Regression evaluation metrices for train**\n"}
{"snippet": "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_test,predictions))\nprint(\"Mean square error (MSE):\", metrics.mean_squared_error(y_test,predictions))\nprint(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_test,predictions)))\n", "intent": "**Regression evaluation metrices for test**\n"}
{"snippet": "def rmse(predictions, actuals): \n    return math.sqrt(((predictions-actuals)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train),  \n           rmse(m.predict(X_valid), y_valid),  \n           m.score(X_train, y_train),  \n           m.score(X_valid, y_valid)]  \n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(\"RMSE Train\\t\\tValid\\t\\t   R2 train\\t\\tR2 valid\\t\\t OOB\\n\", res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "metrics.r2_score(preds, y_valid)\n", "intent": "Somehow our $R^2$ is better than this... its ok...\n"}
{"snippet": "y_pred = pipe_base_model_cv.predict(X_test)\nscore = f1_score(y_test, y_pred, average='macro')\nprint(score)\n", "intent": "Accuracy on test set\n"}
{"snippet": "y_testpred = model.predict(x_test)\ntpredictions = [round(value) for value in y_testpred]\ntaccuracy = accuracy_score(y_test, tpredictions)\nprint(\"Train Accuracy : %.2f%%\" % (taccuracy * 100.0))\n", "intent": "The Feature Importance plot shows which features are contributing more for the model development. The more the F1 Score the more important they are.\n"}
{"snippet": "split = 0.75\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\ny_pred = predict(X_train, y_train, X_test, k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "predictions = regr.predict(test[x])\nSSreg = np.mean((predictions - test[y]) ** 2)\nSStot =  np.mean((test[y] - np.mean(test[y])) ** 2)\nprint 1 - SSreg/SStot\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "y_pred_train = regressor.predict(X_train)\n", "intent": "And predict. First let us try the training set:\n"}
{"snippet": "y_pred_test = regressor.predict(X_test)\n", "intent": "Let's try the test set:\n"}
{"snippet": "cv = ShuffleSplit(len(iris.target), n_iter=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "You can use all of these cross-validation generators with the cross_val_score method:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "f1-score is the geometric average of precision and recall.\n"}
{"snippet": "y_test = gaussian.predict(test_df)\n", "intent": "Ahora ya podemos aplicar el modelo a nuestros datos de test:\n"}
{"snippet": "roc_knn_p = roc_curve(y_test, result_k_p.predict_proba(x_test).T[1])\ngetROCcurve(roc_knn_p, 'ROC Curve Optimized for Precision w/ AUC: ')\n", "intent": "[See the sklearn plotting example here.](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint(\"Done: {}\".format(getTime()))\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint(\"Done: {}\".format(getTime()))\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in testData]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint(\"Done: {}\".format(getTime()))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "y_predict = KNN.predict(X_train)\n", "intent": "Use the model to make a prediction on the test set. Assign the prediction to a variable named y_pred\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = GV_GBC.predict(X_test)\nprint(classification_report(y_pred, y_test))\n", "intent": "The error metrics. Classification report is particularly convenient for multi-class cases.\n"}
{"snippet": "y_pred = LR_L2.predict(X_test)\nprint(classification_report(y_pred, y_test))\n", "intent": "Check the errors and confusion matrix for the logistic regression model.\n"}
{"snippet": "y_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "y_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test_new), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test_new).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\ndef measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred),\n                      'recall': recall_score(y_true, y_pred),\n                      'f1': f1_score(y_true, y_pred)},\n                      name=label)\n", "intent": "A function to return error metrics.\n"}
{"snippet": "y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n                                 measure_error(y_test, y_test_pred_gr, 'test')],\n                                axis=1)\n", "intent": "These test errors are a little better than the previous ones. So it would seem the previous example overfit the data, but only slightly so.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_train_pred_gr_sugar = GR_sugar.predict(X_train)\ny_test_pred_gr_sugar  = GR_sugar.predict(X_test)\ntrain_test_gr_sugar_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_gr_sugar),\n                                         'test':  mean_squared_error(y_test, y_test_pred_gr_sugar)},\n                                          name='MSE').to_frame().T\ntrain_test_gr_sugar_error\n", "intent": "The error on train and test data sets. Since this is continuous, we will use mean squared error.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_train_pred = DTC.predict(X_train)\ny_test_pred  = DTC.predict(X_test)\ntrain_test_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n                                         'test':  mean_squared_error(y_test, y_test_pred)},\n                                          name='MSE').to_frame().T\ntrain_test_error\n", "intent": "A function to return error metrics.\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "data_faces_pred_faces = reg_model_faces.predict(faces_response_vec.reshape(-1,1))[:,0]\nfaces_sse = np.sum((data_faces - data_faces_pred_faces) **2)\nfaces_sse\n", "intent": "Let's calculate the SSE for this faces model.\n"}
{"snippet": "data_faces_pred_full = reg_model_full.predict(designmat_full)[:,0]\nfull_sse = np.sum((data_faces-data_faces_pred_full)**2)\nprint('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))\n", "intent": "And let's predict and calculate the SSE again, so we can compare it with the faces bodies model.\n"}
{"snippet": "data_faces_pred_faces = reg_model_faces.predict(faces_response_vec.reshape(-1,1))[:,0]\n", "intent": "As we can see, these values correspond to `slope` and `intercept` computed above. \nLet's see how good this model is by calculating the SSE:\n"}
{"snippet": "data_faces_pred_full = reg_model_full.predict(designmat_full)[:,1]\nfull_sse = np.sum((data_faces-data_faces_pred_full)**2)\nprint('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))\n", "intent": "And let's predict and calculate the SSE again for the faces voxel, so we can compare it with the faces bodies model.\n"}
{"snippet": "pred_fake_train = model_fake.predict(fake_x_train)\ncorr_fake_train = correlate(pred_fake_train, fake_y_train)[0]\nprint('Correlation with training data: %.04f' % (corr_fake_train))\n", "intent": "Calculate model performance by correlating the predicted and real training data\n"}
{"snippet": "pred_fake_test = model_fake.predict(fake_x_test)\ncorr_fake_test = correlate(pred_fake_test, fake_y_test)[0]\nprint('Correlation with testing data: %.04f' % (corr_fake_test))\n", "intent": "Calculate model performance by correlating the predicted and real testing data\n"}
{"snippet": "pred_fake_train_outlier = model_fake_outlier.predict(fake_x_train)\ncorr_fake_train_outlier = correlate(pred_fake_train_outlier, fake_y_train_outlier)[0]\nprint('Correlation with training data: %.04f' % (corr_fake_train_outlier))\n", "intent": "Calculate model performance by correlating the predicted and real training data with an outlier \n"}
{"snippet": "pred_fake_test_outlier = model_fake_outlier.predict(fake_x_test)\ncorr_fake_test_outlier = correlate(pred_fake_test, fake_y_test)[0]\nprint('Correlation with testing data: %.04f' % (corr_fake_test_outlier))\n", "intent": "Calculate model performance by correlating the predicted and real testing data with an outlier\n"}
{"snippet": "pred_train = model_train.predict(designmat_response_train)\npred_test = model_train.predict(designmat_response_test)\n", "intent": "Now we'll predict the both the training and test data using the two design matrices we created above. \n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "pred_gradient = model_gradient.predict(smoothed[1260:].reshape(-1,100))\npred_acc_gradient = correlate(data_test, pred_gradient)\npred_acc_gradient[pred_acc_gradient>.9] = np.nan\n", "intent": "Calculate the prediction accuracy of this gradient encoding model\n"}
{"snippet": "pred_sem3_train = model_semantic3.predict(designmat_sem3_train)\n", "intent": "Predict the training data and correlate those predictions with the real response amplitudes.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xx, yy, zz, aa = np.mgrid[-0:10:0.5, 0:10:0.5, 0:10:0.5, 0:10:0.5]\ngrid = np.c_[xx.ravel(), yy.ravel(), zz.ravel(), aa.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n", "intent": "As you can see this is performing much better; however a few times the neural netweok is getting caught in the local minima\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nlabels = km.labels_\nsilhouette_score(X,labels,metric='euclidean')\n", "intent": "But which is right?\nNote that the model also stores what the points for the model are in the `.labels_` attribute.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nlabels = km.labels_\nsilhouette_score(X,labels,metric='euclidean')\n", "intent": "But which is right?\nNote that the model also stores what the points for the model are in the `.labels_` attribute.\n"}
{"snippet": "f1_score(y,y_pred)\n", "intent": "F1 score for ridge regression:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error \ndef rmse(ytrue, ypredicted): \n    return np.sqrt(mean_squared_error(ytrue, ypredicted))\n", "intent": "* Write a function `rmse` that takes in truth and prediction values and returns the root-mean-squared error. User sklearn's `mean_squared_error`. \n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\ndef measure_error(y_true, y_pred, label): \n    return pd.Series({'accuracy': accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred), \n                      'recall': recall_score(y_true, y_pred), \n                      'f1': f1_score(y_true, y_pred)}, \n                      name = label)\n", "intent": "There are 161 nodes and a maximum depth of 19.  A function to return error metrics. \n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "accuracy_score(Y, labels)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "silhouette_score(X, Y)\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print classification_report(Y, labels)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "print('Training set r2: ', r2_score(train_results.TOTAL_VALUE, train_results.predicted))\nprint('Validation set r2: ', r2_score(valid_results.TOTAL_VALUE, valid_results.predicted))\n", "intent": "We can use the metrics that scikit-learn provides.\n"}
{"snippet": "classificationSummary(valid_y, rf.predict(valid_X))\n", "intent": "Confusion matrix and metrics\n"}
{"snippet": "from sklearn.metrics import classification_report\npredictions = grid_search.predict(X_test)\nprint grid_search.score(X_test, predictions)\nprint classification_report(y_test, predictions)\n", "intent": "By default, the `GridSearchCV` object will be re-fit using the best values of the hyperparameters.  \nPredictions can then be made using the object.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint cross_val_score(pipeline, X, y, n_jobs=-1)\n", "intent": " Now we will print the accuracies of the three cross validation folds.\n"}
{"snippet": "xx = np.linspace(0, 12, 100)\nxx = xx.reshape(xx.shape[0], 1)\nyy = regressor.predict(xx)\n", "intent": "The folloing will create some points that we can use to visualize the regression hyperlane.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, y_test)\nprint('loss:', loss)\nprint('accuracy:', accuracy)\n", "intent": "Now we can evaluate the model on the test data to get a sense of how we did.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(labels, pred_labels)\nacc = accuracy_score(labels, pred_labels)\nprint(acc)\nprint(mat)\n", "intent": "Check the performance of the model\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(test_labels, pred_labels)\nacc = accuracy_score(test_labels, pred_labels)\nprint(acc)\nprint(mat)\n", "intent": "Check the performance of the model\n"}
{"snippet": "predict = model.predict(test_images, verbose=1)\n", "intent": "Freeze the layers that remain in our model.\n"}
{"snippet": "print (\"Predicted %d, Label: %d\" % (list(classifier.predict(test_data[0:1]))[0], test_labels[0]))\ndisplay(0)\n", "intent": "We can make predictions on individual images as well. Note: the predict method accepts an array of samples as input, and returns a generator.\n"}
{"snippet": "def print_proba_table(prob_list, stride=1):\n    mnist_classes = [i for i in range(len(prob_list[0]))]\n    print(\"Class:\", *mnist_classes, sep=\"\\t\")\n    print(\"index\", *[\"---\" for i in range(len(mnist_classes))], sep=\"\\t\")\n    counter = 0\n    for prob in prob_list[::stride]:\n        print(counter*stride, *[round(prob[i], 3) for i in range(len(mnist_classes))], sep=\"\\t\")\n        counter += 1\nprint_proba_table(clf.predict_proba(X_test), stride=4)\n", "intent": "`clf.predict` tells us the actual predictions made on the test set.\n"}
{"snippet": "print_proba_table(clf.predict_proba(X_test), stride=10)\n", "intent": "Not so easy now, is it? But is 94.8% accuracy good \"enough\"? Depends on your application.\n"}
{"snippet": "print(confusion_matrix(y_test,rfc_pred))\nprint(classification_report(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(college_data['Cluster'],km.labels_))\nprint(classification_report(college_data['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "y_min_predicted = model.predict(x_min)\ny_max_predicted = model.predict(x_max)\nprint(f\"Actual Min Value: {y_min_actual}\")\nprint(f\"Predicted Min Value: {y_min_predicted}\")\nprint(f\"Actual Max Value: {y_max_actual}\")\nprint(f\"Predicted Max Value: {y_max_predicted}\")\n", "intent": "We can also use the predict function to calculate predicted values\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\npredicted = model.predict(X)\nmse = mean_squared_error(y, predicted)\nr2 = r2_score(y, predicted)\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-squared (R2 ): {r2}\")\n", "intent": "There are a variety of ways to quantify the model, but MSE and R2 are very common\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = model.predict(X_test_scaled)\nMSE = mean_squared_error(y_test_scaled, predictions)\nr2 = model.score(X_test_scaled, y_test_scaled)\nprint(f\"MSE: {MSE}, R2: {r2}\")\n", "intent": "Step 5) Quantify your model using the scaled data\n"}
{"snippet": "def compute_loss(X, y, w):\n    X = expand(X)\n    return np.mean([max(0, 1 - y[i] * w.dot(X[i])) for i in range(X.shape[0])], axis=0)\ndef compute_grad(X, y, w):\n    X = expand(X)\n    return np.mean([-1 * (y[i] * w.dot(X[i]) < 1) * y[i] * X[i] for i in range(X.shape[0])], axis=0)\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "y_pred = net.forward(X_test)\ny_pred = [np.argmax(y_pred[i]) for i in range(len(y_pred))]\ny_test_ = [np.argmax(y_test[i]) for i in range(len(y_test))]\nprint accuracy_score(y_pred, y_test_)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)\n", "intent": "Play full session with an untrained agent\n"}
{"snippet": "encoded_values = encoder.predict(train_features)\n", "intent": "Replace 'srch_destination_id' with trained reduced destination features \n"}
{"snippet": "forest_prediction = rfor.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test,forest_prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "prediction = KNN_model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test, rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "new_predict = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "x_test = np.array(['you are not happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "x_test = np.array(['SFO is nice'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob))\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve**:\n"}
{"snippet": "def get_loss(x, y):\n    return tf.reduce_mean(tf.square(get_preds(x) - y))\n", "intent": "The following functions evaluate the loss and the its gradient.\n"}
{"snippet": "mpg = mod.predict(97)\nprint(mpg[0][0])\n", "intent": "It would expect about 24.6 mpg\n"}
{"snippet": "print (clf.predict([[150, 0]]))\n", "intent": "<h3>Make Prediction</h3>\n"}
{"snippet": "print cross_val_score(RidgeCV ,X, y)\n", "intent": "---\nIs it better than the Linear regression? If so, why would this be?\n"}
{"snippet": "print cross_val_score(ElasticNetCV ,X, y)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "from sklearn.cross_validation import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(KNN, X, y, cv=10, n_jobs=-1)\nprint 'Mean Cross-Validated Score:', np.mean(scores)\n", "intent": "Use 10 folds. How does the performace compare to the baseline accuracy?\n"}
{"snippet": "for i in range(10):\n    print(i, np.bincount((np.array(model.predict(testX))[:,i] >= 0.5).astype(np.int_)))\n", "intent": "Now let's look at our predictions in the same way:\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": "Try some other values for yourself. Does the prediction make sense?\nOK, that works fine. Now let's see how good our classifier is on our test set.\n"}
{"snippet": "train_and_evaluate(svc_1, X_train, X_test, y_train, y_test)\n", "intent": "Let's measure precision and recall on the evaluation set, for _each class_. \n"}
{"snippet": "style_layer = 'relu2_1'\nlayer = vgg_net[style_layer]\nfeats, height, width, channels = [x.value for x in layer.get_shape()]\nsize = height * width * channels\nfeatures = tf.reshape(layer, (-1, channels))\nstyle_gram_matrix = tf.matmul(tf.transpose(features), features) / size\nstyle_expected = style_features[style_layer]\nstyle_losses.append(2 * tf.nn.l2_loss(style_gram_matrix - style_expected) / style_expected.size)\n", "intent": "Now we extract the style layer information\n"}
{"snippet": "preds = estimator.predict(input_fn=get_input_fn(my_test_data, 1, 1, shuffle=False))\nprint()\nfor p, s in zip(preds, sentences):\n    print('sentence:', s)\n    print('good review:', p[0], 'bad review:', p[1])\n    print('-' * 10)\n", "intent": "Now, let's generate predictions for the sentences\n"}
{"snippet": "RN50_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_RN50]\ntest_accuracy = 100*np.sum(np.array(RN50_predictions)==np.argmax(test_targets, axis=1))/len(RN50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint \"precision for category 0:\"\nPrec=precision_score(Y, Z,labels=\"0\",average=\"micro\")*100\nprint Prec\n", "intent": "i.b) Another way to calculate the precision for category 0:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nAcc=accuracy_score(Y, Z)*100\nprint('Precision for the model = %.1f%%'%(Acc))\n", "intent": "ii.a) One way to calculate the accuracy:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nAcc=accuracy_score(Y, Z)*100\nprint('Accuracy for the model = %.1f%%'%(Acc))\n", "intent": "ii.a) One way to calculate the accuracy:\n"}
{"snippet": "predict = LR.predict(x_train)\nprint(\"Mean squared error is: %.4f\"%mean_squared_error(y_train, predict))\n", "intent": "<img src=\"https://i.stack.imgur.com/eG03B.png\", width=300, height = 250>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(mean_squared_error(bos.PRICE, lm.predict(X)))\nprint(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred_prob = logi_reg.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nConcatRocData('Logistic Regression', fpr, tpr, auc)\n", "intent": "85% of not survival (0) prediction are good  => 152 / (152+27)  \n80% of survival (1)  prediction are good => 93 / (93+23)\n"}
{"snippet": "bag_preds = bag_mod.predict(testing_data) \nrf_preds = rf_mod.predict(testing_data)\nada_preds = ada_mod.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def evaluate(predictions, true_labels):\n    correct = 0\n    incorrect = 0\n    for i in range(len(predictions)):\n        if predictions[i] == true_labels[i]:\n            correct += 1\n        else:\n            incorrect += 1\n    print(\"\\tAccuracy:   \", correct / len(predictions))\n    print(\"\\tError rate: \", incorrect / len(predictions))\n", "intent": "And predictions evaluator:\n"}
{"snippet": "def recall(actual, preds):\n    return None \nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    return None \nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    return tp/(pred_pos)\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    act_pos = (actual==1).sum()\n    return tp/act_pos\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    prec = tp/(pred_pos)\n    act_pos = (actual==1).sum()\n    recall = tp/act_pos\n    return 2*(prec*recall)/(prec+recall)\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum((actual-preds)**2)/len(actual)\nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "def mse(actual, preds):\n    return None \nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "dbscan = \nclustering_labels_3 = dbscan.fit_predict(dataset_2)\n", "intent": "What happens if we run DBSCAN with the default parameter values?\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    return float(tp)/(pred_pos)\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now that I have fit our model, I evaluate its performance by predicting off the test values!\n"}
{"snippet": "def f1(preds, actual):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    prec = float(tp)/(pred_pos)\n    act_pos = (actual==1).sum()\n    recall = float(tp)/act_pos\n    return 2*(prec*recall)/(prec+recall)\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "print(bmi_life_model.predict([21.07931]))\nlaos_life_exp = bmi_life_model.predict([21.07931])\nlaos_life_exp = bmi_life_model.predict([21.07931])[0][0]\nlaos_life_exp\n", "intent": "3. Predict using the model\nPredict using a BMI of 21.07931 and assign it to the variable laos_life_exp.\n"}
{"snippet": "print(\"Model prediction =\", model.predict([X_test[0]]))\nprint(\"Hand computed prediction =\", model.coef_[0] * X_test[0] + model.intercept_)\nprint(\"Correct output =\", y_test[0])\n", "intent": "In the same way as before, we can compute the estimated output with the `predict()` method.\n"}
{"snippet": "print(\"Model prediction =\", model.predict([x_test[0]]))\nprint(\"Hand computed prediction =\", model.coef_[0] * x_test[0] + model.intercept_)\nprint(\"Correct output =\", y_test[0])\n", "intent": "In the same way as before, we can compute the estimated output with the `predict()` method.\n"}
{"snippet": "scores = cross_val_score(model_knn, X, y, cv=10)\nprint(scores)\n", "intent": "with majority vote method\n"}
{"snippet": "X = spam.iloc[:, 1:201]\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(knn, X, y, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "Use 10 folds. How does the performace compare to the baseline accuracy?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndef my_accuracy(trained_model):\n    return None\nassert my_accuracy(trained_knn) == accuracy_score(trained_knn['y_test'],\n                                                  trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating accuracy.\n"}
{"snippet": "from sklearn.metrics import precision_score\ndef my_precision(trained_model):\n    return None\nassert my_precision(trained_knn) == precision_score(trained_knn['y_test'],\n                                                    trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating precision.\n"}
{"snippet": "from sklearn.metrics import recall_score\ndef my_recall(trained_model):\n    return None\nassert my_recall(trained_knn) == recall_score(trained_knn['y_test'],\n                                              trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating recall.\n"}
{"snippet": "predict = gmm.predict(features).collect()\n", "intent": "Note that we are looking at optimistic in-sample errors.\n"}
{"snippet": "Y_pred = clf.predict(X_test_pca)\n", "intent": "Now we want to use the test set to see how well we did (cross-validation).\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(classification_report(Y_test, Y_pred, target_names=lfw_people.target_names))\nprint(confusion_matrix(Y_test, Y_pred, labels=range(len(lfw_people.target_names))))\n", "intent": "There are nice convenient ways to display the results.\n"}
{"snippet": "VGG19_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "(model.predict(X) == y).mean()\n", "intent": "This is an in-sample prediction. Training error in both sklearn and statsmodels. Both are equivalent\n"}
{"snippet": "y_predict = lr.predict(X)\ny_predict[:10]\n", "intent": "All supervised estimators have a **`predict`** method that accepts **`X`**, a numpy array with data you would like to get the predicted label.\n"}
{"snippet": "new_scores = cross_val_score(lr, X, y, cv=10)\nnew_scores\n", "intent": "If you let the **`cv`** parameter in **`cross_val_score`** be an int, it will perform stratified kfold cross validation with that many folds.\n"}
{"snippet": "y_pred_test = regressor.predict(X_test)\n", "intent": "Next, let's try the test set:\n"}
{"snippet": "cv = ShuffleSplit(n_splits=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "You can use all of these cross-validation generators with the `cross_val_score` method:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "The values of all these values above are in the closed interval [0, 1], where 1 means a perfect score.\n"}
{"snippet": "y_yhat = test.map(lambda x: (x.label, model.predict(x.features)))\nerr = y_yhat.filter(lambda x: x[0] != x[1]).count() / float(test.count())\nprint(\"Error = \" + str(err))\n", "intent": "Evaluate on test data\n"}
{"snippet": "strengths = gmm.predict_proba(district[['median_price']])\n", "intent": "Further, if you want the probabilities around the assignments, you can use the `predict_proba` method:\n"}
{"snippet": "print('KMeans: ARI =', metrics.adjusted_rand_score(y, cluster_labels))\nprint('Agglomerative CLustering: ARI =', \n      metrics.adjusted_rand_score(y, ag.labels_))\n", "intent": "Calculate the Adjusted Rand Index (`sklearn.metrics`) for the resulting clustering and for ` KMeans` with the parameters from the 4th question.\n"}
{"snippet": "sgd_holdout_mse = mean_squared_error(y_valid, \n                                        sgd_reg.predict(X_valid_scaled))\nsgd_holdout_mse\n", "intent": "Make a prediction for hold-out  set `(X_valid_scaled, y_valid)` and check MSE value.\n"}
{"snippet": "X_new = [[3, 5,  4, 2], [5, 4, 3, 2]]\ntarget_idx = list(knn.predict(X_new))\ntarget_val = list()\nfor i in range(len(target_idx)):\n    target_val.append(str(target_idx[i]) + ': ' + str(iris.target_names[target_idx[i]]))\nprint(target_val)  \n", "intent": "- Returns the NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "<b>Classification Accuracy</b>\n- Proportion of correct predictions\n- Common evaluation metric for classification problems\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "<b>Computing RMSE for the Sales predictions</b>\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "<b>Classification Accuracy:</b> Percentage of correct predictions.\n"}
{"snippet": "print((TP + TN)/float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "<b>1. Classification Accuracy: </b> Overall, how often is the classifier correct?\n"}
{"snippet": "print((FP + FN)/float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "<b>2. Classification Error: </b> Overall, how often is the classifier incorrect?\n- Also known as 'Misclassification Rate'.\n"}
{"snippet": "print(\"Mean squared error on test data:\")\nprint(ed.evaluate('mean_squared_error', data={X: x_test, y_post: y_test}))\nprint(\"Mean absolute error on test data:\")\nprint(ed.evaluate('mean_absolute_error', data={X: x_test, y_post: y_test}))\n", "intent": "Calculate evalution metrics.\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob[0]))\n", "intent": "AUC is the percentage of the ROC plot that is underneath the curve:\n"}
{"snippet": "print('Residual sum of squares: %.2f' % np.mean((model.predict(X) - y) ** 2))\n", "intent": "Calculating RSS (Residual Sum of Squares) cost function value for our model:\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint('F1 Score: %s' % f1_score(y_test_binarized, predictions_binarized))\n", "intent": "- F1 Score = F1 Score summarizes precision and recall with a single statistic.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test_binarized, predictions_binarized, target_names=['male'], labels=[1]))\n", "intent": "- classification_report - This is a convenience function provided by scikit-learn that reports precision, recall and F1 score:\n"}
{"snippet": "precision = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision')\nprint(\"Precision: \", np.mean(precision))\nrecall = cross_val_score(classifier, X_train, y_train, cv=5, scoring='recall')\nprint(\"Recall: \", np.mean(recall))\n", "intent": "- Precision and Recall\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import hamming_loss, jaccard_similarity_score\nprint(hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[0.0, 1.0], [1.0, 1.0]])))\n", "intent": "- Multi-label classification performance metrics\n    - Hamming Loss\n    - Jaccard Similarity\n"}
{"snippet": "cluster_assignments = estimator.predict(image_flattened)\n", "intent": "Next, we predict the cluster assignment for each of the pixels in the original image.\n"}
{"snippet": "col_predict = ['price', 'grade', 'sqft_living', 'sqft_above']\nX_test = df[col_predict].copy()\nfor i in col_predict:\n    if i!='bathrooms':\n        X_test[i] = np.log(X_test[i])\nmodel.predict(X_test)\n", "intent": "The $R^2$ score seems to be fine, now we can use the full dataset `df` to predict potential values for <font color='blue'>**bathrooms**</font>:\n"}
{"snippet": "predict_value = lm_for_impute.predict(boston_impute_df.drop(['AGE','y'],axis=1))\n", "intent": "Previously we have fit our model, now it is time to do some prediction:\n"}
{"snippet": "tt = np.linspace(qso['time'].min(), qso['time'].max(), 500)\nqso_gp_sqex1_pred, qso_gp_sqex1_mse = qso_gp_sqex1.predict(tt.reshape(-1,1), eval_MSE=True)\nqso_gp_ex1_pred, qso_gp_ex1_mse = qso_gp_ex1.predict(tt.reshape(-1,1), eval_MSE=True)\n", "intent": "Finally, we can plot the model with its confidence band.\n"}
{"snippet": "note_predictions = list(classifier.predict(X_test, as_iterable=True))   \n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "print(classification_report(y_test, predictions))\nprint('\\n')\nprint(confusion_matrix(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "new_predictions = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "Y_p = lr.predict(X)\nprint Y_p[:10]\n", "intent": "To predict prices we will use `lr.predict()` function.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint accuracy_score(y_test, y_pred)\n", "intent": "Function [`accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "```\nXfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)```\n"}
{"snippet": "rf_cv = cross_val_score( \nprint('The RF model FoM = {:.4f} +/- {:.4f}'.format( \n", "intent": "**Problem 3c**\nUse 10-fold cross validation to estimate the FoM for the random forest model.\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "```\nrng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)```\n"}
{"snippet": "yprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n", "intent": "```\nyprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)```\n"}
{"snippet": "test_vectors = model_vectorizer.transform(test.data)\nlabels = model_classifier.predict(test_vectors)\n", "intent": "```\ntest_vectors = model_vectorizer.transform(test.data)\nlabels = model_classifier.predict(test_vectors)```\n"}
{"snippet": "classPredictA = gnb_clean.predict(news_clean_AA)\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "mse4 = mean_squared_error(y, cpredict4)\nprint(\"\\nThe root mean squared error is \" + str(np.sqrt(mse4)))\nprint(\"\\nThe mean absolute error is \" +str(mean_absolute_error(y, cpredict4)))\nprint(\"\\nThe correlation coefficient is \" + str(np.corrcoef(y, cpredict4)[0,1]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint('expected:', messages.label[5])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "predicts = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "rf_cv = cross_val_score(rf_clf, train_X, train_y, cv=10)\nprint('The RF model FoM = {:.4f} +/- {:.4f}'.format(np.mean(rf_cv), np.std(rf_cv, ddof=1)))\n", "intent": "**Problem 3c**\nUse 10-fold cross validation to estimate the FoM for the random forest model.\n"}
{"snippet": "bag_y_pred = bagging.predict(testing_data)\nrf_y_pred = randomForest.predict(testing_data)\nab_y_pred = adaBoost.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    return tp/(pred_pos)\nprint(precision(y_test, nb_pred))\nprint(precision_score(y_test, nb_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    act_pos = (actual==1).sum()\n    return tp/act_pos \nprint(recall(y_test, nb_pred))\nprint(recall_score(y_test, nb_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    prec = tp/(pred_pos)\n    act_pos = (actual==1).sum()\n    recall = tp/act_pos\n    return 2*(prec*recall)/(prec+recall)\nprint(f1(y_test, nb_pred))\nprint(f1_score(y_test, nb_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "lr_pred = lr.predict(X_test)\nrf_pred = rf.predict(X_test)\nab_pred = ab.predict(X_test)\ndt_pred = dt.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum((actual-preds)**2)/len(actual)\nprint(mse(y_test, dt_pred))\nprint(mean_squared_error(y_test, dt_pred))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "test_image_predicted_label_probabilities = model.predict(test_image_tensor)[0]\nfor idx, prob in enumerate(test_image_predicted_label_probabilities):\n    print(labels[idx].ljust(11), ': %.05f' % prob)\n", "intent": "Let's look at the predicted label probabilities:\n"}
{"snippet": "activations = activation_model.predict(test_image_tensor)\n", "intent": "Next, we pass in the test image tensor into the above activation model to get all the layer outputs:\n"}
{"snippet": "predictions = lgmodel.predict(X_test)\n", "intent": "** Agora preveja valores para os dados de teste. **\n"}
{"snippet": "phot_y = np.empty_like(test_y)\nphot_gal = np.logical_not(test_X[:,0] - test_X[:,-1] < 0.145)\nphot_y[phot_gal] = 'GALAXY'\nphot_y[~phot_gal] = 'STAR'\nprint(\"The baseline FoM = {:.4f}\".format(accuracy_score(test_y, phot_y)))\n", "intent": "**Problem 5a** \nCalculate the FoM for the SDSS photometric model on the test set. \n"}
{"snippet": "recall_score(y_train_0, y_train_pred)\n", "intent": "Recall -> True Positive Rate -> ratio of positive instances correctly detected by the classifier -> TP / TP + FN \n"}
{"snippet": "y = km.predict(data)\n", "intent": "Predict the clusters for each data point\n"}
{"snippet": "clf.predict(df[iris.feature_names])\npd.crosstab(df.species, clf.predict(df[iris.feature_names]))\n", "intent": "Now predict the classes and check the crosstabs:\n"}
{"snippet": "results = clf.predict(X_test)\npd.crosstab(y_test, results)\n", "intent": "This tree is much simpler than the original tree based on all the data.\n"}
{"snippet": "scores = cross_val_score(clf, X_train, y_train, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "What does cross-validation look like if we use only our Train set?\n"}
{"snippet": "scores = cross_val_score(clf, X_train, y_train, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "Does limiting the max depth affect our average model performance with this training data?\n"}
{"snippet": "scores_precision = cross_validation.cross_val_score(f, wine.values, grape.values, cv=5, scoring='precision')\nscores_precision\n", "intent": "Furthermore, we can customize the scoring method by specifying the `scoring` parameter:\n"}
{"snippet": "def mean_square_error(X, y, beta):\n    return (X.dot(beta) - y).dot(X.dot(beta) - y) / X.shape[0]\nmse = mean_square_error(X_bt, y, beta)\nprint('MSE: %.02f RMSE: %.02f' % (mse, np.sqrt(mse)))\n", "intent": "**Question:** To quantify the error, we can use the mean square error (MSE). Implement this the quadratic loss function given above:\n"}
{"snippet": "from numpy import log, exp\ndef log_loss(X, y, beta):\n    return np.sum(log(1 + exp(-y * X.dot(beta)))) / X.shape[0]\n", "intent": "**Question:** Implement the loss function of a logistic regression:\n"}
{"snippet": "new_preds = rf_clf.predict(new_X)\nprint(\"The model has an accuracy of {:.4f}\".format(accuracy_score(new_y, new_preds)))\n", "intent": "**Problem 6b**\nCalculate the accuracy of the model predictions on the new data.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "bn_solver.evaluate(mnist.test.images, mnist.test.labels)\n", "intent": "* With the batchnormalization, the loss is lower and it's more accurate too!\n"}
{"snippet": "result = model.evaluate(x=data.test.images,\n                        y=data.test.labels)\n", "intent": "Now that the model has been trained we can test its performance on the test-set. This also uses numpy-arrays as input.\n"}
{"snippet": "y_pred = model.predict(x=images)\n", "intent": "Get the predicted classes as One-Hot encoded arrays.\n"}
{"snippet": "y_pred = model.predict(x=data.test.images)\n", "intent": "We can plot some examples of mis-classified images from the test-set.\nFirst we get the predicted classes for all the images in the test-set:\n"}
{"snippet": "result = model2.evaluate(x=data.test.images,\n                         y=data.test.labels)\n", "intent": "Once the model has been trained we can evaluate its performance on the test-set. This is the same syntax as for the Sequential Model.\n"}
{"snippet": "y_pred = model2.predict(x=data.test.images)\n", "intent": "We can plot some examples of mis-classified images from the test-set.\nFirst we get the predicted classes for all the images in the test-set:\n"}
{"snippet": "y_pred = model3.predict(x=images)\n", "intent": "We then use the restored model to predict the class-numbers for those images.\n"}
{"snippet": "predict(image_path='images/parrot_cropped1.jpg')\n", "intent": "We can then use the VGG16 model on a picture of a parrot which is classified as a macaw (a parrot species) with a fairly high score of 79%.\n"}
{"snippet": "missing_ages = np.where(np.isnan(titanic_df['Age']))[0]\nimpute_X_missing = \nX_missing_minmax = scaler.transform(impute_X_missing)\nage_preds = lr_age.predict(X_missing_minmax)\n", "intent": "**Problem 3n**\nUse the age regression model to predict the ages for passengers with missing data.\n"}
{"snippet": "predict(image_path=image_paths_test[0])\n", "intent": "We can also try an image from our new test-set, and again the VGG16 model is very confused.\n"}
{"snippet": "result = model.evaluate(x=data.test.images,\n                        y=data.test.labels)\n", "intent": "We then evaluate its performance on the test-set.\n"}
{"snippet": "result = model.evaluate(x_test_pad, y_test)\n", "intent": "Now that the model has been trained we can calculate its classification accuracy on the test-set.\n"}
{"snippet": "y_pred = model.predict(x=x_test_pad[0:1000])\ny_pred = y_pred.T[0]\n", "intent": "In order to show an example of mis-classified text, we first calculate the predicted sentiment for the first 1000 texts in the test-set.\n"}
{"snippet": "model.predict(tokens_pad)\n", "intent": "We can now use the trained model to predict the sentiment for these texts.\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO) \nshutil.rmtree(path = OUTDIR, ignore_errors = True)\ntf.summary.FileWriterCache.clear() \ntf.estimator.train_and_evaluate(estimator = model, \n                                train_spec = train_spec, \n                                eval_spec = eval_spec)\n", "intent": "Run the following cell to start the training and evaluation as you specified them above:\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntf.summary.FileWriterCache.clear() \ntrain_and_evaluate(OUTDIR, num_train_steps = 500)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntf.summary.FileWriterCache.clear() \ntrain_and_evaluate(OUTDIR, num_train_steps = 2000)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "shutil.rmtree(path = \"babyweight_trained_dnn\", ignore_errors = True) \ntrain_and_evaluate(\"babyweight_trained_dnn\")\n", "intent": "Finally, we train the model!\n"}
{"snippet": "missing_ages = np.where(np.isnan(titanic_df['Age']))[0]\nimpute_X_missing = titanic_df[['Pclass', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'S', 'Q', 'C']].iloc[missing_ages]\nX_missing_minmax = scaler.transform(impute_X_missing)\nage_preds = lr_age.predict(X_missing_minmax)\n", "intent": "**Problem 3n**\nUse the age regression model to predict the ages for passengers with missing data.\n"}
{"snippet": "OUTDIR = \"mnist/learned\"\nshutil.rmtree(path = OUTDIR, ignore_errors = True) \nhparams = {\"train_steps\": 1000, \"learning_rate\": 0.01}\ntrain_and_evaluate(OUTDIR, hparams)\n", "intent": "This is the main() function\n"}
{"snippet": "OUTDIR = \"mnist/learned\"\nshutil.rmtree(OUTDIR, ignore_errors = True) \nhparams = {\"train_steps\": 1000, \"learning_rate\": 0.01}\ntrain_and_evaluate(OUTDIR, hparams)\n", "intent": "This is the main() function\n"}
{"snippet": "predict = model.predict(X_test)\n", "intent": "    predict = model.predict(X_test)\n"}
{"snippet": "labels = res.predict(X)\n", "intent": "We can get the means of the two Gaussians and \n"}
{"snippet": "ResNet50_predictions = [np.argmax(model_ResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "banknotes_predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob))\n", "intent": "**Note**: The number of thresholds = the number of unique probabilities in our prediction array\n"}
{"snippet": "print(\"Accuracy: %0.3f\" % dt.score(X_test, y_test))\nprint('ROC AUC: %0.3f' % roc_auc_score(y_test, dt.predict_proba(X_test)[:,1]))\n", "intent": "Notice two of the default parameters: **min_samples_leaf = 1** and **max_depth = None**\n"}
{"snippet": "predictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "metrics.mean_squared_error([1, 2, 3, 4, 5], [5, 4, 3, 2, 1])\n", "intent": "While the opposite scenario should have a mean squared error of 8:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ym = np.mean(y_tr) \ny_tr_pred = regr.predict(X_tr)\nRSS_tr = np.sum((y_tr_pred-y_tr)**2)\nTSS_tr = np.sum((y_tr-ym)**2)\nRsq_tr = 1- RSS_tr/TSS_tr\nprint(\"R^2 = {0:f}\".format(Rsq_tr))\n", "intent": "We next compute the MSE and the R^2 on the training data\n"}
{"snippet": "def predict(a, beta):\n    yhat = hypothesis(a,beta)\n    return yhat\n", "intent": "You should see the likelihood increasing as number of Iterations increase.\n"}
{"snippet": "y_pred = predict_y(W, b, X_test, 3)\nprint('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))\n", "intent": "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:\n"}
{"snippet": "y_pred_test = regressor.predict(X_test)\n", "intent": "As we can see in the plot above, the line is able to capture the general slope of the data, but not many details.\nNext, let's try the test set:\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % np.mean((lm.predict(X) - bos.PRICE) ** 2))\n", "intent": "This is simple the mean of the residual sum of squares.\n------ \nYour turn: Calculate the mean squared error and print it.\n"}
{"snippet": "predict = rfm.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = clf.predict(X)\nprint(y,y_pred)\n", "intent": "First, let's predict the species from the measurements.  Because the classifier is clearly not perfect, we expect some mis-classifications.\n"}
{"snippet": "predicted  = lrm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predicted = pl.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = knn.predict(X_Test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print(classification_report(y_Test,pred))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "predictions = lm.predict(X_Test)\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "predictions = logmodel.predict(X_Test)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(df['Cluster'], kmeans.labels_), '\\n')\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)\n", "intent": "* We make prediction.\n* Now lets put them all together.\n"}
{"snippet": "from sklearn.metrics import homogeneity_score\nfor name, est in estimators.items():\n    print('completeness', name, homogeneity_score(df[name],df['species']))\n    print('homogeneity', name, homogeneity_score(df['species'],df[name]))\n", "intent": "The plot looks good, but it isn't clear how good the labels are until we compare them with the true labels. \n"}
{"snippet": "cross_val_score(model_NB, X_train_countVectorizer, yelp_data_final_update['Review_category_useful_notuseful'], cv=5)\n", "intent": "Naive Bayes model on useful vs not useful:\n"}
{"snippet": "cross_val_score(model_NB, X_train_countVectorizer, yelp_data_final_update['Review_category_cool_notcool'], cv=5)\n", "intent": "Naive Bayes model on cool vs not cool:\n"}
{"snippet": "cross_val_score(model_NB, X_train_countVectorizer, yelp_data_final_update['Review_category_funny_notfunny'], cv=5)\n", "intent": "Naive Bayes model on funny vs not funny:\n"}
{"snippet": "h = .02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = clf_linear.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\npylab.contourf(xx,yy,Z,cmap=pylab.cm.Paired)\npylab.axis('off')\npylab.scatter(X[:,0],X[:,1],c=colormap[y], s=50,cmap=pylab.cm.Paired)\n", "intent": "Let's see how the linear SVM separated the space.\n"}
{"snippet": "print 'Training MSE: ', np.mean((regr1.predict(X_train) - y_train)**2)\nprint 'Test MSE: ', np.mean((regr1.predict(X_test) - y_test)**2)\n", "intent": "We can calculate the Mean Squared Error on the test set:\n"}
{"snippet": "X_test = np.random.random((200, 100))\ny_test = np.random.randint(0, 10, 200)\ny_test = to_categorical(y_test)\nloss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\nprint('\\n', loss_and_metrics)\nclasses = model.predict_classes(X_test, batch_size=32)\nproba = model.predict_proba(X_test, batch_size=32)\n", "intent": "- Like `sklearn`, we have nice predict and evaluation functions!\n- Here's an example:\n"}
{"snippet": "print(\"R-Squared Score: {}\".format(r2_score(y_test, y_pred)))\nprint(\"Mean Absolute Error: {}\".format(mean_absolute_error(y_test, y_pred)))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\n", "intent": "<h1>Challenge 5</h1>\n"}
{"snippet": "mean_squared_error(y_test, test_set_pred1)\n", "intent": "Mean Square = \n0.066664630206035261\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, test_set_pred1)\n", "intent": "R2 score for 30 days stock & CMC\n0.99973097446459092\n"}
{"snippet": "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\nuc = UnigramChunker(train_sents)\nprint(uc.evaluate(test_sents))\n", "intent": "Trainieren und evaluieren Sie den UnigramChunker auf dem CoNLL 2000 Korpus.\n"}
{"snippet": "def vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n", "intent": "The loss function encourages a good reconstruction and discourages a deviation of the learned latent z distribution from the N(0,1) distribution.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print (np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn, transformed_data, transformed_labels, cv=5)\n", "intent": "Let's cross validate each.\n"}
{"snippet": "knn.predict(wine_data_test)\n", "intent": "And finally, let's use our fitted model to predict on new data.\n"}
{"snippet": "cross_val_score(tree, wine_data, wine_labels, cv=4)\n", "intent": "And let's cross-validate again. Let's only run it four times.\n"}
{"snippet": "cross_val_score(gnb, wine_data, wine_labels, cv=4)\n", "intent": "And of course, let's cross-validate to see how well we did. Let's only run it four times.\n"}
{"snippet": "lr.predict(wine_mag_test)\n", "intent": "And finally, we predict.\n"}
{"snippet": "print('Number of errors: %i' % sum(clf.predict(X)!=y))\nplot_decision_regions(X,y,clf)\n", "intent": "*How can we evaluate the model??*\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nscaled_X_val = scaler.transform(X_val)                  \ny_pred_dt = clf_dt.predict(scaled_X_val)\ny_pred_nb = clf_nb.predict(scaled_X_val)                      \nacc_dt = accuracy_score(y_pred_dt,y_val)                            \nacc_nb = accuracy_score(y_pred_nb,y_val) \nprint(\"The accuracy of Decision Tree: {} %\".format(acc_dt))\nprint(\"The accuracy of Gaussian Naive Bayes: {} %\".format(acc_nb))\n", "intent": "Q12. Using the accuracy_score function, determine the accuracy of the two classifiers.\n"}
{"snippet": "cost = tf.losses.mean_squared_error(output_reshape, predictions)\n", "intent": "Very standard way of defining costs/optimizer etc..\n"}
{"snippet": "feature_cols = vehicles.columns[1:]\npreds1 = treereg1.predict(oos[feature_cols])\npreds2 = treereg2.predict(oos[feature_cols])\npreds3 = treereg3.predict(oos[feature_cols])\nprint(preds1)\nprint(preds2)\nprint(preds3)\n", "intent": "Q: How do we get predictions from our models?\n"}
{"snippet": "predict = clf.predict(x_test)\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "metrics.mean_absolute_error(y_test,predict)\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "print(classification_report(y_test,pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ypred2 = clf2.predict(Xtest)\n", "intent": "Random Forest Regressor does worse than classifier with probabilities.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in resnet_test]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%% ' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_score,recall_score\nprec_dt = precision_score(y_pred_dt,y_val)\nprec_nb = precision_score(y_pred_nb,y_val)\nrecall_dt = recall_score(y_pred_dt,y_val)\nrecall_nb = recall_score(y_pred_nb,y_val)\nprint(\"The precision of Decision Tree: {} %\".format(prec_dt))\nprint(\"The precision of Gaussian Naive Bayes: {} %\".format(prec_nb))\nprint(\"The recall of Decision Tree: {} %\".format(recall_dt))\nprint(\"The recall of Gaussian Naive Bayes: {} %\".format(recall_nb))\n", "intent": "Q13. Determine the precision and recall using precision_score and recall_score.\n"}
{"snippet": "probas = np.asarray( [ clf.predict_proba(X) for clf in classifiers_ ] )\navg_proba = np.average( probas, axis = 1, weights = None )\navg_proba\n", "intent": "Code when you wish to obtain the predicted probability for the final class.\n"}
{"snippet": "import numpy as np\nrandom_input = np.random.rand(1, 1, 3, 3)\nprediction = predictor.predict({'input': random_input.tolist()})\nprint(prediction)\n", "intent": "Invoking prediction:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(trans_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(college['Cluster'], kmeans.labels_))\nprint(classification_report(college['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "prediction = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "** Create a scatterplot of the real test values versus the predicted values. **\n"}
{"snippet": "prediction = logmodel.predict(X_test)\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "prediction = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\nkmeans.predict(X_new)\n", "intent": "Of course, we can predict the labels of new instances:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_dt = f1_score(y_pred_dt,y_val)\nf1_nb = f1_score(y_pred_nb,y_val)\nprint(\"The F1-score of Decision Tree: {} %\".format(f1_dt))\nprint(\"The F1-score of Gaussian Naive Bayes: {} %\".format(f1_nb))\n", "intent": "Q14. Determine the F1-score of the two classifiers.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Run more predictions\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "---\nCross-validate the $R^2$ of an ordinary linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "y_pred = ridgeregcv.predict(X_test)\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "y_pred = lassoreg.predict(X_test)\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "y_pred_class=knn.predict(x)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "def mean_squared_error(X, y, beta_array):\n    y_hat = np.dot(X, beta_array)\n    mean_sq_err = np.mean((y_true - y_hat)**2)\n    return mean_sq_err\n", "intent": "This function calculates the mean of the squared errors using a dot product between the `X` predictor matrix and the `beta_array`:\n"}
{"snippet": "scores = cross_val_score(knn, xs, y, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "Plot the cross-validated mean accuracy for each score. What is the best accuracy?\n"}
{"snippet": "alive = np.random.randint(1, size=len(Y_test))\ndead = np.random.randint(1, size=len(Y_test)) + 1\ncointoss = np.random.randint(2, size=len(Y_test))\nhalf = np.random.randint(1, size=len(Y_test)) + 0.5\nprint ('alive:','\\t\\t', round(metrics.log_loss(Y_test, alive), 9), '\\n'\n      'dead:', '\\t\\t', round(metrics.log_loss(Y_test, dead), 9), '\\n'\n      'Coin:', '\\t\\t', round(metrics.log_loss(Y_test, cointoss), 9), '\\n'\n      'half:', '\\t\\t', round(metrics.log_loss(Y_test, half), 9))\n", "intent": "Need review the R code: \n"}
{"snippet": "alive = np.random.randint(1, size=len(Y_test)) + 1\ndead = np.random.randint(1, size=len(Y_test))\ncointoss = np.random.randint(2, size=len(Y_test))\nhalf = np.random.randint(1, size=len(Y_test)) + 0.5\nprint ('alive:','\\t', round(metrics.log_loss(Y_test, alive, eps=0.99), 9), '\\n'\n      'dead:', '\\t', round(metrics.log_loss(Y_test, dead, eps=0.99), 9), '\\n'\n      'Coin:', '\\t', round(metrics.log_loss(Y_test, cointoss, eps=0.99), 9), '\\n'\n      'half:', '\\t', round(metrics.log_loss(Y_test, half), 9))\n", "intent": "Need review the R code: \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores1 = cross_val_score(clf1,X,y,cv=6)              \nav_score1 = sum(scores1)/len(scores1)            \nscores2 = cross_val_score(clf2,X,y,cv=6)             \nav_score2 = sum(scores2)/len(scores2)            \nprint(\"Average Cross Validation Score for clf1: {}\".format(av_score1))\nprint(\"Average Cross Validation Score for clf2: {}\".format(av_score2))\n", "intent": "Q9. Calculate the 6-fold cross validation score using the cross_val_score() function. Parameter 'cv' defines the number of folds.\n"}
{"snippet": "plot_fit(x_test, predict(x_test, w0, w1), x, y)\n", "intent": "Let's try plotting the result again.\n"}
{"snippet": "w0, w1 = w\nplot_fit(x_test, predict(x_test, w0, w1), x, y)\n", "intent": "Plotting this solution, as before:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nY_test_pred = ((np.dot(Phi_test, v_opt[1::]) + v_opt[0]) >= 0)*1 \naccuracy_score(Y_test, Y_test_pred)\n", "intent": "This time we should get a better result for the accuracy on the test set.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nnaive_score = accuracy_score(y_true, y_naive_prediction)\nprint(\"Accuracy score of naive model: {}\".format(naive_score))\n", "intent": "Checking accuracy score of naive model\n"}
{"snippet": "logistic_regression_auc = roc_auc_score(y_test, pred_prob)\nlogistic_regression_accuracy = accuracy_score(y_test, predicted_labels)\nprint(\"Accuracy score of Logistic Regression Model: {}\".format(logistic_regression_accuracy))\nprint(\"AUROC of Logistic Regression Model: {}\".format(logistic_regression_auc))\n", "intent": "Checking accuracy score of Logistic Regression Model\n"}
{"snippet": "test_data_pred_prob = log_regression_model.predict_proba(test)[:,1]\n", "intent": "Next, prediction for test set has to be made. To check how well model perform it needs to be submitted on Kaggle to get results.\n"}
{"snippet": "def evaluate(x, y, expr, x_value, y_value):\n    pass\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint(\"SUCCESS!\")\n", "intent": "Now try compiling and running a simple function:\n"}
{"snippet": "ss = ((fit.predict(bodyfat[subsets[3]]) - bodyfat['fat'])**2).sum()\n", "intent": "Using the model output, we can extract the residual sums of squares and use this to calculate AIC.\n"}
{"snippet": "model_selection.cross_val_score(f, wine.values, grape.values, cv=5,\n                                 scoring='f1_weighted')\n", "intent": "Furthermore, we can customize the scoring method by specifying the `scoring` parameter:\n"}
{"snippet": "from sklearn.metrics import explained_variance_score,mean_squared_error,r2_score\ndef performance_metrics(y_true,y_pred):\n    rmse = mean_squared_error(y_true,y_pred)\n    r2 = r2_score(y_true,y_pred)\n    explained_var_score = explained_variance_score(y_true,y_pred)\n    return rmse,r2,explained_var_score\n", "intent": "Q9. Complete the function below, which returns the above mentioned metrics. Import the necessary tools via scikit-learn\n"}
{"snippet": "predi = list(clasifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predi = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = logi.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predi = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predi = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = thesvm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "score = sa_model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "data = loadmat(os.path.join('Data', 'spamTest.mat'))\nXtest, ytest = data['Xtest'].astype(float), data['ytest'][:, 0]\nprint('Evaluating the trained Linear SVM on a test set ...')\np = utils.svmPredict(model, Xtest)\nprint('Test Accuracy: %.2f' % (np.mean(p == ytest) * 100))\n", "intent": "Execute the following cell to load the test set and compute the test accuracy.\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_tensors]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = regressor.predict(X_val)\nrmse,r2,explained_var_score = performance_metrics(y_val,y_pred)\nprint(\"Root mean squared error:{} \\nR2-score:{} \\nExplained variance score:{}\".format(rmse,r2,explained_var_score))\n", "intent": "Q11. Generate predictions from the validation set, and output the above-mentioned scores.\n"}
{"snippet": "Series(cross_val_score(logreg, features_array, target, cv=5, scoring='roc_auc')).describe()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "accuracy_score(pipe.predict(X_test), y_test)\n", "intent": "Functions that are applicable to estimators are also applicable to Pipelines. That is one of the most powerful premise of the pipeline after all. \n"}
{"snippet": "import seaborn as sns\nsns.heatmap(confusion_matrix(pipe.predict(X_test), y_test), annot=True,  fmt='');\n", "intent": "By now, we know that this score is not very meaningful, let's look at the confusion matrix!\n"}
{"snippet": "x.value = 3\ny.value = 4\nf.evaluate()\n", "intent": "And we can run this graph to compute $f$ at any point, for example $f(3, 4)$.\n"}
{"snippet": "x.value = DualNumber(3.0)\ny.value = DualNumber(4.0)\nf.evaluate()\n", "intent": "Now let's see if the dual numbers work with our toy computation framework:\n"}
{"snippet": "x.value = DualNumber(3.0, 1.0)  \ny.value = DualNumber(4.0)       \ndfdx = f.evaluate().eps\nx.value = DualNumber(3.0)       \ny.value = DualNumber(4.0, 1.0)  \ndfdy = f.evaluate().eps\n", "intent": "Yep, sure works. Now let's use this to compute the partial derivatives of $f$ with regards to $x$ and $y$ at x=3 and y=4:\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test, tuned_forest_predictions) \n", "intent": "Make predictions for the test data.\n"}
{"snippet": "tree_predictions = tree.predict(X_test) \n", "intent": "Make a prediction with trained model for test data.\n"}
{"snippet": "forest_predictions = rf.predict(X_test) \n", "intent": "Make a prediction for test data.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nscaled_X_val = None                  \ny_pred_dt = clf_dt.predict(scaled_X_val)\ny_pred_nb = clf_nb.predict(scaled_X_val)                      \nacc_dt = None                            \nacc_nb = None\nprint(\"The accuracy of Decision Tree: {} %\".format(acc_dt))\nprint(\"The accuracy of Gaussian Naive Bayes: {} %\".format(acc_nb))\n", "intent": "Q12. Using the accuracy_score function, determine the accuracy of the two classifiers.\n"}
{"snippet": "tscv = TimeSeriesSplit(n_splits=7)\nscores = cross_val_score(logit, X_train_sparse, y_train, cv=tscv, scoring='roc_auc')\nnp.mean(scores)\n", "intent": "0.92729257732863013\n"}
{"snippet": "test_pred = searchCV.predict_proba(X_test)[:,1]\n", "intent": "0.93372138833945972\n"}
{"snippet": "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\nprint(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\nprint(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\n", "intent": "Predict the class of a single flower.\n"}
{"snippet": "austen_sentence_vectors = vectorizer.transform(austen_test_sentences)   \nausten_sentence_classifications = classifier.predict(austen_sentence_vectors)   \nausten_sentence_classifications[:20]                                    \n", "intent": "    Write a script that prints Austen-like sentences written \n    by Melville, and Melville-like sentences written by Austen.\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2)) / (len(bos.PRICE - lm.predict(X))))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\nresnet_test_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % resnet_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lm.predict( X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "** Calculate the Mean Absolute Error, Mean Squared Error, and the Root Mean Squared Error.**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "**Using Grid object to predict the class !! **\n"}
{"snippet": "predict=dtc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred = dt.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = lg.predict(X_val)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "def evaluate(sess, X, Y):\n    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\n    print (sess.run(tf.reduce_mean(tf.cast(tf.equal (predicted, Y), tf.float32))))\n", "intent": "- The training function is also the same\n- For evaluation of accuracy, we need a slight change from the sigmoid version:\n"}
{"snippet": "def evaluate(sess, X, Y):\n    print(sess.run(inference([[55., 40.]])))\n    print(sess.run(inference([[50., 70.]])))\n    print(sess.run(inference([[90., 20.]])))\n    print(sess.run(inference([[90., 70.]])))\n", "intent": "We evaluate the resulting model:\n"}
{"snippet": "clf.predict(np.matrix([1, 2, 1, 1]))\n", "intent": "+ Can you modify the code to get a different result?\n"}
{"snippet": "print \"predicted outcome\", knn.predict(iris.data[:,2:])\nprint \"observed outcome\", iris.target\n", "intent": "+ Lets print out the raw outcome var vs predicted outcome\n"}
{"snippet": "print(classification_report(y_test,predict))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "(data['recommendation'] == gmm.predict(X)).value_counts()\n", "intent": "The clusters above are the assidgned political party found for the individual voters by the GMM model trained on the candidate data.\n"}
{"snippet": "print (sklearn.metrics.classification_report(y_test, y_pred))\n", "intent": "[Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n"}
{"snippet": "preds_test = model_sk.predict(X_test)\npreds_train = model_sk.predict(X_train)\n", "intent": "Predicting both train and test sets to evaluate model\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true.ravel() - y_pred.ravel()) / y_true.ravel())) * 100\n", "intent": "There is no MAPE implementation in sklearn (because this metric is undefined when real value is zero). Below one can find my own implementation\n"}
{"snippet": "def relative_error(grad, grad_num):\n    return np.sum((grad - grad_num) ** 2, axis=1) * 1. / np.sum(grad ** 2, axis=1)\n", "intent": "Plotting error curves\n"}
{"snippet": "y_pred_test = model_knn_sklearn.predict(X_test)\n", "intent": "Predict answers on the test set\n"}
{"snippet": "acc = accuracy_score(y_test, y_pred_test)\n", "intent": "Accuracy score with sklearn function\n"}
{"snippet": "y_pred_test = model_knn_weighted_sklearn.predict(X_test)\n", "intent": "Predict answers on the test set\n"}
{"snippet": "preds_test = model.predict(X_test)\npreds_train = model.predict(X_train)\n", "intent": "Predicting both train and test sets to evaluate model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kms.labels_))\nprint(classification_report(df['Cluster'],kms.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"----------------------------------------\\nMETRICS:\")\nprint(classification_report(y_test, preds_test))\n", "intent": " Confusion matrix, Precision, Recall, F1-Scores\n"}
{"snippet": "score_train = model_dense.evaluate(X_train.reshape((len(X_train), img_cols * img_rows)), y_train, verbose=0)\nscore_test = model_dense.evaluate(X_test.reshape((len(X_test), img_cols * img_rows)), y_test, verbose=0)\n", "intent": "Table to store the results of the experiments\n"}
{"snippet": "score_train = model_cnn.evaluate(X_train, y_train, verbose=0)\nscore_test = model_cnn.evaluate(X_test, y_test, verbose=0)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_dense.evaluate(X_train.reshape((-1, img_cols * img_rows)), y_train, verbose=0)\nscore_test = model_dense.evaluate(X_test.reshape((-1, img_cols * img_rows)), y_test, verbose=0)\n", "intent": "Table to store the results of the experiments\n"}
{"snippet": "score_train = model_column_onelayer.evaluate(X_train, y_train, verbose=0)\nscore_test = model_column_onelayer.evaluate(X_test, y_test, verbose=0)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_column_bidir.evaluate(X_train, y_train, verbose=0)\nscore_test = model_column_bidir.evaluate(X_test, y_test, verbose=0)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_pixel_bidir.evaluate(X_train.reshape(-1, img_cols * img_rows, 1), y_train, verbose=1)\nscore_test = model_pixel_bidir.evaluate(X_test.reshape(-1, img_cols * img_rows, 1), y_test, verbose=1)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_cnn_rnn_bidir.evaluate(X_train.reshape(-1, img_rows, img_cols, 1), y_train, verbose=1)\nscore_test = model_cnn_rnn_bidir.evaluate(X_test.reshape(-1, img_rows, img_cols, 1), y_test, verbose=1)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "preds_train = model.predict(X_train)\npreds_val = model.predict(X_val)\n", "intent": "And final accuracy of classification:\n"}
{"snippet": "predict=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "Below there are two functions that to plot the results and calculate different metrics.\nAlso I implement MAPE metric.\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def tv_loss_test(correct):\n    content_image = 'styles/tubingen.jpg'\n    image_size =  192\n    tv_weight = 2e-2\n    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.0001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "It looks like this classifier is worse, but perhaps more robust in general and generalizes better to new data.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "We can get test accuracy above `95%` after 12 epochs, but there is still a lot of margin for improvements via parameter tuning.\n"}
{"snippet": "print 'time_from_registration, %.3f' % roc_auc_score(df_all.label, df_all.time_from_registration)\nprint 'title_seen_cnt, %.3f' % roc_auc_score(df_all.label, df_all.title_seen_cnt)\n", "intent": "Let's check if some features give us a good baseline:\n"}
{"snippet": "predict = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "otherlabels = rf.predict(otherfeatures)\n", "intent": "If you have the features of some previously unseen objects, you can then predict their class using the Random Forest's ```predict``` method:\n"}
{"snippet": "predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(cd['Cluster'],kmeans.labels_))\nprint(classification_report(cd['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict default payment next month for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, clf.predict(X_test)))\n", "intent": "<div class=\"alert alert-success\">\n</div>\n"}
{"snippet": "scores = model_selection.cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(int(x))    \nprint(r2)\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "print(svc.predict([[150000, 40]]))\n", "intent": "Or just use predict for a given point:\n"}
{"snippet": "f1_score(test_label_y, predicted)\n", "intent": "<b>accuracy_score for bag of words with alpha 0.001  is 0.874</b>\n"}
{"snippet": "print(classification_report(y_test,predict))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "f1_score(test_label_y, predicted)\n", "intent": "<b>Accuracy for Multinomial NB with BOW is 0.897</b>\n"}
{"snippet": "print(\"precesion for MultinomialNB with BOW with alpha 1.701 is\",precision_score(test_label_y, predicted, average=\"macro\"))\nprint(\"Recall for MultinomialNB with BOW with alpha 1.701 is\",recall_score(test_label_y, predicted, average=\"macro\"))  \n", "intent": "<b>F1 score for Multinomial NB with BOW is 0.939</b>\n"}
{"snippet": "f1_score(test_label_y, predicted_tfidf)\n", "intent": "<b>accuracy_score for TF-iDF with alpha 0.001 is 0.890</b>\n"}
{"snippet": "forecast = model.predict(data_test_joined_2_regressor)\nforecast\n", "intent": "**Make a prediction**\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\nX_test = X\nprediction_result = knn.predict(X_test)\nprint(Y)\nprint(prediction_result)\nY == prediction_result\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\"\n- Can predict for multiple observations at once\n"}
{"snippet": "print(metrics.mean_absolute_error(y_true, y_pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "resnet50_pred = [np.argmax(resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\nresnet50_test_accuracy = 100*np.sum(np.array(resnet50_pred)==np.argmax(test_targets, axis=1))/len(resnet50_pred)\nprint('Test accuracy: %.4f%%' % resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict=svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "score <- evaluate(model, x = x_test, y = y_test)\nprint(score)\n", "intent": "After training we can get the final loss for the test set by using the evaluate() fucntion.\n"}
{"snippet": "x_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)\n", "intent": "After the model is fit, we can use it to make a prediction.\n"}
{"snippet": "x_input = array([[70,75], [80,85], [90,95]])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)\n", "intent": "The shape of the input for making a single prediction must be 1 sample, 3 time steps, and 3 features, or [1, 3, 3]\n"}
{"snippet": "def get_embeddings(filenames):\n  faces = [extract_face(f) for f in filenames]\n  samples = asarray(faces, 'float32')\n  samples = preprocess_input(samples, version=2)\n  model = VGGFace(model='resnet50', \n                  include_top=False, input_shape=(224, 224, 3),\n  pooling='avg')\n  yhat = model.predict(samples) \n  return yhat\n", "intent": "Predicting face embeddings for a list of photographs.\n"}
{"snippet": "bg_fg_labels = model.predict(region_means)\nprint(bg_fg_labels)\n", "intent": "We see K-means algorithm found two clusters centered around 160 and the other clustered centered around 57. \n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint('loss:', loss)\nprint('accuracy:', accuracy)\n", "intent": "Now we can evaluate the model on the test data to get a sense of how we did.\n"}
{"snippet": "img, x = get_image(\"../assets/kitty.jpg\")\npredictions = model.predict(x)\nimshow(img)\nfor pred in decode_predictions(predictions)[0]:\n    print(\"predicted %s with probability %0.3f\" % (pred[1], pred[2]))\n", "intent": "We load an image into memory, convert it into an input vector, and see the model's top 5 predictions for it.\n"}
{"snippet": "img, x = get_image('../data/101_ObjectCategories/airplanes/image_0003.jpg')\nprobabilities = model_new.predict([x])\nprint(probabilities)\n", "intent": "To predict a new image, simply run the following code to get the probabilities for each class.\n"}
{"snippet": "from sklearn.metrics import classification_report\npred = kmeans.labels_\ny_test = new_df['Cluster']\nprint(classification_report(y_test,pred))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "metrics.adjusted_rand_score(clustering_KMeans.labels_,clustering_ward.labels_)  \n", "intent": "L'Adjusted Random Index est un coefficient qui permet de comparer deux clusterings : \nhttp://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "prediction = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predict(trX)\n", "intent": "Theano's \"predict\" output\n"}
{"snippet": "pred = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "tfidf_pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$\n"}
{"snippet": "test_pred = lcv.predict_proba(X_test_sparse)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "predictions = lr.predict(new_data_reindexed)\npredictions = np.maximum(predictions, 0)\npredictions.astype(int) \n", "intent": "All good! Our new data has the same format as our training DataFrame, and we can make predictions.\n"}
{"snippet": "pred_test_lda = lda_clf.predict(X_test)\nprint('Prediction accuracy for the test dataset')\nprint('{:.2%}'.format(metrics.accuracy_score(y_test, pred_test_lda)))\n", "intent": "To verify that over model was not overfitted to the training dataset, let us evaluate the classifier's accuracy on the test dataset:\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "predict(2016, 'Kansas', 'Connecticut') \n", "intent": "Lets follow kansas through the tournament\n"}
{"snippet": "predict(2016, 'Villanova', 'Oklahoma') \n", "intent": "Lets follow Villanova\n"}
{"snippet": "predict(1984, 'California', 'Hawaii')\n", "intent": "Future and past prediction will not work and the predict method will use data from the closest year where there is data available instead.\n"}
{"snippet": "predict(2017, 'Hawaii', 'California')\n", "intent": "Future prediction will not work and the predict method will use data from the most recent year where there is data available instead.\n"}
{"snippet": "from timeit import timeit\ndef GLC_time_evaluate(data, time, rmse):\n    model = []\n    time.append(timeit(lambda : GLC_linear_regression(data, model), number=1))\n    rmse.append(model[0].get('training_rmse'))\n", "intent": "Let's define an additional function for each package that will handle timing and evaluating our models.\n"}
{"snippet": "np_time = []\nnp_rmse = []\nrows_range = range(5000, 90001, 2500)\nfor n_rows in rows_range:\n    sf = data.head(n_rows)\n    SKL_time_evaluate(sf, np_time, np_rmse)\n", "intent": "Time scikit-learn backed by SFrame on the first 90K rows of data and save the results.\n"}
{"snippet": "n = len(X_test_pca)\np = len(X_test_pca.columns)\nstart2=time()\nt_y=t_knn.predict(X_test_pca)\nend2=time()\nRSS = sum((y_test-t_y)**2)\nt_aic=2*p+n*math.log(RSS/n)\nt_bic = n*math.log(RSS/n)+p*math.log(n)\nt_r2=adj_r2_score(knn,X_test_pca,y_test)\n", "intent": "Evaluate the model with metrics\n"}
{"snippet": "n = len(X_test_pca)\np = len(X_test_pca.columns)\nstart2=time()\np_y=ada_neigh.predict(X_test_pca)\nend2=time()\nRSS = sum((y_test-p_y)**2)\nAIC=2*p+n*np.log(RSS/n)\nBIC = n*np.log(RSS/n)+p*np.log(n)\nR2=adj_r2_score(ada_neigh,X_test_pca,y_test)\n", "intent": "Evaluate the model with metrics\n"}
{"snippet": "knn.predict_proba([[3, 5, 4, 2],])\n", "intent": "You can also do probabilistic predictions:\n"}
{"snippet": "model = VGG19(weights='imagenet', include_top=False)\nvgg19_feature_train = model.predict(x_train)\nvgg19_feature_test = model.predict(x_test)\n", "intent": "Build VGG19 and extract the last-layer features for both the training set and the test set.\n"}
{"snippet": "model2 = ResNet50(weights='imagenet', include_top=False)\nresnet_feature_train = model2.predict(x_train)\nresnet_feature_test = model2.predict(x_test)\n", "intent": "Build ResNet50 and extract the last-layer features for both the training set and the test set.\n"}
{"snippet": "def train_and_test(n):\n    random.seed(n)\n    return predict(df_train, df_test, model_type='chained_rf', output_prediction=True, seed=n).f_label_1_predicted\nn = 1000\nr = random.randint(1, 100000000)\nresult = parallelize(range(r, r+n), train_and_test)\npredicted_test = sum(result)/n\npredicted_test = df[['date']].join(predicted_test).dropna()\npredicted_test['date'] = (predicted_test['date'] + pd.DateOffset(1)).map(lambda x: '%s/%s/%s' % (x.date().day, x.date().month, x.date().year))\npredicted_test.columns = ['date', 'label']\n", "intent": "Below are the predictions we made on the test set. The results were uploaded to Kaggle and got a 52.9% AUC score.\n"}
{"snippet": "roc_auc_score(y_test, lm.predict(X_test))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "print metrics.classification_report(C.TrueAdmit, C.RF1preds)\n", "intent": "Random Forest wit `n_classifiers = 1` has pretty different precision than with the default which is 10\n"}
{"snippet": "tr_resolver = neural_net.evaluate(tr_word_lstm, tr_attn_layer, tr_scorer, all_words, all_markables, coref_features.minimal_features)\n", "intent": "Let's evaluate on the entire dataset.\n"}
{"snippet": "tags = model.predict(sentence)\nprint (tags[0:3])\n", "intent": "- we provide the `predict()` function that returns the set of tags obtained for the specific input by the model.\n"}
{"snippet": "shutil.rmtree('babyweight_trained_dnn', ignore_errors = True) \ntrain_and_evaluate('babyweight_trained_dnn')\n", "intent": "Finally, we train the model!\n"}
{"snippet": "shutil.rmtree('babyweight_trained_wd', ignore_errors = True) \ntrain_and_evaluate('babyweight_trained_wd')\n", "intent": "Finally, we train the model!\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, ypred)\n", "intent": "We can check our classification accuracy by comparing the true values of the test set to the predictions:\n"}
{"snippet": "print \"Class predictions according to Sklearn:\" \nprint sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(y, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(y, labels))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, y_test)\nprint('loss:', loss)\nprint('accuracy:', accuracy)\n", "intent": "We can then evaluate the model much like we would in sklearn: \n"}
{"snippet": "print classification_report(insults_test[\"Insult\"], predicted, target_names=['Insult', \"Neutral\"])\n", "intent": "Check the classification report\n"}
{"snippet": "wing_size_predictions = insects_regression.predict(X_insects)\n", "intent": "3\\. Use the `predict` method to compute predicted values your data (could be the data you used to train the model, or another dataset entirely.\n"}
{"snippet": "wells_predictions = wells_regression.predict(X_wells)\nprint(wells_predictions[:10])\n", "intent": "Now that the regression has been fir, we can use the `predict` method to forecast whether our model thinks a family will switch wells.\n"}
{"snippet": "insects_predictions = p.predict(X_insects)\n", "intent": "Since the last object in our pipeline is a `LinearRegression`, the pipeline has a `predict` method.\n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "sklearn.metrics.mean_squared_error(bos.PRICE, lm.predict(X))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint(predicted)\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "score = model.evaluate(x_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test)\nprint(\"\\n Testing Accuracy:\", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "cross_val_score(model_lr,features,target,cv=10).mean()\n", "intent": "**3) Implement cross-validation for your logistic regression model. Select the number of folds. Explain your choice.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,model.predict(fit_test))\nprint cm,'\\n'\nprint \"Precision:\",float(cm[1,1])/(cm[0,1]+cm[1,1])\nprint \"Recall:\",float(cm[1,1])/(cm[1,0]+cm[1,1]),'\\n'\n", "intent": "**4) Display the confusion matrix, classification report, and AUC.**\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test,predicted)\n", "intent": "This above returns the same score we calculated earlier for the model.\n"}
{"snippet": "y_pred = clf.predict(cnn_codes_test)\nprint(\"For params: %s we got results: acc: %.2f, F1: %.2f\" % \n      (str(clf.best_params_), acc(y_test, y_pred), f1_score(y_test, y_pred, average='micro')))\n", "intent": "And finally, I found the best parameters and got results:\n"}
{"snippet": "results = []\nfor model in models:\n    y_pred = model['model'].predict(cnn_codes_test)\n    print(\"Testing model with parameters %s is done\" % str(model['params']))\n    results.append(y_pred)\n", "intent": "The difference between their accuracies is small, so I decided to use all models. \n"}
{"snippet": "score = mod.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ndef print_mean_squared_error():\n    mse =  \n    print(\"Mean squared error: {:.2f}\".format(mse))\n", "intent": "Write a function that:\n* Takes y_test en y_pred as parameters.\n* Calculates the mean squared error\n* Prints the result.\n"}
{"snippet": "print_mean_squared_error(y_test, y_pred)\n", "intent": "Let's calculate the mean squared error for our model.\n"}
{"snippet": "h=.1\nx_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\ny_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\nxx, yy = numpy.meshgrid(np.arange(x_min, x_max, h), numpy.arange(y_min, y_max, h))\nZ = logclf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n", "intent": "* First we plot the decision boundary, followed by training and testing data points.\n"}
{"snippet": "print_r2_score(y_test, y_pred)\n", "intent": "Now we calculate the $R^2$ score for our trained model.\n"}
{"snippet": "with np.load('data/mystery_data_new.npz') as data:\n    popularity_new = data['popularity_new']\nprint \"Predicted L2 Error:\", l2_error(popularity_new, predicted_popularity_new)\n", "intent": "At the end of the year, we tally up the popularity numbers for each celeb and check how well we did on our predictions.\n"}
{"snippet": "print(classification_report(y_test,y_pred))\n", "intent": "It looks like our KNN model is already pretty good. Let's check the `classification_report`.\n"}
{"snippet": "from collections import Counter\nprint(Counter(y_clean))\ny_clean.shape[0]\nbase_line = np.ones(y_clean.shape[0])*4\nprint('Estimated baseline performance:  ',accuracy_score(y_clean, base_line))\n", "intent": "Estimate the baseline performance.\n"}
{"snippet": "COD = r2_score(y,cv_predict_multi)\nMAE = mean_absolute_error(y,cv_predict_multi)\nRMSE = np.sqrt(mean_squared_error(y, cv_predict_multi))\nCC = np.corrcoef(y,cv_predict_multi)\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "auto_numeric_log = np.log(auto_numeric)\nX_multi_log = auto_numeric_log.drop(['price'],axis=1).values\ny_log = auto_numeric_log['price']\ncv_predict_multi_log = cross_val_predict(new_lr, X_multi_log, y_log, cv = kf)\nCOD = r2_score(y_log,cv_predict_multi_log)\nMAE = mean_absolute_error(y_log,cv_predict_multi_log)\nRMSE = np.sqrt(mean_squared_error(y_transf, cv_predict_multi_log))\nCC = np.corrcoef(y_log,cv_predict_multi_log)\n", "intent": "Now re-build a Linear Regression model on the transformed dataset and report the R^2, RMSE, MAE and CC metrics.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\ndef get_silhouette_score(data, model):\n    pass\nprint \"Silhouette Score for KMeans with 5 clusters = %lf\" % 0.0\nprint \"Silhouette Score for KMeans with 25 clusters = %lf \" % 0.0\nprint \"Silhouette Score for KMeans with 50 clusters = %lf \" % 0.0\n", "intent": "**c.** Calculate the Silhouette Score using 500 sample points for all the kmeans models.\n"}
{"snippet": "print fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response)\n", "intent": "**b.** Now that you've added the categorical data, let's see how it works with a linear model!\n"}
{"snippet": "preds = model.predict(test_sample)\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    pass \n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "h=.1\nx_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\ny_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logclf.predict(np.c_[xx.ravel(), yy.ravel()])\nprint yy\nprint xx.ravel()\n", "intent": "* First we plot the decision boundary, followed by training and testing data points.\n"}
{"snippet": "trainPredict = model.predict(X_train)[:,0]\ntestPredict = model.predict(X_test)[:,0]\nfrom sklearn.metrics import mean_squared_error\nprint \"MSE train %.3f\"%mean_squared_error(trainPredict, y_train)\nprint \"MSE test  %.3f\"%mean_squared_error(testPredict, y_test)\n", "intent": "Obtenemos predicciones para los datos de entrenamiento y para los de test.\n"}
{"snippet": "preds =  model.predict(X_test)[:,0]\n", "intent": "predict test prices\n"}
{"snippet": "predict = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print 'predicted:', spam_detector.predict(tfidf4)[0]\nprint 'expected:', messages.label[3]\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "all_predictions = spam_detector.predict(messages_tfidf)\nprint all_predictions\n", "intent": "Hooray! You can try it with your own texts, too.\nA natural question is to ask, how many messages do we classify correctly overall?\n"}
{"snippet": "print classification_report(messages['label'], all_predictions)\n", "intent": "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:\n"}
{"snippet": "predictions = nb_detector.predict(msg_test)\nprint confusion_matrix(label_test, predictions)\nprint classification_report(label_test, predictions)\n", "intent": "And overall scores on the test set, the one we haven't used at all during training:\n"}
{"snippet": "print svm_detector.predict([\"Hi mom, how are you?\"])[0]\nprint svm_detector.predict([\"WINNER! Credit for free!\"])[0]\n", "intent": "So apparently, linear kernel with `C=1` is the best parameter combination.\nSanity check again:\n"}
{"snippet": "print 'before:', svm_detector.predict([message4])[0]\nprint 'after:', svm_detector_reloaded.predict([message4])[0]\n", "intent": "The loaded result is an object that behaves identically to the original:\n"}
{"snippet": "def ols_predict(x_input, y_intercept, coefficients):\n    y_intercept + (coefficient)(Xvar)\n    return \n", "intent": "Below, write the function that would use this output to predict new data elements (finish the return statement).\n"}
{"snippet": "cm = sklearn.metrics.confusion_matrix(y_train,clf.predict(X_train))\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "r2 = r2_score(auto_numeric['price'], predicted3)\nrmse = np.sqrt(mean_squared_error(auto_numeric['price'], predicted3))\nmae = mean_absolute_error(auto_numeric['price'], predicted3) \ncoeff = np.corrcoef(auto_numeric['price'],predicted3)[0,1]\nprint(\"r2: {}\\tRMSE: {}\\tMAE: {}\\tCC: {}\".format(r2, rmse, mae, coeff))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "mymodel_predictions = [np.argmax(mymodel.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(mymodel_predictions)==np.argmax(test_targets, axis=1))/len(mymodel_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "fpred = forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,fpred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = log.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "preds = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "h=.1\nx_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\ny_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logclf.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "* First we plot the decision boundary, followed by training and testing data points.\n"}
{"snippet": "split = len(null_collapsed_df)/2\ntrain = null_collapsed_df[:split]\ntest  = null_collapsed_df[split:]\ntest['Prediction'] = train['Expected'].median()\nbasic_trainer.evaluate(test)\n", "intent": "The Null DF has no usable input data, so we'll simply use the median Expected value as the prediction. It doesn't count on the leaderboard anyway.\n"}
{"snippet": "def KM_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:]   -   centroids[assignment[i]])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "def km_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:] - centroids[assignment[i]])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "def km_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:] - centroids[int(assignment[i])])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "def km_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:]   -   centroids[assignment[i]])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "predict=model.predict(data[['Population','Violent_crime']])\nprint(\"Mean squared error: %.2f\"\n      % np.mean((predict - data['Property_crime']) ** 2))\n", "intent": "<b>Calculate mean square error:</b>\n"}
{"snippet": "model1.predict([10000,750])\n", "intent": "A person with FICO score 750 and want to request a loan amount $10,000. From the above model, we can know the interest rate he need to pay monthly.\n"}
{"snippet": "model.predict([[10000,750]])\n", "intent": "The f-score for \"1\" prediction is lower than 50%. There are many green cross (differences between predicted result and real result) in above plot. \n"}
{"snippet": "data['predict']=model.predict(data[['Sepal_length', 'Sepal_width', 'Petal_length', 'Petal_width']])\ndata['diff'] = np.where(data['spec_num'] == data['predict'], 1, 0)\ndata['diff'].value_counts()\ndata['diff'].unique()\npredict = data[data['diff'] == 0]\n", "intent": "<h2>Run cross validation the logistic model</h2>\n"}
{"snippet": "y_pred = clf.predict(X_test)\ndef print_cluster(images, y_pred, cluster_number):\n    images = images[y_pred==cluster_number]\n    y_pred = y_pred[y_pred==cluster_number]\n    print_digits(images, y_pred, max_n=10)\nfor i in range(10):\n     print_cluster(images_test, y_pred, i)\n", "intent": "Predict and show predicted clusters.\n"}
{"snippet": "r2_score(y_test, load_pred)\n", "intent": "**1.4 (5 points)** To validate the regression, find $R^2$ of your predction (Your answer needs to be rounded to 2-decimal places).\n"}
{"snippet": "incept_predictions = [np.argmax(incept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(incept_predictions)==np.argmax(test_targets, axis=1))/len(incept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(pipeline, X_train, y_train)\n", "intent": "Cross-validation with a pipeline\n---------------------------------\n"}
{"snippet": "def get_accuracy(model, datas, labels):\n    preds = np.array([np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in datas])\n    accuracy = 100*np.sum(preds==np.argmax(labels, axis=1))/len(preds)\n    return accuracy\ndef get_report(trained_model):\n    print('Train accuracy: %.4f%%' % get_accuracy(trained_model, train_tensors, train_targets))\n    print('Valid accuracy: %.4f%%' % get_accuracy(trained_model, valid_tensors, valid_targets))\n    print('Test accuracy:  %.4f%%' % get_accuracy(trained_model, test_tensors, test_targets))\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\nclassification_report(y_test, y_pred)\n", "intent": "Metrics for each class: precision, recall, f1-score and support. \nhttps://en.wikipedia.org/wiki/Precision_and_recall\n"}
{"snippet": "y_pred = classifier.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "Metrics for each class: precision, recall, f1-score and support. \nhttps://en.wikipedia.org/wiki/Precision_and_recall\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_Xception = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Xception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preds=rtf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn import metrics\nprint \"Addjusted rand score:{:.2}\".format(metrics.adjusted_rand_score(y_test, y_pred))\nprint \"Homogeneity score:{:.2} \".format(metrics.homogeneity_score(y_test, y_pred)) \nprint \"Completeness score: {:.2} \".format(metrics.completeness_score(y_test, y_pred))\nprint \"Confusion matrix\"\nprint metrics.confusion_matrix(y_test, y_pred)\n", "intent": "Show different performance metrics, compared with \"original\" clusters (using the knowb number class)\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix \nprint(classification_report(data['Cluster'],kmeans.labels_))\nprint(confusion_matrix(data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predict=lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predict))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "preds=nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "preds=pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "preds=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "glass[\"y_pred\"] = linreg.predict(X)\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "linreg.predict(2)\n", "intent": "- Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object.\n"}
{"snippet": "log_preds = learn.predict(is_test=True)\npreds = np.argmax(log_preds, axis=1) \nprobs = np.exp(log_preds[:,1])\n", "intent": "**Without using TTA (run either of them)**\n"}
{"snippet": "metrics.mean_squared_error(y, np.exp(y_pred_log))\n", "intent": "Not a fair comparison! MSE for the second model is in log-space.\n"}
{"snippet": "X_test = test.loc[:, feature_cols]\ny_test = test.loc[:, 'price']\ny_pred = treereg.predict(X_test)\ny_pred\n", "intent": "**Bonus**: Use the fitted model to check your answers.\n"}
{"snippet": "from sklearn.model_selection import KFold\nkf = KFold(5, shuffle=True)\nnp.mean(-cross_val_score(dtr, X, y, cv=kf, scoring='neg_mean_squared_error'))\n", "intent": "- Train your model and calculates its MSE in five-fold cross-validation.\n"}
{"snippet": "silhouette_score(iris_scaled, km.labels_)\n", "intent": "Here we get the average silhouette score for all points in our dataset:\n"}
{"snippet": "silhouette_score(iris_features_df_scaled, km.labels_)\n", "intent": "Here we get the average silhouette score for all points in our dataset:\n"}
{"snippet": "X_test_scaled = scaler.transform(X_test)\ntest_preds = knc.predict(X_test_scaled)\ntest_preds\n", "intent": "Now let's scale the whole validation dataset and predict the whole thing!\n"}
{"snippet": "train_preds = knc.predict(X_train_scaled)\ntrain_preds[:10]\n", "intent": "Now, let's see how well it learned the training set.\n"}
{"snippet": "test_preds = knr.predict(X_test_scaled)\ntest_preds[:10]\n", "intent": "Let's use our trained KNeighborsRegressor to predict the value for our scaled datapoint:\n"}
{"snippet": "train_preds = kn.predict(X_ts)\n", "intent": "Now, let's see how well it learned the training set.\n"}
{"snippet": "confusion = np.zeros((len(classes), len(classes)))\nfor ii, train_class in enumerate(classes):\n    for jj in range(ii, len(classes)):\n        confusion[ii, jj] = roc_auc_score(y == train_class, y_pred[:, jj])\n        confusion[jj, ii] = confusion[ii, jj]\n", "intent": "Compute confusion matrix using ROC-AUC\n"}
{"snippet": "train_preds = knc.predict(X_train_scaled)\n", "intent": "Now, let's see how well it learned the training set.\n"}
{"snippet": "test_preds = knr.predict(X_test_scaled)\ntest_preds[:5]\n", "intent": "Let's use our trained KNeighborsRegressor to predict the value for our scaled datapoint:\n"}
{"snippet": "y_pred_class = knn.predict(X_sc)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "kfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(tree, iris.data, iris.target, cv=kfold)))\n", "intent": "This way, we can verify that it is indeed a really bad idea to use three-fold (nonstratified) cross-validation on the iris dataset:\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(tree, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\n", "intent": "The following code splits the dataset into 50% training set and 50% test set for 10 iterations:\n"}
{"snippet": "xval_chance = (-1 * logreg.intercept_[0])/logreg.coef_[0][0]\nprint xval_chance*logreg.coef_[0][0] + logreg.intercept_\nprint xval_chance\nlogreg.predict_proba([[xval_chance]])\n", "intent": "Since we only have $\\beta_0$ and $\\beta_1$,\n$$ \\beta_0 + \\beta_1x = 0$$\n$$ -\\beta_0  = \\beta_1x $$\n$$ x = \\frac{-\\beta_0}{\\beta_1} $$\n"}
{"snippet": "def accuracy_score(actual, predicted):\n    return np.array(actual == predicted).mean()\n", "intent": "Run this code block to define an accuracy function.\n"}
{"snippet": "accuracy_score(predictions, titanic_target[:5])\n", "intent": "Run this block of code to test the accuracy of the prediction.\n"}
{"snippet": "test_ypred = gd_best.predict(test_x)\nprint ('Gradient Boost Test MSE:', mean_squared_error(test_ypred, test_y))\nprint ('Gradient Boost Test R2:',r2_score(test_ypred, test_y))\n", "intent": "Report the final MSE and R^2.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "clf.predict([[1,1,1,1]])\n", "intent": "We can use the decision tree for prediction.\nFirst lets predict the values of a feature vector containing [1,1,1,1]:\n"}
{"snippet": "final_pred = lin_model.predict(X_test)\nprint('Test set rmse: ', rmse(final_pred, y_test))\n", "intent": "Finally, select one final model to make predictions for your test set. This is often the model that performed best on the validation data.\n"}
{"snippet": "svm_test_pred = svm_model.predict(X_test)\nnp.mean(svm_test_pred == y_test)\n", "intent": "Choose your best classifier and use it to predict on the test set. Report the mean accuracy and confusion matrix. \n"}
{"snippet": "lin_val_pred = lin_model.predict(X_val_my_feats)\nsecond_train_error = rmse(lin_pred, y_train)\nsecond_val_error = rmse(lin_val_pred, y_val)\nprint(\"Training RMSE:\", second_train_error)\nprint(\"Test RMSE:\", second_val_error)\n", "intent": "**Question 1.5:** What is the rmse for both the prediction of X_train and X_val?\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Error: %.2f%%\" % (100-scores[1]*100))\n", "intent": "Let's evaluate our model on the test dataset.\n"}
{"snippet": "y_pred_lstm = lstm_model.predict(XTest_batch)\nprint(yTest.shape)\nprint(y_pred_lstm.shape)\nprint(\"MAE: {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\nprint(\"MSE: {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n", "intent": "Generate predictions and check the mean squared error!\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred)\n", "intent": "There is a special `classification_report` function that gives some of the main performance metrics:\n"}
{"snippet": "print regr.predict(bill_gates['sqft_living'].reshape(-1, 1))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Bill_gates%27_house.jpg/2560px-Bill_gates%27_house.jpg\">\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse_train = mean_squared_error(yhat_train, y_train)\nmse_valid = mean_squared_error(yhat_valid, y_valid)\nprint(\"Training MSE:   {:.3f}\".format(mse_train))\nprint(\"Validation MSE: {:.3f}\".format(mse_valid))\n", "intent": "Finally, we can use the `mean_square_error` method to compute the MSE of the predictions.  \n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "yhat_train = deg9regpipe.predict(X_train)\nyhat_valid = deg9regpipe.predict(X_valid)\nmse_train = mean_squared_error(yhat_train, y_train)\nmse_valid = mean_squared_error(yhat_valid, y_valid)\nprint(\"Training MSE:   {:.3f}\".format(mse_train))\nprint(\"Validation MSE: {:.3f}\".format(mse_valid))\nxplot = np.linspace(-60,60,100).reshape(-1,1)\nyplot = deg9regpipe.predict(xplot) \ndam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])\n", "intent": "Again, we'll fit the model, print errors, and make a plot. How do the resulting errors and plot compare to the linear model? \n"}
{"snippet": "predictions_train = clf.predict(df_train[features])\nprobs_train = clf.predict_proba(df_train[features])\ndisplay(predictions_train)\n", "intent": "    We are able to achive more that 99% accuracy for TRAIN and ~98% accuracy for TEST VALIDATION\n"}
{"snippet": "print(iris_model.predict(iris_features_test))\nprint(iris_target_test)\n", "intent": "Compare predictions with actual results\n"}
{"snippet": "iris_model.predict_proba(iris_features_test)\n", "intent": "Predict probabilities\n"}
{"snippet": "validation_data = np.load('features_validation.npy')\nval_pred_class = model.predict_classes(validation_data,verbose=0) \nprint('Accuracy on validation set: ',np.mean(val_pred_class.ravel()==val_labels)*100,'%')\nprint('\\nVal loss & val_acc')\nprint(model.evaluate(validation_data,val_labels,verbose=0))\n", "intent": "<a id='sec5'></a>\n___\n"}
{"snippet": "y_predict = pipe.predict(x_test)\n", "intent": "Now we can see how how good is our fit by testing the model on data it hasn't seen yet. \n"}
{"snippet": "print (\"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2))\nprint (\"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2))\n", "intent": "Now, calculate the mean squared error using just the test data and compare to mean squared from using all the data to fit the model. \n"}
{"snippet": "print (np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "The residual sum of squares: \n"}
{"snippet": "print (np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "Mean squared error: \n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = knn.predict(X1)\ny_true = df_Target.values\n", "intent": "Build Confusion Matrix\n"}
{"snippet": "metrics.silhouette_score(dn, labels, metric='euclidean')\n", "intent": "Find the Silhoutte Score and plot\n"}
{"snippet": "metrics.silhouette_score(newDF, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score to measure your analysis\n"}
{"snippet": "metrics.accuracy_score(labels, y)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "metrics.silhouette_score(y, labels, metric=\"euclidean\")\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print metrics.classification_report(y,labels)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "def vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n", "intent": "We will also need to build our intermediate layer taking into account the loss from both the regular learner and the mean and variance vectors\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: {:0.2f}'.format(accuracy_score(y_test, y_pred)))\n", "intent": "[sklearn.metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_Model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "tree_predictions = tree.predict(X_test) \n", "intent": "Make a prediction with the trained model on the test data.\n"}
{"snippet": "pred = model.predict(np.sign(data[cols]))\npred[:15]\n", "intent": "In the prediction, a `+1` means a positive return is expected and a `-1` means a negative return is expected.\n"}
{"snippet": "y_pred = treereg.predict(X_test)\ny_pred\n", "intent": "<a id=\"testing\"></a>\n---\nTesting model trained on training data on testing split\n"}
{"snippet": "bikes.loc[:,'predictions'] = lr_temp.predict(X)\nbikes\n", "intent": "- Store `lr_all`'s fitted values in a new `predictions` column of the `bikes` DataFrame.\n"}
{"snippet": "glass.loc[:,'y_pred'] = linreg.predict(X)\nglass\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1] \ny_pred = logit_pred_proba > .5\nmetrics.confusion_matrix(y_test, y_pred)\n", "intent": "-- it has no predictive power over a null model that just predicts 0 every time\n"}
{"snippet": "X_test = test.loc[:, feature_cols]\ntreereg.predict(X_test)\n", "intent": "**Bonus**: Use the fitted model to check your answers.\n"}
{"snippet": "from sklearn.metrics import r2_score\ny_pred_train = model.predict(X_train)\nprint ('r squared on train data {}%'.format(round(r2_score(y_train, y_pred_train)*100,2)))\ny_pred = model.predict(X_test)\n", "intent": "R^2 (coefficient of determination) regression score function.\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_predict,y_test)))\n", "intent": "from sklearn import metrics\n"}
{"snippet": "S = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nS\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "cv_scores = cross_val_score(rf, X_train, y_train, cv=3)\n", "intent": "Perfrom cross-validation.\n"}
{"snippet": "best = clf.best_estimator_\nprint 'Best estimator: ', best\nscores = cross_validation.cross_val_score(best, X_test, y_test, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Then we can check the accuracy on the **Test Set**:\n"}
{"snippet": "dt_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_predict = mod.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = logmod.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "expectedF = Y_test\npredictedF = classifierForest.predict(X_test)\npredictedS = classifierSVC.predict(X_test)\nprint(expectedF)\nprint(predictedF)\nprint(predictedS)\n", "intent": "Let's test how good the system is doing\n"}
{"snippet": "y_predF = classifierForest.predict(X_real)\ny_predS = classifierSVC.predict(X_real)\n", "intent": "Then we make the predictions with both classifiers\n"}
{"snippet": "decoded_imgs = autoencoder.predict(x_test)\n", "intent": "Epoch 50/50\n60000/60000 [==============================] - 4s - loss: 0.0958 - val_loss: 0.0942\n"}
{"snippet": "theta_min = np.matrix(result[0])\npredictions = predict(theta_min, X)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint ('accuracy = {0}%'.format(accuracy))\n", "intent": "We can then use this function to score the training accuracy of our classifier.\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint(\"accuracy = {}%\".format(accuracy))\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "forest_predictions = rf.predict(X_test) \n", "intent": "Make predictions for the test data.\n"}
{"snippet": "w1 = 0\nw2 = 0\nw3 = 0\nw4 = 0\ndef mlpredict(dp):\n    return w1 + w2*dp['maxtemp'] + w3*dp['mintemp'] + w4*dp['maxvib']\n", "intent": "Note: Regression, in this context means that the output is a continuous dependent variable.\n"}
{"snippet": "mse = sklearn.metrics.mean_squared_error(Y_test, Y_pred)\nprint(mse)\n", "intent": "Falta adicionar o r_squared\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(metrics.confusion_matrix(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the training set\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(metrics.confusion_matrix(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the tree on the validation dataset\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **training set**\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "y_pred = my_tuned_tree.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the model on a stratified test set\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the model on the **validation set**\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test,tuned_forest_predictions)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "accuracy = metrics.accuracy_score(Y_test, y_hat)\nprint(\"Accuracy:-->\" ,accuracy)\n", "intent": "Cross_val_score were calculated on fewer examples so might have small scores compare to simple holdout strategy evaluation experiment. \n"}
{"snippet": "y_hat = my_tuned_model.predict(X_test)\naccuracy = metrics.accuracy_score(Y_test, y_hat) \nprint(accuracy)\n", "intent": "<b> Note:</b> I am using test.csv dataset (10000 examples) as a hold out dataset to evaluate the tuned model.\n"}
{"snippet": "print(\"****** Test Data ********\")\ny_pred = my_tuned_model.predict(np.asfarray(X_test))\nprint(\"Accuracy : - \" , metrics.accuracy_score(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\ndisplay(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Evaluate the model on a test dataset\n"}
{"snippet": "scores = model.evaluate(x_test, y_test_wide, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\nscores\n", "intent": "Score some test data\n"}
{"snippet": "x_unseen = 0.78\nmodel.predict(x_unseen)\n", "intent": "This model can now be use to make predictions for *y* given new values of *x*:\n"}
{"snippet": "bills = np.arange( 10, 70, 5 )\nfor bill in bills:\n    predict_tip = model.predict(bill)\n    print(\"Predicted tip for meal costing %.2f = %.2f\" % ( bill,  predict_tip ) )\n", "intent": "We can make predictions from this model:\n"}
{"snippet": "test_x = x[0:5]\nmodel.predict(test_x)\n", "intent": "Let's try to predict the first five values of the original data (note: normally we would use a separate test dataset in a real evaluation).\n"}
{"snippet": "budgets = np.arange( 0, 400, 50 )\nfor spend in budgets:\n    predict_sales = model.predict(spend)\n    print(\"Predicted sales for TV advertising spend of %.2f = %.2f\" % ( spend,  predict_sales ) )\n", "intent": "We can now use this to make predictions\n"}
{"snippet": "test_x = x[0:1]\nprint(test_x)\nprint(\"Predicted Sales = %.2f\" % model.predict(test_x))\nprint(\"Actual Sales = %.2f\" % df[\"Sales\"].iloc[0])\n", "intent": "Again we can make predictions for sales based on new values for the 3 input features:\n"}
{"snippet": "print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, linreg.predict(X_train_scaled)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, linreg.predict(X_holdout_scaled)))\n", "intent": "**<font color='red'>Question 1:</font> What are mean squared errors of model predictions on train and holdout sets?**\n"}
{"snippet": "xinput = np.array([[3, 5, 4, 2], [3, 5, 2, 2]])\npred_class_numbers = knn.predict(xinput)\nprint( iris.target_names[pred_class_numbers] )\n", "intent": "We can also predict for multiple input examples at once:\n"}
{"snippet": "predicted = model.predict(dataset_test)\npredicted\n", "intent": "Make predictions for the test set, based on the model that we just built:\n"}
{"snippet": "print(classification_report(target_test, predicted, target_names=[\"negative\",\"positive\"]))\n", "intent": "We can quickly compute a summary of these statistics using scikit-learn's provided convenience function:\n"}
{"snippet": "acc_scores =  cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\nprint(acc_scores)\nprint(type(acc_scores))\n", "intent": "Similarly, for 10-fold cross validation we get an array with 10 accuracy scores, one for each fold:\n"}
{"snippet": "score1 = cross_val_score(model1, X, Y,cv=5, scoring=\"accuracy\")\nprint(\"For k=1 : \", score1)\nscore2 = cross_val_score(model2, X, Y,cv=5, scoring=\"accuracy\")\nprint(\"For k=3 : \", score2)\nscore3 = cross_val_score(model3, X, Y,cv=5, scoring=\"accuracy\")\nprint(\"For k=5 : \", score3)\nprint(\"Mean score for :::\")\nprint(\"k=1 :\", score1.mean())\nprint(\"k=3 :\", score2.mean())\nprint(\"k=5 :\", score3.mean())\n", "intent": "Step 4. Use 5-fold cross-validation to evaluate the accuracy achieved by a KNN classifier on the *Diabetes* dataset for: K=1, K=3, K=5\n"}
{"snippet": "y_pred = nb_classifier.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "y_pred = my_tree.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "y_pred = my_svm.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "y_pred = my_nn.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, lasso_cv.predict(X_train_scaled)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, lasso_cv.predict(X_holdout_scaled)))\n", "intent": "**<font color='red'>Question 4:</font> What are mean squared errors of tuned LASSO predictions on train and holdout sets?**\n"}
{"snippet": "class_rep_knn = classification_report(test_labels,pred_labels_knn)\nprint(\"Decision Tree: \\n\", class_rep_tree)\nprint(\"Logistic Regression: \\n\", class_rep_log)\nprint(\"Nearest Neighbors: \\n\", class_rep_knn)\n", "intent": "Repeat the code above to compare nearest neighbors with the other two models.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", mean_squared_error(ys, predictions) ** .5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions) ** .5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "acc = accuracy_score(y_test, y_pred)\nprint(acc)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "resnet50_prediction = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_prediction) == np.argmax(test_targets, axis=1))/len(resnet50_prediction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The testing accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "Z = logreg.predict(X_testing)\n", "intent": "<hr />\n<b>\n With the training data fit, we can now run prediction on the test data using the '*predict()*' function\n</b>\n"}
{"snippet": "y_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predict)\n", "intent": "<b>Then we score the predicted output from model on our test data against our ground truth test data.</b>\n"}
{"snippet": "y_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predict)\n", "intent": "<b>Then we score the predicted output from the model on our test data against our ground truth test data.</b>\n"}
{"snippet": "roc_auc_score(y_valid, ctb_valid_pred)\n", "intent": "**We got almost 0.75 ROC AUC on a hold-out set.**\n"}
{"snippet": "ex_2d_x = np.array([[400, 400]], dtype=np.float32)\npredict = k_means_estimator.predict(input_fn=lambda: input_fn_2d(ex_2d_x), as_iterable=False)\n", "intent": "*Note that the predict() function expects the input exactly like how we specified the feature vector*\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=1)\nprint\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Evaluation based on the evaluation metrics passed at compile time can be run on your test data with the evaluate method.\n"}
{"snippet": "mdir = tempfile.mkdtemp()\nenv = gym.make('CartPole-v0')\nenv = wrappers.Monitor(env, mdir, force=True)\nfor episode in range(100):\n    state = env.reset()\n    done = False\n    while not done:\n        action = np.argmax(model.predict(state.reshape(1, 4))[0])\n        nstate, reward, done, info = env.step(action)\n        state = nstate\n", "intent": "Cool, not bad at all!\nLet's test this fully trained agent and see how it does.\n"}
{"snippet": "mdir = tempfile.mkdtemp()\nenv = gym.make('CartPole-v0')\nenv = wrappers.Monitor(env, mdir, force=True)\nfor episode in range(100):\n    state = env.reset()\n    done = False\n    while not done:\n        action = np.argmax(model.predict(state.reshape(1, 4))[0])\n        nstate, reward, done, info = env.step(action)\n        state = nstate\n", "intent": "How about running your fully-trained greedy agent?\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(iris.data, cls[1])\n", "intent": "Now calculate the Silhouette Coefficient for the total cluster.\n"}
{"snippet": "labelPred = valObs.map(lambda lp: (lp.label, linRegSGD.predict(lp.features)))\nvalLRSGDMAE = calcMAE(labelPred)\nprint(\"Validation MAE:\\n\\tBenchmark: = {0:.3f}\\n\\tLRSGD = {1:.3f}\"\n      .format(valBenchMAE, valLRSGDMAE))\nprint(\"\\nThe difference between Benchmark and SGD = {0:.3f}\".format(valBenchMAE-valLRSGDMAE))\n", "intent": "We will check the new model on the validation set:\n"}
{"snippet": "predLabel = testObsNum.map(lambda lp:(modelNum.predict(lp.features), lp.label))\nmetrics = BinaryClassificationMetrics(predLabel)\nAUROC = metrics.areaUnderROC\nprint(AUROC)\n", "intent": "Let us try out the model on the test set, and see how they fare:\n"}
{"snippet": "y_train_pred = BikeOLSModel.predict(x_train)\ny_test_pred = BikeOLSModel.predict(x_test)\nr2_test = r2_score(y_test, y_test_pred)\nr2_train = r2_score(y_train, y_train_pred)\nBikeOLS_r2scores = {'test': r2_test, 'training': r2_train}\nprint('Training and test R2 scores are: ', BikeOLS_r2scores)\n", "intent": "<HR>\nTraining and test R2 scores are:  {'test': 0.40540416900870035, 'training': 0.4065387827969087}\n<HR>\n"}
{"snippet": "split = 0.67\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\ny_pred = predict(X_train, y_train, X_test, k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "np.sqrt(mean_squared_error(np.ones(len(temp_valid['total_bedrooms']))*temp_train['total_bedrooms'].mean(),\n                           temp_valid['total_bedrooms']))\n", "intent": "RMSE on a validation set is 64.5. Let's compare this with the best constant prediction - what if we fill NaNs with mean value:\n"}
{"snippet": "mse = cross_val_score(linreg, X, y, scoring='mean_squared_error', cv=10)\nrmse = np.mean(np.sqrt(-mse))\nprint (rmse)\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "all_pred = spam_detect_model.predict(message_tfidf) \n", "intent": "Prediction on all messages\n"}
{"snippet": "predictions = pipeline.predict(msg_test)\n", "intent": "Now we can directly pass data into the pipeline and the pre processing will be done along side the training of the model.\n"}
{"snippet": "test = data[50:60]\nprint(neigh.predict(test))\n", "intent": "Prepare Test Data and Predict\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50.predict(np.expand_dims(feature,axis=0))) for feature in test_Resnet50]\naccuracy = 100 * np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets,axis=1))/len(Resnet50_predictions)\nprint ('Testing accuracy: {:3f}%'.format(accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def cv_score(X, y, n_neighbors=4, weights='distance', n_samples=X.shape[0]):\n", "intent": "* Use cross validation to evaluate the performance of model\n* Use MRAE as performance metric\n"}
{"snippet": "n_neighbors_list = np.arange(1, 7)\nMRAEs_list = []\nfor n_neighbors in n_neighbors_list:\n    print('when n_neighbors:', n_neighbors)\n    MRAEs = cv_score(X, y, n_neighbors)\n    print('MRAE is between %.3f +/- %.3f \\n' % (MRAEs.mean(), MRAEs.std()))\n    MRAEs_list.append(MRAEs.mean())\n", "intent": "* use cross validataion to select the optimal k, which is 2 (see the plot below)\n"}
{"snippet": "n_neighbors_list = np.arange(1, 7)\nMRAEs_list = []\nfor n_neighbors in n_neighbors_list:\n    print('when n_neighbors:', n_neighbors)\n    MRAEs = cv_score(X, y, n_neighbors, n_samples=10000)\n    print('MRAE is between %.3f +/- %.3f \\n' % (MRAEs.mean(), MRAEs.std()))\n    MRAEs_list.append(MRAEs.mean())\n", "intent": "* use cross validataion to select the optimal k, which is 2 (see the plot below)\n"}
{"snippet": "mse = sklearn.metrics.mean_squared_error(Y_test, Y_pred)\nprint(mse)\n", "intent": "**Mean Squared Error**\n"}
{"snippet": "y_pred = mlog.predict(X_test)\nprint('Weighted f1 score for test dataset',f1_score(y_test, y_pred, average='weighted'))\n", "intent": "Weighted f1 score for test dataset\n"}
{"snippet": "pred =knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print(classification_report(y_test,pred),confusion_matrix(y_test,pred))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "y_pred = linreg.predict(X)\nglass['y_pred'] = y_pred\nglass.head()\n", "intent": "**Using the `LinearRegression` object we have fit, create a variable that are our predictions for `ri` for each row's `al` in the data set.**\n"}
{"snippet": "my_car.set_loss(loss_function)\n", "intent": "Next, we will set the loss function, using the loss function that you defined in part a.\n"}
{"snippet": "forecast = my_model.predict(future_dates)\nforecast.tail(3)\n", "intent": "Let's create our forecast and analize our prediction components.\n"}
{"snippet": "forecast = my_model.predict(future_dates)\nmy_model.plot_components(forecast);\n", "intent": "Let's create our forecast and analize our prediction components.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = net.forward(X)\nloss = criterion.forward(predictions, Y)\nans = [np.argmax(p) for p in predictions]\ncheck = [np.argmax(p) for p in Y]\naccuracy_score(ans,check)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "def gram_matrix(input_tensor):\n  channels = int(input_tensor.shape[-1])\n  a = tf.reshape(input_tensor, [-1, channels])\n  n = tf.shape(a)[0]\n  gram = tf.matmul(a, a, transpose_a=True)\n  return gram / tf.cast(n, tf.float32)\ndef get_style_loss(base_style, gram_target):\n", "intent": "Again, we implement our loss as a distance metric . \n"}
{"snippet": "model.load_weights(checkpoint_path)\nloss,acc = model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Then load the weights from the checkpoint, and re-evaluate:\n"}
{"snippet": "print('Accuracy score', accuracy_score(y_test, y_pred))\n", "intent": "Let's also compute accuracy\n"}
{"snippet": "import pdb\ndef buggy_loss(y, y_hat):\n  pdb.set_trace()\n  huber_loss(y, y_hat)\nprint(\"Type 'exit' to stop the debugger, or 's' to step into `huber_loss` and \"\n      \"'n' to step through it.\")\ntry:\n  buggy_loss(1.0, 2.0)\nexcept:\n  pass\n", "intent": "Check out exercise 2 towards the bottom of this notebook for a hands-on look at how eager simplifies model debugging.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "- on untrained data (X_test)\n"}
{"snippet": "def muffin_or_cupcake(flour, sugar):\n    if(model.predict([[flour, sugar]]))==0:\n        print('You\\'re looking at a muffin recipe!')\n    else:\n        print('You\\'re looking at a cupcake recipe!')\n", "intent": "__Step 6:__ Predict New Case\n"}
{"snippet": "predict_D1=modelD1.predict(X_D1_val[:,None])\n", "intent": "Generamos las predicciones:\n"}
{"snippet": "from sklearn import metrics\nprint('MSE1:', metrics.mean_squared_error(Y_D1_val, predict_D1))\n", "intent": "Calculamos la Media del Error Cuadrado para el modelo 1 (variable predictora 'RM')\n"}
{"snippet": "predict_fullmodel=full_model.predict(X_val)\n", "intent": "* Report the mean square error on the test set\n"}
{"snippet": "print('MSE_MLP:', metrics.mean_squared_error(Y_val, prediction_MLP))\n", "intent": "* Report the mean square error on the test set.\n"}
{"snippet": "print('MSE_centr_model:', metrics.mean_squared_error(Y_val, predict_centr_model))\n", "intent": "Report the mean square error on the test set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(\"\")\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred_train = from_log(model_grid.predict(X_ohe_train))\ny_pred_valid = from_log(model_grid.predict(X_ohe_valid))\n", "intent": "Making a predictions on train and validation data:\n"}
{"snippet": "new_pred = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "test_point_indexes=100\nno_features=100\npredicted_clss=sig_clf.predict(test_x_onehotCoding[test_point_indexes])\nprint(\"Predicted Class :\", predicted_clss[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_indexes]),4))\nprint(\"Actual Class :\", test_y[test_point_indexes])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_features]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_indexes],test_df['Gene'].iloc[test_point_indexes],test_df['Variation'].iloc[test_point_indexes], no_features)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 1000\npredicted_cls = sig_clf.predict(test_x_onehotCoding)\nprint(\"Predicted Class :\", predicted_cls[test_point_index])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 1000\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>5.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>5.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>5.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "MSE = np.mean((rgrs.predict(x_ts) - y_ts) ** 2)\nprint(\"Residual sum of squares:\", MSE )\nRSS = rgrs.score(x_ts, y_ts)\nprint('The R^2 score for the Model:', RSS)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "y_pred = model.predict_proba(X_test)\ny_pred[:5]\n", "intent": "After having trained a logistic regression model we predict the clicks for the test set\n"}
{"snippet": "y_pred_train_2 = from_log(model_grid.predict(X_ohe_train_2))\ny_pred_valid_2 = from_log(model_grid.predict(X_ohe_valid_2))\n", "intent": "Again make predictions on train and valid datasets and plot real and predicted prices:\n"}
{"snippet": "def mean_squared_error(a, b):\n    return tf.reduce_mean(tf.square(a - b))\n", "intent": "This function creates a TensorFlow operation for calculating the Mean Squared Error between the two input tensors.\n"}
{"snippet": "y_pred = one_class_clf.predict(X[~y.index.isin(idxs)])\n", "intent": "Since the *RandomUnderSampler* returns the indexes, we can use them to get the rest of the samples in order to validate the classifier.\n"}
{"snippet": "y_pred = nb_clf.predict(X[~y.index.isin(idxs)])\ny_true = y[~y.index.isin(idxs)]\n", "intent": "Using the same approach, I'll validate it with the rest of the samples\n"}
{"snippet": "confidence_intervals = np.arange(.975, .9999, .0001)\nbest = (0, 0, 0, 0)\nfor interval in confidence_intervals:\n    t = chi2.ppf(interval, 5) ** .5\n    y_pred = [1 if y_ > t else 0 for y_ in y_score]\n    auc = average_precision_score(y_true=y_true, y_score=y_pred)\n    if auc > best[1]:\n        precision, recall, _ = precision_recall_curve(y_pred, y_true)\n        best = (interval, auc, precision, recall)\nprint best\n", "intent": "Here with confidence interval 97.5%, the AUC was .05 <br />\nThis isn't a poor classifier. So, let's try to tune the confidence interval.\n"}
{"snippet": "t = chi2.ppf(0.975, 5) ** .5\ny_pred = [1 if y_ > t else 0 for y_ in y_score]\nauc = average_precision_score(y_true=y_true, y_score=y_pred)\nprint auc\n", "intent": "Now, let's predict using the confidence interval\n"}
{"snippet": "confidence_intervals = np.arange(.999000, .999999, .000001)\nbest = (0, 0, 0, 0)\nfor interval in confidence_intervals:\n    t = chi2.ppf(interval, 5) ** .5\n    y_pred = [1 if y_ > t else 0 for y_ in y_score]\n    auc = average_precision_score(y_true=y_true, y_score=y_pred)\n    if auc > best[1]:\n        precision, recall, _ = precision_recall_curve(y_pred, y_true)\n        best = (interval, auc, precision, recall)\nprint best\n", "intent": "Here, we had a poor auc of .051 <br />\nLet's try to tune the confidence interval\n"}
{"snippet": "X = df_final[['student_Yes', 'balance', 'income']]\ny = df_final[['default_Yes']]\ny = np.ravel(y)\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(logregl2, X, y, cv=5)\nscores\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "mse = metrics.mean_squared_error(y_test, predictions)\nd_mse = metrics.mean_squared_error(y_test, d_predictions)\nprint \"MSE / LR = {0:.4}, Dummy = {1:0.4}\".format(mse, d_mse)\n", "intent": "$$ MAE(y, \\hat y) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} ( y_i - \\hat y_i )^2 $$\n"}
{"snippet": "mae = metrics.mean_absolute_error(y_test, predictions)\nd_mae = metrics.mean_absolute_error(y_test, d_predictions)\nprint \"MAE / LR = {0:.4}, Dummy = {1:0.4}\".format(mae, d_mae)\n", "intent": "$$ MAE(y, \\hat y) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} | y_i - \\hat y_i | $$\n"}
{"snippet": "y_pred_train_rf_best = from_log(rf_best.predict(X_ohe_train_rf))\ny_pred_valid_rf_best = from_log(rf_best.predict(X_ohe_valid_rf))\n", "intent": "As earlier, let's make a predictions on validation set and plot real and predicted values on train and validation datasets:\n"}
{"snippet": "print 'MAE %.2f' % mean_absolute_error(y, y_hat)\nprint 'MSE %.2f' % mean_squared_error(y, y_hat)\n", "intent": "Residuals:\n* $\\frac{1}{n} \\sum_i |\\hat{y}^{(i)}-y^{(i)}|$\n* $\\frac{1}{n} \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$\n"}
{"snippet": "print('evaluating 800 epochs model')\nscore = model_800.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test error:', (1-score[2])*100, '%')\nprint('evaluating 56 epochs model')\nscore = model_56.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test error:', (1-score[2])*100, '%')\n", "intent": "First the evaluations for the network of the previous notebook, with 800 and 56 epochs of training.\n"}
{"snippet": "print('evaluating new model')\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test precision:', score[2]*100, '%')\nprint('Test error:', (1-score[2])*100, '%')\n", "intent": "As we can see, after only 100 iterations the error on the training set was already pretty low. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"Train MSE:\", mean_squared_error(Y_train,model.predict(X_train)))\nprint(\"Test MSE:\", mean_squared_error(Y_test,model.predict(X_test)))\n", "intent": "measure mean squared error\n"}
{"snippet": "sns.regplot(bos.PRICE, lm.predict(X))\n", "intent": "The results appear to be normally distributed. There are some values below 0, which we should remove, as prices for houses cannot be negative.\n"}
{"snippet": "score, acc = model.evaluate(X_test_pad, y_test)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n", "intent": "It definitely trains faster...\n"}
{"snippet": "img = prepare_image('imgs/dog2.jpg')\nout = model.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/dog2.jpg'/>\n"}
{"snippet": "img = prepare_image('imgs/sloth.jpg')\nout = model.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/sloth.jpg'/>\n"}
{"snippet": "r2 =  metrics.r2_score(test_y, pred_y) \nprint( \"mean squared error = \", metrics.mean_squared_error(test_y, pred_y))\nprint( \"r2 score (coef determination) = \", metrics.r2_score(test_y, pred_y))\n", "intent": "<h2>R2 score</h2>\n<br />\n"}
{"snippet": "test_predictions = final_model.predict(X_test_scaled)\n", "intent": "Make predictions for test samples:\n"}
{"snippet": "def accuracy(tp, fp, fn, tn):\n    return (\"insert your code here\")\ndef recall():\ndef specificity ():\ndef precision():\ndef NegativePredictiveValue():\ndef f1_score(tp, fp, fn, tn):\n", "intent": "Refer to either class notes or textbook for these definitions. \n"}
{"snippet": "from sklearn import metrics\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\nprint (\"var:\", y.var())\n", "intent": "* From Sklearn metrics import MSE and R-square methods and apply to the problem\n"}
{"snippet": "expected = y\npredicted = model.predict(X)\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "**Using either confusion matrix or classification report, see which one seems to be apt in NB ?**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\nprint('R^2 score: %.2f' % r2_score(y_test, y_pred))\n", "intent": "related game: http://guessthecorrelation.com/\n"}
{"snippet": "sgd_clf.predict([X[36000]])\n", "intent": "Since it gives a output of **True**, hence our binary classifier correctly identified the digit 5 from our dataset.\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "2/F1 = 1/recall + 1/precision\n1. f1/2 < smaller of recall and precision\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Evaluating the accuracy of SGDClassifier**\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n", "intent": "First we'll make the Confusion Matrix. For this we need predictions.\n"}
{"snippet": "knn_clf.predict([some_digit])\n", "intent": "**KNeighborsClassifier** supports **multilabel classification** but not all classifiers do.\n"}
{"snippet": "preds_proba = rf.predict_proba(X_val_analize)\ntrue_class_probs = preds_proba[:,1]\n(true_class_probs > 0.023).sum()\n", "intent": "Forest is highly biased onto False assumptions, let's pick elements with the probability being true not > 0.5, but >bias.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n    target_names=twenty_test.target_names))\nmetrics.confusion_matrix(twenty_test.target, predicted)\n", "intent": "scikit-learn further provides utilities for more detailed performance analysis of the results:\n"}
{"snippet": "scaled_svm_clf1.predict([[5.5, 1.7]])\n", "intent": "Step 3: Test the model using a sample data\n"}
{"snippet": "Xception_predictions = [np.argmax(my_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(labels_test_exception, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "accuracy = np.mean(model.predict(X_test) == y_test.argmax(axis=1))\nprint(\"Accuracy Rate: {}%\".format(round(accuracy, 2) * 100))\n", "intent": "* Note how much were overfitting here (training and validation results are diverging).\n"}
{"snippet": "from sklearn.utils import shuffle\nclass PerceptronClassifier(object):\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n        self.iter = 0\n    def fit(self, X, y, epochs=100):\n        return np.dot(x, self.weights) + self.bias\n    def predict(self, x):\n", "intent": "<a name=\"POS-2\">Solution for POS Exercise 2</a>\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint (mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = LR.predict(x_test1)\nLR.score(x_test1,y_test1)\n", "intent": "**Use the Model to predict on x_test and evaluate the model using metric(s) of Choice.**\n"}
{"snippet": "predictions = dTree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "p = np.array([0.1, 0.1, 0.05, 0.6, 0.3], dtype=np.float32)\ny = np.array([0, 0, 0, 1, 0], dtype=np.float32)\ndef poisson_loss(y, p):\n    return (p - y * np.log(p)).mean()\npoisson_loss(y, p)\n", "intent": "* https://github.com/fchollet/keras/pull/479/commits/149d0e8d1871a7864fc2d582d6ce650512de371c\n"}
{"snippet": "coeffs = np.arange(0.1, 0.91, 0.1)\nmse = [mean_squared_error(y_test, ridge_pred * c + vw_pred * (1 - c)) for c in coeffs]\nm = np.argmin(mse)\ncoeffs[m], mse[m]\n", "intent": "Let's try to combine predictions\n"}
{"snippet": "print('R^2: {:.3f}'.format(r2_score(y_m, cvp_m)))\nprint('RMSE: {:.3f}'.format(mean_squared_error(y_m, cvp_m)**(0.5)))\nprint('MAE: {:.3f}'.format(mean_absolute_error(y_m, cvp_m)))\nprint('CC: {:.3f}'.format(np.corrcoef(cvp_m, y_m)[1,0]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "y=tf.nn.embedding_lookup(np.eye(8238,dtype=np.float32),y_)\nloss = -tf.reduce_sum(y*tf.log(tf.clip_by_value(y_hat,1e-10,1.0)))+tf.nn.l2_loss(U)+tf.nn.l2_loss(V)\n", "intent": "We need to modify the loss function a bit for technical reasons.\n"}
{"snippet": "A_pre = best_model.predict(test[['B','C','D','G','H']])\n", "intent": "**c) Use the model to predict \"A\" using the testing dataset. Report the OS prediction accuracy. (10pts)**   \n"}
{"snippet": "test_pred = logitCV.predict_proba(test_X)[:,1]\ntest_pred = np.array(list(\"{:.6f}\".format(x) for x in test_pred))\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "def squared_error(ys_original,ys_line):\n    return sum((ys_line-ys_original)**2)\n", "intent": "<img src=\"rsq.png\" />\n"}
{"snippet": "deep_features_model.evaluate(image_testing_SFrame)\n", "intent": "As we can see, deep features provide us with significantly better accuracy (about 78%)\n"}
{"snippet": "predictions = []\nfor row in questions[1:]:\n    predition = clf.predict([row])\n    print(features_to_english(row), \"=>\", fast_or_slow(predition))\n    predictions.append(predition[0])\nprint(predictions)\n", "intent": "Please note that the set do not have to be same as in the training model\n"}
{"snippet": "from azureml import services\n@services.publish(workdspace_id, authorization_token)\n@services.types(crim=float, zn=float, indus=float, chas=float, nox=float, rm=float, age=float, \n                dis=float, rad=float, tax=float, ptratio=float, black=float, lstat=float)\n@services.returns(float)\ndef demoservice(crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat):\n    feature_vector = [crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat]\n    return lm.predict(feature_vector)\n", "intent": "Azure ML API Services are not yet functional\n"}
{"snippet": "loss, accuracy = model.evaluate(x_test, \n                                utils.onehot(targets_test, num_classes), \n                                batch_size=128)\nprint()\nprint('Test Loss:', loss)\nprint('Test Accuracy', accuracy)\n", "intent": "When the model is fitted Keras also makes it easy to evaluate  the performance. Like above all it takes is a single function call.\n"}
{"snippet": "adjusted_rand_score(y2, ag.labels_)\n", "intent": "Score ARI for K-MEANS and Agglomerative Clustering:\n"}
{"snippet": "probabilities = logistic_mod.predict_proba(X_test)\nprint(probabilities[:15,:])\n", "intent": "Next, execute the code in the cell below to compute and display the class probabilities for each case. \n"}
{"snippet": "probabilities = logistic_mod.predict_proba(x_test)\nprint(probabilities[:15,:])\n", "intent": "Next, execute the code in the cell below to compute and display the class probabilities for each case. \n"}
{"snippet": "nr.seed(498)\ncv_estimate = ms.cross_val_score(rf_clf, Features, Labels, \n                                 cv = outside) \nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))\n", "intent": "Now, you will run the code in the cell below to perform the outer cross validation of the model. \n"}
{"snippet": "nr.seed(498)\ncv_estimate = ms.cross_val_score(ab_clf, Features, Labels, \n                                 cv = outside) \nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))\n", "intent": "Now, you will run the code in the cell below to perform the outer cross validation of the model. \n"}
{"snippet": "probabilities = ab_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)    \n", "intent": "Now, execute the code in the cell below to score and evaluate the model.\nOnce you have executed the code, answer **Question 4** on the course page.\n"}
{"snippet": "nr.seed(498)\ncv_estimate = ms.cross_val_score(clf, Features, Labels, \n                                 cv = outside) \nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))\n", "intent": "Now, you will run the code in the cell below to perform the outer cross validation of the model. \n"}
{"snippet": "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "Let's calculate some regression evaluation metrics.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Let's see how well the model works by putting its predictions up against the real thing.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n", "intent": "So how did our model perform?\n"}
{"snippet": "xgb = XGBClassifier(random_state=33, n_jobs=4)\nkf = KFold(random_state=33,n_splits=5,shuffle=True)\nprint ('Mean ROC-AUC CV score:', np.mean(cross_val_score(xgb, X_train, y_train, scoring='roc_auc',cv=kf)))\n", "intent": "Let's check quality of XGBoost via CV with 5 shuffling folds.\n"}
{"snippet": "sms_predictions = spam_detection_model.predict(sms_tfidf)\nprint(','.join(list(p for p in sms_predictions)[:500]))\n", "intent": "No to put the entire dataset through the model and see how well it performs.\n"}
{"snippet": "print(confusion_matrix(y_test, terse_predictions))\nprint(classification_report(y_test, terse_predictions))\n", "intent": "Finally, create a confusion matrix and classification report to see how we did.\n"}
{"snippet": "y_pred = gp.predict(x_train[:, None])\nrmse = __________________\nprint '(in-sample) rmse =', rmse\n", "intent": "We can measure the training error (RMSE):\n"}
{"snippet": "def test_train_error(theta0):\n    gp = GaussianProcess(theta0=theta0, nugget=.5)\n    __________________\n    __________________\n    __________________\n    return dict(__________________=__________________,)\ntest_train_error(100)\n", "intent": "Refactor this into a function:\n"}
{"snippet": "y_pred = gp.predict(x_train[:, None])\nrmse = np.sqrt(np.mean((y_pred - y_train)**2))\nprint '(in-sample) rmse =', rmse\n", "intent": "We can measure the training error (RMSE):\n"}
{"snippet": "x_test= np.random.uniform(0, 10, size=1000)\ny_test = np.random.normal(np.cos(x_test), dy)\ny_pred = gp.predict(x_test[:, None])\nrmse = np.sqrt(np.mean((y_pred - y_test)**2))\nprint '(out-of-sample) rmse =', rmse\n", "intent": "To measure the \"test\" error, we need different data.  Since this is a simulation, we can have as much as we want:\n"}
{"snippet": "import sklearn.cross_validation\nscores = sklearn.cross_validation.cross_val_score(clf, X, y)\nprint scores.mean(), scores.std()\n", "intent": "Does that look good?  Too good?  If it looks too good to be true, it probably is...\n"}
{"snippet": "n = len(y)\ncv = sklearn.cross_validation.StratifiedKFold(n, n_folds=10, shuffle=True)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "I think it is important to stratify the sampling, although our book does not make a big deal about that:\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedKFold(___________________________)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Oops, what did I get wrong here?  Can you fix it?\n"}
{"snippet": "xgb = XGBClassifier(random_state=33, n_jobs=4)\nkf = KFold(random_state=33,n_splits=5,shuffle=True)\nprint ('Mean ROC-AUC CV score:', np.mean(cross_val_score(xgb, X_train, y_train, scoring='roc_auc',cv=kf)))\n", "intent": "Let's check quality again.\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedShuffleSplit(________________________________)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Oops, what did I get wrong again?\n"}
{"snippet": "cv = sklearn.cross_validation.LeavePLabelOut(df.site, p=2)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "There is also a version that holds out $P$ labels, and lets you choose $P$:\n"}
{"snippet": "n = len(y)\ncv = sklearn.cross_validation.LeaveOneOut(n)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "What did I not mention that whole time?  Leave-one-out cross-validation.  That is because it takes forever.\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n    scores += [sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)]\nprint\n", "intent": "Let's return to the stratified, repeated, 10-fold cross-validation approach:\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=________________________________)\n    scores += [sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)]\nprint\nprint np.mean(scores)\n", "intent": "To make this really, really fair, we should use the same random splits in each experiment...\n"}
{"snippet": "import sklearn.metrics\ny_pred = ['Other' for i in range(len(y))]\nsklearn.metrics.accuracy_score(y, y_pred, sample_weight=sample_weight)\n", "intent": "What is in `sample_weights`?\n"}
{"snippet": "clf.predict_proba(X[test[:10]])\n", "intent": "Did I promise a little more for this week in the syllabus?\nProbability prediction:\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "I think it is important to stratify the sampling, although our book does not make a big deal about that:\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n    scores += list(sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv))\n", "intent": "And I don't want you to ever rely on just 10 samples.  How can you do ten repetitions of 10-fold cross validation?\n"}
{"snippet": "xgb = XGBClassifier(random_state=33, n_jobs=4,max_depth=4, min_child_weight=1, n_estimators=200)\nkf = KFold(random_state=33,n_splits=5,shuffle=True)\nprint ('Mean ROC-AUC CV score:', np.mean(cross_val_score(xgb, X_train, y_train, scoring='roc_auc',cv=kf)))\n", "intent": "Now check new hyperparameters via our CV.\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456+rep)\n    scores += [sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)]\nprint\nprint np.mean(scores)\n", "intent": "To make this really, really fair, we should use the same random splits in each experiment...\n"}
{"snippet": "np.round(clf.predict_proba(X[test[:10]]), 2)\n", "intent": "Use `np.round` to make this look nicer:\n"}
{"snippet": "c.predict_proba(X[test[0]])\n", "intent": "We can make probability predictions for most classifiers in `sklearn`:\n"}
{"snippet": "y_pr_pred = c.predict_proba(X[test])\n", "intent": "So yes...  How does it do on things it say are 80% chance real, overall?\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\n", "intent": "Evaluate the model on test data\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\navg_ar_score = complete_ar_score = adjusted_rand_score(iris.target, avg_pred)\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "X_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test)\ny_pred = clf.predict(X_test_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nscoreTrain = model.evaluate(x_train,y_train, verbose=0)\nprint(\"Accuracy: \", score[1])\nprint(\"Accuracy of Train :\", scoreTrain[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "y_pred = model.predict(X_test.values.reshape(-1, 1))\ny_pred\n", "intent": "In `scikit-learn` this will use the `.predict()` function\n"}
{"snippet": "skf_5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\ncv_scores = cross_val_score(gcv.best_estimator_, X_crossvalid, y_crossvalid, cv=skf_5, scoring='neg_log_loss')\n", "intent": "Split with 3 folds was used to reduce parameters search time. Cross-validation can be run with large amount of splits.\n"}
{"snippet": "y_pred_mean = [y_train.mean()] * len(y_test)\nprint('RMSE (model):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('RMSE: (dumb model):', np.sqrt(metrics.mean_squared_error(y_test, y_pred_mean)))\n", "intent": "One example is a model that always predicts the mean value\n"}
{"snippet": "y_pred_p = lr.predict_proba(X_test)\ny_pred_l1_p = lr_l1_penalty.predict_proba(X_test)\ny_pred_l2_p = lr_l2_penalty.predict_proba(X_test)\n", "intent": "We can get predicted probabilities as well as predictions\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_test_2, y_pred_2)\nprint(f\"AUC: {auc}\")\n", "intent": "from [https://www.ncss.com/software/ncss/roc-curves-ncss](https://www.ncss.com/software/ncss/roc-curves-ncss)\n"}
{"snippet": "y_pred = lr.predict(X_test)\nprint(roc_auc_score(y_test, y_pred))\nconfusion_matrix(y_test, y_pred)\n", "intent": "Looking better! Let's evaluate both the basic and new model on their respective test sets\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\naccuracy_scores = cross_val_score(knn, X_train, y_train, scoring=\"f1\")\nnp.mean(accuracy_scores)\n", "intent": "You might have noticed the classes are skewed - use an appropriate metric for evaluation\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "Again, we have to coerce our data into a `[n_samples, n_features]` feature matrix:\n"}
{"snippet": "yfit = model.predict(Xtest)\n", "intent": "Predict the labels of the test set:\n"}
{"snippet": "def predict(i):\n    image = mnist.test.images[i]\n    actual_label = np.argmax(mnist.test.labels[i])\n    prediction = tf.argmax(y,1)\n    predicted_label = sess.run(prediction, feed_dict={images: [image]})\n    print (\"Predicted: %d, actual: %d\" % (predicted, actual))\n    pylab.imshow(mnist.test.images[i].reshape((28,28)), cmap=pylab.cm.gray_r) \n    return predicted_label, actual_label\npredict(5)\n", "intent": "A method to make predictions on a single image\n"}
{"snippet": "import xgboost as xgb\nxgb_clf = xgb.XGBClassifier()\nscores = cross_val_score(xgb_clf, X, y, cv=shuffle_validator, scoring='accuracy')\nprint('Accuracy: {:.4f} (+/- {:.2f})'.format(scores.mean(), scores.std()))\n", "intent": "**Just to finish with my new favorite**\n"}
{"snippet": "y_pred = random_forest.predict_proba(X_valid)\n", "intent": "Compare with baselines of 4 classes: \n"}
{"snippet": "np.mean((y_test - regr.predict(X_test))**2)**0.5\n", "intent": "Linear regression model RMSE result over test_daata\n"}
{"snippet": "def get_sentiment(review):\n    vectorized_review = update_input_layer(review).reshape(1,-1)\n    res = model.predict(vectorized_review)\n    if(res >= 0.5):\n        return \"POSITIVE\"\n    else:\n        return \"NEGATIVE\"\n", "intent": "Let's try the model to predict some reviews:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.\n    for i, layer in enumerate(style_layers):\n        gram = gram_matrix(feats[layer])\n        style_loss += style_weights[i]*torch.sum((gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "class CombinedClassifier:\n    def __init__(self, clf1, clf1_features, clf2, clf2_features):\n        self.clf1 = clf1\n        self.clf2 = clf2\n        self.clf1_features = clf1_features\n        self.clf2_features = clf2_features\n    def predict(self, X): pass\n    def predict_proba(self, X): pass\nclf3 = CombinedClassifier(clf1, numerical, clf2, dummy_categorical)\n", "intent": "Build a third model (named `clf3`) that combine these two classifiers by completing the following class definition.\n"}
{"snippet": "from treeinterpreter import treeinterpreter as ti, utils\nselected_rows = [31, 85]\nselected_df = X_train.iloc[selected_rows,:].values\nprediction, bias, contributions = ti.predict(rf, selected_df)\n", "intent": "Using `treeintrerpreter` I obtain 3 objects: predictions, bias (average value of the dataset) and contributions.\n"}
{"snippet": "reduced_x_features\ngradient_boosted.predict_proba([[45, 0, 6, 1, 15000, 9.62, 6.91]])\n", "intent": "This one has perfect performance so we will use that to predict a new campaign:\n"}
{"snippet": "evaluate('hard', ['better', 'good']) \n", "intent": "Embeddings can also reveal details like grammatical properties:\n"}
{"snippet": "evaluate('china', ['moscow', 'russia']) \n", "intent": "They can also help us answer questions about the world:\n"}
{"snippet": "evaluate('nurse', ['man', 'doctor']) \n", "intent": "However, embeddings can also reveal problematic *biases* in language. \n"}
{"snippet": "from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_val_score\ntscv = TimeSeriesSplit(n_splits=5)\ncv = cross_val_score(lr, X_train_scaled, y_train, scoring = 'neg_mean_absolute_error', cv=tscv)\nmae = cv.mean()*(-1)\nmae\n", "intent": "For Cross Validation (CV) on Time Series data, we will use **TimeSeries Split** for CV.\nLet's see Mean Absolute Error for our simplest model\n"}
{"snippet": "cr = classification_report(y_test,predictions)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "labels = kmeans.predict(data)\nprint(labels)\n", "intent": "From our *k*-means model we just built, we can see the labels to which each data point is assigned.\n"}
{"snippet": "yhat = neigh.predict(X_test)\nyhat[0:5]\n", "intent": "we can use the model to predict the test set:\n"}
{"snippet": "yhat = clf.predict(X_test)\nyhat [0:5]\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_test, yhat, average='weighted') \n", "intent": "You can also easily use the __f1_score__ fron sklearn library:\n"}
{"snippet": "from sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)\n", "intent": "Lets try jaccard index for accuracy:\n"}
{"snippet": "salary_pred = lin_reg.predict(test_set)\nsalary_pred\n", "intent": "Now we have a model and can call the `predict` function on it with inputs. \n"}
{"snippet": "r2_score(test_set_full[\"Salary\"], salary_pred)\n", "intent": "There's also a separate `r2_score` method that will calculate the $r^2$.\n"}
{"snippet": "next_25 = stepwise_fit.predict(n_periods=25)\nnext_25\n", "intent": "After your model is fit, you can forecast future values using the `predict` function, just like in sci-kit learn:\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "We will define a loss metric - namely - *Mean Absolute Percentage Error* which calculated Mean Absolute Error in percentage\n"}
{"snippet": "predicted_class = result.predict(train_cols)\naccuracy_score(data.admit, predicted_class)\n", "intent": "Answer: A one unit increase in GPA doubles the odds of being admitted.  \n"}
{"snippet": "reg.predict(np.array([2,3,40]).reshape(1,-1))\n", "intent": "Let's see how much power consumption the fitted tree predicts for a Wednesday at 2am if it is 40F:\n"}
{"snippet": "clf.predict([[2., 2.]])\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "def square_error(s, s_est):\n    y = np.mean(np.power((s - s_est), 2))\n    return y\nMSE_tr = square_error(S_tr, s_hat)\nMSE_tst = square_error(S_tst, s_hat)\nprint('Average square error in the training set (baseline method): {0}'.format(MSE_tr))\nprint('Average square error in the test set (baseline method): {0}'.format(MSE_tst))    \n", "intent": "Compute the mean square error over training and test sets, for the baseline estimation  method.\n"}
{"snippet": "def square_error(s, s_est):\n    y = np.mean(np.power((s - s_est), 2))\n    return y\nprint('Average square error in the training set (baseline method): {0}'.format(MSE_tr))\nprint('Average square error in the test set (baseline method): {0}'.format(MSE_tst))    \n", "intent": "Compute the mean square error over training and test sets, for the baseline estimation  method.\n"}
{"snippet": "transfer_model_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(transfer_model_predictions)==np.argmax(test_targets, axis=1))/len(transfer_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(df['admit'], probas[:,1])\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "dtree_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(colleges['Cluster'], model.labels_))\nprint(classification_report(colleges['Cluster'], model.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "ridge_cv_score = cross_val_score(model_ridge, X_train, y_train, scoring='neg_mean_absolute_error',\n                                 cv=TimeSeriesSplit(n_splits=5))\nlgb_cv_score = cross_val_score(model_lgb, X_train, y_train, scoring='neg_mean_absolute_error',\n                                 cv=TimeSeriesSplit(n_splits=5))\n", "intent": "Let's recheck cross_val_score for models.\n"}
{"snippet": "dataset_tempe['LR prediction'] = lr_tempe.predict(X_tempe_train)\ndataset_tempe['RFR prediction'] = regr_rf.predict(X_tempe_train)\ndataset_tempe['SVR prediction'] = svr_rbf.predict(X_tempe_train)\nprint('Dataframe shape:', dataset_tempe.shape)\n", "intent": "After training the regressor, each regression model is fed with input of feature matrix.\n"}
{"snippet": "dataset_tempe_extend['LR future prediction'] = lr_tempe.predict(X_tempe_train_extend_inter)\n", "intent": "The extended feature matrix is fed to Linear Regression model to predict the global temperature in future.\n"}
{"snippet": "dataset_tempe['predicted'] = LR_tempe.predict(X_tempe_train)\ndataset_tempe.head()\n", "intent": "Here, the global temperature anomaly from original data is compared with temperature anomaly predicted by trained modeled.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(km.labels_, df['Cluster']))\nprint(classification_report(km.labels_, df['Cluster']))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y_hat = line_reg.predict(train_data[['X']])\n", "intent": "We can also use the trained model to render predictions.\n"}
{"snippet": "preds_bag = model_bag.predict(testing_data)\npreds_rf = model_rf.predict(testing_data)\npreds_ab = model_ab.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1))) \n    pred_pos = np.sum(preds_nb==1) \n    return tp/(pred_pos)\nprint(precision(y_test, preds_svm))\nprint(precision_score(y_test, preds_svm))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "y_pred_full = isof.predict(X_full[['x1','x2']])\n", "intent": "Very fast! How about the quality?\n"}
{"snippet": "def f1(preds, actual):\n    return 2/(1/precision(preds, actual) + 1/(recall(preds, actual)))\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "preds_tree = tree_mod.predict(X_test)\npreds_rf = rf_mod.predict(X_test)\npreds_ada = ada_mod.predict(X_test)\npreds_reg =reg_mod.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_train_predicted = model.predict_proba(X_train)[:,1]\ny_test_predicted = model.predict_proba(X_test)[:,1]\nprint('Train AUC %.4f' % roc_auc_score(y_train,y_train_predicted))\nprint('Test AUC %.4f' % roc_auc_score(y_test,y_test_predicted))\n", "intent": "We will predict probabilities of our TARGET=1,\n  P(TARGET=1|X) and use it for finding AUC_ROC metric.\n"}
{"snippet": "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n    scores = model_selection.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n", "intent": "Apply the weighted voting schema we just implemented and test it.\n"}
{"snippet": "clf.predict(X.head())\n", "intent": "The classifier is now trained on data! We can use it to do stuff like predicting iris types based on features we supply.\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2))/len(bos.PRICE))\nprint((np.mean((bos.PRICE - lm.predict(X)) ** 2)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print classification_report(y_test, KNNOP_pred)\n", "intent": "The recall on survival has gone up while dropping the precision slightly from our first model.\n"}
{"snippet": "pred_quant = res.predict()\npred_quant\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "nb_test = pg.NaiveBayes.from_samples(pg.ExponentialDistribution, df, y, verbose=False)\nnb_test.predict_proba(df);\n", "intent": "Last but not least - timings (as I said - very fast, even on sparse data)\n"}
{"snippet": "y_val_pred = clf.predict(X_validation)\naccuracy_score(Y_validation, y_val_pred)\n", "intent": "i) Classification accuracy\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(clf.predict(X_validation), Y_validation)\n", "intent": "iii) Confusion matrix\n"}
{"snippet": "print(classification_report(Y_validation, y_val_pred))\n", "intent": "iv) Classification Report\n"}
{"snippet": "confusion_matrix(clf.predict(X_validation), Y_validation)\n", "intent": "iii) Confusion matrix\n"}
{"snippet": "print(classification_report(Y_validation, y_val_pred))\n", "intent": "iv) Classification Report including Prediction, recall, F1 score\n"}
{"snippet": "y_val_pred = model.predict(X_validation)\naccuracy_score(Y_validation, y_val_pred)\n", "intent": "i) classification accuracy\n"}
{"snippet": "confusion_matrix(model.predict(X_validation), Y_validation)\n", "intent": "iii) confusion matrix\n"}
{"snippet": "y = clf.predict_proba(test_df)\n", "intent": "Make predictions on the test data\n"}
{"snippet": "x_test = np.array(['play ball'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "score = -cross_val_score(model, train_data, train_targets, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\nprint(score)\n", "intent": "Let's train a baseline model with the default parameters:\n"}
{"snippet": "vals = np.array([100,30,20000,7,3]).reshape(1,-1)\nmodel.predict(vals)\n", "intent": "Now we can use the model to predict avg grade for a given set of features\n"}
{"snippet": "vals = np.array([100,30,7,3]).reshape(1,-1)\nmodel.predict(vals)\n", "intent": "Now we can use the model to predict avg grade for a given set of features\n"}
{"snippet": "lm.predict(120.0)\n", "intent": "Once we have a model, we can make prediction:\n"}
{"snippet": "Employee.loc[6, 'Year'] = lm.predict(120.0)\nEmployee\n", "intent": "We may then impute the missing '`Year`' with our prediction.\n"}
{"snippet": "cv_score_dt=cross_val_score(decision_tree_model,X_test,y_test,cv=30)\n", "intent": "Cross Validating Model\n---\n"}
{"snippet": "cv_score_svm=cross_val_score(svm_model,X_test,y_test,cv=20)\n", "intent": "Cross Validating the Model\n---\n"}
{"snippet": "iris['predicted'] = model.predict(x)\niris\n", "intent": "It's helpful to attach our predictions back to our dataframe to see what they are in context to the original training set.\n"}
{"snippet": "note_predictions = classifier.predict(X_test)\nnote_predictions = list(note_predictions)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "dnn_predictions = dnn_classifier.predict(x=X_test, as_iterable=False)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "def gb_mse_cv(params, random_state=random_state, cv=kf, X=train_data, y=train_targets):\n    params = {'n_estimators': int(params['n_estimators']), \n              'max_depth': int(params['max_depth']), \n             'learning_rate': params['learning_rate']}\n    model = LGBMRegressor(random_state=random_state, **params)\n    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n    return score\n", "intent": "The interface of hyperop.fmin differs from Grid or Randomized search. First of all we need to create a function to minimize.\n"}
{"snippet": "pred_rforest = rforest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(metrics.classification_report(y_test, pred_rforest))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print(confusion_matrix(data.Cluster, kmeans.labels_))\nprint(classification_report(data.Cluster, kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred_init = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test, pred_init))\nprint(classification_report(y_test, pred_init))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "pred_pipe = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "results = model.evaluate(X_test, y_test)\n", "intent": "- Keras model can be evaluated with evaluate() function\n- Evaluation results are contained in a list\n    - Doc (metrics): https://keras.io/metrics/\n"}
{"snippet": "ResNetFifty_predictions = [np.argmax(ResNetFifty_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNetFifty]\ntest_accuracy = 100*np.sum(np.array(ResNetFifty_predictions)==np.argmax(test_targets, axis=1))/len(ResNetFifty_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "valid_mae = mean_absolute_error(y_valid, ridge_pred)\nvalid_mae, np.expm1(valid_mae)\n", "intent": "As we can see, the prediction is far from perfect, and we get MAE $\\approx$ 1.3 that corresponds to $\\approx$ 2.7 error in \n"}
{"snippet": "predictions = my_model.predict(test_X)\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))\n", "intent": "We similarly evaluate a model and make predictions as we would do in scikit-learn.\n"}
{"snippet": "glass.loc[:, 'predictions'] = linreg.predict(X)\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score, log_loss\nlr_preds = lr.predict_proba(xtest)[:,1]\nprint(\"auc:      {}\".format(roc_auc_score(ytest, lr_preds)))\nprint(\"log_loss: {}\".format(log_loss(ytest,lr_preds)))\n", "intent": "Let us compute the score on one fold\n"}
{"snippet": "gbm_preds = gbm.predict_proba(xtest)[:,1]\nprint(\"auc:      {}\".format(roc_auc_score(ytest, gbm_preds)))\nprint(\"log_loss: {}\".format(log_loss(ytest,gbm_preds)))\n", "intent": "Once fitted, we use the model to predict\n"}
{"snippet": "num_sampled = 10\nloss = tf.reduce_mean(\n  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n                 num_sampled, vocabulary_size))\n", "intent": "We can then compute the loss\n"}
{"snippet": "cross_val_score(clf, X_train, y_train_0, cv=10, scoring='accuracy')\n", "intent": "What if you would like to perform 10-fold CV test? How would you do that\n"}
{"snippet": "precision_score(y_train_0, y_train_pred) \n", "intent": "Note the result here may vary from the video as the results from the confusion matrix are different each time you run it.\n"}
{"snippet": "recall_score(y_train_0, y_train_pred) \n", "intent": "Note the result here may vary from the video as the results from the confusion matrix are different each time you run it.\n"}
{"snippet": "f1_score(y_train_0, y_train_pred)\n", "intent": "Note the result here may vary from the video as the results from the confusion matrix are different each time you run it.\n"}
{"snippet": "predicted = regressor.predict(X['test'])\nmse = mean_squared_error(y['test'], predicted)\nprint (\"Error: %f\" % mse)\n", "intent": "Evaluate our hypothesis using test set. The mean squared error (MSE) is used for the evaluation metric.\n"}
{"snippet": "multireg.predict([0,0])\n", "intent": "** Now let's use predict function to predict mpg for light, heavy and medium cars **\n"}
{"snippet": "RSS = sum((y-linreg.predict(X))**2) \nTSS = sum((y-y.mean())**2) \nR_Squared = 1 - float(RSS)/TSS\nprint(R_Squared)\n", "intent": "This is the definition of R_Squared. Let's first calculate it = this is the hard way!\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc,roc_auc_score\ny_hat_probability = lm.predict_proba(X).T[1]  \nprint(y_hat_probability)\nprint(roc_auc_score(y, y_hat_probability))\nvals = roc_curve(y, y_hat_probability)  \n", "intent": "Answer: Raise the threshold for predicting that emails are spam to be higher, which would make less email classified as spam\n"}
{"snippet": "accuracy_score = classifier.evaluate(X_test, y_test)[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "iris_predictions = list(classifier.predict_classes(X_test))\nprint(classification_report(y_test,  iris_predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,  iris_predictions))\nprint('\\n')\n", "intent": "** Now create a classification report and a Confusion Matrix. Does anything stand out to you?**\n"}
{"snippet": "print(confusion_matrix(y_test, rfc_pred))\nprint(classification_report(y_test, rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(\"\\n\")\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "print(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "print(accuracy_score(gs.predict(Xtestlr), ytestlr))\n", "intent": "Yes GridSearchCV gives the same value for C which is 0.1 as obtained earlier by explicitly looping through different values of C.\n"}
{"snippet": "print 'Silhouette Score:', metrics.silhouette_score(X_scaled, labels, metric='euclidean')\n", "intent": "And to compute the clusters' silhouette coefficient:\n"}
{"snippet": "print 'Silhouette Score:', metrics.silhouette_score(X_scaled, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score to measure your analysis\n"}
{"snippet": "r2_test = r2_score(lin_model, test_feats, test_targets)\nprint \"r2_score:\", r2_test  \n", "intent": "Compute the R$^2$ score on the **test** data using the above function.\n"}
{"snippet": "y_pred_train = least_squares.predict(X_train)\ny_pred_test = least_squares.predict(X_test)\nprint(\"Train MSE: %.4f\" % mean_squared_error(y_pred_train, y_train))\nprint(\"Test MSE: %.4f\" % mean_squared_error(y_pred_test, y_test))\n", "intent": "*Exercise*: Compute the MSE for the train and test set. How does it compare to the situation before?\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf_svm.predict(X_test)\nprint \"Test Precision: {}\".format(metrics.precision_score(y_test, y_pred))\nprint \"Test Recall: {}\".format(metrics.recall_score(y_test, y_pred))\nprint \"Test F-Score: {}\".format(metrics.f1_score(y_test, y_pred))\n", "intent": "The scikit-learn metrics package offers the basic evaluation routines.\n"}
{"snippet": "x = np.array(trainX)\ny = np.array(trainY)\np4 = np.poly1d(np.polyfit(x, y, 3))\nfrom sklearn.metrics import r2_score\nr2 = r2_score(np.array(trainY), p4(np.array(trainX)))\nprint(r2)\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "labels = kmeans.fit_predict(X)\nlabels\n", "intent": "  * Train K-means op de dataset en haal de uiteindelijke labels op door middel van `fit_predict()`.\n"}
{"snippet": "kmeans.fit_predict(X)\n", "intent": "  * Train K-means op de dataset en haal de uiteindelijke labels op door middel van `fit_predict()`.\n"}
{"snippet": "errors_manhattan = 1 - f1_score(y_test, y_pred_manhattan, average=None)\nerrors_manhattan\n", "intent": "* Bereken de F1 score per target (digit) en per distance-measure\n* Zet de F1 scores om in error scores (1 - F1 score)\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint confusion_matrix(ytest, clf.predict(xtest))\n", "intent": "We might be less accurate bit we are certainly not overfit.\n"}
{"snippet": "WordFreqTe=np.zeros((len(datasetTest.data),len(vocabTrain)))\nWordFreqTe.shape\nfor idx1, tewords in enumerate(dsTestlower):\n    for idx2, vword in enumerate(vocabTrain):\n        WordFreqTe[idx1,idx2]=tewords.count(vword)\nypred =clf.predict(WordFreqTe)\n", "intent": "  * Classificeer de test data met behulp van de getrainde Naive Bayes.\n  * Bereken de gemiddelde F1-score (average='macro')\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "  * Classificeer de test data aan de hand van de best scorende SVM-parameter settings.\n"}
{"snippet": "errors = 1 - f1_score(y_test, y_pred, average=None)\nerrors\n", "intent": "* Bereken per digit de error tussen de voorspelde waarden en de targets door middel van (1 - F1 score)\n"}
{"snippet": "y_pred = clfMulPar.predict(X_test)\n", "intent": "  * Classificeer de test data aan de hand van de best scorende SVM-parameter settings.\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "  * Classificeer de test data aan de hand van de best scorende MLP-parameter settings.\n"}
{"snippet": "errors = 1 - f1_score(y_test, y_pred, average=None)\nerrors\n", "intent": "  * Bereken per digit de error tussen de voorspelde waarden en de targets door middel van (1 - F1 score)\n"}
{"snippet": "labels_sw = .fit_predict(X_sw)\n", "intent": "Elke window een label geven en kijken wat het resultaat is.\n"}
{"snippet": "y_predSVM = clfSVM.predict(X_pred_c)\n", "intent": "SVM geeft het beste resultaat. Dus we gebruiken die om de voorspelling te doen.\n"}
{"snippet": "labels_sw = km.fit_predict(X_sw)\n", "intent": "Elke window een label geven en kijken wat het resultaat is.\n"}
{"snippet": "metrics.fowlkes_mallows_score(y, labels)\n", "intent": "*Fowlkes-Mallows* can be used when the ground truth class assignments of the samples is known.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nX = extended[['2015 Volume Sold (Liters) sum']]\ny = extended[['2016 Sales Q1']]\nransac = sklearn.linear_model.RANSACRegressor()\ncross_val_score(ransac, X, y, cv=10)\n", "intent": "We can do better than a test/train split,  by doing cross validation.\n"}
{"snippet": "print('precision:', precision_score(expected_labels, our_labels)) \nprint('recall:', recall_score(expected_labels, our_labels)) \nprint('accuracy:', accuracy_score(expected_labels, our_labels)) \n", "intent": "Precision/Recall/Accuracy stats\n"}
{"snippet": "from sklearn.model_selection import KFold\nkf = KFold(5, shuffle=True)\nnp.mean(-cross_val_score(dtr, X, y, cv= kf,\n                        scoring='neg_mean_squared_error'))\n", "intent": "- Train your model and calculates its MSE in five-fold cross-validation.\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint predicted\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"sh-expo.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "log_preds, y = learn.TTA() \nprobs = np.mean(np.exp(log_preds),0)\naccuracy_np(probs, y), metrics.log_loss(y, probs)\n", "intent": "Training loss and validation loss are getting closer and smaller. We are on right track.\n"}
{"snippet": "def predict_result(model,x_test,img_size_target,batch_size): \n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test1 = model.predict([x_test],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict([x_test_reflect],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    preds_avg = (preds_test1 +preds_test2)/2\n    return preds_avg\n", "intent": "Again plot some sample images including the predictions.\n"}
{"snippet": "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\nmask_valid = np.array([downsample(x) for x in y_valid])\n", "intent": "Again plot some sample images including the predictions.\n"}
{"snippet": "test_acc, test_batches = 0, 0\nfor x_batch, y_batch in get_batches((X_test, y_test), batch_size):\n    net.training = False\n    predictions = net.forward(x_batch)\n    test_acc += accuracy_score(np.argmax(predictions, axis=1), y_batch)\n    test_batches += 1\nprint('\\t test accuracy:\\t %s' % (test_acc / test_batches * 100))\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "def predict(ratings, similarity, type='item'):\n    if type == 'item':\n        pass\n    return \n", "intent": "<img src=\"user_sim.gif\">\n<img src=\"item_sim.gif\">\n"}
{"snippet": "print metrics.accuracy_score(test_label,p)\n", "intent": "*Hint : Use metrics.accuracy_score*\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"MSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\", (mean_squared_error(ys, predictions))**.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "xs.append(4)\nys.append(50)\npredictions.append(8)\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "print(classification_report(Y_test, Y_pred))\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "These quantities are also related to the ($F_1$) score, which is defined as the harmonic mean of precision and recall.\n$F1 = 2\\frac{P \\times R}{P+R}$\n"}
{"snippet": "labels = model.predict(patches_hog)\nlabels.sum()\n", "intent": "Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains an airplane:\n"}
{"snippet": "sand_pred = gnb.predict(data_test)\nS = sand_pred.reshape(M_tst, N_tst)\n", "intent": "And run the model we trained on the first image\n"}
{"snippet": "prediction = knn.predict(X_test)\nprediction\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = predict_y(W, b, X_test, 3)\naccuracy_score(y_test, y_pred)\n", "intent": "Let's load a convenience function from scikit-learn to assess the overall accuracy\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_pred, y_test))\n", "intent": "    1. Compute the F1 (or other accuracy) scores for the TF ANN\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n", "intent": "Number of correct classified inputs, by the total number of inputs.\n    (True postive + True negative) / Total nb\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X))**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "score = accuracy_score(clf.predict(Xtestlr), ytestlr)\nscore\n", "intent": "The same best value for c was obtained using GridSearchCV\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier does not seem to be better. The test score for this classifier (71%) is lower than the original which was 77%\n"}
{"snippet": "model.load_weights('mnist.model.best.hdf5')\nscore = model.evaluate(x_test, y_test, verbose=0)\nscorex = 100*score[1]\nprint(\"Accuracy: %.2f%%\" % scorex )\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "InceptionV3Preditction = [np.argmax(model_inc.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(InceptionV3Preditction)==np.argmax(test_targets, axis=1))/len(InceptionV3Preditction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "pre = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test,pre))\nprint('\\n')\nprint(classification_report(y_test,pre))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "prediction = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, pred))\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, pred))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "predictions = svm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint(\"Test set predictions :\\n {}\".format(y_pred))\n", "intent": "<h3>Evaluating Model</h3>\n"}
{"snippet": "print(classification_report(y_test, predictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, predictions))\n", "intent": "** Now create a classification report and a Confusion Matrix. Does anything stand out to you?**\n"}
{"snippet": "x_test = np.array(['she is my better half'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\nx_test = np.array(['i am her better half'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "y_hat = model.predict(x_test)\ncifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', \n                  'frog', 'horse', 'ship', 'truck']\n", "intent": "This may give you some insight into why the network is misclassifying certain objects.\n"}
{"snippet": "dog_breed_predictions = [\n    np.argmax(model.predict(np.expand_dims(tensor, axis=0))) \n    for tensor in test_tensors]\ntest_accuracy = (\n    100 * np.sum(\n        np.array(dog_breed_predictions) == \n        np.argmax(test_targets, axis=1)) /\n    len(dog_breed_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [\n    np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) \n    for feature in test_VGG16]\ntest_accuracy = (\n    100 * np.sum(\n        np.array(VGG16_predictions) == np.argmax(test_targets, axis=1)) /\n    len(VGG16_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "xception_predictions = [\n    np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) \n    for feature in test_Xception]\ntest_accuracy = (\n    100 * np.sum(\n        np.array(xception_predictions) == \n        np.argmax(test_targets, axis=1)) /\n    len(xception_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mutual_info_score\nmax_mutual_info = 0\ncount = 0\ncolumn = \"\"\nfor col1 in marketing_dataframe.columns.values:\n    temp = mutual_info_score(marketing_dataframe[col1],marketing_dataframe[\"outcome\"])\n    if temp > max_mutual_info:\n        max_mutual_info = temp\n        column = col1\nprint \"The highest Mutual Information is\",max_mutual_info,\"for column\",column\n", "intent": "Which attribute has the highest Mutual Information with the 'outcome' attribute?\n"}
{"snippet": "x = [[0.2, .15, 0.68, 0.05, 0.328]]\ngrid_knn.predict(x)\n", "intent": "This simple technique gives us the best K value.\nWe can use the best model from grid_knn to make predictions.\n"}
{"snippet": "silhouette_score(X, labs2)\n", "intent": "Derive silhouette score of country data and labels\n"}
{"snippet": "point = np.array([[0, 0]])\nmodel.predict(point)\n", "intent": "Make prediction on point (0,0). Works same way as sklearn.\n"}
{"snippet": "preds = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "new_data = np.asarray([0.18,0.15]).reshape(1,-1)\npred3 = knn3.predict(new_data)\npred5 = knn5.predict(new_data)\nprint (\"The knn3 model thinks new_data belongs to class {}\".format(pred3[0]))\nprint (\"The knn5 model thinks new_data belongs to class {}\".format(pred5[0]))\n", "intent": "Apply model on a new point\n"}
{"snippet": "knn3.predict_proba(new_data)\n", "intent": "Look at class probabilities\n"}
{"snippet": "metrics.mean_absolute_error(y_true, y_pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "metrics.mean_squared_error(y_true, y_pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "pred_labels = lr.predict(X)\n", "intent": "Plot the probabilities and the predictions\n"}
{"snippet": "null_acc = np.repeat(0.522, repeats=(y_test.shape[0]))\nlog_loss(y_test, null_acc)\n", "intent": "Now let's add some context to the log loss value by using the null accuracy\n"}
{"snippet": "accuracy_score(y_test, labels_60)\n", "intent": "Does this give a better accuracy score?\n"}
{"snippet": "model.predict_proba([[0,0]])\n", "intent": "What if you want the probabilities of each class?\n"}
{"snippet": "preds_r = rtree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "new_text = [\"I had a decent time at this restaurant. \\\nThe food was delicious but the service was very poor. \\\nI recommend the salad but do not eat the french fries.\"]\nnew_text_transform = vect.transform(new_text)\nprint nb.predict(new_text_transform)\nprint nb.predict_proba(new_text_transform)\n", "intent": "How do you assess this model? \n<br>\nLet's try it on some new text\n"}
{"snippet": "preds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "We can evaluate the (new) predictions:\n"}
{"snippet": "preds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "And lets check the error statistics...\n"}
{"snippet": "y_hat = alpha + np.dot(X_test, beta)\npreds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "Using the estimated values, we can make predictions for the test set:\n"}
{"snippet": "samples = fit.extract(permuted=True)  \ny_hat = np.mean(samples[\"alpha\"].T + np.dot(X_test, samples[\"beta\"].T), axis=1)\npreds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "Extract results and compute error statistics:\n"}
{"snippet": "def compute_error(trues, predicted):\n    corr = np.corrcoef(predicted, trues)[0,1]\n    mae = np.mean(np.abs(predicted - trues))\n    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n    rmse = np.sqrt(np.mean((predicted - trues)**2))\n    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n    return corr, mae, rae, rmse, r2\n", "intent": "Compute error statistics of the model's imputations:\n"}
{"snippet": "train_preds = lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_seed, shuffle=True)\n    NNF = NearestNeighborsFeats(n_jobs=20, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf)   \n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "train_gini = 2*metrics.roc_auc_score(train_X['Cancer'], logistic_fit.predict()) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\ntest_gini = 2*metrics.roc_auc_score(test_X['Cancer'], logistic_fit.predict(test_X)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)\n", "intent": "    - Coefficient Stability - sign and p-values\n"}
{"snippet": "print(confusion_matrix(y_test, preds_r))\nprint(\"\\n\")\nprint(classification_report(y_test, preds_r))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "Y_pred = model.predict(X_val)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(Y_val,axis = 1) \nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \nplot_confusion_matrix(confusion_mtx, classes = range(10)) \n", "intent": "<h1><a name=\"cm\">Confusion matrix</a></h1>\n"}
{"snippet": "print(classification_report(predict, y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster '], kmeans.labels_))\nprint('\\n\\n')\nprint(classification_report(df['Cluster '], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"timg.jpeg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "x_test = np.array(['not happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", loss,acc)\n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_logreg))\n", "intent": "AKA: \"Missclassification Rate\"\n"}
{"snippet": "print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred_logreg))\n", "intent": "How sensitive is the classifier to detecting positive instances?\nAKA \"True Positive Rate\" or \"Recall\"\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred))\n", "intent": "AKA: \"Missclassification Rate\"\n"}
{"snippet": "predictions = lm.predict(X_test)\npredictions[0:10]\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "np.mean((bos.PRICE - lm.predict(X))**2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "plot_clusters(clust.predict(X), clust.cluster_centers_)\n", "intent": "How'd `scikit-learn` do?\n"}
{"snippet": "print \"Complete Match: \", (model.predict(X) == predict(model, X, n_jobs=4)).all()\nprint model.predict(X[:20])\nprint predict(model, X[:20], n_jobs=4)\n", "intent": "Now let's make sure that we are getting the same results for both of them.\n"}
{"snippet": "(model.predict_proba(X[:100]) - predict_proba(model, X[:100], n_jobs=4)).sum()\n", "intent": "We can also connfirm that this is producing the correct result.\n"}
{"snippet": "metrics.accuracy_score(y_test, output)\n", "intent": "Getting the accuracy on the test set.\n"}
{"snippet": "train_output = forest.predict(X_train)\n", "intent": "Using model to precit training set values (example of what is not supposed to be done)\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nevs = explained_variance_score(y_test, y_predict)\nprint('EVS: {}'.format(round(evs,2)))\n", "intent": "**Explained Variance**  \nAnother **performance measure** we can use is the **Explained Variance Score (EVS)**.\n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. \n"}
{"snippet": "score = logistic.predict_proba(X)\nscore_lalonde = dataV2.copy()\nscore_lalonde[\"prop_0\"] = score[:,0]\nscore_lalonde[\"prop_1\"] = score[:,1]\nscore_lalonde.head(10)\n", "intent": "Once our data is trained, we now calculate the propensity scores:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions_Resnet50 = [np.argmax(model_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(predictions_Resnet50)==np.argmax(test_targets, axis=1))/len(predictions_Resnet50)\nprint('Resnet50 test accuracy: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('predicted:', spam_detector.predict(tfidf4)[0])\nprint('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "all_predictions = spam_detector.predict(messages_tfidf)\nprint(all_predictions)\n", "intent": "Hooray! You can try it with your own texts, too.\nA natural question is to ask, how many messages do we classify correctly overall?\n"}
{"snippet": "print(classification_report(messages['label'], all_predictions))\n", "intent": "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:\n"}
{"snippet": "predictions = nb_detector.predict(msg_test)\nprint (confusion_matrix(label_test, predictions))\nprint (classification_report(label_test, predictions))\n", "intent": "And overall scores on the test set, the one we haven't used at all during training:\n"}
{"snippet": "print (svm_detector.predict([\"Hi mom, how are you?\"])[0])\nprint (svm_detector.predict([\"WINNER! Credit for free!\"])[0])\n", "intent": "So apparently, linear kernel with `C=1` is the best parameter combination.\nSanity check again:\n"}
{"snippet": "resnet50_pred=[np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nresnet50_tacc=100*np.sum(np.array(resnet50_pred)==np.argmax(test_targets, axis=1))/len(resnet50_pred)\nprint('Test accuracy on Resnet50 model: {}'.format(resnet50_tacc))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "np.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "preds = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "lambda_ = \ntrain_loss = get_cross_entropy_loss(logits=predictions_student, labels=batch_train_labels)\ntrain_loss += lambda_ * distill_kl_loss\n", "intent": "**Define the joint training loss**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test_scaled, predictions)\nr2 = model.score(X_test_scaled, y_test_scaled)\nprint(f\"MSE: {MSE}, R2: {r2}\")\n", "intent": "Step 5) Quantify your model using the scaled data\n"}
{"snippet": "print 'Sum of squared Errors: %f' % np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "print 'Mean squared Error: %f' % np.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "prune_acc_train = round(accuracy_score(optimal_tree.predict(X_train), y_train), 3)\nnoprune_acc_train = round(accuracy_score(dtree.predict(X_train), y_train), 3)\nprint('The pruned tree has an accuracy score of {score} on training data.'.format(score=prune_acc_train))\nprint('The un-pruned tree has an accuracy score of {score} on training data.'.format(score=noprune_acc_train))\n", "intent": "(i) Compare the training error rates between the pruned and un-pruned trees. Which is higher?\n"}
{"snippet": "prune_acc_test = round(accuracy_score(optimal_tree.predict(X_test), y_test), 3)\nnoprune_acc_test = round(accuracy_score(dtree.predict(X_test), y_test), 3)\nprint('The pruned tree has an accuracy score of {score} on test data.'.format(score=prune_acc_test))\nprint('The un-pruned tree has an accuracy score of {score} on test data.'.format(score=noprune_acc_test))\n", "intent": "(j) Compare the test error rates between the pruned and unpruned trees. Which is higher?\n"}
{"snippet": "rfc_pred = rForest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(rfc_pred, y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "tree_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, preds))\nprint(\"\\n\")\nprint(classification_report(y_test, preds))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "print(classification_report(y_test, rf_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(colleges['Cluster'], km.labels_))\nprint(classification_report(colleges['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred1 = model1.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = logm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = log_reg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\nconfusion_matrix(y_test, y_pred)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "scores = cross_val_score(lr, X, y, cv=10, scoring='neg_mean_squared_error')\nscores\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "predictF = dforest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print('Decision Tree: \\n')\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint('Decision Forest: \\n')\nprint(classification_report(y_test,predictF))\nprint('\\n')\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "preds = pipe.predict(X_test)\nprint(confusion_matrix(y_test, preds))\nprint(\"\\n\")\nprint(classification_report(y_test, preds))\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "R50_predictions = [np.argmax(R50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_R50]\ntest_accuracy = 100*np.sum(np.array(R50_predictions)==np.argmax(test_targets, axis=1))/len(R50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'R2 with PCA with 60 components: ', r2_score(y_test,pipe.predict(X_test))\n", "intent": "Insight: with this rudimental, untuned linear regression on 60 principal components we obtained an R2 of 58%\n"}
{"snippet": "print 'R2 with PCA with 60 components: ', r2_score(y_test,pipe.predict(X_test))\nprint 'RMSE with PCA with 60 components: ', mean_squared_error(y_test,pipe.predict(X_test))\n", "intent": "Insight: with this rudimental, untuned linear regression on 60 principal components we obtained an R2 of 58%\n"}
{"snippet": "test_predictions = lin_reg.predict(X_test)\n", "intent": "Finally, let's see how either of the two models performs against our Test Set\n"}
{"snippet": "best_model = gs.best_estimator_\ntest_predictions = best_model.predict(X_test)\nprint 'Test R2: ',r2_score(y_test, test_predictions)\nprint 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\nprint 'Test MAE: ',mean_absolute_error(y_test, test_predictions)\n", "intent": "Now testing with Test Dataset:\n"}
{"snippet": "model, predictions, r2, mse, mae, rmse = nonlinear_reg(X_train_intns, y_train, 2)\ntest_predictions = model.predict(X_test)\nprint 'Test R2: ',r2_score(y_test, test_predictions)\nprint 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\nprint 'Test MAE: ',mean_absolute_error(y_test, test_predictions)\n", "intent": "Now doing nonlinear regressions with Interaction Features\n"}
{"snippet": "test_predictions_lin_reg = lin_reg.predict(X_test)\ntest_predictions_tree_reg = best_model_dtree.predict(X_test)\ntest_predictions_best_model_svr = best_model_svr.predict(X_test)\ntest_predictions_best_model_kneigh = best_model_kneigh.predict(X_test)\n", "intent": "Now we evaluate the predictions on the test set\n"}
{"snippet": "model.predict(x)\n", "intent": "Predicting on the training points should return very good results, and in fact, does.\n"}
{"snippet": "np.linalg.norm(model.predict(x) - y)**2 / np.linalg.norm(y)**2\n", "intent": "I personally like to divide this error by the squared norm of y to get the ratio of the total variance reduced.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\npreds = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nyValidationPrediction = gnb_prediction\nconfusionMatrixValidation = confusion_matrix(yvalidation,yValidationPrediction)\nlogLossValidation = log_loss(yvalidation,yValidationPrediction)\nplot_confusion_matrix(confusionMatrixValidation)\nconfusionMatrixValidation\n", "intent": "By using the prediction of the Gaussian Naive Bayes model, compute and display the confusion matrix on the validation set.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\n", "intent": "Evaluate the model on the `test_images` and `test_labels` tensors.\nDocumentation : https://keras.io/models/model/\n"}
{"snippet": "results = model.predict(X_test)\npd.crosstab(y_test, results)\n", "intent": "This tree is much simpler than the original tree based on all the data.\n"}
{"snippet": "from sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\nimport numpy as np\nrange_n_clusters = XXXXX\nfor n_clusters in range_n_clusters:\n    Z = XXXXX\n    cluster_labels=XXXXX\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters ={},\".format(n_clusters)+\" the average silhouette_score is :{}\".format(silhouette_avg))\n", "intent": "http://scikit-learn.org/stable/modules/clustering.html\nPlease fullfill the the XXXXX part.\n"}
{"snippet": "Resnet50_model_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_model_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\ncomplete_ar_score\navg_ar_score = adjusted_rand_score(iris.target, avg_pred)\navg_ar_score\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))\n", "intent": "We now compute some commonly used measures of prediction \"goodness\".  \nFor more detail on these measures see\n[6],[7],[8],[9]\n"}
{"snippet": "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))\nprint(\"Precision = %f\" %(skm.precision_score(test_truth,test_pred)))\nprint(\"Recall = %f\" %(skm.recall_score(test_truth,test_pred)))\nprint(\"F1 score = %f\" %(skm.f1_score(test_truth,test_pred)))\n", "intent": "As before, we now compute some commonly used measures of prediction \"goodness\". \n"}
{"snippet": "def predict(theta, X):\n    h_theta = h_of_theta(theta, X)\n    return np.round(h_theta)\np = predict(optimal_theta, X)\nprint '\\nTraining Accuracy: ', np.mean((p == y) * 100)\n", "intent": "Let's compute the accuracy on the training set:\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'images': mnist.test.images}, y=mnist.test.labels,\n    batch_size=batch_size, shuffle=False)\nmodel.evaluate(input_fn)\n", "intent": "Evaluate the Model. Define the input function for evaluating.\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(df['Real'], df['Predicted']))\nprint('MSE:', metrics.mean_squared_error(df['Real'], df['Predicted']))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(df['Real'], df['Predicted'])))\n", "intent": "**Validation and Evaluation**\n"}
{"snippet": "predictions = lm.predict(X_test)\nlen(X_train)\n", "intent": "** Use lm.predict() to predict off the X_test set of the data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = clf.predict(X_test)\ny_true = y_test\nconfusion_matrix(y_true, y_pred)\n", "intent": "How do they compare to the true values (of species?)\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = clf.predict(X_test)\ny_true = y_test\nconfusion_matrix( y_pred, y_true, labels=[\"Normal\", \"Failed\"])\n", "intent": "How do they compare to the true values \n"}
{"snippet": "model.load_weights('pre_trained_spacy_model.h5')\nloss, acc = model.evaluate(x_test, y_test)\nprint(model.metrics_names)\nprint(loss, acc)\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "model.load_weights('task_specific_spacy_model.h5')\nloss, acc = model.evaluate(x_test, y_test)\nprint(model.metrics_names)\nprint(loss, acc)\n", "intent": "Lets try with the second model:\n"}
{"snippet": "def get_projected_rides(minute):\n    best_model = polynomial_regression_fit(times, counts, 2)\n    projections = polynomial_regression_predict(best_model.params, 2, times)\n    return projections[minute]\nmins = 1323\nrides = get_projected_rides(mins)\nprint \"The number of rides at\", mins, \"is\", rides\n", "intent": "The best fit by R-squared, AIC and BIC is a second degree polynomial expression.\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Erstelle die Vorhersagen (en. predictions) aus den Testdaten und werte dann Classification Report und Confusion Matrix aus.**\n"}
{"snippet": "X_new = [ [3, 5, 4, 2], \n          [5, 4, 3, 2] ]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Schauen wir uns nun die Vorhersagen des trainierten Modells an.\n"}
{"snippet": "model1.predict(data)\n", "intent": "y = 1 / (1 + np.exp(-(w.T).dot(data.T)))\ny\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.mean_absolute_error(y_test, y_pred))\n", "intent": "Mean absolute error is the average absolute value of the difference between the predicted and actual values:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "Mean Squared Error (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "from sklearn import metrics\nprint \"The accuracy is %.2f\" % metrics.accuracy_score([1,1], [1,1])\n", "intent": "You may have also noticed that I often do something like\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint \"The accuracy is %.2f\" % accuracy_score([1,1], [1,1])\n", "intent": "But I could also do something like this\n"}
{"snippet": "print 'accuracy_score:', accuracy_score(Y_test, Y_pred)\nprint 'precision:', precision_score(Y_test, Y_pred)\nprint 'recall:', recall_score(Y_test, Y_pred)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "print cross_val_score(dt, X, y, cv=5).mean()\nprint cross_val_score(rf, X, y, cv=5).mean()\nprint cross_val_score(et, X, y, cv=5).mean()\n", "intent": "Let's compare the 3 models (re-init Decision Tree)\n"}
{"snippet": "metrics.accuracy_score(y, labels)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "knn.predict([[3, 5, 4, 2]])\n", "intent": "In order to make a **prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "print metrics.classification_report(y, labels)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "X_prob = logreg.predict_proba(X_test)\nprint X_prob[:10]\nprint \" \"\nprint X_prob[:10].T\nprint \" \"\nprint X_prob[:10].T[1]\nprint \"\"\nprint X_prob[:10][1]\n", "intent": "* print_metrics(x, y, y_pred, y_test, X_test, model)\n* metrics_list(x, y, X_test, Y_test, model, model_name)\n"}
{"snippet": "y_hat = knn.predict(wine_df)\n(y_hat == y). mean()\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "model.predict_proba(point)\n", "intent": "Probabilities of each class.\n"}
{"snippet": "cross_val_score(model, X, y, cv = 5,scoring =\"precision\").mean()\n", "intent": "Cross validate with precision and recall\n"}
{"snippet": "def get_error(X, y, actual, model):\n    y = np.transpose([y])\n    actual = np.transpose([actual])\n    model = model.predict(X)\n    model = np.transpose([model])\n    train_error = (model - y) ** 2\n    train_error = np.average(train_error)\n    test_error = (model - actual) ** 2\n    test_error = np.average(test_error)\n    return train_error, test_error\n", "intent": "Hint: Try looking at the predict method in the LinearRegression object\n"}
{"snippet": "def get_error(X, y, actual, model):\n", "intent": "Hint: Try looking at the predict method in the LinearRegression object\n"}
{"snippet": "actual = train_df[target]\npred = dt.predict(train_df[predictors])\nmse = np.mean((actual - pred) ** 2)\nmse\n", "intent": "mean((actual - pred) ** 2)\n"}
{"snippet": "predict = list(learn.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "score = model.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Was it worth the wait?\n"}
{"snippet": "predict = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predict = naive_bayes.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predict_pipeline = pipeline.predict(X_test1)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "iris_svm_predict = svm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "final_model = grid.best_estimator_ \nprediction = final_model.predict(X_test) \nprint('confusion matrix:\\n')\nprint(metrics.confusion_matrix(prediction, Y_test))\nprint(\"\")\nprint('classification report\\n')\nprint(metrics.classification_report(prediction, Y_test, digits = 4))\n", "intent": "<div class=\"span5 alert alert-info\">\n<h2><center> Predict with the best estimator\n"}
{"snippet": "predictions_lg = linear_regression.predict(car_test['cyl displ hp weight accel yr'.split()])\npredictions_lg[:10]\n", "intent": "8.2 Generate predictions with a model\n-------------------------------------\n"}
{"snippet": "cv_scores = model_selection.cross_val_score(linreg, dd_examples, dd_targets, \n                                             cv=kfold, \n                                             scoring='neg_mean_squared_error', \n                                             n_jobs=-1) \nprint(cv_scores) \n", "intent": "There is an easier way, with `cross_validation`, which will do this routine for you.\n"}
{"snippet": "y_pred = fit.predict(X)\nprint(metrics.r2_score(y, y_pred))\n", "intent": "The categorical variable (Romance) was significant. \n"}
{"snippet": "pred_label = ['PG-13' for d in range(len(X_test))]\nacc_pred = accuracy_score(y_test,pred_label)\nprint \"Always Predict PG-13 Accuracy Score: \", acc_pred\n", "intent": "Make a baseline stupid predictor that always predicts the label that is present the most in the data. Calculate its accuracy on a test set.\n"}
{"snippet": "score, acc = model.evaluate(x_test, y_test,\n                            batch_size=32,\n                            verbose=2)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n", "intent": "OK, let's evaluate our model's accuracy:\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100.0*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\nimport numpy as np\nprint np.mean(y == y_pred)\nprint knn.score(X, y)\n", "intent": "Let's compute the accuracy. This is known as **training accuracy** because we are testing on the same data we used to train the model.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predicted_price=lr.coef_*x_train+lr.intercept_\nlr_predict=lr.predict(x_train)\n", "intent": "By looking into the attributes of your model, write down an equation for predicting the price of a car given the engine-power.\n"}
{"snippet": "print ('the coefficient of determination is:', r2_score(target_price,ml_predict))\nprint ('the Root Squared Error is:', sqrt(mean_squared_error(target_price,ml_predict)))\nprint ('the Mean Absolute Error is :', mean_absolute_error(target_price,ml_predict))\nprint ('the correlation coefficient is:', np.corrcoef(target_price,ml_predict))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "new_auto_numeric=auto_numeric.copy(deep=True)\nnew_auto_numeric['engine-size']=transformed_size\ntrain_transformed=new_auto_numeric.drop('price',axis=1)\ntransf_predict = cross_val_predict(lr, train_transformed, target_price, cv = kf)\nprint ('the coefficient of determination is:', r2_score(target_price,transf_predict))\nprint ('the Root Squared Error is:', sqrt(mean_squared_error(target_price,transf_predict)))\nprint ('the Mean Absolute Error is :', mean_absolute_error(target_price,transf_predict))\nprint ('the correlation coefficient is:', np.corrcoef(target_price,transf_predict))\n", "intent": "Now re-build a Linear Regression model on the transformed dataset and report the R^2, RMSE, MAE and CC metrics.\n"}
{"snippet": "result=confusion_matrix(y,clf.predict(X))\nprint(result)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "print(\"The Coefficient of Determination (R^2) is: {:.5f}\".format(r2_score(y_mlr,result_mlr)))\nprint(\"The Root Mean Squared Error (RMSE) is: {:.5f}\".format(np.sqrt(mean_squared_error(y_mlr,result_mlr))))\nprint(\"The Mean Absolute Error (MAE) is: {:.5f}\".format(mean_absolute_error(y_mlr,result_mlr)))\nprint(\"The Correlation Coefficient (CC) is: \\n{}\".format(np.corrcoef(result_mlr,y_mlr)))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "score = model.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "But, even with just 10 epochs, we've outperformed our Tensorflow version considerably!\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(features, axis=0))) for features in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "IcebergModel.evaluate(x=X_band,y=target)\n", "intent": "Evaluating the model on the traning data and it gets a 96.5 % accuarcy . \n"}
{"snippet": "from sklearn.metrics import classification_report\npred_label = IcebergModel.predict(x=X_band)\npred_label[pred_label>0.5]=1\npred_label[pred_label<=0.5]=0\nprint(classification_report(target,pred_label))\n", "intent": "Classification report on the training data .  \n"}
{"snippet": "score = model.evaluate(features, targets)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(features_test, targets_test)\nprint(\"\\n Testing Accuracy:\", score[1])\n", "intent": "**Note:** Worth coming back to this later once I better understand how keras and NN work to try to get better results.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "metrics = model_res.evaluate(test_res, test_targets)\nprint('Test accuracy: {:.4f}'.format(metrics[1]))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics.cluster import adjusted_rand_score\nprint \"Comparing Y (truth) to BMM:   \", adjusted_rand_score(Y, Y_BMM)\nprint \"Comparing Y (truth) to KMeans:\", adjusted_rand_score(Y, Y_KMeans)\n", "intent": "> Report the rand index score using the class code as ground truth label for both algorithms and comment on your findings.\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nnp.sqrt(metrics.mean_squared_error(y_test, predictions))\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "trn_features = model.predict(trn_data, batch_size=batch_size)\nval_features = model.predict(val_data, batch_size=batch_size)\n", "intent": "...and their 1,000 imagenet probabilties from VGG16--these will be the *features* for our linear model:\n"}
{"snippet": "predictions = lstm_basic_model.predict(X_test)\n", "intent": "**Prediction using basic LSTM model**\n"}
{"snippet": "def model_lstm_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n    return trainScore[0], testScore[0]\n", "intent": "**Error/accuracy score of basic LSTM model**\n"}
{"snippet": "opt_predictions = optimized_lstm_model.predict(X_test)\n", "intent": "**Prediction using Optimized LSTM model**\n"}
{"snippet": "model_lstm_score(optimized_lstm_model,X_train, y_train, X_test, y_test)\n", "intent": "**Error/accuracy score of Optimized LSTM model**\n"}
{"snippet": "predictions = gru_basic_model.predict(X_test)\n", "intent": "**Prediction using Basic GRU model**\n"}
{"snippet": "def model_gru_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n    return trainScore[0], testScore[0]\n", "intent": "**Error/accuracy score of Basic GRU model**\n"}
{"snippet": "opt_predictions = gru_opt_model.predict(X_test)\n", "intent": "**Prediction using Optimized GRU model**\n"}
{"snippet": "model_gru_score(gru_opt_model, X_train, y_train, X_test, y_test)\n", "intent": "**Error/accuracy score of Optimized GRU model**\n"}
{"snippet": "predictions = regression_model.predict(X_test)\n", "intent": "   **Use the model for data prediction **\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "* Determiner un metrique tel que Accuracy\n"}
{"snippet": "nb.predict(unknown_dtm)\n", "intent": "Question: Why do we use this one? Why did we need to define a new one for the cross validation step above?\n"}
{"snippet": "list(zip(['dickinson', 'anthem'], nb.predict(unknown_dtm), nb.predict_proba(unknown_dtm)))\n", "intent": "Let's zip this together with the name of the poems to make sense of the output\n"}
{"snippet": "X_train, X_test, X_validation = np.split(X, [int(.5 * len(X)), int(.7 * len(X))])\nY_train, Y_test, Y_validation = np.split(Y, [int(.5 * len(Y)), int(.7 * len(Y))])\nplot_train_test_error(*calc_train_test_error(X_train, X_test, Y_train, Y_test))\n", "intent": "**Exercise 3 **  Vary the train ,test and validation ratios and observe how overfitting changes.\n"}
{"snippet": "print(lr.predict(X_train))\nprint(y_train)\n", "intent": "Apply the model and evaluate it on the training data:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores_sklearn = cross_val_score() \n", "intent": "Compare the result of your implementation with the result of the ``cross_val_score`` method in scikit-learn.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 86%?\n"}
{"snippet": "my_model_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(my_model_predictions)==np.argmax(test_targets, axis=1))/len(my_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions)**0.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\nprobs[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "metrics.silhouette_score(Xs, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "rf_pred = rf.predict_proba(df_test[rhs_columns])\nfpr, tpr, _ = roc_curve(df_test['inclusion'], rf_pred[:,1])\nroc_auc = auc(fpr,tpr)\nprint(\"Area under the ROC Curve, RF: %f\" % roc_auc)\n", "intent": "Print the AUC for Random Forest\n"}
{"snippet": "rf.predict_proba(X_test.head())\n", "intent": "We can see the probabilities of churn\n"}
{"snippet": "model.load_weights('imdb.model.best.hdf5')\nscore = model.evaluate(x_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test)\nprint(\"\\n Testing Accuracy:\", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "RESNET50_predictions = [np.argmax(RESNET50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_RESNET50]\ntest_accuracy = 100*np.sum(np.array(RESNET50_predictions)==np.argmax(test_targets, axis=1))/len(RESNET50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test, rfc_predictions))\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "pred = dnn.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "cv_scores = cross_val_score(RF_clf,X_train, y_train, cv=10, scoring='accuracy')\nprint('The accuracy scores for the iterations are {}'.format(cv_scores))\nprint('The mean accuracy score is {}'.format(cv_scores.mean()))\n", "intent": "Compute k-fold cross validation on training dataset and see mean accuracy score\n"}
{"snippet": "model.predict([np.array([3]), np.array([6])])\n", "intent": "We can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user \n"}
{"snippet": "Y_sample = clf2.predict(X_sample)\nY_sample\n", "intent": "predict the labels for the kaggle sample set\n"}
{"snippet": "prediccion_logreg = modelo_logreg.predict(Xtest)\ncf_mat = confusion_matrix(ytest, prediccion_logreg)\nprint(\"Clientes correctamente clasificados: {}\".format(cf_mat[0][0] + cf_mat[1][1]))\nprint(\"Clientes buenos clasificados como malos: {}\".format(cf_mat[1][0]))\nprint(\"Clientes malos clasificados como buenos: {}\".format(cf_mat[0][1]))\n", "intent": "El modelo tuvo un rendimiento excelente. Vamos a visualizar los falsos positivos y negativos que obtuvo el modelo.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Training phase did a good prediction but testing prediction was low, still I might not consider as a better classifier.\n"}
{"snippet": "choser_Lr_test=chosen_Lr.predict(Test_X)\nprint classification_report(Test_y,choser_Lr_test)\n", "intent": "Checking metrics on the Test set. We see that we almost get the same metrics as the training set. Which means that we did not overfit.\n"}
{"snippet": "Xc_predictions = [np.argmax(Xc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xc]\ntest_accuracy = 100*np.sum(np.array(Xc_predictions)==np.argmax(test_targets, axis=1))/len(Xc_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_hat = model.predict(x_test)\ny_hat_two = model_two.predict(x_test)\ncifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n", "intent": "This may give you some insight into why the network is misclassifying certain objects.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=1)\nprint(\"Accuracy: \", score)\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\nprint(train_predict.shape)\nprint(test_predict.shape)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "val_pseudo = bn_model.predict(conv_val_feat, batch_size=batch_size)\n", "intent": "To do this, we simply calculate the predictions of our model...\n"}
{"snippet": "predictions  = dforest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(data['Cluster'],kmeans.labels_))\nprint(classification_report(data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "name, model = (\"glove_small_tfidf\", etree_glove_small_tfidf)\nprint(name, cross_val_score(model, X, y, cv=5).mean())\n", "intent": "benchmark all the things!\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(gtdb_test.target, predicted, target_names=gtdb_test.target_names))\n", "intent": "An accuracy of 85.8% is a much better choice. Here is a more detailed performance analysis of the results:\n"}
{"snippet": "t0 = time.time()\npredicted = gs_all_clf.predict(gtdb_test.data)\nt1 = time.time()\nprint(t1-t0, time.ctime(t1))\nnp.mean(predicted == gtdb_test.target)\n", "intent": "The comparitive accuracy is 89.7% - much better!\n"}
{"snippet": "scaler.transform([[4.7, 3.1]])\nprint(clf2.decision_function(scaler.transform([[4.7, 3.1]])))\nclf2.predict(scaler.transform([[4.7, 3.1]]))\n", "intent": "Let us evaluate on the previous instance to find the three-class prediction. Scikit-learn tries the three classifiers. \n"}
{"snippet": "y_pred=clf.predict(X_test4)\nprint (metrics.classification_report(y_test, y_pred, target_names=['setosa','versicolor','virginica']))\n", "intent": "Measure precision & recall in the testing set, using all attributes, and using only petal measures\n"}
{"snippet": "def plot_gen(G, n_ex=16):\n    plot_multi(G.predict(noise(n_ex)).reshape(n_ex, 28,28), cmap='gray')\n", "intent": "This is just a helper to plot a bunch of generated images.\n"}
{"snippet": "print(\"Score of model with test data defined above:\")\nprint(rf_model.score(X_test, y_test))\nprint()\npredicted = rf_model.predict(X_test)\nprint(\"Classification report:\")\nprint(metrics.classification_report(y_test, predicted)) \nprint()\n", "intent": "Let's look at the classification performance on the test data:\n"}
{"snippet": "label_idx = model_rf.predict([random_iris])\nlabel_idx\n", "intent": "Can we use our model to predict the type?\n"}
{"snippet": "age_z = lin_reg.predict(random_patient)\nage_z\n", "intent": "Now let's use our model to predict!\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "**This looks really promising!!! Checking the predicted values now**\n"}
{"snippet": "acc= accuracy_score(Y_test, y_pred)\nacc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"family2.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "arr = bst.predict(xgboost.DMatrix(x_test_new[['team1', 'team2', 'p1', 'p2']]))\narr\n", "intent": "Wow! Finally our model better than constant predictions! Congratulations! Don't hesitate, submit!\n"}
{"snippet": "acc = accuracy_score(Y_test, Y_pred)\nacc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "model.load('checkpoints/ckpt--150')\nmodel.predict()\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def data_D(sz, G):\n    real_img = X_train[np.random.randint(0,n,size=sz)]\n    X = np.concatenate((real_img, G.predict(noise(sz))))\n    return X, [0]*sz + [1]*sz\n", "intent": "Create a batch of some real and some generated data, with appropriate labels, for the discriminator.\n"}
{"snippet": "part_test = np.array(['57111'])\ncomplaint_test = np.array(['BRAKE PEDAL IS SOFT'])\nX_new_part_labelencoded = enc_label.transform(part_test)\nX_new_part_onehot = enc_onehot.transform(X_new_part_labelencoded.reshape(-1,1))\nX_new_complaint_counts = count_vect.transform(complaint_test)\nX_new_complaint_tfidf = tfidf_transformer.transform(X_new_complaint_counts)\nX_new_combined_tfidf = sparse.hstack((X_new_part_onehot, X_new_complaint_tfidf), format='csr')\npredicted = clf.predict(X_new_combined_tfidf)\nprint(predicted)\n", "intent": "This should return 0:\n"}
{"snippet": "cross_val_score(basic_pipe, train, y, cv=kf).mean()\n", "intent": "We can also cross-validate with it as well and get a similar score as we did with our scikit-learn column transformer pipeline from above.\n"}
{"snippet": "test = logreg.predict_proba(X_test)\nprint(test)\n", "intent": "- If y_pred_prob > 0.3, than y_pred_class = 1\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images. This will be our \"test accuracy\". \n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "After we test the model again we see that we improved test accuracy up to 55%\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images.\n"}
{"snippet": "yhat= LR.predict(X_test)\nacc=np.mean(yhat==y_test)\nacc\n", "intent": "Now we can predict using our test set:\n"}
{"snippet": "yhat=lr.predict(X_test[['PROF']])\nyhat[0:5]\n", "intent": "We can make a prediction:\n"}
{"snippet": "yhat=lr.predict(X_test)\nyhat[0:5]\n", "intent": " We can make a prediction:\n"}
{"snippet": "try:\n    print(accuracy_score(predicted_labels, expected_labels))\n    print(confusion_matrix(y_pred=predicted_labels, y_true=expected_labels))\nexcept NameError:\n    print(\"You shouldn't know the answers, but this results in ~ 0.71 accuracy score\")\n", "intent": "<img src=\"../img/locally_best_tree.png\">\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(X))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "lin_test_pred = lin_reg.predict(X_test)\nsecond_train_error = rmse(lin_pred, y_train)\nsecond_test_error = rmse(lin_test_pred, y_test)\nprint(\"Training RMSE:\", second_train_error)\nprint(\"Test RMSE:\", second_test_error)\n", "intent": "**Question 1.5:** What is the rmse for both the prediction of X_train and X_test?\n"}
{"snippet": "def mean_squared_error(X, y, fit_func):\n    return ((fit_func(X).squeeze() - y.squeeze()) ** 2).mean()\n", "intent": "Minimize\n$$C(\\mathbf{w}) = \\sum_j (\\mathbf{x}_j^T \\mathbf{w} - y_j)^2$$\n"}
{"snippet": "(X.dot(reg.coef_.T) == reg.predict(X)).all()\n", "intent": "<center><img src=\"images/row_mult.png\" style=\"height: 200px;\"></img></center>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_acc = accuracy_score(y_true=y_test, y_pred=clf.predict(X_test))\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\n", "intent": "Below, we can see that the test accuracy of the imbalanced set, and it went from .846 to .916. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_acc = accuracy_score(y_true=y_test, y_pred=clf1.predict(X_test))\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\n", "intent": "Below, we can see the test accuracy after tuning Random Forest without SMOT. \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\nprint ()\nprint (classification_report(y_test, y_pred_test))\n", "intent": "We can see below that even though I drastically improved recall, precision is pretty low compared to the recall. \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\nprint ()\nprint (classification_report(y_test, y_pred_test))\n", "intent": "I got similar precision and recall as in the model for both genders.\n"}
{"snippet": "rmse = sqrt(mean_squared_error(y3, y3_pred))\nprint('Root Mean Squared Error:{:.3f}'.format(rmse))\nprint('Mean Absolute Error:{:.3f}'.format(mean_absolute_error(y3, y3_pred)))\nprint('Correlation Coefficient (CC):{:.3f}'.format(np.corrcoef(y3,y3_pred)[0,1]))      \n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "eval_result = model.evaluate(input_fn=input_test)\naverage_loss = eval_result[\"average_loss\"]\nprint(\"Average loss: %s\" % average_loss)\n", "intent": "Next we'll evaluate the trained model.\n"}
{"snippet": "train_sizes=[50,100,1000,5000]\nfor train_size in train_sizes:\n    clf, logistic=train_models(train_size, train_vector, train_labels)\n    predictions_svm=clf.predict(test_vector_small)\n    predictions_log=logistic.predict(test_vector_small)\n    print (\"support vector\")\n    print (\"train size: \",train_size,\"has accuracy:\",accuracy_score(test_labels_small, predictions_svm))\n    print\n    print (\"logistic\")\n    print (\"train size: \",train_size,\"has accuracy:\",accuracy_score(test_labels_small, predictions_log))\n", "intent": "Try training the model with different test sets\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint()\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predict = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predict = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "from __future__ import division\ndef calculate_training_error(pred, true):\n", "intent": "This function will be used to calculate the training error for the Naive Bayes models\n"}
{"snippet": "dec_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "for_pred = forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "classification_report(y_test, for_pred)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "pred = knn.predict(x_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": eval_data},\n    y=eval_labels,\n    num_epochs=1,\n    shuffle=False)\neval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\nprint(eval_results)\n", "intent": "After training, evaluate the model.\n"}
{"snippet": "x_test = np.array(['I won the prize'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint(r2)\n", "intent": "Now we measure $R^2$ error:\n"}
{"snippet": "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\nexample_counts = count_vectorizer.transform(examples)\nprint(example_counts)\npredictions = classifier.predict(example_counts)\npredictions \n", "intent": "And there we have it: a trained spam classifier. We can try it out by constructing some examples and predicting on them.\n"}
{"snippet": "print (\"Predicted %d, Label: %d\" % (classifier.predict(test_data[0]), test_labels[0]))\ndisplay(0)\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "print(\"Accuracy score: {:.4f}\".format(accuracy_score(y_train, no_survivors)))\n", "intent": "Now use the appropriate functions from `sklearn` (already imported) to compare with your results. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nX = news_A_clean.drop('class', axis=1)\ny_pred = gnb.predict(X)\ny_true = news_A_clean['class']\ncm = confusion_matrix(y_true, y_pred)\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "print('Prediction for (4,-2.5):', lr_mod.predict([[4,-2.5]]))\n", "intent": "You will be provided with one or more specific points for which you will asked to generate predictions. \n"}
{"snippet": "confmatr = confusion_matrix(y, classifier.predict(X))\nconfmatr\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "confmatrix = confusion_matrix(y_true=y, y_pred=classifier.predict(X))\nconfmatrix\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": predict_data_batch[0]},\n    y=None,  \n    num_epochs=1,\n    shuffle=False)\npredict_results = mnist_classifier.predict(input_fn=predict_input_fn)\nfor i, p in enumerate(predict_results):\n    print(\"Correct label: %s\" % predict_data_batch[1][i])\n    print(\"Prediction: %s\" % p)\n", "intent": "Now make a few predictions.\n"}
{"snippet": "fullDim_adjustedRandScore = adjusted_rand_score(kmeans.labels_, y)\nfullDim_adjustedRandScore\n", "intent": "But since the adjusted rand score function is tolerant to these kind of difference we would get the same result if we had executed:\n"}
{"snippet": "confmatrix = confusion_matrix(y_true=y, y_pred=classifier.predict(X))\nconfmatrix\n", "intent": "<span style=\"color:red\">the second confusion matrix is ok (ok for your wrong model that is) but the whole answer is presented in a very confusing way\n"}
{"snippet": "inceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(inceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predictions = dtree.predict(test_x)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predictions = rfc.predict(test_x)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(test_y, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = list(classifier.predict(test_x, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(data['Cluster'], km.labels_))\nprint(classification_report(data['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "results = m.evaluate(input_fn=generate_input_fn(test_file), steps=100)\nprint('evaluate done')\nprint('Accuracy: %s' % results['accuracy'])\n", "intent": "Let's see how the model did. We will evaluate all the test data.\n"}
{"snippet": "predictions = model.predict(test_x)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = nb.predict(test_x)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions = pipeline.predict(test_x)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "trans_resnet_predictions = [np.argmax(trans_resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(trans_resnet_predictions)==np.argmax(test_targets, axis=1))/len(trans_resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint \"R-squared = %f\" % metrics.r2_score(dataset['Y'],a*dataset['BMI']+b)\n", "intent": "So the error $R^2 = 0.296$. We can also use the scikit-learn module `metrics` to compute $R^2$ with one command: \n"}
{"snippet": "theta1 = compomics_import.plot_regression_path(simple['x1'],simple['y'],0.1,30)\nprint \"theta1 = %f\" % theta1\nprint \"R-squared = %f\" % metrics.r2_score(simple['y'],theta1*simple['x1'])\n", "intent": "We can trace the updates made by the function `linear_regression()` and plot the iteration path on the cost function $J$:\n"}
{"snippet": "from sklearn import metrics\nprint metrics.r2_score(y,cv_predictions)\n", "intent": "We can now compute an estimate of the generalization performance of the model using these predictions:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ncv_predictions = cross_val_predict(model,X,y,cv=5)\nprint cv_predictions\n", "intent": "In scikit-learn we can use the `cross_val_predict()` function in the module `sklearn.cross_validation` to do the same thing:\n"}
{"snippet": "print \"R-squared = %f\" % metrics.r2_score(target[folds==1],model.predict(data_norm_poly_norm[folds==1]))\n", "intent": "what is the $R^2$ performance on the test set?\n"}
{"snippet": "test_file  = str(\"adult.test.csv\") \nresults = m.evaluate(input_fn=generate_input_fn(test_file), \n                     steps=200)\nprint('evaluate done')\nprint(results)\nprint('Accuracy: %s' % results['accuracy'])\n", "intent": "Let's see how the model did. We will evaluate all the test data.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test,predictions)\n", "intent": "Now we can use the $sklearn.metrics$ to compute the accuracy of these predictions.\n"}
{"snippet": "print metrics.accuracy_score(y_test,predictions_zero)\n", "intent": "What is the accuracy of this model?\n"}
{"snippet": "print metrics.roc_auc_score(dataset_mel['label'], dataset_mel['Eccentricity'])\n", "intent": "<strong>Exercise</strong>\n- What is the AUC for the feature \"Solidity\" in the data set \"dataset_mel\"?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint accuracy_score(target, km.labels_)\n", "intent": "- What is the accuracy of the K-means++ class assignments?\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "`How did our model perform?`\n"}
{"snippet": "accuracy = (TP + TN)/float(TP + TN + FP + FN)\nprint(accuracy)\nprint(accuracy_score(y_test, y_pred))\n", "intent": "`The same as what we saw using the confusion matrix from scikit's metrics module!`\n"}
{"snippet": "def tf_node_loss(logits, labels):\n  return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n", "intent": "Define a per-node loss function for training.\n"}
{"snippet": "y_pred = img_kmeans.fit_predict(img_new)\n", "intent": "We can use our prediction to index into \n"}
{"snippet": "        for t, transition in enumerate(episode):\n            total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n            estimator_value.update(transition.state, total_return)\n            baseline_value = estimator_value.predict(transition.state)            \n            advantage = total_return - baseline_value\n            estimator_policy.update(transition.state, advantage, transition.action)\n", "intent": "<img src='./picture/reinforce.png' width=600 height=550 />\n<img src='./picture/reinforce_baseline.png' width=600 height=550 />\n"}
{"snippet": "test_file  = str(\"adult.test.csv\") \nresults = m.evaluate(input_fn=generate_input_fn(test_file, num_epochs=1, shuffle=False), \n                     steps=None)\nprint('evaluate done')\nprint('\\nAccuracy: %s' % results['accuracy'])\n", "intent": "Let's see how the model did. We will evaluate all the test data.\n"}
{"snippet": "print \"accuracy  {:.3}\".format(accuracy_score(ytest, ypred2))\nprint \"precision {:.3}\".format(precision_score(ytest, ypred2))\nprint \"recall    {:.3}\".format(recall_score(ytest, ypred2))\n", "intent": "Results from testing the model on the unbalanced data\nThese are the definitive metrics that inform us about how well this classifier will perform.\n"}
{"snippet": "print \"accuracy  {:.3}\".format(accuracy_score(ytest, ypred3))\nprint \"precision {:.3}\".format(precision_score(ytest, ypred3))\nprint \"recall    {:.3}\".format(recall_score(ytest, ypred3))\n", "intent": "Results from testing the model on the unbalanced data\nThese are the definitive metrics that inform us about how well this classifier will perform.\n"}
{"snippet": "h = .02  \nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "<h2>Visualize Classifier</h2>\n<p>As mentioned above, the technique of using a meshgrid allows us to show multidimentional descision boundaries</p>\n"}
{"snippet": "h = .02  \nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "<h2>Visualize Classifier</h2>\n<p>As mentioned above, the technique of using a meshgrid allows us to show multidimentional descision boundaries</p>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model2.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(df2['parsed_salary'], lm.predict(feature_set))\n", "intent": "The following calculates the area under these curves (AUC):\n"}
{"snippet": "print('Confusion Matrix: (note that False is first row and column)')\nprint(metrics.confusion_matrix(valid_dataframe['Quasar'],RFPredictions))\nprint('\\n')\nprint('Classification Report:')\nprint(metrics.classification_report(valid_dataframe['Quasar'],RFPredictions))\nprint('\\n')\nprint('Validation score: %.4f' % RFScore)\n", "intent": "Below is the confusion matrix, precision, recall, and f1-score on the validation set.\n"}
{"snippet": "CNPredictions = 1 - np.argmax(CNPredictions, axis=1)\nTrueValid = 1 - np.argmax(valid_dataframe[['EncodedQuasar','EncodedNonQuasar']].values, axis=1)\nprint('Confusion Matrix: (note that Non-quasar is first row and column)')\nprint(metrics.confusion_matrix(TrueValid,CNPredictions))\nprint('\\n')\nprint('Classification Report:')\nprint(metrics.classification_report(TrueValid,CNPredictions))\n", "intent": "Below is the confusion matrix, precision, recall, and f1-score on the validation set.\n"}
{"snippet": "def print_rmse(model, name, input_fn):\n  metrics = model.evaluate(input_fn=input_fn, steps=1)\n  print ('RMSE on {} dataset = {} USD'.format(name, np.sqrt(metrics['average_loss'])*SCALE))\n", "intent": "*Step 5: Evaluate Regressors Using a Reduced Feature Space*\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "my_model = my_model_2\nmy_preds = my_model.predict(test_data)\npreds = my_preds\nmost_likely_labels = decode_predictions(preds, top=1, class_list_path='../input/resnet50/imagenet_class_index.json')\nfor i, img_path in enumerate(img_paths[:2]):\n    display(Image(img_path))\n    print(most_likely_labels[i])\n", "intent": "**Method 2: Results**\n"}
{"snippet": "b_size = 32                         \nepochs = 5                          \nmodel.load_weights('./notMNIST.hdf5')\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\nscore = model.evaluate(test_dataset, test_labels, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "The Network can be visualised as below:\n"}
{"snippet": "predictions = model.predict(im)  \nprint(predictions)\n", "intent": "Here, we use the **model.predict** function of Keras to generate prediction for this image. \n"}
{"snippet": "test_preds = np.matmul(X_test_level2, np.array([best_alpha, 1-best_alpha]))\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds) \ntest_preds = lr.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds) \nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "def get_regression_metrics(model, actual, predicted):\n    regr_metrics = {\n                        'Root Mean Squared Error' : metrics.mean_squared_error(actual, predicted)**0.5,\n                        'Mean Absolute Error' : metrics.mean_absolute_error(actual, predicted),\n                        'R^2' : metrics.r2_score(actual, predicted),\n                        'Explained Variance' : metrics.explained_variance_score(actual, predicted)\n                   }\n    df_regr_metrics = pd.DataFrame.from_dict(regr_metrics, orient='index')\n    df_regr_metrics.columns = [model]\n    return df_regr_metrics\n", "intent": "Create a helper function to calculate regression metrics\n"}
{"snippet": "clf.predict_proba(vectorizer.transform(['This movie is not remarkable or superb']))\n", "intent": "No, because the \"not\" negates the whole sentiment of remarkable, touching, and superb. It clearly is a rotten review\n"}
{"snippet": "print(classification_report(y_test, pred))\nprint('\\n')\nprint(\"Confusion Metric: \\n\", confusion_matrix(y_test, pred))\nprint('\\n')\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\n", "intent": "Evaluating the model\n"}
{"snippet": "print(\"MAE: \", metrics.mean_absolute_error(y_test, pred))\nprint(\"MSE: \", metrics.mean_squared_error(y_test, pred))\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Calculating Errors **\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\navg_ar_score = adjusted_rand_score(iris.target, avg_pred)\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "print('MAE: ', metrics.mean_absolute_error(y_test, pred))\nprint('MSE: ',metrics.mean_squared_error(y_test, pred))\nprint('RMSE: ',np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Calculating Errors **\n"}
{"snippet": "print(metrics.r2_score(y_test, pred))\n", "intent": "** R2 Score for the model **\n"}
{"snippet": "print('MAE: ', metrics.mean_absolute_error(y_test, pred))\nprint('MSE: ', metrics.mean_squared_error(y_test, pred))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Errors for the model **\n"}
{"snippet": "for i in range(10):\n    p4 = np.poly1d(np.polyfit(x, y, i))\n    r2_train = r2_score(np.array(trainY), p4(np.array(trainX)))\n    r2_test = r2_score(testy, p4(testx))\n    print \"degree: %s\" % i\n    print \"train R^2: %s\" % r2_train\n    print \"test R^2: %s\" % r2_test\n    print \"\"\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "train_11_pred = grid_logit_1.best_estimator_.predict(\n    non_title_train['Title'].values)\ntrain_11_avg_pre = average_precision_score(y_train_1,\n                                           train_11_pred)\ntrain_auc_11_score = roc_auc_score(y_train_1, train_11_pred)\ntrain_f1_11_score = f1_score(y_train_1, train_11_pred)\nprint(\"Training data scores:\")\nprint(\"AUC score:\", train_auc_11_score)\nprint(\"Average precision:\", train_11_avg_pre)\nprint(\"f1_score:\", train_f1_11_score)\n", "intent": "**model performance**\n"}
{"snippet": "X_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n       iris_dataset['target_names'][prediction]))\n", "intent": "Let's create a new example and ask the kNN model to classify it\n"}
{"snippet": "y_pred = knn.predict(X_train)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\n", "intent": "Feeding all test examples to the model yields all predictions\n"}
{"snippet": "print(\"Shape of probabilities: {}\".format(lr.predict_proba(X_test).shape))\nprint(\"Predicted probabilities:\\n{}\".format(\n      lr.predict_proba(X_test[:6])))\n", "intent": "The output of predict_proba is a _probability_ for each class, with one column per class. They sum up to 1.\n"}
{"snippet": "print(\"f1_score of random forest: {:.3f}\".format(\n        f1_score(y_test, rf.predict(X_test))))\nprint(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\n", "intent": "Note that the F1-measure completely misses these subtleties\n"}
{"snippet": "print metrics.mean_absolute_error(y_true, y_pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(classification_report(y_test, pred))\n", "intent": "Precision, recall, F1-score\n- Now yield 10 per-class scores\n"}
{"snippet": "predicted_rfc = rfc.predict(X_train)\npredicted_gbc = gbc.predict(X_train)\npredicted_etc = etc.predict(X_train)\npredicted_abc = abc.predict(X_train)\npredicted_svc = svc.predict(X_train)\npredicted_knc = knc.predict(X_train)\npredicted_dtc = dtc.predict(X_train)\npredicted_ptc = ptc.predict(X_train)\npredicted_lrc = lrc.predict(X_train)\n", "intent": "Now we'll use the _predict_ method over our training atributes to build every prediction object. \n"}
{"snippet": "print(metrics.classification_report(expected, predicted_rfc))\nprint(metrics.classification_report(expected, predicted_gbc))\nprint(metrics.classification_report(expected, predicted_etc))\nprint(metrics.classification_report(expected, predicted_abc))\nprint(metrics.classification_report(expected, predicted_svc))\nprint(metrics.classification_report(expected, predicted_knc))\nprint(metrics.classification_report(expected, predicted_dtc))\nprint(metrics.classification_report(expected, predicted_ptc))\nprint(metrics.classification_report(expected, predicted_lrc))\n", "intent": "If you feel confortable to see every classification report, feel free to execute this code below (will be deprecated in next version). \n"}
{"snippet": "predictions_rfc = rfc.predict(X_test)\npredictions_gbc = gbc.predict(X_test)\npredictions_etc = etc.predict(X_test)\npredictions_abc = abc.predict(X_test)\npredictions_svc = svc.predict(X_test)\npredictions_knc = knc.predict(X_test)\npredictions_dtc = dtc.predict(X_test)\npredictions_ptc = ptc.predict(X_test)\npredictions_lrc = lrc.predict(X_test)\n", "intent": "Now we'll predict with our test dataset to see the adherence of our models. \n"}
{"snippet": "mse_rfc = mean_squared_error(predictions_rfc, Y_test)\nmse_abc = mean_squared_error(predictions_abc, Y_test)\nmse_etc = mean_squared_error(predictions_etc, Y_test)\nmse_gbc = mean_squared_error(predictions_gbc, Y_test)\nmse_svc = mean_squared_error(predictions_svc, Y_test)\nmse_knc = mean_squared_error(predictions_knc, Y_test)\nmse_dtc = mean_squared_error(predictions_dtc, Y_test)\nmse_ptc = mean_squared_error(predictions_ptc, Y_test)\nmse_lrc = mean_squared_error(predictions_lrc, Y_test)\n", "intent": "Let's store our Mean Squared Error for each classifier.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "Once we have a model, here is how we calculate the mean absolute error:\n"}
{"snippet": "test_fiter_more.cross_val_accuracy_score()\n", "intent": "**train test score**\n"}
{"snippet": "scores = model.evaluate(X, Y)\nprint \"Training Dataset %s: %.2f\"%(model.metrics_names[0], scores[1])\nprint \"Training Dataset %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100)\n", "intent": "In this part, we can calculate the accuracy fo this model on training dataset \n"}
{"snippet": "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\nresult = cross_val_score(estimator, X, y_onehot, cv=kfold)\nprint(\"Accuracy: %.2f%% (+-%.2f%%)\"%(result.mean()*100, result.std()*100))\n", "intent": "10-fold cross validation\n"}
{"snippet": "print metrics.mean_squared_error(y_true, y_pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "scores = loaded_model.evaluate(X, Y, verbose=0)\nprint(\"Traning %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))\n", "intent": "Evaluate the loaded model\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\nprint('\\n', preds[:8])\nprint(probs[:8])\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "conv_val_feat = vgg640.predict(val, batch_size=batch_size, verbose=1)\nconv_trn_feat = vgg640.predict(trn, batch_size=batch_size, verbose=1)\n", "intent": "We can now pre-compute the output of the convolutional part of VGG.\n"}
{"snippet": "def data_D(sz, G):\n    real_img = X_train[np.random.randint(0,n,size=sz)]\n    X = np.concatenate((real_img, G.predict(noise(sz))))\n    return X, [0]*sz + [1]*sz  \n", "intent": "Create a batch of some real and some generated data, with appropriate labels, for the discriminator.\n"}
{"snippet": "p = top_model.predict(arr_hr[0:1])\np.shape\n", "intent": "Now we can pass any image through this CNN and it will produce it in the style desired!\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n", "intent": "** Now create a classification report and a Confusion Matrix. Does anything stand out to you?**\n"}
{"snippet": "y_predictions = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], km.labels_))\nprint(classification_report(df['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4))\nprint('expected:', messages['label'][3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "y_predict = nb.predict(X_test) \n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, y_predict))\nprint(classification_report(y_test, y_predict))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "y_predict = pipeline.predict(X_test)\nprint(confusion_matrix(y_test, y_predict))\nprint(classification_report(y_test, y_predict))\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "kpred = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred  = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "X_test = test[feature_cols]\ny_test = test.price\ny_pred = treereg.predict(X_test)\ny_pred\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each observation?\n"}
{"snippet": "print(classification_report(y_test, pred_rf))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred=best_model.predict(dtest)\n", "intent": "Look at evaluation metrics with the scikit-learn function on the validation data. \n"}
{"snippet": "adult_10_clusters = adult_models[0].predict(adult_train_scaled)\nadult_11_clusters = adult_models[1].predict(adult_train_scaled)\nadult_12_clusters = adult_models[2].predict(adult_train_scaled)\nadult_13_clusters = adult_models[3].predict(adult_train_scaled)\nadult_14_clusters = adult_models[4].predict(adult_train_scaled)\n", "intent": "Compare to 0.801917 peak performance on project 1 and 0.666667 dummy performance.\n"}
{"snippet": "adult_clusters = adult_models[2].predict(adult_train_scaled)\n", "intent": "Compare to 0.801917 peak performance on project 1 and 0.666667 dummy performance.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(train_diff, ar1ma1.fittedvalues)\nprint(r2)\n", "intent": "Looks like the combo has the lowest AIC score\n"}
{"snippet": "eigen_total = sum(eigenValues)\ndef eigen_score(x):\n   ExplainedVariance = (eigenValues[x]/eigen_total) * 100\n   return \"Explained Variance\", x+1, ExplainedVariance\n", "intent": "Next, Calculate the explained variance\n"}
{"snippet": "print \"Accuracy \" + str(accuracy_score(Y_test, Y_pred))\nprint \"Precision \" + str(precision_score(Y_test, Y_pred))\nprint \"Recall \" + str(recall_score(Y_test, Y_pred))\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "y_train_predict = svm.predict(X_train)\nprint(y_train_predict)\nprint(y_train)\n", "intent": "3) Apply / evaluate\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, pred_test)\n", "intent": "or >> MSE: from sklearn.metrics import mean_squared_error\n"}
{"snippet": "fpr[\"lr\"], tpr[\"lr\"]  = roc_mean(fpr_list, tpr_list)\nauc_mean[\"lr\"] = roc_auc_score(y_test, probs, average=None)\nplot_ROC(fpr[\"lr\"], tpr[\"lr\"], auc_mean[\"lr\"])\n", "intent": "plot the mean ROC curve\n"}
{"snippet": "cross_val_score(grid_search, X, y, cv=5)\n", "intent": "Nested Cross-Validation\n-------------------------\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nprint \"RMSE:\", mean_squared_error(ys,predictions)\nprint \"MAE: \", mean_absolute_error(ys,predictions)\n", "intent": "print \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n"}
{"snippet": "y_pred=knn.predict(X_test)\ny_pp=knn.predict_proba(X_test)\ny_pp\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(y, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(y, labels))\nprint(\"Silhouette Score: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\",np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "acc = accuracy_score(y_test, pred)\nprint acc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "fpr[\"rf\"], tpr[\"rf\"]  = roc_mean(fpr_list, tpr_list)\nauc_mean[\"rf\"] = roc_auc_score(y_test, probs, average=None)\nplot_ROC(fpr[\"rf\"], tpr[\"rf\"], auc_mean[\"rf\"])\n", "intent": "plot the ROC mean curve\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsil_score = silhouette_score(X, labels1).mean()\nprint sil_score\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "submission = tpot_classifier.predict(titanic_submission)\n", "intent": "And now we are ready to predict and classify the test results\n"}
{"snippet": "submission[\"Survived\"] = model.predict(submission)\n", "intent": "Im going to add a new Column Survived and use the model to predict the \"Survived\" values . such as below..\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'predicted:', spam_detect_model.predict(tfidf4)[0]\nprint 'expected:', messages.label[3]\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "def compute_rmse(y_pred, y_true):\n    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\ndef evaluate(estimate,test=movielens_test):\n    ids_to_estimate = zip(test['user_id'], test['movie_id'])\n    estimated = np.array([estimate(u,i) for (u,i) in ids_to_estimate])\n    real = test.rating.values\n    return compute_rmse(estimated, real)\n", "intent": "``evaluate`` will compute the precision of the recommender system by using the RMSE metric:\n"}
{"snippet": "def rec1(user_id, item_id,train=movielens_train):\n    return 1\nprint 'Error: %s' % evaluate(rec1)\n", "intent": "+ Write a function that, given a user, scores any film with the mean scoring of that user.\n"}
{"snippet": "elast = ElasticNet(alpha=optimal_elast.alpha_, l1_ratio=optimal_elast.l1_ratio_)\nelast_scores = cross_val_score(elast, scaled_data, y, cv=10)\nprint 'R^2: ',elast_scores\nprint 'R^2 mean: ',np.mean(elast_scores)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "print(clf.predict([[2., 2.]]))\n", "intent": "Now, we try to classify the test data with coordinates (2,2). The classifier classifies it as category 1 which is same as the coordinate (1,1).\n"}
{"snippet": "fpr[\"nn\"], tpr[\"nn\"]  = roc_mean(fpr_list, tpr_list)\nauc_mean[\"nn\"] = roc_auc_score(y_test, probs, average=None)\nplot_ROC(fpr[\"nn\"], tpr[\"nn\"], auc_mean[\"nn\"])\n", "intent": "plot the mean ROC curve\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf_svm.predict(X_test)\nprint(\"Test Precision: {}\".format(metrics.precision_score(y_test, y_pred)))\nprint(\"Test Recall: {}\".format(metrics.recall_score(y_test, y_pred)))\nprint(\"Test F-Score: {}\".format(metrics.f1_score(y_test, y_pred)))\n", "intent": "The scikit-learn metrics package offers the basic evaluation routines.\n"}
{"snippet": "y_pred = model.predict(X_te)\nscore = y_te == y_pred\nscore = model.score(X_te, y_te)\nsum(score)\n", "intent": "Make prediction on test data\n"}
{"snippet": "bio.show_event_error(*errors[0])\n", "intent": "These errors you can then inspect in detail via `show_event_error`:\n"}
{"snippet": "cv_scores = []\nfor n in range(20,300):\n    cv_score = my_cross_val_score(X_train[:n], y_train[:n], num_folds=5)\n    cv_scores.append(cv_score)\n", "intent": "Let's try the latter first:\n"}
{"snippet": "cv_scores = []\nfor k in range(2,51):\n    cv_score = my_cross_val_score(X_train, y_train, num_folds=k)\n    cv_scores.append(cv_score)\n", "intent": "What if we use all the data and vary the number of folds?\n"}
{"snippet": "print(\"Estimated Logistic Coefficients: {}\".format(str(gd.coeffs)))\nprint(\"Predicted Values on Training Data: {}\".format(str(gd.predict(X)[:5])))\n", "intent": "Now that the model has been fit, we can see the estimated coefficients and the final predictions on the training data.\n"}
{"snippet": "print('Cross validated score with 5 folds (scaled pipeline): {}'\n      .format(np.mean(cross_val_score(p_scaled, X1, y1, scoring='accuracy', cv=5))))\nprint('Cross validated score with 5 folds (unscaled pipeline): {}'\n      .format(np.mean(cross_val_score(p_unscaled, X1, y1, scoring='accuracy', cv=5))))\n", "intent": "5\\. Use `cross_val_score` to compute the average `accuracy` of a 5-fold cross validation of the scaled model versus the unscaled model.\n"}
{"snippet": "model.predict(Xtest), ytest\n", "intent": "End of the aside.\nLet's consider the full matrix of predicted and actual values.\n"}
{"snippet": "silhouette_score(wine, wine['cluster'], metric = 'euclidean')\n", "intent": "- Silhouette score over range of K\n- SSE / Inertia over range of K\n"}
{"snippet": "test_svm = test_norm[features_for_svm].copy()\ntest_svm = pd.concat([pd.get_dummies(test_norm.alchemy_category), test_svm], axis=1)\nsvm_pred = svm_classifier.predict(test_svm)\nsvm_pred = svm_all_train.predict_proba(test_svm)[:,1]\n", "intent": "Using the model **with** alchemy category\n"}
{"snippet": "def precision(actual, preds):\n    true_pos = len(np.intersect1d(np.where(preds == 1), np.where(actual == 1)))\n    pred_pos = (preds == 1).sum()\n    return true_pos / pred_pos\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    true_pos = len(np.intersect1d(np.where(preds == 1), np.where(actual == 1)))\n    act_pos = (actual == 1).sum()\n    return true_pos / act_pos\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    return 2 * precision(preds, actual) * recall(preds, actual) / (precision(preds, actual) + recall(preds, actual))\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "lr_pred = lr.predict(X_test)\ndtr_pred = dtr.predict(X_test)\nrfr_pred = rfr.predict(X_test)\nabr_pred = abr.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum(np.square(actual - preds)) / len(actual)\nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "bc_pred = bc.predict(testing_data)\nrfc_pred = rfc.predict(testing_data)\nabc_pred = abc.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test)\nprint \"\\n=======confusion matrix==========\"\nprint confusion_matrix(y_test, y_pred)\n", "intent": "Print the confusion matrix for the decision tree model\n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint all_predictions\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset.\n"}
{"snippet": "x, y = make_xy(critics, vectorizer)\nprob = clf.predict_proba(x)[:, 0]\nprint(len(prob))\n", "intent": "We can see mis-predictions as well.\n"}
{"snippet": "y_pred = clf.predict(X_test)\ny_true = y_test\n", "intent": "- Accuracy\n- Precision\n- Recall\n- F1 Score(f-score)\n"}
{"snippet": "test_pred = model.predict(X_test)\nprint(model.score(X_test,Y_test))\n", "intent": "A little lower training score than before, but let's see how our test score is\n"}
{"snippet": "model.predict_proba(np.array([1500,60]))\n", "intent": "For fun, let's predict if a 1500 sq ft house with a 60 quality rating will be in imy budget. \n"}
{"snippet": "print(metrics.classification_report(Y_test, test_preds))\n", "intent": "However, the confusion matrix tells a different story. 11/16 (~70%) are false positives\n"}
{"snippet": "expected = yTest\npredicted = dtModel.predict(xTest)\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "This model has **scored 66.52%**\n"}
{"snippet": "y_pred = logit_cv2.predict_proba(X_test)[:, 1]\ny_pred.shape\n", "intent": "my best cv csore 0.98267878\n"}
{"snippet": "test_pred = logit_cv2.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "test_pred = logit_cv.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_test, Y_pred)\nprint(acc)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "print(accuracy_score(gs_cv.predict(Xtestlr),ytestlr))\n", "intent": "It does not give the same value for C. The above grid search seems to indicate that C of 0.001 would be the best. \n"}
{"snippet": "from sklearn.model_selection import StratifiedKFold,KFold\nkf=KFold(n_splits=5, shuffle=True, random_state=2017)\ndef auc_to_gini_norm(auc_score):\n    return 2*auc_score-1\ndef eval_rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\ndef eval_rmsle(y, pred):\n    return mean_squared_error(y, pred)**0.5\n", "intent": "** Cross validation **\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = clf.predict(train[columns])\nprint(roc_auc_score(train['high_income'], predictions))\n", "intent": "* Print out the AUC score between `predictions` and the `high_income` column of `train`.\n"}
{"snippet": "predictions= lgr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print (classification_report(y_test,pred2))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "prediction= lg.predict(df_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print (classification_report(y_test,prediction))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf_pipeline.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred, digits=4))\n", "intent": "Compute common classification metrics and evaluate the models. Decide which model performs best on the given problem.\n"}
{"snippet": "from sklearn import metrics\nprint()\nprint(\"ML MODEL REPORT\")\nprint(\"Accuracy: {}\".format(metrics.accuracy_score(y_test, y_pred)))\nprint(\"Confusion matrix:\")\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred,\n                                            target_names=names))\n", "intent": "Evaluate the models using standard methods.\n"}
{"snippet": "X_test = full_matrix[idx_split:,:]\ntest_pred = lgb_model.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "We first define a rmsle evaluation function \n"}
{"snippet": "predictions = list(classifier.predict(X_test))\npredictions\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "X_new =np.array([[3, 5, 4, 2], [5, 4, 3, 2]])\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "print (TP + TN) / float(TP + TN + FP + FN)\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "print (FP + FN) / float(TP + TN + FP + FN)\nprint 1 - metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "y_pred = np.dot(X_test[:, causl], w_causl)\nfrom sklearn import metrics\nrmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n    (metrics.explained_variance_score(y_test, y_pred), rmse))\n", "intent": "Let us check how good the true model predicts the phenotypes on the test set\n"}
{"snippet": "y_pred = model_l1_cv.predict(X_test)\n", "intent": "Let us now look at how good the model predicts the phenotype on the test set.\n"}
{"snippet": "multi_normal, y_pred = anomaly.predict(X, Xval, e, Xtest, ytest)\n", "intent": "1. use CV data to find the best $\\epsilon$\n2. use all data (training + validation) to create model\n3. do the prediction on test data\n"}
{"snippet": "predictions_cv = cross_validation.cross_val_predict(clf, xtest, ytest, cv=10)\npredictions = clf.predict_proba(xtest) \n", "intent": "Do the same thing on the test data. We also grab the predictions for the test data as a probability that the event in question results in injury.\n"}
{"snippet": "weight_avg=logit_lv3_train_pred*0.5+ xgb_lv3_train_pred*0.5\nprint(auc_to_gini_norm(roc_auc_score(y_train, weight_avg)))\n", "intent": "We can always still do a simple weight average, to bring the two together and see if there any extra juice to be squeezed\n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n", "intent": "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by\nadding up the predictions of all the trees:\n"}
{"snippet": "ext_df['prob'] = model.predict_proba(X)[:,1]\next_df['prediction'] = ext_df.prob.apply(lambda x: 1 if x>=0.5 else 0)\next_df = ext_df.sort_values(by = 'prob', ascending = False)\n", "intent": "Results looks a little to good. Let's check the observations scores and the weights assigned to the input features:\n"}
{"snippet": "predictions=model.predict(final_test_tensors, verbose=1)\n", "intent": "Predict complete test set\n"}
{"snippet": "mseFull = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint (mseFull)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print metrics.accuracy_score(new_data['loan_given'], new_data['predicted_class'])\n", "intent": "My prediction of the accuracy of the model for unseen test data is:\n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint(rss/len(bos))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "sum(logistic.predict(XX) == YY) / len(XX)\n", "intent": "The score is calculated as \n"}
{"snippet": "class IsolationForest2(IsolationForest):    \n    def predict(self, X):\n        func =  -self.decision_function(X)\n        return (func - min(func)) / (max(func) - min(func))\n", "intent": "Plot the graph how the quality depends on **n_estimators**.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "logit_val_auc=roc_auc_score(y_localval, logit_valprediction)\nlogit_val_gininorm=2*logit_val_auc-1\nprint('Logistic Regression Validation AUC is {:.6f}'.format(logit_val_auc))\nprint('Logistic Regression Validation Normalised Gini Coefficient is {:.6f}'.format(logit_val_gininorm))\n", "intent": "The prediction step is rather straightforward - exactly the same as the one for Random Forest\n"}
{"snippet": "Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nvgg_bottleneck = VGG16(weights='imagenet', include_top=False, pooling=POOLING)\nvgg_bottleneck.compile('sgd','mse')\ntrain_vgg_bf = vgg_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_vgg_bf = vgg_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('VGG train bottleneck features shape: {} size: {:,}'.format(train_vgg_bf.shape, train_vgg_bf.size))\nprint('VGG valid bottleneck features shape: {} size: {:,}'.format(valid_vgg_bf.shape, valid_vgg_bf.size))\n", "intent": "[See here](https://github.com/keras-team/keras/issues/9394) for the bug I mention below\n"}
{"snippet": "predictions = lgb_model.predict(test_x)\npredictions  = np.expm1(predictions)\n", "intent": "---\n<a id=\"predictions\"></a>\n"}
{"snippet": "b4 = vgg.get_batches(batch_path, batch_size=8)\nimgs,labels = next(b4)\npredictions = vgg.predict(imgs, True)\npredicted_labels = predictions[2]\nplots(imgs, titles=predicted_labels)\n", "intent": "Test a batch and see how it does\n"}
{"snippet": "test_input_fn = create_test_input_fn(df_test)\npredictions = linear_estimator.predict(test_input_fn)\ni = 0\nfor prediction in predictions:\n    true_label = df_test['label'][i]\n    predicted_label = prediction['class_ids'][0]\n    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == 5: break\n", "intent": "The Estimator returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples.\n"}
{"snippet": "score = tl_model.evaluate(test_tl, test_targets)\nprint('Test Accuracy: {}%', score[-1]*100)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.load_weights=('best_model.hdf5')\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "scores = cross_val_score(lr, features_array, target, cv=5, scoring='roc_auc')\nscores.min(), scores.mean(), scores.max()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom scipy.stats import sem\nscores = cross_val_score(pipeline, twenty_train_small.data,\n                         twenty_train_small.target, cv=3, n_jobs=3)\nscores.mean(), sem(scores)\n", "intent": "Such a pipeline can then be cross validated or even grid searched:\n"}
{"snippet": "print('R-squared=', metrics.explained_variance_score(y, y_pred))\n", "intent": "* Better than Linear Regression without cross validation\n"}
{"snippet": "n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=2017).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, train_target, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "**Define a cross validation strategy**\n"}
{"snippet": "scores_neigh = cross_val_score(neigh, df2_reduced, target_variable, cv=5)\nprint(\"Cross-validated scores for each step: \\n{}\".format(scores_neigh))\n", "intent": "Check the accuracy of models with <b>cross validation</b>.<br>\n"}
{"snippet": "scores_regr = cross_val_score(regr, df2_reduced, target_variable, cv=10)\nscores_neigh = cross_val_score(neigh, df2_reduced, target_variable, cv=10)\nscores_tree = cross_val_score(tree, df2_reduced, target_variable, cv=10)\nprint(\"Accuracy LogisticRegression: %0.2f (+/- %0.2f)\" % (scores_regr.mean(), scores_regr.std() * 2))\nprint(\"Accuracy KNeighborsClassifier: %0.2f (+/- %0.2f)\" % (scores_neigh.mean(), scores_neigh.std() * 2))\nprint(\"Accuracy DecisionTreeClassifier: %0.2f (+/- %0.2f)\" % (scores_tree.mean(), scores_tree.std() * 2))\n", "intent": "<i>Cross-validation to understand best performing algorithm</i>\n"}
{"snippet": "y_pred = tree.predict(df2_test)\ny_pred\n", "intent": "<i>Make predictions with DecisionTreeClassifier because it was the most accurate method on train data based on cross-validation</i>\n"}
{"snippet": "predictions = clf.predict_proba( [\n        [3, 5, 0],\n        [3, 6, 0],\n        [3, 7, 0]\n        ]\n)\npredictions\n", "intent": "Now compare reality (above) with our prediction engine....\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "re-run predictions on this grid object\n"}
{"snippet": "start = round(len(filtered_fare)/2)\ny_pred = fare_model_2.predict(start=start, end=len(filtered_fare)+48, dynamic=True)\npred = [np.nan for _ in range(0, start)] + list(y_pred)\npred = np.array(pred)\n", "intent": "With a durbin_watson statistic of around 2 ~ we've gotten rid of our serial correlation!  Now let's plot the data!\n"}
{"snippet": "X = np.array(means)\nX = X.reshape(-1, 1)\nmodel = GeneralMixtureModel.from_samples(\n    [NormalDistribution, ExponentialDistribution],\n    n_components=3, X=X)\nlabels = model.predict(X)\nlabel_mapping = [\"Normally Distributed\", \"Exponentially Distributed\"]\nfor elem in zip(set(labels), np.bincount(labels)):\n    print(label_mapping[elem[0]], elem[1])\nmodel.plot( n=100000, edgecolor='c', color='c', bins=50 )\n", "intent": "Now that we have some sense of our data, let's see if we can take things further by fitting a distribution to it.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) \\\n                         for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/ \\\n    len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Train Accuracy: \", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "def rmsle(preds, true):\n    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n    return float(rmsle)\n", "intent": "__Define utility function__\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(actual == 1), np.where(preds == 1)))\n    ap = (actual == 1).sum()\n    return tp/ap \nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(actual, preds):\n    return 2 * (precision(actual, preds) * recall(actual, preds)) / (precision(actual, preds) + recall(actual, preds)) \nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "pred_lin = mod_lin.predict(X_test)\npred_dec = mod_dec.predict(X_test)\npred_ada = mod_ada.predict(X_test)\npred_rf = mod_rf.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return ((actual - preds)**2).mean() \nprint(mse(y_test, pred_dec))\nprint(mean_squared_error(y_test, pred_dec))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "y_pred_bag = model_bag.predict(testing_data)\ny_pred_rf = model_rf.predict(testing_data)\ny_pred_ada = model_ada.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n", "intent": "Testing the model\nNow, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "mean, std = naturality_score(test_loader, batch_size=batch_size, \n                             gpu_id=gpu_id, weights_path=capsNN_weights_path, model=None)\n", "intent": "Let us first see, what would be the highest possible score, by computing the modified inception score for original MNIST validation images.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, x, y, cv=10)\nprint(accs)\nprint(np.mean(accs))\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\ny_pp\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "def rmsle(preds, true):\n    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n    return float(rmsle)\n", "intent": "__Define utility functions__\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "The calculation of mean absolute error in the Melbourne data is\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, \n                         scoring='neg_mean_absolute_error',\n                         cv=5)\nprint(scores)\n", "intent": "Finally get the cross-validation scores:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(clf, X=digits.data, y=digits.target, cv=cv)\n", "intent": "We can do all this in one line using the `cross_val_score` method\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(\"Report of SVM prediction\")\nprint(classification_report(y_test, svm_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of logistic regression prediction\")\nprint(classification_report(y_test, lr_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of neural network prediction\")\nprint(classification_report(y_test, mlp_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of combined prediction\")\nprint(classification_report(y_test, predictions))\n", "intent": "The accuracy of the algorithms are compared based on precision, f1-score, etc. The reports are shown below.\n"}
{"snippet": "print(\"Report of SVM prediction\")\nprint(classification_report(y_test, svm_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of logistic regression prediction\")\nprint(classification_report(y_test, lr_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of neural network prediction\")\nprint(classification_report(y_test, mlp_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of combined prediction\")\nprint(classification_report(y_test, predictions))\n", "intent": "The accuracy of the algorithms are compared based on precision, f1-score, etc. The reports are shown below.\n"}
{"snippet": "new_observation = [3, 5, 4, 2]\nknn.predict(new_observation)\n", "intent": "- New observations are called \"out-of-sample\" data\n- Uses the information it learned during the model training process\n"}
{"snippet": "y_pred_gs = clf_gs_cv.predict(Xtestlr)\naccuracy_score(y_pred_gs, ytestlr)\n", "intent": "The ``GridSearchCV`` method returned a different ``C`` and different score.\n"}
{"snippet": "num_test = X_test.shape[0]\npredicted = mlp1.predict(X_test)\nnum_correct = [target[predicted[index]] for index, target in enumerate(y_test)]\naccuracy = sum(num_correct) / num_test\nprint(\"Accuracy on test set with (NH = 10):\\t\" + str(accuracy))\n", "intent": "<h4> Check the accuracy on test set </h4>\n"}
{"snippet": "num_test = X_test.shape[0]\npredicted = mlp2.predict(X_test)\nnum_correct = [target[predicted[index]] for index, target in enumerate(y_test)]\naccuracy = sum(num_correct) / num_test\nprint(\"Accuracy on test set with (NH = 20):\\t\" + str(accuracy))\n", "intent": "<h4> Check the accuracy on test set </h4>\n"}
{"snippet": "def rmsle(preds, true):\n    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n    return float(rmsle)\ndef eval_rmsle(y, pred):\n    return mean_squared_error(y, pred)**0.5\n", "intent": "__Define Utility Function__\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nmodel_valid_accuracy_comparisons[\"Simple Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the tree on the validation dataset\n"}
{"snippet": "y_pred = my_tree.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nmodel_test_accuracy_comparisons[\"Simple Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the tree on the validation dataset\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **training set**\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nmodel_valid_accuracy_comparisons[\"Better Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_pred, rownames=['True'], colnames=['Predicted'], margins=True, dropna = False)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "y_pred = my_tuned_tree.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nmodel_test_accuracy_comparisons[\"Tuned Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the performance of the tuned tree\n"}
{"snippet": "print(\"****** Test Data ********\")\ny_pred = my_tuned_model.predict(np.asfarray(X_test))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\ndisplay(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Evaluate the model on a test dataset\n"}
{"snippet": "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, km.labels_))\n", "intent": "We save you some time and write these performance metrics for you.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nyhat_train = model.predict(x_train)\nacc_train = accuracy_score(y_train, yhat_train)\nprint(\"accuracy score for train dataset: \")\nprint(str(acc_train*100) + \" %\\n\")\nyhat_test = model.predict(x_test)\nacc_test = accuracy_score(y_test, yhat_test)\nprint(\"accuracy score for test dataset: \")\nprint(str(acc_test*100) + \" %\\n\")\n", "intent": "After the model has been fit, we can use the model to predict class labels from the data.\n"}
{"snippet": "p_valid = np.zeros_like(y_valid)\nfor flip in [False, True]:\n    temp_x = x_valid\n    if flip:\n        temp_x = img.flip_axis(temp_x, axis=2)\n    p_valid += 0.5 * np.reshape(model.predict(temp_x, verbose=1), y_valid.shape)\n", "intent": "Little bit of TTA, predict on both horisontal orientations.\n"}
{"snippet": "y_localpred=xgr.predict(x_localval)\n", "intent": "**Validation with x_localval**\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(features, axis=0))) for features in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('\\nTest Accuracy: %.4f%%' % test_accuracy )                          \n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_expect = y_test\ny_predict = clf.predict(x_test)\nprint(metrics.classification_report(y_expect, y_predict))\n", "intent": "Wow 97% is a really a good score for our model!! \n"}
{"snippet": "feature_1 = model_1_xgboost.predict(xgb.DMatrix(xgtrain))\nfeature_2 = model_2_rf.predict(xgtrain)\nfeature_3 = model_3_et.predict(xgtrain)\nfeature_4 = model_4_xgboost.predict(xgb.DMatrix(xgtrain2))\nfeature_5 = model_5_rf.predict(xgtrain2)\nfeature_6 = model_6_et.predict(xgtrain2)\nfeature_7 = model_7_ridge.predict(xgtrain2)\nfeature_8 = model_8_lasso.predict(xgtrain2)\nfeature_9 = model_11_kr.predict(xgtrain2)\n", "intent": "Stacking the above 9 models to see if it produces better output\n"}
{"snippet": "feature_1 = model_1_xgboost.predict(xgb.DMatrix(xgtrain))\nfeature_2 = model_2_rf.predict(xgtrain)\nfeature_3 = model_3_et.predict(xgtrain)\nfeature_7 = model_7_ridge.predict(xgtrain)\nfeature_8 = model_8_lasso.predict(xgtrain)\nfeature_9 = model_9_sgd.predict(xgtrain)\nfeature_10 = model_10_perceptron.predict(xgtrain)\nfeature_11 = model_11_kr.predict(xgtrain)\nfeature_12 = model_12_svr.predict(xgtrain)\n", "intent": "Stacking the above 9 models to see if it produces better output\n"}
{"snippet": "print clf.predict(scaler.transform([[4.7, 3.1]]))\nprint clf.decision_function(scaler.transform([[4.7, 3.1]]))\n", "intent": "Let's how our classifier can predict the class of a certain instance, given its sepal length and width:\n"}
{"snippet": "from sklearn import metrics\ny_train_pred = clf.predict(X_train)\nprint metrics.accuracy_score(y_train, y_train_pred)\n", "intent": "Let's see how good our classifier is on our training set, measuring accuracy:\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint metrics.accuracy_score(y_test, y_pred)\n", "intent": "To get a better idea of the expected performance of our classifier on unseen data, que must measure  accuracy on the testing set\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred, target_names=iris.target_names)\nprint metrics.confusion_matrix(y_test, y_pred)\n", "intent": "Let's try some additinoal measures: Precision, Recall and F-score, and show the confusion matrix\n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n", "intent": "Calculate the mean and standard error of cross-validation accuracy\n"}
{"snippet": "import html\ntext_input = 'Guten Morgen'\nresult = predictor.predict(text_input)\nprint(html.unescape(result))\n", "intent": "Now it's your time to play. Input a sentence in German and get the translation in English by simply calling predict. \n"}
{"snippet": "def lsq_loss(y_true, y_pred):\n    return np.mean(0.5 * (y_true - y_pred)**2)\n", "intent": "We will also need to specify a loss function. For now let's use the least squared loss, which we will code below.\n"}
{"snippet": "'MSE loss:', lsq_loss(XOR_out, acts[-1])\n", "intent": "You'll notice that these are now for each observation.\n"}
{"snippet": "def ndcg5_score(preds, dtrain):\n    labels = dtrain.get_label()\n    top = []\n    for i in range(preds.shape[0]):\n        top.append(np.argsort(preds[i])[::-1][:5])\n    mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int)\n    score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1))\n    return 'ndcg5', score\n", "intent": "To see the model performance as it advance we are going to define the score function, for this competition is the *NDCG5*:\n"}
{"snippet": "quote = vectorizer.transform(['This movie is not, touching, or superb in any way'])\nclf.predict_proba(quote)\n", "intent": "> It is a clear misclassification, which is not suprising since most words in the review are positive words\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_test_pred)))\n", "intent": "8.once you're satisfied with training, check the R2score on the test set\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Model 1 Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predictions=svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "new_data = np.linspace(0,10,10)\ntest_in_fn = train_in_func = tf.estimator.inputs.numpy_input_fn({'x':new_data},shuffle=False)\nlist(est.predict(input_fn=test_in_fn))\n", "intent": "Running on a brand new test data\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\nXception_test_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy of Xception: %.4f%%' % Xception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import html\nresult = predictor.predict(\"I love translation\")\nprint(html.unescape(result))\n", "intent": "Now it's your time to play. Input a sentence in English and get the translation in French by simply calling predict. \n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "- Back to [Table of Contents](\nNow let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:\n"}
{"snippet": "sns.regplot(lm.predict(),lm.resid,fit_reg = False)\n", "intent": "What I'm doing below is plotting the fitted values (i.e. predicted training values versus the residuals).\n"}
{"snippet": "probability_pred = lm.predict_proba(df[['gre','gpa',1,2,3]])[:,1]\npredictions = (probability_pred >= 1) + 0   \n(lm.predict(df[['gre','gpa',1,2,3]]) == predictions).sum()\n", "intent": "If $P(Y=1|X) > .5$, predict Y = 1 otherwise predict 0.\nLet's verify this manually:\n"}
{"snippet": "ref_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, ref_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "spam_detect_model.predict(tfidf4)[0]\n", "intent": "Classify our single random message and see how we do.\n"}
{"snippet": "all_pred = spam_detect_model.predict(messages_tfidf)\n", "intent": "Run the prediction on all messages in the tfidf:\n"}
{"snippet": "predictions = [0 if x<0.5 else 1 for x in predictor.predict(x_test)]\nwith open('ann_result.csv', 'w') as output_file:\n    output_file.write('PassengerId,Survived\\n')\n    output_file.write('\\n'.join(['{},{}'.format(id, prediction) for id,prediction in zip(test_ids,\n                                                                                         predictions)]))\n", "intent": "After complete the training, we can make the predictions and generate the submition file:\n"}
{"snippet": "y_pred = [a['predictions'] for a in estimator.predict(test_input)]\n", "intent": "** Create a prediction input function and then use the .predict method off your estimator model to create a list or predictions on your test data. **\n"}
{"snippet": "import html\ntext_input = \"Hey, how you're doing?\"\nresult = predictor.predict(text_input)\nprint(html.unescape(result))\n", "intent": "Now it's your time to play. Input a sentence in English and get the translation in French by simply calling predict. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(model.labels_, data['Cluster']))\nprint(classification_report(model.labels_, data['Cluster']))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = log_model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pipe_predictions = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = lm.predict(X=x_test)\n", "intent": "**Make predictions based on the fitted model**\n"}
{"snippet": "pred = logistic.predict_proba(X)\ndf['propensity score'] = pred[:, 1]\ndf.head()\n", "intent": "We assign this score to each observation in the dataframe\n"}
{"snippet": "preddtree = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "random_pred = rtree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,random_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,random_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions_int = 10**best_int_ridge.predict(price_test_final[int_feats])\npredictions_int_df = pd.DataFrame.from_items([\n    ('PID', price_test_final.index),\n    ('SalePrice', predictions_int)\n])\n", "intent": "Our predictions approximate the original distribution. \n"}
{"snippet": "sample = [6.4,3.2,4.5,1.5]\npredictor.predict(sample)\n", "intent": "We can now use this endpoint to classify. Run an example prediction on a sample to ensure that it works.\n"}
{"snippet": "from sklearn.metrics import accuracy_score,adjusted_rand_score\nprint(accuracy_score(digits.target, labels))\nprint(adjusted_rand_score(digits.target, labels))\n", "intent": "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:\n"}
{"snippet": "def kl_loss(pij, qij):\n", "intent": "Implement a function to compute the KL-divergence between two distribution $p_{ij}$ and $q_{ij}$.\n"}
{"snippet": "lgb_model = lgb.train(lgb_params, lgb.Dataset(X_trn, y_trn), train_round)\npredictions = lgb_model.predict(X_tst)\n", "intent": "**trian use all data**\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "predictions = model.predict(test_images)\n", "intent": "With the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "predictions = model.predict(img)\nprint(predictions)\n", "intent": "Now predict the image:\n"}
{"snippet": "results = model.evaluate(test_data, test_labels)\nprint(results)\n", "intent": "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n"}
{"snippet": "import sklearn as sk\nfrom astropy.table import Table\nprecision = sk.metrics.precision_score(y_test, test_pred, average=None) * 100.\nrecall = sk.metrics.recall_score(y_test, test_pred, average=None) * 100.\nplotScatter(precision, recall, np.arange(n_classes), 'Precison', 'Recall')    \n", "intent": "Compute the precison and recall of each class and visualize which classes needs more data augmentation.\n"}
{"snippet": "y_pred = knn.predict(X)\n", "intent": "- Every SNLS object is associated to an SED in pickle catalog\n"}
{"snippet": "words = [\"awesome\", \"blazing\"]\npayload = {\"instances\" : words}\nresponse = bt_endpoint.predict(json.dumps(payload))\nvecs = json.loads(response)\nprint(vecs)\n", "intent": "The payload should contain a list of words with the key as \"**instances**\". BlazingText supports content-type `application/json`.\n"}
{"snippet": "print('\nprint('Homogeneity {}'.format(metrics.homogeneity_score(y, labels)))\nprint('completeness {}'.format(metrics.completeness_score(y, labels)))\nprint('V-measure {}'.format(metrics.v_measure_score(y, labels)))\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`. (Optional! We won't cover this until Wednesday!)**\n"}
{"snippet": "print(cross_val_score(lr, ss_like_pca_df.iloc[:,:1], y, cv=5).mean())\n", "intent": "- What is the mean cross val score?\n"}
{"snippet": "Incep_predictions = [np.argmax(Incep_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Incep_predictions)==np.argmax(test_targets, axis=1))/len(Incep_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score)\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "tweets = [\"love the weather\",\"\npredictions = pipeline.predict(tweets)\npredictions\n", "intent": "It's worth noting that as have the classifiers, we can try them a couple of new tweets.\n"}
{"snippet": "new_data = np.asarray([0.18,0.15]).reshape(1,-1)\npred3 = knn3.predict(new_data)\npred5 = knn5.predict(new_data)\nprint \"The knn3 model thinks new_data belongs to class {}\".format(pred3[0])\nprint \"The knn5 model thinks new_data belongs to class {}\".format(pred5[0])\n", "intent": "Apply model on a new point\n"}
{"snippet": "accuracy_score(y_test, labels_70)\n", "intent": "Does this give a better accuracy score?\n"}
{"snippet": "def clean_tweets(tweets):\n    tweets = [re.sub(hashtag_pattern, 'HASHTAG', t) for t in tweets]\n    tweets = [re.sub(twitter_handle_pattern, 'USER', t) for t in tweets]\n    return [re.sub(url_pattern, 'URL', t) for t in tweets]\ndef test_tweets(tweets, model):\n    tweets = clean_tweets(tweets)\n    features = countvectorizer.transform(tweets)\n    predictions = model.predict(features)\n    return list(zip(tweets, predictions))\n", "intent": "Use the `test_tweet` function below to test your classifier's performance on a list of tweets. Write your tweets \n"}
{"snippet": "sampleData = dfCheck[:1]\nsampleDataFeatures = np.asarray(sampleData.drop('Outcome',1))\nsampleDataFeatures = (sampleDataFeatures - means)/stds\npredictionProbability = diabetesLoadedModel.predict_proba(sampleDataFeatures)\nprediction = diabetesLoadedModel.predict(sampleDataFeatures)\nprint('Probability:', predictionProbability)\nprint('prediction:', prediction)\n", "intent": "We will now use the first record to make our prediction.\n"}
{"snippet": "predictions = []\ndistance_from_hyperplane = []\nfor array in np.array_split(new_test_data_matrix[:,:-1], 100):\n    result = linear_predictorf.predict(array)\n    predictions += [r['predicted_label'] for r in result['predictions']]\n    distance_from_hyperplane += [r['score'] for r in result['predictions']]\ndistance_from_hyperplane_test_fair = np.array(distance_from_hyperplane)    \npredictions_test_fair = np.array(predictions)\npd.crosstab(np.where(new_test_data_matrix[:,-1] == 1, 1, 0), predictions_test_fair,\n            rownames=['actuals'], colnames=['predictions'])\n", "intent": "Now, we can calculate the predictions using our fair linear model.\n"}
{"snippet": "def test_tweets(tweets, model):\n    tweets = [clean(tweet) for tweet in tweets]\n    features = vectorizer.transform(tweets)\n    predictions = model.predict(features)\n    return list(zip(tweets, predictions))\n", "intent": "Use the `test_tweet` function below to test your classifier's performance on a list of tweets. Write your tweets \n"}
{"snippet": "y_pred = model1.predict(X_train1)\nprint('train data: ',accuracy_score(y_train1, y_pred))\ny_pred = model1.predict(X_test1)\nprint('test data: ',accuracy_score(y_test1, y_pred))\n", "intent": "First lets go back to our original best model ('Model1') and see if it is overfitting the training data:\n"}
{"snippet": "epsilon = 1e-7 \nloss = tf.losses.log_loss(y, y_proba) \n", "intent": "Build it from scratch, and try TF's built in functions.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1),dtype=bool)\n", "intent": "90% accuracy! Great no?!?! Not so fast buster!!!! Lets create a reallllly bad classifier that predicts 0 for everything, and see how it performs.\n"}
{"snippet": "print(\"RF scores: \", cross_val_score(forest_clf,X_train,y_train, cv=3,scoring='accuracy'))\nprint(\"SGD scores: \", cross_val_score(sgd_clf,X_train,y_train, cv=3, scoring='accuracy'))\n", "intent": "Let's take a look at how the random forest classifer performs!\n"}
{"snippet": " predictions = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint('Confusion Matrix:')\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint('')\nprint('Classification Reprt:')\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\npoly_scores = cross_val_score(clf_poly, iris.data, iris.target, cv=5)\nprint(\"Scores for linear kernel \\n-------\")\nprint(scores)\nprint(scores.mean())\nprint(\"\\n Scores for poly kernal\\n-------\")\nprint(poly_scores)\nprint(poly_scores.mean())\nprint(clf_poly.predict([[ 5.8,  2.8,  5.1,  2.4], [2, 1, 0, 2, 0, 2]]))\nprint(Y_test)\n", "intent": "Now we can use K-Fold cross validation instead to split the data into K (5 in this case) sections keeping one for testing\n"}
{"snippet": "test_preds = best_alpha * pred_lr + (1 - best_alpha) * pred_lgb \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "input = {\n  'observations': np.ones((observation_space_mapping[roboschool_problem])),\n}\nresult = predictor.predict(input)\nresult\n", "intent": "Now let us predict the actions using a dummy observation\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_seed, shuffle=True) \n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf)\n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "raw_preds = model.predict(X_test)\n", "intent": "We first generate raw predictions and then compare those outcomes with the observed results.\n"}
{"snippet": "print(\"Mean squared error of the baseline model\")\nmean_squared_error(df['value'],df['Baseline Prediction'])\n", "intent": "**Mean squared error of our baseline and see how it looks**\n"}
{"snippet": "mean_squared_error(df['value'],df['Baseline Prediction'])\n", "intent": "**Mean squared error of our baseline and see how it looks**\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall_score(y_test, ypf)\n", "intent": " - Use [Kavin Markham's notebook](https://github.com/justmarkham/scikit-learn-videos/blob/master/09_classification_metrics.ipynb) as reference\n"}
{"snippet": "print((10 + 0 + 20 + 10) / 4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "import numpy as np\nprint((10**2 + 0**2 + 20**2 + 10**2) / 4)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt(((10**2 + 0**2 + 20**2 + 10**2) / 4)))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "classification_error = (FP + FN) / float(TP + TN + FP + FN)\nprint(classification_error)\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "scores = predictor.predict(input_image.asnumpy())\n", "intent": "Now we can use the predictor object to classify the input image:\n"}
{"snippet": "y_pred_class = nb.predict(X_test_dtm)\n", "intent": "**Naive bayes is fast as seen above**\n- This matters when we're using 10-fold cross-validation with a large dataset\n"}
{"snippet": "y_pred_class = logreg.predict(X_test_dtm)\n", "intent": "**This is a lot slower than Naive Bayes**\n- Naive Bayes cannot take negative numbers while Logistic Regression can\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "This is a good model if you care about the probabilities.\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "Mean Squared Error (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "X_new = [[3,5,4,2], [5,4,3,2]]\nknn.predict(X_new)    \n", "intent": " - New observation are called 'out of sample' data\n    - Uses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics \nprint metrics.accuracy_score(y, y_pred)\n", "intent": " - PROPORTION OF OUR CORRECT PREDICTIONS\n    - COMMON ELALUATION METRIC FOR CLASSIFICATION PROBLEMS\n"}
{"snippet": "acc = accuracy_score(y_test, y_pred)\nprint acc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "iris_predictor.predict([6.4, 3.2, 4.5, 1.5]) \n", "intent": "Invoking prediction:\n"}
{"snippet": "predctions = lm.predict(X_test)\n", "intent": "<h3>Predections </h3>\n"}
{"snippet": "y_pred_baseline = baseline_fit.predict(x_train_constant)\nrmse_baseline = sm.tools.eval_measures.rmse(y_train,y_pred_baseline)\nprint(\"Baseline RMSE is:\",rmse_baseline)\n", "intent": "**Observations: Here we can see that newspaper sales are not statistially significant and therefore this validates our earlier observation.**\n"}
{"snippet": "loss = tf.losses.mean_squared_error(y_label, y_network)\n", "intent": "Now we can define the loss function, which our network is going to optimize the weights for:\n"}
{"snippet": "AUC_scores_mix=[]\nw_range=range(10, 100, 1)\nfor w in w_range:\n    y_pred_mix=(w/100)*y_pred_nw2+y_pred_nw*((100-w)/100)\n    AUC_scores_mix.append(roc_auc_score(y_test_genres, y_pred_mix, average='macro'))\n", "intent": "We identify the best accuracy of the model from the weights of the predictions obtained in the images and text models.\n"}
{"snippet": "AUC_scores_mix=[]\nw_range=range(10, 100, 1)\nfor w in w_range:\n    y_pred_mix=(w/100)*y_pred_genres2+y_pred_genres*((100-w)/100)\n    AUC_scores_mix.append(roc_auc_score(y_test_genres, y_pred_mix, average='macro'))\n", "intent": "To make the merge of the models is identified how much should weigh the prediction of each model in such a way that the roc_AUC_score is maximized.\n"}
{"snippet": "w_0 = np.random.rand()\nw_1 = np.random.rand()\nw_0, w_1, my_loss(w_0, w_1, X, Y)\n", "intent": "Those are huge gradients! \n"}
{"snippet": "y_genetic = genetic_regressor.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\nscore_genetic = genetic_regressor.score(X_test, y_test)\ny_tree = tree_regressor.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\nscore_tree = tree_regressor.score(X_test, y_test)\ny_neural = neural_regressor.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\nscore_neural = neural_regressor.score(X_test, y_test)\n", "intent": "All three regressors will try and evaluate the function over our space:\n"}
{"snippet": "resids = bos.PRICE - lm.predict(X)\nMSE = np.mean( resids**2)\nprint(MSE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print(np.mean(cross_val_score(reg,X_train,y_train)))\nprint(np.mean(cross_val_score(rf,X_train,y_train)))\nprint(np.mean(cross_val_score(reg2,X_train2,y_train2)))\nprint(np.mean(cross_val_score(cv.best_estimator_,X_train2,y_train2)))\n", "intent": "Random forest regressor seems to be better\n"}
{"snippet": "import numpy as np\nrandom_input = np.random.rand(1, 1, 3, 3)\nprediction = predictor.predict({'inputs': random_input.tolist()})\nprint(prediction)\n", "intent": "Invoking prediction:\n"}
{"snippet": "for clf in classifiers:    \n    train_predict(clf, X_train, y_train, X_test, y_test)\n", "intent": "Evaluating all classifiers.\n"}
{"snippet": "dummy_clf = DummyClassifier(strategy='most_frequent', random_state=1)\ntrain_predict(dummy_clf, X_train, y_train, X_test, y_test)\n", "intent": "Creating a Dummy Classifier which Predicts the Majority Class and XGBoost untuned model\n"}
{"snippet": "scores = sklearn.model_selection.cross_val_score(log_reg, X, y, cv=5)\nprint 'cross-val scores: ', scores\nprint 'mean cross-val: ', scores.mean(), '+/-', scores.std()\n", "intent": "The model fitted on the training dataset has a 79% accuracy, which is close to the accuracy of the model on itself, suggesting good generalization.\n"}
{"snippet": "y_pred = sklearn.model_selection.cross_val_predict(log_reg, X, y, cv=3)\nprint 'accuracy:', sklearn.metrics.accuracy_score(y, y_pred)\nprint 'precision:', sklearn.metrics.precision_score(y, y_pred)\nprint 'recall:', sklearn.metrics.recall_score(y, y_pred)\n", "intent": "Cross-validated accuracy is closer to 79% on average, and remains fairly consistent, suggesting good generalizability.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ndef mean(numbers):\n    return float(sum(numbers)) / max(len(numbers), 1)\nscores = cross_val_score(regr, X_train, y_train)\navg_score=mean(scores)\nprint'Cross validation score examples are ' + str(scores[:3])\nprint'Cross validation average score is ' + str(avg_score)\nprint(\"Because this cross validation has very high average score which are close to 1, so this model is robust\")\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "predictions = lrmodel.predict( X_test)\n", "intent": "***\nNow that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "***\nFinally, lets evaluate our model's performance by calculating the residual sum of squares, and the various loss errors.\n"}
{"snippet": "predictions = lr_model.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "print(VGG19_model.evaluate(test_VGG19, test_targets))\nprint(VGG19_model.evaluate(train_VGG19, train_targets))\nprint(VGG19_model.evaluate(valid_VGG19, valid_targets))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "img_out = cml.predict({input_name: data})[output_name]\n", "intent": "Now, invoke the model and grab the output\n"}
{"snippet": "print(classification_report(y_test, rf_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = SVC_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "clf.predict(np.matrix([6.3, 3.3, 6.0, 2.5]))\n", "intent": "+ Can you modify the code to get a different result?\n"}
{"snippet": "score = model.evaluate(X_test,Y_test,verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:',score[1])\n", "intent": "EVALUATION AND PREDICTION\n"}
{"snippet": "values = dataset.values   \nX = values[:,:8]\ny = values[:,8]\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits =3 , random_state = 7)\nresult = cross_val_score(model,X, y, cv= kfold, scoring = 'accuracy' )\nprint('result.mean():', result.mean())\n", "intent": "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values like LDA.\n"}
{"snippet": "prob_y_0 = clf_0.predict_proba(X)\nprob_y_0 = [p[1] for p in prob_y_0]\nprint(\"roc_auc_score::\", roc_auc_score(y,prob_y_0))\n", "intent": "Ok... and how does this compare to the original model trained on the imbalanced dataset?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, StratifiedKFold\ncv=StratifiedKFold(n_splits=10, random_state=RANDOM_STATE)\nscores = cross_val_score(classifier, cpm, class_labels, cv=cv)\n", "intent": "**10-fold Cross-validation by explicitly calling the StratifiedKFold method to control the random_state param.**\n"}
{"snippet": "preds = lr.predict(X_valid)\n", "intent": "Wouldn't you expect the coefficient for `t2m_fc_mean` to be around 0 and the bias to be close to one? Why is this not the case?\n"}
{"snippet": "df = create_sub(rf.predict(X_test), 'rf.csv')\n", "intent": "That's a little better. But even after trying a range of parameter combinations I was not able to beat the linear regression score. Hmmm?\n"}
{"snippet": "pred1 = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred = my_tuned_tree.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the model on a stratified test set\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the model on the **validation set**\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "accuracy1 = metrics.accuracy_score(pred1, y_test)\naccuracy1\n", "intent": "Evaluate the trained classifier\n"}
{"snippet": "score = cross_val_score(clf1, X_train_plus_valid, y_train_plus_valid, cv=10)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "pred5 = my_tuned_model.predict(X_test)\n", "intent": "Evaluate the performance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "my_model.predict(X_train)\nfor i in range(my_model.n_estimators_):\n    print(\"** \", i, \" \", my_model.base_estimator_types[i])\n    y_pred = my_model.last_X_stack_queries[:, i]\n    accuracy = metrics.accuracy_score(y_train, y_pred) \n    print(\"Accuracy: \" +  str(accuracy))\n    print(metrics.classification_report(y_train, y_pred))\n    print(\"Confusion Matrix\")\n    display(pd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Measure the perfromance of the individual models in the stack\n"}
{"snippet": "my_model = SuperLearnerClassifier()\nscores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\nprint(scores)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the perofrmance of the SuperLearnerClassifier\n"}
{"snippet": "my_model = SuperLearnerClassifier(use_probs=True, stack_layer_classifier_type=\"logreg\")\nscores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\nprint(scores)\n", "intent": "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used.\n"}
{"snippet": "pred2 = r_tree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "my_model = SuperLearnerClassifier(use_probs=True, include_base_features_at_stack = True, stack_layer_classifier_type = \"logreg\")\nscores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\nprint(\"Mean accuracy \", mean(scores))\nprint(scores)\n", "intent": "Evaluate the impact of adding original descriptive features at the stack layer.\n"}
{"snippet": "print(\"Individual accuracies\")\ny_pred = my_model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"** Ensemble  Accuracy: \" +  str(accuracy))\nfor i in range(my_model.n_estimators_):\n    y_pred = my_model.last_X_stack_queries[:, i]\n    accuracy = metrics.accuracy_score(y_test, y_pred) \n    print(\"** \", i, \" \", my_model.base_estimator_types[i], \" Accuracy: \" +  str(accuracy))\n", "intent": "Measure the strength of the individual classifiers within the ensemble by measureing the accuracy of their predictions on a test set. \n"}
{"snippet": "kappa_matrix = np.zeros((my_model.n_estimators_, my_model.n_estimators_))\nfor i in range(my_model.n_estimators_):\n    for j in range(my_model.n_estimators_):\n        kappa = cohen_kappa_score(my_model.last_X_stack_queries[:, i], my_model.last_X_stack_queries[:, j], labels=None, weights=None)\n        kappa_matrix[i][j] = kappa\nprint(kappa_matrix)\n", "intent": "Measrue the disagreement between base estimators by calculating the Cohen's kappa metric between each of their classicications.\n"}
{"snippet": "pred2 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "def get_silhouette_score(x, estimator):\n    cls = estimator\n    cls_labels = cls.fit_predict(x)\n    score = silhouette_score(x, cls_labels)\n    return score\n", "intent": "We can see the silhouette score for **the best model, `KMeans(2, max_iter=500, init='k-means++', algorithm='elkan', n_init=8)`**\n"}
{"snippet": "pred = lasso.predict(X_test)\n", "intent": "Seuraavaksi on aika ennustaa malli koko harjoitusdatalla ja tarkastella sen antaman ennusteen luotettavuutta.\n"}
{"snippet": "pred = ridge.predict(X_test)\n", "intent": "Ennustetaan malli koko datasetille ja tarkastetaan selitysaste.\n"}
{"snippet": "from theano import tensor as T\nraise NotImplementedError(\"TODO: add any other imports you need\")\ndef evaluate(x, y, expr, x_value, y_value):\n    raise NotImplementedError(\"TODO: implement this function.\")\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint \"SUCCESS!\"\n", "intent": "This exercise requires you to compile a Theano function and call it to execute `\"x + y\"`.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "Evaluate model using k fold cross validation\n"}
{"snippet": "log.predict(X_test)\nlog.score(X,y)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "scores = model.evaluate(X_test_pad, y_test, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "own_predictions = [np.argmax(own_model.predict(np.expand_dims(feature, axis=0))) for feature in test_own]\ntest_accuracy = 100*np.sum(np.array(own_predictions)==np.argmax(test_targets, axis=1))/len(own_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ridge_scores = cross_val_score(opt_ridge, predictors, y, cv=10)\nprint \"Cross-validated scores:\", ridge_scores\nprint \"Mean: \", np.mean(ridge_scores)\n", "intent": "---\nIs it better than the Linear regression? If so, why would this be?\n"}
{"snippet": "elastic_scores = cross_val_score(opt_elastic, predictors, y, cv=10)\nprint \"Cross-validated scores:\", elastic_scores\nprint \"Mean: \", np.mean(elastic_scores)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom math import sqrt\nprint \"RMSE:\", sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "metrics.silhouette_score(dn, labels, metric='euclidean')\nlen(dn)\n", "intent": "Find the Silhoutte Score and plot\n"}
{"snippet": "metrics.accuracy_score(labels,y)\n", "intent": "Plot the predicted vs actual classifcations to see how our clustering analysis compares\n"}
{"snippet": "metrics.silhouette_score(y,labels,metric='euclidean')\n", "intent": "Check the centroids to see where each cluster is lying \n"}
{"snippet": "pred = ir.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "roc_auc_score(y_test, clf.predict_proba(X_test)), accuracy_score(y_test, clf.predict(X_test))\n", "intent": "**ROC AUC and share of correct predictions on validation set.**\n"}
{"snippet": "linscores = linmodel.evaluate(X_test.reshape((-1,28*28)), \n                              Y_test, \n                              verbose=2)\nprint(\"%s: %.2f%%\" % (linmodel.metrics_names[1], linscores[1]*100))\n", "intent": "For a better measure of the quality of the model, let's see the model accuracy for the test data. \n"}
{"snippet": "linpredictions = linmodel.predict(X_test.reshape((-1,28*28)))\nshow_failures(linpredictions)\n", "intent": "Here are the first 10 test digits the linear model classified to a wrong class:\n"}
{"snippet": "scores = model.evaluate(X_test.reshape((-1,28*28)), Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Accuracy for test data.  The model should be somewhat better than the linear model. \n"}
{"snippet": "linscores = linmodel.evaluate(X_test.reshape((-1, 28*28)), Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (linmodel.metrics_names[1], linscores[1]*100))\n", "intent": "For a better measure of the quality of the model, let's see the model accuracy for the test data. \n"}
{"snippet": "linpredictions = linmodel.predict(X_test.reshape((-1, 28*28)))\nshow_failures(linpredictions)\n", "intent": "Here are the first 10 test digits the linear model classified to a wrong class:\n"}
{"snippet": "scores = model.evaluate(X_test.reshape((-1, 28*28)), Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Accuracy for test data.  The model should be somewhat better than the linear model. \n"}
{"snippet": "print np.hstack([knr.predict(xin[:5]),(S*y)[:5]])\n", "intent": " The sub-blocks show the windows of the the `y` data that are being\nprocessed by the nearest neighbor estimator. For example,\n"}
{"snippet": "print np.allclose(knr.predict(xin),S*y)\n", "intent": " Or, more concisely checking all entries for approximate equality,\n"}
{"snippet": "def computeTestScores(test_x, test_y, clf, cv):\n    kFolds = sklearn.cross_validation.KFold(test_x.shape[0], n_folds=cv)\n    scores = []\n    for _, test_index in kFolds:\n        test_data = test_x[test_index]\n        test_labels = test_y[test_index]\n        scores.append(sklearn.metrics.accuracy_score(test_labels, clf.predict(test_data)))\n    return scores\n", "intent": "Test the performance of our tuned KNN classifier on the test set.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "** Now take that grid model and create some predictions using the test set and create classification reports and confusion matrices for them**\n"}
{"snippet": "model.predict(x_test)\n", "intent": "Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXception]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf.predict_proba(X_test)\nfor i in range(0, len(clf.predict_proba(X_train_unlabeled))):\n    print(i, clf.predict_proba(X_train_unlabeled)[i])\n    if clf.predict_proba(X_train_unlabeled)[i][0] > clf.predict_proba(X_train_unlabeled)[i][1]:\n        if clf.predict_proba(X_train_unlabeled)[i][0] > 0.9:\n", "intent": "[response here]\n[response here]\n"}
{"snippet": "cm_test = confusion_matrix(test_target,\n                 clf_tf_idf.predict(vectorizer_tf_idf.transform(test_data)))\ncm_test\n", "intent": "there are many false negatives; maybe because slightly neutral reviews were more often classified as negative.\n"}
{"snippet": "from sklearn.metrics import recall_score\nprint('Recall: {:.2%}'.format(recall_score(test_target, y_predict)))\n", "intent": "----\nCalcute the recall on the test data\n<br>\n<details><summary>\nClick here for the recall score...\n</summary>\n<br>\n78.38%\n</details>\n"}
{"snippet": "cm = confusion_matrix(y, gnb.predict(X))\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "rmse = np.sqrt(mean_squared_error(b, multib_pred))\nprint (\"The Root mean squared error is : {}\" .format(rmse))\nprint (\"The Mean absolute error is : {}\" .format(mean_absolute_error(b, multib_pred)))\nprint (\"The Correlation Coefficient is : {}\" .format((np.corrcoef(b, multib_pred))[0][1]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "print (\"The Coefficient of Determination is : {}\" .format(r2_score(y, y_pred)))\nrmse = np.sqrt(mean_squared_error(y, y_pred))\nprint (\"The Root mean squared error is : {}\" .format(rmse))\nprint (\"The Mean absolute error is : {}\" .format(mean_absolute_error(y, y_pred)))\nprint (\"The Correlation Coefficient is : \\n  {}\" .format((np.corrcoef(y, y_pred))[0][1]))\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "print np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "ResNet50_model_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_model_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=10)\nprint scores\nprint scores.mean()\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "dt_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\nsb.boxplot(dt_scores)\n", "intent": "Alright! We finally have our demo classifier. Let's create some more visuals of its performance so we have something to put in our report.\n"}
{"snippet": "line_Yhat_query = line_reg.predict(\n    np.reshape(X_query, (len(X_query),1)))\n", "intent": "Use the regression model to render predictions at each `X_query` point.\n"}
{"snippet": "line_Yhat = line_reg.predict(train_data[['X']])\n", "intent": "To plot the residual we will also predict the $Y$ value for all the training points:\n"}
{"snippet": "score = model.evaluate(test_x, test_y_onehot, batch_size=128)\nprint(\"LOSS (evaluated on the test dataset)=     {}\".format(score[0]))\nprint(\"ACCURACY (evaluated on the test dataset)= {}\".format(score[1]))\n", "intent": "Now, you'll probably want to evaluate or save the trained model.\n"}
{"snippet": "pred_y=model.predict(test_x).argmax(axis=1)\nfrom sklearn.metrics import classification_report\nprint( classification_report(test_y,pred_y) )\n", "intent": "Output the classification report (see if the trained model works well on the test data):\n"}
{"snippet": "score = model.evaluate(x_test_e, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "idx = 83\nexp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=6)\nprint('Document id: %d' % idx)\nprint('Probability(christian) =', c.predict_proba([newsgroups_test.data[idx]])[0,1])\nprint('True class: %s' % class_names[newsgroups_test.target[idx]])\n", "intent": "We then generate an explanation with at most 6 features for an arbitrary document in the test set.\n"}
{"snippet": "print \"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2)\nprint \"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2)\n", "intent": "Now, calculate the mean squared error using just the test data and compare to mean squared from using all the data to fit the model. \n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=[age])\nclassifier.train(train_inpf)\nresult = classifier.evaluate(test_inpf)\nclear_output()  \nprint(result)\n", "intent": "The following will train and evaluate a model using only the `age` feature:\n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns)\nclassifier.train(train_inpf)\nresult = classifier.evaluate(test_inpf)\nclear_output()\nfor key,value in sorted(result.items()):\n  print('%s: %s' % (key, value))\n", "intent": "You could retrain a model on these features by changing the `feature_columns` argument to the constructor:\n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns+my_categorical_columns)\nclassifier.train(train_inpf)\nresult = classifier.evaluate(test_inpf)\nclear_output()\nfor key,value in sorted(result.items()):\n  print('%s: %s' % (key, value))\n", "intent": "It's easy to use both sets of columns to configure a model that uses all these features:\n"}
{"snippet": "results = model.evaluate(test_inpf)\nclear_output()\nfor key,value in sorted(result.items()):\n  print('%s: %0.2f' % (key, value))\n", "intent": "After the model is trained, evaluate the accuracy of the model by predicting the labels of the holdout data:\n"}
{"snippet": "result_batch = mobilenet_classifier.predict(image_batch)\n", "intent": "Now run the classifier on the image batch.\n"}
{"snippet": "result = model.predict(image_batch)\nresult.shape\n", "intent": "Test run a single batch, to see that the result comes back with the expected shape.\n"}
{"snippet": "result_batch = model.predict(image_batch)\nlabels_batch = label_names[np.argmax(result_batch, axis=-1)]\nlabels_batch\n", "intent": "Run the image batch through the model and comvert the indices to class names.\n"}
{"snippet": "predictions_single = model.predict(img)\nprint(predictions_single)\n", "intent": "Now predict the image:\n"}
{"snippet": "example_batch = normed_train_data[:10]\nexample_result = model.predict(example_batch)\nexample_result\n", "intent": "Now try out the model. Take a batch of `10` examples from the training data and call `model.predict` on it.\n"}
{"snippet": "print np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2)\n", "intent": "The residual sum of squares: \n"}
{"snippet": "print(classification_report(y_test, y_pred_gbc))\n", "intent": "From the confusion matrix we can conclude that the model was able to predict **97%** correct class. Which is great model.\n"}
{"snippet": "def run_final_classifier(path, forest=os.path.join('.', 'trained_classifier.p')):\n    clf = pickle.load(open('trained_classifier.p', 'r'))\n    images = listdir(path)\n    print \"%-40s\" % \"filename\", \"%-40s\" % \"predicted_class\"\n    print \"\".join([\"-\" for i in range(60)])\n    for image in images:\n        predict = clf.predict(extractFeaturesFromOneImage(path + '/' + image))\n        print \"%-40s\" % image, \"%-40s\" % predict\n", "intent": "4) **A function that can take it a path and a pickled forest**\n"}
{"snippet": "accs = []\nfor k in k_values:\n    knn = \n    scores = cross_val_score(\n    accs.append(np.mean(\n", "intent": "Plot the cross-validated mean accuracy for each score. What is the best accuracy?\n"}
{"snippet": "expected = y\npredicted = clf.predict(X)\nconfusionMatrix = confusion_matrix(expected,predicted)\nprint(confusionMatrix)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", mean_squared_error(ys, predictions)**.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "print(neigh.predict(df.iloc[500:,2:]))\nprint(neigh.predict_proba(df.iloc[500:,2:]))\n", "intent": "Prepare test data and predict\n"}
{"snippet": "print(neigh.predict([[1.5]]))\nprint(neigh.predict_proba([[1.5]]))\n", "intent": "Predict and Print Results\n"}
{"snippet": "metrics.silhouette_score(matrix, labels, metric=\"euclidean\")\n", "intent": "Find the Silhoutte Score and plot\n"}
{"snippet": "logreg.predict_proba(3)[:, 1]\n", "intent": "**Interpretation:** A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'assorted'.\n"}
{"snippet": "print np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2)\n", "intent": "Mean squared error: \n"}
{"snippet": "accuracy_score([1,2,3],[3,2,1])\n", "intent": "0.99 wow it should be overfitting\n"}
{"snippet": "m_lin.predict(...)\n", "intent": "Predict the charges for person a (age 20, bmi 20, smoker) and person b (age 50, bmi 20, non-smoker)\n"}
{"snippet": "m_tree.predict(...)\n", "intent": "Predict the charges for person a (age 20, bmi 20, smoker) and person b (age 50, bmi 20, non-smoker)\n"}
{"snippet": "import sklearn.metrics as met\ndef err(y, yhat):\n    print(np.sqrt(met.mean_squared_error(y_true=y, y_pred=yhat)))\n    print(met.mean_absolute_error(y_true=y, y_pred=yhat))\n", "intent": "Make a convenience function err that prints the root mean square error (rmse) and the mean absolute error (mae). Print rmse and mae for test data.\n"}
{"snippet": "yhat_tree_train = m_tree.predict(X=X_train)\nprint(np.sqrt(met.mean_squared_error(y_true=y_train, y_pred=yhat_tree_train)))\nprint(met.mean_absolute_error(y_true=y_train, y_pred=yhat_tree_train))\n", "intent": "Compute the rmse and mae for the train set.\n"}
{"snippet": "yhat_tree_test = m_tree.predict(X=X_test)\nprint(np.sqrt(met.mean_squared_error(y_true=y_test, y_pred=yhat_tree_test)))\nprint(met.mean_absolute_error(y_true=y_test, y_pred=yhat_tree_test))\n", "intent": "Compute the rmse and mae for the test set.\n"}
{"snippet": "err(y_train, m_svm.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "err(y_train, m_nn.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "err(y_train, m_rf.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "newX = np.array([1, 4, 120])\nresults.predict(newX)\n", "intent": "What if a new car had 4 cylinders and 120 horsepower? \n"}
{"snippet": "yhat_nb_test = m_nb.predict(X=X_test)\n", "intent": "Predict the labels of the test data, using the trained model\n"}
{"snippet": "yhat_logit_test = m_logit.predict(X=X_test)\n", "intent": "Predict the labels of the test data, using the trained model.\n"}
{"snippet": "yhat_tree_test = m_tree.predict(X=X_test)\nconfusion_matrix_plot(y_test, yhat_tree_test)\n", "intent": "What does the confusion matrix look like?\n"}
{"snippet": "yhat_rf_test = m_rf.predict(X=X_test)\nconfusion_matrix_plot(y_test, yhat_rf_test)\n", "intent": "What does the confusion matrix look like?\n"}
{"snippet": "yhat_nn_test = m_nn.predict(X=X_test)\nconfusion_matrix_plot(y_test, yhat_nn_test)\n", "intent": "Show the confusion matrix for the test set.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\n", "intent": "Now that the classifier is trained, we can use it to predect the classes of our test data and have a look at the accuracy.\n"}
{"snippet": "probs = clf.predict_proba(X_test)\n", "intent": "But since the accuracy is only part of the story, let's get out the probability of belonging to each class so that we can generate the ROC curve.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\n", "intent": "Let's see if the accuracy has improved\n"}
{"snippet": "probs = clf.predict_proba(X_test)\n", "intent": "The accuracy is more or less unchanged, but we do get a slightly better ROC curve\n"}
{"snippet": "newPoint_1 = np.array([6,9])\nnewPoint_2 = np.array([2,4])\nprint classifier.predict([newPoint_1, newPoint_2])\nfig_ml_in_10\n", "intent": "Predict a Label\n===============\n"}
{"snippet": "cross_val_score(model_lr, X=X, y=ending_fact, scoring='accuracy')\n", "intent": "ngram_range = (1, 2)\n"}
{"snippet": "print X.shape\ncross_val_score(model_lr, X=X, y=y, scoring='accuracy')\n", "intent": "ngram_range=(1, 10)\n"}
{"snippet": "print X.shape\ncross_val_score(model_lr, X=X, y=y, scoring='accuracy')\n", "intent": "ngram_range=(1, 11)\n"}
{"snippet": "y = model.predict(data)\ny_class = np.argmax(y, axis=1)\n", "intent": "Now, we will obtain the predictions from the model.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\ny_pred_1 = knn.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred_1))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntestSet_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % testSet_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_avg = np.ones(len(y_test)) * np.mean(y_train)\nmse_avg = mean_squared_error(y_test, y_avg)\nprint(np.mean(y_train))\nprint(np.ones(len(y_test)))\nprint(\"\\n y_avg\\n\\n\", y_avg)\nprint(mse_avg)\nmae_avg = mean_absolute_error(y_test, y_avg)\n", "intent": "This section predicts the housing prices using average values of training samples.\n"}
{"snippet": "from sklearn import metrics\nmse=metrics.mean_squared_error(y_test,predictions)\nrmse=np.sqrt(mse)\nrmse\n", "intent": "plotting residuals. normally distributed residuals means model was a good choice for the data\n"}
{"snippet": "sklearn.metrics.accuracy_score(y, y_hat, normalize=True)\n", "intent": "Accuracy for Multi Class\n========================\n$$ \\frac{N_{\\text{true}}}{N_{\\text{all}}} $$\n"}
{"snippet": "prediction = kM.predict(df['Cluster'],kM.la)\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred = knn.predict(X_test)\n", "intent": "Let's evaluate our KNN model!\n"}
{"snippet": "print (classification_report(predictions, y_test))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "pipeline_pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "spam_detect_model.predict(tfidf20)[0]\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "roc_auc_score(df[\"admit\"], predicted_classes)\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "network_model_predictions = [np.argmax(network_model.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\ntest_accuracy = 100*np.sum(np.array(network_model_predictions)==np.argmax(test_targets, axis=1))/len(network_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_y_dnn = classifier.predict(X_test)['classes']\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predicted_y_rf = randForest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "import numpy as np\nfor n in xrange(3):\n    idx = np.random.permutation(X.index)\n    X_shuffle, y_shuffle = X.loc[idx] , y.loc[idx]\n    l = cross_val_score(model, X_shuffle, y_shuffle, cv=5)\n    print l, l.mean()\n", "intent": "- This would definitely be useful if we aren't sure the data has been sorted in some way.\n- We could also shuffle our data around, to avoid this\n"}
{"snippet": "metrics.explained_variance_score(Y_test,predicted_Y)\n", "intent": "We could quickly calculate $R^2$ to find if we are doing fine on the model fit\n"}
{"snippet": "predicted_class = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predicted_class = mnb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predicted_class_2 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = svm_model.predict(X_test)\n", "intent": "Now let's predict using the trained model.\n"}
{"snippet": "import math\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"MSE:\", mean_squared_error(ys, predictions)\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"MSE:\", mean_squared_error(ys, predictions)\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "test = features[50:61]\npredict = df.predict(test)\nactual = list(target[50:61])\nprint predict\nprint actual\n", "intent": "Prepare Test Data and Predict\n"}
{"snippet": "best_score = -999\nn_clusters = 0\nfor i in range(2, 31):\n    temp_score = gmm_score(i)\n    if(temp_score > best_score):\n        best_score = temp_score\n        n_clusters = i\n    print(\"Total score for %s clusters is %s\" % (i, temp_score))\nprint(\"The best score was %s for %s clusters\" % (best_score,n_clusters))\n", "intent": "* Report the silhouette score for several cluster numbers you tried. \n* Of these, which number of clusters has the best silhouette score?\n"}
{"snippet": "y_pred = model.predict(X)\n", "intent": "So we're almost $12,800 off, on average.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(XceptionModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==\n                           np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = pipeline.predict(X2_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "nb.predict_proba(X_test_dtm)[false_positives][0]\n", "intent": "It's also possible to see how strong a model's prediciton is. Lets see how strongly the model classified the above examples.\n"}
{"snippet": "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob\n", "intent": "Despite the mix of positive and negative terms in both of the above examples, the model decisively predicted the classes incorrectly.\n"}
{"snippet": "knn.predict([[3, 5, 1, .2]])\n", "intent": "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "df['Predictions'] = reg.predict(x_values)\ndf[\"Pred_Error\"] = df['Predictions'] - df['Outcome']\ndf.head()\n", "intent": "So now we have simple trained dataset. now to make a prediction.\n"}
{"snippet": "def predict(W, b, X):\n    m = X.shape[1]\n    WT = T.transpose(W, 0, 1)\n    A = ...\n    return A\n", "intent": "**Expected Output:**\n```\nW = tensor([[ 0.1903], [ 0.1226]])\nb = tensor(1.9254)\ndW = tensor([[ 0.6775], [ 1.4163]])\ndb = tensor(0.2192)\n```\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint -cross_val_score(model, X, y, cv=10, scoring=\"mean_absolute_error\").mean()\nprint -np.median(cross_val_score(model, X, y, cv=10, scoring=\"median_absolute_error\"))\n", "intent": "Now with cross-validation.\n"}
{"snippet": "certainty = clf.predict_proba(transformed[:, :-222])\n", "intent": "We may also look at prediction confidences to find if there are cases where the random forest is uncertain which value is most probable:\n"}
{"snippet": "y_predictions = logistic.predict(x)\n", "intent": "Then, we do the predicitons.\n"}
{"snippet": "print(classification_report(y, y_predictions))\n", "intent": "In addition we printed classification report.\n"}
{"snippet": "df['propensity_score'] = logistic.predict_proba(x)[:,1]\ndf.head()\n", "intent": "The last part of this task is to add propensity scores to the dataframe.\n"}
{"snippet": "y_pred = best_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "Now that we have found our best model, we will evaluate its accuracy on the test set as well as display its confusion matrix.\n"}
{"snippet": " def softmax_loss(x, y):\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": " def softmax_loss(x, y):\n    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    N = x.shape[0]\n    loss = -np.sum(np.log(probs[np.arange(N), y]+1e-8)) / N\n    dx = probs.copy()\n    dx[np.arange(N), y] -= 1\n    dx /= N\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "def compute_loss(X, y, w):\ndef compute_grad(X, y, w):\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": " def softmax_loss(f, y):\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "y_pred = np.array(zip(*model.predict_proba(X))[1])\ny_pred[:10]\n", "intent": "Note that each prediction gives a probability for each class: not setosa, yes setosa. We want the latter.\n"}
{"snippet": "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_test,predictions))\nprint(\"Mean square error (MSE):\", metrics.mean_squared_error(y_test,predictions))\nprint(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_test,predictions)))\n", "intent": "**Regression evaluation metrices**\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "**As expected, the classification report card is bad**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "**Then you can re-run predictions on this grid object just like you would with a normal model**\n"}
{"snippet": "print(classification_report(y_test,grid_predictions))\n", "intent": "**Classification report shows improved F1-score**\n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve**:\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % mean_squared_error(Y_test, pred_test))\n", "intent": "Now we will get the mean square error\n"}
{"snippet": "class_predict = log_model2.predict(X_test)\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(messages['label'], all_predictions))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"400\">\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "- (Like before, we need to reshape these *x* values into a ``[n_samples, n_features]`` features matrix.)\n"}
{"snippet": "print roc_auc_score(y, y_pred_proba)  \n", "intent": "Let's first look at this without cross-validation.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "- How's the accuracy?\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "- Generate some new data and predict the label:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "- View the classification report and confusion matrix for this classifier.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "- Check how accurate our unsupervised clustering was in finding similar digits within the data:\n"}
{"snippet": "N_EPOCHS = 5\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n", "intent": "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the split.\n"}
{"snippet": "N_EPOCHS = 5\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n", "intent": "Finally, we train our model...\n"}
{"snippet": "test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n", "intent": "...and get our new and vastly improved test accuracy!\n"}
{"snippet": "test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n", "intent": "...and get our best test accuracy yet! \n"}
{"snippet": "print(classification_report(y_test,rfc_pred))\nprint(confusion_matrix(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print \"Classification report for classifier\", model.__class__.__name__, \"\\n\"\nprint metrics.classification_report(expected, predicted), \"\\n\"\nprint \"Confusion matrix:\" \nprint metrics.confusion_matrix(expected, predicted)\n", "intent": "We could use `sklearn`'s `classification_report` and `confusion_matrix` to quickly view a bunch of performance metrics.\n"}
{"snippet": "print(classification_report(y_test,y_pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n    processed_tweets = process_all(unlabeled_tweets)\n    stripped_text = processed_tweets[\"text\"].apply(strip, rare_words=rare_words)\n    X = tfidf.transform(stripped_text)\n    pred = classifier.predict(X)\n    return pred\nclassifier = learn_classifier(X, y, 'best')\n", "intent": "We're almost there! It's time to write a nice little wrapper function that will use our model to classify unlabeled tweets from tweets_test.csv file.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "vgg_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res50]\ntest_accuracy = 100*np.sum(np.array(vgg_predictions)==np.argmax(test_targets, axis=1))/len(vgg_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = dct.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "print(classification_report(pred,y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "MSE = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nMSE\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "np.unique(model.predict(D), return_counts=True)\n", "intent": "That looks quite promising.\nUnfortunately, the model has put a lot of tags into the same cluster again:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "new_bad_review = \"This movie really sucked. I can't believe how long it dragged on. The actors are absolutely terrible.\"\nfeatures = tfidf.transform([new_bad_review])\nmodel.predict(features)\n", "intent": "We can also use our model to classify new reviews, all we have to do is extract the tfidf features from the raw text and send them to the model:\n"}
{"snippet": "new_bad_review = \"This movie really sucked. I can't believe how long it dragged on. The actors are absolutely talentless.\"\nfeatures = tfidf.transform([new_review])\nmodel.predict(features)\n", "intent": "We can also use our model to classify new reviews, all we have to do is extract the tfidf features from the raw text and send them to the model:\n"}
{"snippet": "svc.predict([[3, 1], [2,4]])\n", "intent": "Once we have learned the boundary then we can predict the label of novel samples\n"}
{"snippet": "pipeline.predict([\"10 things you need to do...\"])\n", "intent": "What? That was it?! \nThat's right. You've just built a machine learning classifier for clickbait and it was that easy to train. Let's test it out:\n"}
{"snippet": "predicted_labels = cross_val_predict(pipeline, training.full_content, training.label)\npipeline_performance(training.label, predicted_labels)\n", "intent": "We had pretty high accuracy but can we do better?\nRather than using just the title, we could use the title and description together.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "preds_base = logregcv.predict_proba(X_train_base)[:,1]\nfpr1, tpr1, thresholds = roc_curve(y_train_base, preds_base)\nroc_auc1 = auc(fpr1, tpr1)\nprint(\"AUC of the base model:\", roc_auc1)\n", "intent": "- Check AUC of the base model:\n"}
{"snippet": "y_pred = model.predict(X_test_pca)\nprint \"Overall prediction accuracy:\", model.score(X_test_pca, y_test)\nprint \nprint classification_report(y_test, y_pred, target_names=target_names)\nprint confusion_matrix(y_test, y_pred, labels=range(n_classes))\n", "intent": "Let's evaluate this 'best' model.\n"}
{"snippet": "train_y_hat = model.predict(train_X)\nprint np.sqrt(metrics.mean_squared_error(train_y, train_y_hat))\ntest_y_hat = model.predict(test_X)\nprint np.sqrt(metrics.mean_squared_error(test_y, test_y_hat))\n", "intent": "(Check http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html as needed)\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nscore2 = model2.evaluate(x_test, y_test, verbose=0)\nscore3 = model3.evaluate(x_test, y_test, verbose=0)\nscore4 = model4.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\nprint(\"Accuracy2: \", score2[1])\nprint(\"Accuracy3: \", score3[1])\nprint(\"Accuracy4: \", score4[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "Resnet50_predictions=[np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lr2.predict_proba(X_test_std[0,:].reshape(1, -1))\n", "intent": "Predict the probability of a pattern in the test set.\n"}
{"snippet": "(lr2.predict_proba(X_test_std[0,:].reshape(1, -1)),\nlr3.predict_proba(X_test_std[0,:].reshape(1, -1)))\n", "intent": "Predict the probability of a pattern in the test set.\n"}
{"snippet": "resnet50_model_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_model_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = linear_model.predict( X_test)\npredictions\n", "intent": "Calculate the accuracy score for the above model.\n"}
{"snippet": "y_pred = dnn_clf_bn.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The best params are reached during epoch 2, that's much faster than earlier. Let's check the accuracy:\n"}
{"snippet": "y_pred = dnn_clf.predict(X_train1)\naccuracy_score(y_train1, y_pred)\n", "intent": "Since batch normalization did not help, let's go back to the best model we trained earlier and see how it performs on the training set:\n"}
{"snippet": "uniques, idx = np.unique(data.beer_beerid, return_index=True)  \npred = pd.Series(model.predict(X[idx, :]), index=data.beer_beerid[idx], name=\"predictions\") \\\n    .sort(ascending=False, inplace=False)\npred_name = pd.Series(pred.values, beer_names[pred.index], name=\"predictions\")\nprint \"Top recommendations for %s.\" % user\nprint pred_name[:5]\n", "intent": "Pretty bad cross-validation scores, but mind you we have only a handful reviews.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) \n                           for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "inceptionV3_predictions = [np.argmax(inceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inceptionV3]\ntest_accuracy = 100*np.sum(np.array(inceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(metrics.classification_report(y_test,predictions))\n", "intent": "<font color=green>The total number of confusions dropped from **287** to **256**. [241+46=287, 246+10=256]</font>\n"}
{"snippet": "print(metrics.classification_report(y_test,predictions))\n", "intent": "<font color=green>The total number of confusions dropped even further to **209**.</font>\n"}
{"snippet": "this_voxel = 10\nkp,ai = best_model[this_voxel]\na = alphas[ai]\nfm = preproc.construct_feature_matrix(np.atleast_2d(wedge_features[:,kp]).T,hrf_length=L)\nyhat = model_grid.ix[kp,a].predict(fm)\n", "intent": "Check out predictions for one of the voxels -- for both the training and validation data.\n"}
{"snippet": "X_new = [[0], [10]]\nlr.predict(X_new)\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\".\n- Can predict for multiple observations at once.\n"}
{"snippet": "def evaluate(model, test_x, test_y):\n    y_prob = model.predict(test_x, verbose=0)\n    y_pred = np.argmax(y_prob, axis=-1)\n    y_true = np.argmax(test_y, 1)\n    score, accuracy = model.evaluate(test_x, test_y, batch_size=32)\n    print(\"\\nAccuracy = {:.4f}\".format(accuracy))\n    print(\"\\nError Rate = {:.4f}\".format(1. - accuracy))\n    return accuracy\n", "intent": "This method defines a few evaluation metrics that will be used to evaluate the performance of a trained model.\n"}
{"snippet": "y_probs_logit = clf.predict_proba(X_test)\ny_probs_logit_left = y_probs_logit[:,1]\n", "intent": "Guardemos las probabilidades de ambas clases en un array y $p(y=1)$ en otro:\n"}
{"snippet": "hide_code\nmodel.load_weights('weights.best.model.hdf5')\nscore = model.evaluate(x_test, y_test)\nscore\n", "intent": "We should have an accuracy greater than 10%. Let's try to reach the level 60-70%.\n"}
{"snippet": "beer = np.random.choice(beer_names.index)\nbeer_idx = (data.beer_beerid == beer).values\nX_beer = X[beer_idx, :][0]  \nprint \"%s will give beer %s a rating of %.1f\" % (user, beer_names[beer], model.predict(X_beer)[0])\n", "intent": "Pretty spectacular. \nJust for the fun of it, how will this user rate a random beer?\n"}
{"snippet": "hide_code\nmodel.load_weights('weights.best.model.hdf5')\nscore = model.evaluate(x_test, y_test)\nscore\n", "intent": "We should have an accuracy greater than 3%\n"}
{"snippet": "hide_code\ngray_model.load_weights('weights.best.gray_model.hdf5')\ngray_score = gray_model.evaluate(x_test2, y_test2)\ngray_score\n", "intent": "Try to reach an accuracy greater than 50%\n"}
{"snippet": "hide_code\nmulti_model.load_weights('weights.best.multi.hdf5')\nmulti_scores = multi_model.evaluate(x_test3, y_test3_list, verbose=0)\nprint(\"Scores: \\n\" , (multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 50% for the second target (background).\n"}
{"snippet": "hide_code\ngray_multi_model.load_weights('weights.best.gray_multi.hdf5')\ngray_multi_scores = gray_multi_model.evaluate(x_test4, y_test4_list, verbose=0)\nprint(\"Scores: \\n\" , (gray_multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (gray_multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (gray_multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 50% for the second target (background).\n"}
{"snippet": "hide_code\ngray_model.load_weights('weights.best.gray_model.hdf5')\ngray_score = gray_model.evaluate(x_test2, y_test2)\ngray_score\n", "intent": "Try to reach an accuracy greater than 80%\n"}
{"snippet": "hide_code\nmulti_model.load_weights('weights.best.multi.hdf5')\nmulti_scores = multi_model.evaluate(x_test3, y_test3_list, verbose=0)\nprint(\"Scores: \\n\" , (multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 25% for the second target (background).\n"}
{"snippet": "hide_code\ngray_multi_model.load_weights('weights.best.gray_multi.hdf5')\ngray_multi_scores = gray_multi_model.evaluate(x_test4, y_test4_list, verbose=0)\nprint(\"Scores: \\n\" , (gray_multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (gray_multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (gray_multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 25% for the second target (background).\n"}
{"snippet": "print (metrics.classification_report(yTest, predRF))\nprint (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, predRF),2))\n", "intent": "Use the `classification_report` from the `metrics` module and the `accuracy_score` to see how well the classifier is doing\n"}
{"snippet": "from lib import gfx  \nfrom sklearn import metrics\nfrom math import sqrt\nrfr_fare = np.expm1(rfr_model.predict(X_test)) \ny_pred = rfr_fare\nrmse_score = sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('\\n\ngfx.plot_actual_vs_predicted(y, y_test, y_pred)\n", "intent": "Evaluates the model on the holdout set:\n"}
{"snippet": "scores = cross_val_score(linreg, X, y, cv=10, scoring='mean_squared_error')\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis = 0))) for feature in test_Resnet50]\ntest_accuracy = 100 * np.sum(np.array(Resnet50_predictions) == np.argmax(test_targets, axis = 1)) / len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print classification_report(y_train,rfc.predict(X_train))\n", "intent": "Check the performance in trainging:\n"}
{"snippet": "DogResnet50_predictions = [np.argmax(DogResnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(DogResnet50_predictions)==np.argmax(test_targets, axis=1))/len(DogResnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_net]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(df[features]))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "metrics.silhouette_score(km_X, labels, metric='euclidean')\n", "intent": "Let's plot the SSE and SC to see where our optimum k lies.\n"}
{"snippet": "Yhat = np.squeeze(np.array([model.predict(x.reshape(1, *x.shape)) for x in X]))\n", "intent": "Predict and compare to truth:\n"}
{"snippet": "W1 = fully_connected(nfeatures, nhidden) \nW2 = fully_connected(nhidden, ncats)\nWs = [W1, W2]\nacc = accuracy(predict(Ws, X_test), Y_test)\nprint(\"Accuracy ({}): {:.2f}\".format(0, acc))\ntrainer = Trainer(X_train, Y_train, AdamOptimizer())\nwhile trainer.epochs < 10:\n    trainer.train(Ws)\nacc = accuracy(predict(Ws, X_test), Y_test)\nprint(\"Accuracy ({}): {}\".format(trainer.epochs, acc))\n", "intent": "Let's try with the Adam optimizer, which usually doesn't require us to change its parameters:\n"}
{"snippet": "y_hat = knn.predict(X_test)\nprint(y_hat)\nprint(y_test)\nprint('Accuracy:', (y_hat == y_test).mean())\n", "intent": "Predict the labels (Iris species) for the test data and compare with the real labels:\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\"\n- Can predict for multiple observations at once\n"}
{"snippet": "y_pred = model.predict(X_val, batch_size=500)\n", "intent": "Stability is reach quite fast. we can now evaluate it on the validation set.\n"}
{"snippet": "accuracy_score(y_val, y_ohe)\n", "intent": "Strange for a prediction of 94%. Let's look at the accuracy on validation set manually\n"}
{"snippet": "y_ohe2 = (y_pred>0.32).astype(int)\naccuracy_score(y_val, y_ohe2)\n", "intent": "Only 28% ... still low but we should keep in mind that we may predict multiclass so we should consider as true all class for example above 0.3\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "Now let's predict performances for the test set. We will be able to compare it to real perfs.\n"}
{"snippet": "clf.predict_proba(iris.data[:1, :])\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "dnn_model = tf.estimator.DNNClassifier(hidden_units=[10,20,10,10], feature_columns=feat_cols, n_classes=2)\ndnn_model.train(input_fn=input_func, steps=1000)\ndnn_model.evaluate(eval_input_func)\n", "intent": "76% means a little improvement... let's use more layers\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions));\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "item_prediction = predict(train_data_matrix, item_similarity, flag='item')\nuser_prediction = predict(train_data_matrix, user_similarity, flag='user')\n", "intent": "And now we apply the predict formula in each case\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Now, let's rerun everything\n"}
{"snippet": "roc_auc_score(df['admit'], predicted_classes)\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\nnp.mean(np.sqrt(-scores))\n", "intent": "min samples = ln(depth)/number of samples\n    - because the number of leaves is 2^d\n"}
{"snippet": "model.load_weights(file_path)\nprint('Predicting....')\ny_pred = model.predict(test_data, batch_size=1024,verbose=1)\n", "intent": "The model is overfit\n"}
{"snippet": "print(\"Final model:\")\nprint(w.get_value())\nprint(b.get_value())\nprint(\"target values for D:\")\nprint(test_set[1])\nprint(\"prediction on D:\")\nprint(predict(test_set[0]))\n", "intent": "Now estimate the number of errors made by the classifier on the training set itself.\n"}
{"snippet": "from sklearn import metrics\ny_pred = linreg.predict(X)\nmetrics.mean_squared_error(y, y_pred)\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "y_pred_class = logreg.predict(X_test)\n", "intent": "Make predictions on the testing set and calculate the accuracy\n"}
{"snippet": "for i in range(n_B):\n    print(i, np.sqrt(mean_squared_error(y_pred[i], y_test)))\n", "intent": "Results of each tree\n"}
{"snippet": "errors = np.zeros(n_estimators)\nfor i in range(n_estimators):\n    y_pred_ = trees[i].predict(X_train.iloc[samples_oob[i]])\n    errors[i] = 1 - metrics.accuracy_score(y_train.iloc[samples_oob[i]], y_pred_)\n", "intent": "Estimate the oob error of each classifier\n"}
{"snippet": "ResNet50_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\nvalid_predict = model.predict(X_valid)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\nprint scores\nprint scores.mean()\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "testPre = text_clf.predict(clf_test.Proposal)\n", "intent": "Let's use our trained model to predict values in the test set\n"}
{"snippet": "custom_predictions = [np.argmax(custom_model.predict(np.expand_dims(feature, axis=0))) for feature in test_custom]\ntest_accuracy = 100*np.sum(np.array(custom_predictions)==np.argmax(test_targets, axis=1))/len(custom_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "train_set = in_dta[:25]\nval_set = in_dta[25:]\nval_test_error(train_set, val_set, test_set)\n", "intent": "Split in_dta into a training set (first 25 examples) and validation set (last 10 examples) and evaluate validation and test errors\n"}
{"snippet": "train_set = in_dta[25:]\nval_set = in_dta[:25]\nval_test_error(train_set, val_set, test_set)\n", "intent": "Split in_dta into a training set (first 10 examples) and validation set (last 25 examples) and evaluate validation and test errors\n"}
{"snippet": "def disagreement_probability(weights, line, points):\n        x = Line.transform_inputs(points)\n        y = line.evaluate(points)\n        g = np.array([np.sign(np.dot(weights, xn)) for xn in x])\n        return 100. * np.sum(y != g) / len(y)\n", "intent": "Calculate disagreement probability\n"}
{"snippet": "import numpy as np\nprint (np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "mse= np.mean((bos.PRICE -lm.predict(X))**2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "a = metrics.precision_score(y_test, y_svc, pos_label='four')\nb = metrics.recall_score(y_test, y_svc, pos_label='four')\n", "intent": "$2ab \\over {a + b}$\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint r2\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "pred = knn_1.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred= logreg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint(f1_score(y_test, y_pred))\n", "intent": "* We can now compute the F-score using these predictions\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import classification_report, roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import label_binarize\ndef print_crossval_eval_measures(model, df, target):\n    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n        scores = cross_val_score(model, df, target, cv=10, scoring=metric)\n        print('mean', metric, ':', scores.mean(), '\\nall scores :', scores, \"\\n\")\nprint_crossval_eval_measures(model1, X, y)\n", "intent": "Part 10 refers to performing cross validation on the whole dataset and comparing the resulting scores to the train test split method.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(features[['mpg','disp']],km.labels_)\n", "intent": "Silhouette measures how similar an observation is to its own cluster compared to the closest cluster\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(adult,labels,metric='euclidean')\n", "intent": "Compute the Silhoutte Score to measure your analysis\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(y, kmeans.labels_)\nprint(acc)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(X,kmeans.labels_)\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "from sklearn.metrics import classification_report\ncls_rep = classification_report(Y, kmeans.labels_)\nprint(cls_rep)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "svc.predict([[200000, 40]])\n", "intent": "Or just use predict for a given point:\n"}
{"snippet": "from sklearn.metrics import accuracy_score,confusion_matrix\npredictions=nb.predict(X_test)\nprint(accuracy_score(predictions, y_test))\nprint(confusion_matrix(predictions, y_test))\n", "intent": "Score the Naive Bayes classifier on the test data\n"}
{"snippet": "print(accuracy_score(predictions, y_test))\nprint(confusion_matrix(predictions, y_test))\n", "intent": "Produce the accuracy score of the logistic regression from the test set\n"}
{"snippet": "predicttions = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "bank_predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "model_class.evaluate(input_func,steps=1000)\ninput_pred_func=tf.estimator.inputs.pandas_input_fn(x=X_test,shuffle=False,num_epochs=1,batch_size=8)\n", "intent": "** Create a prediction input function. Remember to only supprt X_test data and keep shuffle=False. **\n"}
{"snippet": "prediction=log_model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "prediction=model.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "prediction =model1.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "pred=model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(np.array(trainY), p4(np.array(trainX)))\nprint r2\n", "intent": "...even though it fits the training data better:\n"}
{"snippet": "dem_probs = clf.predict(X_pred)\nprint((dem_probs/100) * 538)\n", "intent": "Calculate the number of electoral college votes the Democratic candidate will given the data on each date in the prediction file. \n"}
{"snippet": "pred=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predict=logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred=nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "pred=pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "train_preds = meta_lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = meta_lr.predict(X_test_level2)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint(\"Train RMSE for stacking is \", sqrt(mean_squared_error(y_train_level2, train_preds)))\n", "intent": "Compute the R-squared and RMSE for X_train_level2 and compute the final predictions on X_test_level2\n"}
{"snippet": "y_pred = linreg.predict(X_test)\n", "intent": "y = 0.0465645678742*TV + 0.179158122451*Radio + 0.00345046471118*Newspaper + 2.87696662232\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_train_pred.shape\n", "intent": "Next we need to generate some predictions on the training data and calculate the error for each instance.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "result = [np.round(anomaly_score(x, dist), 4) for x in val_errors]\nprint(result)\n", "intent": "The higher the number, the more likely the sample is to be an anomaly.\n"}
{"snippet": "print(test_VGG16[0].shape)\nprint(np.expand_dims(test_VGG16[0], axis=0).shape)  \nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "def get_next_char(inp):\n    idxs = [char_idx[c] for c in inp]\n    arrs = [np.array(i)[np.newaxis] for i in idxs]\n    p = model.predict(arrs)\n    i = np.random.choice(range(vocab_size), p=p.ravel())\n    return chars[i]\n", "intent": "Helper function to get fourth predicted character given three\n"}
{"snippet": "def predict_single_batch(x, max=True):\n    x = np.tile(x, batch_size).reshape((batch_size, in_len))\n    if max:\n        return model.predict(x, batch_size=batch_size)[0, in_len-1].argmax()\n    else:\n        return np.random.choice(range(vocab_size), p=model.predict(x, batch_size=batch_size)[0,in_len-1])\n", "intent": "Helper method to predict on single batch for stateful networks\n"}
{"snippet": "preds = model.predict(test_features, batch_size=64, verbose=1)\n", "intent": "Get the predictions\n"}
{"snippet": "predict2 = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['Cluster'],kM.labels_))\nprint(classification_report(df['Cluster'],kM.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "mseTotal = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint mseTotal\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "pred_probs = logr.predict_proba(X)[:,1]\n", "intent": "The first column indicates the predicted probability of class 0, and the second column indicates the predicted probability of class 1.\n"}
