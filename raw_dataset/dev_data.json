{"snippet": "import seaborn as sns\nscore_df = pd.DataFrame(results.grid_scores_)\nscore_df['parameters'] = score_df['parameters'].map(lambda score: score['alpha'])\nscore_df['mean_score'] = score_df['cv_validation_scores'].map(lambda scores: np.mean(scores))\nscore_df.plot(kind=\"scatter\", x=\"parameters\", y=\"mean_validation_score\", title=\"Alphas / Mean Scores\")\n", "intent": "Figure out how to plot the impact of your score given the range of parameters used.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_df=0.9, max_features=5000, sublinear_tf=True, use_idf=True)\n", "intent": "Vamos utilizar o Tf-Idf para criar o embedding dos motivos de compra e o K-Means para encontrar clusters.\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA,'train_log1p_recommends.csv'), \n                           index_col='id')\ny_train = train_target['log_recommends'].values\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n        'year': [2000, 2001, 2002, 2001, 2002],\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\ndf\n", "intent": "Pandas `DataFrame` can be easily saved into a SQL database using the function `to_sql()`. Let's create a data frame first.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX, y = iris.data, iris.target\n", "intent": "Grid Searches\n=================\n"}
{"snippet": "df.to_csv('img2targets.csv', index=False)\n", "intent": "Save the data frame. \n"}
{"snippet": "sales = pd.read_csv('data/sales_train_v2.csv')\nshops = pd.read_csv('data/shops.csv')\nitems = pd.read_csv('data/items.csv')\nitem_cats = pd.read_csv('data/item_categories.csv')\ntest = pd.read_csv('data/test.csv')\nsubmission = pd.read_csv('data/test.csv')\n", "intent": "Load the data from the hard drive... Data files must be in a data/ subfolder relatively to the folder where notepad is installed\n"}
{"snippet": "performance_template_df = pd.DataFrame(columns= [\n        ['Params']*3 + [b for a in ['Precision', 'Recall', 'F1_score', 'Support'] for b in [a, a]],\n        ['MaxDepth', 'Nfeature', 'Features'] + ['no', 'yes']*4\n    ])\nperformance_template_df\n", "intent": "Now, let's run the performance evaluation across a number of configurations. We'll collect the results for each configuration into a dataframe.\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/delta.csv', index_col='Aircraft')\ndf['Clusters'] = clusters\ndf['Aircraft'] = df.index\ndf_grouped = df.groupby('Clusters').mean()\nprint(df_grouped.Accommodation)\n", "intent": "You don't have to write any code in this section, but here's one interpretaion of what we have done.\nLet's take a closer look at each cluster.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nrandom_state = np.random.RandomState(3939)\ntest_set_fraction = 0.3\nreviews_train, reviews_test, t_train, t_test = train_test_split(\n    reviews, labels, test_size=test_set_fraction, random_state=random_state)\nprint '\n", "intent": "We set aside 30% of the data set to test the classifier.\n"}
{"snippet": "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n                                                stratify = labels_train, random_state = 123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[feature], data[target], test_size=0.25, random_state=27)\n", "intent": "Divide the data set into train sets and test sets.\n"}
{"snippet": "a = pd.DataFrame(X_train1.columns)\nb = pd.DataFrame(model.feature_importances_)\na.join(b,lsuffix='_feature',rsuffix='_score')\n", "intent": "Importanza delle variabili per il modello:\n"}
{"snippet": "kick = pd.read_csv(\"kick.csv\", low_memory=False)\nkick.head(3)\n", "intent": "The data corresponds to kickstarter projects, data about the projects and whether it was successfully funded or not. \n"}
{"snippet": "y = boston.target\ndf = pd.DataFrame(boston.data,columns=boston.feature_names)\nprint boston.DESCR\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "def factor_betas(pca, factor_beta_indices, factor_beta_columns):\n    assert len(factor_beta_indices.shape) == 1\n    assert len(factor_beta_columns.shape) == 1\n    return pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\nproject_tests.test_factor_betas(factor_betas)\n", "intent": "Implement `factor_betas` to get the factor betas from the PCA model.\n"}
{"snippet": "summary = pd.DataFrame(list(zip(X.columns, \\\n    np.transpose(clf_tree.feature_importances_), \\\n    np.transpose(clf_rf.feature_importances_), \\\n    np.transpose(clf_ext.feature_importances_), \\\n    np.transpose(clf_gb.feature_importances_), \\\n    np.transpose(clf_ada.feature_importances_), \\\n    np.transpose(clf_xgb.feature_importances_), \\\n    )), columns=['Feature','Tree','RF','Extra','GB','Ada','Xtreme'])\nsummary['Median'] = summary.median(1)\nsummary.sort_values('Median', ascending=False)\n", "intent": "For additional insight we compare the *feature\\_importance* output of all the classifiers for which it exists:\n"}
{"snippet": "df1 = pd.DataFrame([pd.Series(np.arange(10, 15)) ,\n                pd.Series(np.arange(20, 25)),             \n                pd.Series(np.arange(20, 25))])\nprint(df1, '\\n')\nprint(df1.shape) \n", "intent": "* **shape**\n - row, column\n"}
{"snippet": "df = pd.read_csv(\n    '/home/data_scientist/data/2001.csv',\n    encoding='latin-1',\n    usecols=(13, 16)\n    )\n", "intent": "We use the `AirTime` column at the Willard airport.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['is_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "test_df = pd.DataFrame(X_test, columns=data.columns[:-1])\ntest_df['subscribed'] = y_test\nfor col in test_df.columns:\n    if isinstance(test_df[col][0], int):\n        test_df[col] = test_df[col].apply(pd.to_numeric, errors='coerce')\n    elif isinstance(test_df[col][0], float):\n        test_df[col] = test_df[col].apply(pd.to_numeric, errors='coerce')\ntest_df['never_contacted'] = test_df['prev_days'] == 999\ntest_df = test_df[list(test_df.columns[:-2]) + list([test_df.columns[-1]]) + list([test_df.columns[-2]])]\nc_df = pd.concat([train_df, test_df], axis=0)\n", "intent": "First, I build a test_df, convert data types, and concatenate test_df and train_df.\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "pd.DataFrame([[1,2], [3,4]])\n", "intent": "DataFrames are 2 dimensional arrays with row and column labels. You can construct them from any 2 dimensional structure you might expect.\n"}
{"snippet": "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\ny = df.iloc[0:100, 4].values\ny = np.where(y == 'Iris-setosa', -1, 1)\nX = df.iloc[0:100, [0,2]].values\n", "intent": "Dataset of three different Iris flower species and their respective features. [Source link](https://archive.ics.uci.edu/ml/datasets/Iris)\n"}
{"snippet": "df = pd.read_csv('poly_regression.csv')\ndata = df.values\nX = data[:,0]\nY = data[:,1]\nX = X.reshape(X.shape[0],1)\nY = Y.reshape(Y.shape[0],1)\n", "intent": "Import data from poly_regression.csv\n"}
{"snippet": "df=pd.read_csv('2013_movies.csv')\n", "intent": "<h1>Challenge 11</h1>\n"}
{"snippet": "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n", "intent": "**Create a dataframe**\n"}
{"snippet": "training_data = pd.read_csv(\"ames_train.csv\")\ntest_data = pd.read_csv(\"ames_test.csv\")\n", "intent": "As a good sanity check, we should at least verify that the data shape matches the description.\n"}
{"snippet": "price_type = df_all.price_type\nprice_type_vectorizer = CountVectorizer(binary=1, dtype='uint8', min_df=10)\nprice_type_matrix = price_type_vectorizer.fit_transform(price_type)\n", "intent": "First let's try using the price type only\n"}
{"snippet": "trips = bq.Query(taxiquery, YEAR=2014).to_dataframe()\nweather = bq.Query(wxquery, YEAR=2014).to_dataframe()\ndata2014 = pd.merge(weather, trips, on='daynumber')\ndata2014[:5]\n", "intent": "<h3> Adding 2014 data </h3>\nLet's add in 2014 data to the Pandas dataframe.  Note how useful it was for us to modularize our queries around the YEAR.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n", "intent": "Map words <----> integer <----> frequency of integer in a doc, so called Term Frequencies.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = train_test_split(X[predictors], y, train_size=0.7, random_state=8)\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "df = pd.read_csv('../datasets/titanic.csv')\n", "intent": "Quelle: [https://www.kaggle.com/c/titanic/data)\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data.references_org_person][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\ndata = fetch_olivetti_faces().images\n", "intent": "400 fotos of human faces. Each face is a 2d array [64x64] of pixel brightness.\n"}
{"snippet": "pca = sklearn.decomposition.KernelPCA(n_components=2, kernel='rbf', gamma=.001)\nXt = pca.fit_transform(X)\n", "intent": "Compare that the kernel PCA:\n"}
{"snippet": "import pandas as pd\ncust_df = pd.read_csv(\"Cust_Segmentation.csv\")\ncust_df.head()\n", "intent": "Before you can work with the data, you must use the URL to get the Cust_Segmentation.csv.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n", "intent": "Split our data into training and testing\n"}
{"snippet": "x = StandardScaler().fit_transform(x)\nprint x.shape\nprint x[0:5]\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet_800600.jpg\")\nimshow(style_image);\n", "intent": "For our running example, we will use the following style image: \n"}
{"snippet": "def get_word_table(table, key, sim_key='similarity', show_sim = True):\n    if show_sim == True:\n        return pd.DataFrame(table, columns=[key, sim_key])\n    else:\n        return pd.DataFrame(table, columns=[key, sim_key])[key]\n", "intent": " We can use analogies to see word associations. For instance, King is to Woman as Queen is to _ , we get:\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled_features,columns=dataframe.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    nb_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- Let's try an RNN for the same Reuters classification task:\n"}
{"snippet": "N = 200\nnp.random.seed(0)\nX, y = sklearn.datasets.make_moons(N, noise = 0.20)\n", "intent": "Create data for a simple binary classification problem.\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(100, 2, 2, 0, weights=[.5, .5], random_state=0) \nclf = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nclf.fit(X, y)\n", "intent": "Let's consider a two class example to keep things simple\n"}
{"snippet": "of_df = pd.read_csv(\"../assets/datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "* Very simple: Only two features!\n    - Eruption Time\n    - Wait Time\n"}
{"snippet": "from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], \n                feature_names=cancer.feature_names, impurity=False, filled=True)\nimport graphviz\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ndisplay(graphviz.Source(dot_graph))\n", "intent": "Analyzing Decision Trees manually\n- Visualize and find the path that most data takes\n"}
{"snippet": "splicing_feature_data = pd.DataFrame(index=event_names)\nsplicing_feature_data['gene_name'] = gene_names\nsplicing_feature_data.head()\n", "intent": "Now let's create `splicing_feature_data` to map these event names to the gene names, and to the `gene_category` from before.\n"}
{"snippet": "def references_org_person(title):\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "categorical_cols = [col for col in train_x.columns if train_x[col].dtype == object]\nfor col in categorical_cols:\n    train_x[col] = train_x[col].fillna(\"NaN\")\nnumerical_cols = [col for col in train_x.columns if any(train_x[col].isnull()) and train_x[col].dtype != object]\nfor col in numerical_cols:\n    train_x[col + \"_nan\"] = train_x[col].map(lambda x: 1 if np.isnan(x) else 0)\n    train_x[col]          = train_x[col].fillna(train_x[col].median())\nsum(train_x.isnull().sum())\n", "intent": "<a id=\"fillnan\"></a>\n"}
{"snippet": "import pandas as pd\npath = '/Users/jim_byers/Documents/GA/GA_Data_Science_course/SEA-DAT1/data/'\nurl = path + 'titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "ss_ff = preprocessing.StandardScaler()\nss_ff.fit(Xff)\nX_ff_scaled = ss_ff.transform(Xff)\nX_ff_scaled = pd.DataFrame(X_ff_scalled, columns=Xff.columns)\ny_ff_scaled = log(ff.area)\n", "intent": "* Try scaling the input and using the log of the area and see if you get a better score.\n* Examine the coefficients\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params,\ncv=5, n_jobs=-1,\nverbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now let's tune the number of neighbors $k$ for kNN:\n"}
{"snippet": "class CountVectorizer:\n    def __init__(self, lowercase=True):\n        self.lowercase = lowercase\n    def __repr__(self):\n        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\ncv = CountVectorizer()\ncv\n", "intent": "By default, the class instance prints the name of the class and memory related information. We will update this to say something more meaningful\n"}
{"snippet": "vectorizer = CountVectorizer(max_features = 1000,\n                             ngram_range=(1, 2),\n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(data['title'])\nX_new_text_features = vectorizer.transform(data['title'])\n", "intent": "Use `CountVectorizer` to generate vectorized text features.\n"}
{"snippet": "from sklearn.datasets import load_digits\nimport numpy as np\ndigits = load_digits()\ndigits.keys()\n", "intent": "Get some data to play with\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(df,y)\nlen(x_train), len(x_test), len(y_train), len(y_test)\n", "intent": "Split your data set into testing and training:\n"}
{"snippet": "study.plot_pca(feature_subset='gene_category: LPS Response', sample_subset='not (pooled)', plot_violins=False, show_point_labels=True)\n", "intent": "Equivalently, I could have written out the plotting command by hand, instead of using `study.interactive_pca`:\n"}
{"snippet": "label = pd.read_csv(\"preprocessed_meta/classes.csv\", index_col = 0)\nenc = LabelBinarizer()\ny_enc = enc.fit_transform(label.title.values.reshape(-1, 1))\n", "intent": "First let's create our target matrix from the dataset we made previously\n"}
{"snippet": "le = LabelEncoder()\nX['Sex'] = le.fit_transform(finalData.Sex) \n", "intent": "The values in the 'Sex' column are 'Male/Female'. So convert them into 1/0 using Label Encoding.\n"}
{"snippet": "from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n", "intent": "Adding `random_state=42` to make this notebook's output constant:\n"}
{"snippet": "data = {'x1': df.value.values, 'y': df.total_foreign_reserve_sgd.values}\ndf_dataset = pd.DataFrame(data=data)\ndf_dataset.head()\n", "intent": "Here's our resulting dataset after feature selection:\n"}
{"snippet": "pca = PCA(n_components=3)\nX = df_2.loc[:, 'A2':'A13_s']\ny = df_2.y\nX.columns\n", "intent": "2. PCA to plot (for classification)\n  - Plot a scatter plot with 2 feature dimensions (or 3 feature dimensions)  \n  - Use colours for y_enc\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "Keras includes some built-in datasets that are useful for learning and practice.\nhttps://keras.io/datasets/\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, clusters_kmeans, random_state=42)\n", "intent": "Train a decision tree using any set of clusters from earlier workshop.\n"}
{"snippet": "import google.datalab.bigquery as bq\ndf = bq.Query(query).execute().result().to_dataframe()\n", "intent": "The problem is to estimate demand for bicycles at different rental stations in New York City.  The necessary data is in BigQuery:\n"}
{"snippet": "authors_list = bq.Query(sql).execute().result().to_dataframe()['first_author'].tolist()\nwrite_list_to_disk(authors_list, \"authors.txt\")\nprint(\"Some sample authors {}\".format(authors_list[:10]))\nprint(\"The total number of authors is {}\".format(len(authors_list)))\n", "intent": "When creating the author list, we'll only use the first author information for each article.  \n"}
{"snippet": "iris_df = pd.DataFrame(IRIS.data, columns=IRIS.feature_names)\n", "intent": "We will load the Iris data into a dataframe for ease of manipulation.\n"}
{"snippet": "diabetes = datasets.load_diabetes()\ndiabetes_df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetes_df.head()\n", "intent": "<b>Step 2:</b> Load the dataset\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(decode_error = 'ignore', binary=True)\nvect.fit(X_train)\nvect.get_feature_names()\n", "intent": "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`.\n"}
{"snippet": "ratings = pd.read_csv('ml-latest-small/ratings.csv')\nratings.head()\n", "intent": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n"}
{"snippet": "y_train = train_data['human-generated']\ncontextVectorizerTF = TfidfVectorizer().fit(train_data['context'])\nx_context = contextVectorizerTF.transform(train_data['context'])\nresponseVectorizerTF = TfidfVectorizer().fit(train_data['response'])\nx_response = responseVectorizerTF.transform(train_data['response'])\nx_train = sparse.hstack([x_context,x_response])\n", "intent": "Generate feature matricies for context and response. Fill free to create\nadditional feature extraction object if necessary.\n"}
{"snippet": "gp_features = pd.DataFrame(index=gps.index)\ngp_features = gp_features.ix[gps.index]\ngp_features['peaktime'] = xx[gps.ix[:, 4:204].as_matrix().argmax(1)]\n", "intent": "Like before, we find the peak, and order the pseudotime-varying genes.\n"}
{"snippet": "X = StandardScaler().fit_transform(circles_X)\n", "intent": "**9.2 Scale the data and fit DBSCAN on it.**\n"}
{"snippet": "url = '../data/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "college = pd.read_csv('./College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = scaler.fit_transform(training_set)\n", "intent": "When there are sigmoid activation functions in the neural network, it is recommendable to apply normalization for feature scaling.\n"}
{"snippet": "import numpy as np\ndef pca(data):\n    centered_data = data - data.mean(axis=0)\n    covariance_matrix = np.cov(centered_data)\n    eigenvals, eigenvecs = np.linalg.eig(covariance_matrix)\n    pc = eigenvecs.T @ centered_data  \n    return pc\n", "intent": "Principal components are the eigenvalues of the autocorrelation matrix of a dataset. The eigenvalues give the ordering of the components.\n"}
{"snippet": "train = pd.read_csv('./data/train.csv')\ntest = pd.read_csv('./data/test.csv')\n", "intent": "* Feature Importance ( Random Forest Classifier and Gradient Boosting Classifier )\n* Feature Engineering for Tree Based Models\n"}
{"snippet": "movies = pd.read_csv(path+'movies.csv')\nmovies.head()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "print(pd.DataFrame({'effect': params.round(0),\n                    'error': err.round(0)}))\n", "intent": "With these errors estimated, let's again look at the results:\n"}
{"snippet": "from skimage.io import imread\nimage = imread('laptop.jpeg')\ntype(image)\n", "intent": "Let's use [scikit-image](http://scikit-image.rg) to load the content of a JPEG file into a numpy array:\n"}
{"snippet": "from keras.preprocessing.sequence import pad_sequences\nMAX_SEQUENCE_LENGTH = 1000\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x_train.shape)\nprint('Shape of data test tensor:', x_test.shape)\n", "intent": "Let's truncate and pad all the sequences to 1000 symbols to build the training set:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "Standardizing the data.\n"}
{"snippet": "reader = tf.TextLineReader()\nkey_op, value_op = reader.read(filename_queue)\n", "intent": "Define an op to dequeue a line from file:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier();\nrefclasscol = strat_train_set_X.columns\nrfc_model.fit(strat_train_set_X, strat_train_set_y);\nscore = np.round(rfc_model.feature_importances_, 3)\nimportances = pd.DataFrame({'feature':refclasscol, 'importance':score})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\nimportances.plot.bar();\n", "intent": "Let's build a basic random forest classifier and extract the features based on their importances\n"}
{"snippet": "vect = CountVectorizer(ngram_range=(1, 2))\ntokenize_test(vect)\n", "intent": "**Approach 4:** Include 1-grams and 2-grams\n"}
{"snippet": "import pandas as pd\nimport seaborn as sns\nsns.set()\ncols = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n          'Engine size (liters)', 'Number of engine cylinders', \n          'Engine horsepower', 'City gas mileage' , \n          'Highway gas mileage', 'Weight (pounds)', \n          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\ndf = pd.DataFrame(cars, columns=cols)\nsns.pairplot(df)\n", "intent": "Try to get some initial idea of the dataset:\n"}
{"snippet": "data = pd.read_csv('~/Desktop/DSI-SF-2-akodate/datasets/401_k_abadie/401ksubs.csv') \ndata.head(2)\n", "intent": "1. Read the 401ksubs.csv data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "cvt = CountVectorizer()\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\nfreqs = [(word, X_all.getcol(idx).sum()) for word, idx in cvt.vocabulary_.items()]\nprint sorted (freqs, key = lambda x: -x[1])\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "with open('data/anna.txt', 'r') as f:\n    text = f.read()\n", "intent": "Then, we'll load the Anna Karenina text file and convert it into integers for our network to use. \n"}
{"snippet": "wbc = pd.read_csv('../../data/wdbc.csv.bz2')\nwbc = wbc.drop(['id'], axis=1)\nwbc = wbc.replace({'M': 1, 'B': 0})\nprint (wbc.keys())\nprint (wbc.shape)\nprint(wbc.dtypes)\nwbc.head(10)\n", "intent": "__1.\tLoad the data. You may drop id or just ignore it in the rest of your analysis.__\n"}
{"snippet": "X1, Xt1, y1, yt1 = train_test_split(df_1.drop('label', axis=1)['text'], df_1.label, test_size=0.25, random_state=4222)\n", "intent": "* create test dataset\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], \n                      'Salary': [25,80,22,36,37,59,74,70,33,102,88], \n             'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2\n", "intent": "Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "X_new, y_new = make_data(N=50)\nX_new_tr, X_new_ts, y_new_tr, y_new_ts = train_test_split(X_new, y_new)\npoly2new = PolynomialRegression(2)\npoly2new.fit(X_new_tr, y_new_tr)\nplot_regr(X_new_tr, y_new_tr, poly2new)\n", "intent": "Let's start from smaller data size, $N=50$:\n"}
{"snippet": "import sklearn.feature_selection as fs\nx_new = fs.VarianceThreshold(threshold = 1).fit_transform(iris.data)\nx_new.shape\n", "intent": "Let's try to remove features with variances smaller than 1:\n"}
{"snippet": "import pandas as pd\ncard = pd.read_csv('./data/card.csv')\nhier.fit(card)\nlabel = hier.labels_\n", "intent": "<p><a name=\"hcase2\"></a></p>\nFit the hierarchical algorithm.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, z_train, z_test = train_test_split(X, z, random_state=1, stratify=z)\n", "intent": "Split our data into training and testing\n"}
{"snippet": "print (\"Length of test rows:\", len(test))\ninspect_test = pd.DataFrame({'Dtype': test.dtypes, 'Unique values': test.nunique() ,\n             'Number of Missing values': test.isnull().sum() ,\n              'Percentage Missing': (test.isnull().sum() / len(test)) * 100\n             }).sort_values(by='Number of Missing values',ascending = False)\ninspect_test\n", "intent": "BMI and smoking status have missing values\n"}
{"snippet": "ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\nmatrix = fill_na(matrix)\ntime.time() - ts\n", "intent": "Producing lags brings a lot of nulls.\n"}
{"snippet": "import pandas as pd\nnames = ['Bob','Jessica','Mary','John','Mel']\nbirths = [968, 155, 77, 578, 973]\nBabyDataSet = list(zip(names,births))\npd.DataFrame(data = BabyDataSet, columns=['Names', 'Births'])\n", "intent": "Check if Pandas is working. After running this cell, you should see a table appear below.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents) \n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(features,label)\n", "intent": "**As we have often done before, we split the data into training and test:**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ny = df.iloc[:,0]\nX = df.iloc[:,1:]\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3, random_state = 4444)\n", "intent": "* Split data into train/test\n"}
{"snippet": "movies = pd.read_csv('challenges_data/2013_movies.csv')\n", "intent": "Fit and evaluate a decision tree classifier for your movie dataset. Examine the rules your tree uses.\n"}
{"snippet": "scaler_model = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "df = pd.DataFrame(data.data, columns=data.feature_names)\n", "intent": "Define the data/predictors as the pre-set feature names: \n"}
{"snippet": "def read_data():\n    data = pd.read_csv('multiclassdata.csv', delimiter=',', skiprows=0)\n    return data.values\n    raise NotImplementedError()\n", "intent": "Read in the data from the file `multiclassdata.csv` using pandas\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n                                                random_state=2)\nprint(Xtrain.shape, Xtest.shape)\n", "intent": "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:\n"}
{"snippet": "centers = [[1,1], [6,6]]\nX, _ = make_blobs(n_samples=400, n_features=2, centers=centers, cluster_std=1.)\n", "intent": "---\nWe'll start with a simple example to build some intuition.\nFirst our toy dataset:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\npca.fit(pre90s_z)\n", "intent": "In order to do so, we will first perform PCA implementation that comes with Python's sklearn machine learning library.\n"}
{"snippet": "coef_vol = masker.inverse_transform(svc.coef_)\nnl.plotting.plot_stat_map(coef_vol, bg_img=anat, dim=-.5)\n", "intent": "Since we have a value for each voxel, we can simply map this back to the volume using our `masker`, and visualize the weights.\n"}
{"snippet": "from itertools import combinations\nauto_engineered_disances = pd.DataFrame()\nfor (feature_one, feature_two) in combinations(features_sorted[-5:], r=2):\n    for name, operation in zip((\"plus\", \"minus\"), (np.add, np.subtract, np.true_divide)):\n        new_name = \"%s_%s_%s\" % (feature_names[feature_one], name, feature_names[feature_two])\n        auto_engineered_disances[new_name] = operation(covtype.data[:, feature_one],\n                                                       covtype.data[:, feature_two])\n", "intent": "Let's add and subtract all the distances.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\n", "intent": " * A thousand something 8x8 handwritten digits\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n", "intent": "On divise en base d'apprentissage et de test :\n"}
{"snippet": "submission = pd.concat([Test_id, pd.DataFrame(pred)], axis=1)\nsubmission.columns = ['id','trip_duration']\nsubmission['trip_duration'] = submission.apply(lambda x : 1 if (x['trip_duration'] <= 0) else x['trip_duration'], axis = 1)\nsubmission.to_csv(\"submission.csv\", index=False)\n", "intent": "Time for Submission\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain, test = train_test_split(dataset, train_size = 0.7)\n", "intent": "** Train / Test set split: Dividing sub-set of test and training **\n"}
{"snippet": "df_test = pd.DataFrame({\n    'AirTime': X_test['AirTime'].values,\n    'Distance': y_test['Distance'].values\n    })\ndf_pred = pd.DataFrame({\n    'AirTime': X_pred['AirTime'].values,\n    'Distance': y_pred\n    })\nax2 = plot_statsmodels_reg(df_test, df_pred)\n", "intent": "Here's an example plot. The blue points are points in the test data, and the red line is the regression model predicted by statsmodels.\n"}
{"snippet": "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))\n", "intent": "4.5 Embarked\n--------------\n**4.5.1 filling missing values**\n"}
{"snippet": "data = pd.read_csv('../../DSI-CHI-1/lessons/week-04/3.4-model-fit-and-sklearn-logistic/assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "classes = pd.read_csv('../../DSI-CHI-1/lessons/week-07/2.3-pca-lab-1/assets/datasets/votes.csv', index_col=0)\ny = classes['Class']\ny.head()\n", "intent": "Next, let's define the x and y variables: \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "le = LabelEncoder()\nan1 = (an[['Cold or Warm Blooded','Covering','Aquatic','Aerial','Lays Eggs']]).apply(lambda x: d[x.name].fit_transform(x))\nan1\n", "intent": "This time we will use the label encoder to convert our non-numerical data, and concatenate them all together:\n"}
{"snippet": "X=cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "dists = pd.DataFrame(dists, columns=df_wide.index)\ndists.index = dists.columns\ndists.ix[0:10, 0:10]\n", "intent": "Convert dists to a Pandas DataFrame, use the index as column index as well (distances are a square matrix).\n"}
{"snippet": "from sklearn.feature_selection import SelectFromModel\nclf=RandomForestRegressor()\nsfm = SelectFromModel(clf,threshold=0.002)\nX_train_new=sfm.fit_transform(X_train, y_train)\nfeature_test_new=sfm.transform(feature_test)\n", "intent": "2,using SelectFromModel to select features\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                 header=None, \n                 names=['Sepal length', 'Sepal width', 'Petal length', 'Petal width', 'target'])\ndf.head()\n", "intent": "Again, let us use [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset.\n"}
{"snippet": "filename = '/home/data_scientist/data/2001.csv'\nusecols = (5, 15, 16)\ncolumns = ['CRSDepTime', 'DepDelay', 'Origin']\nall_data = pd.read_csv(filename, header=0, na_values=['NA'], usecols=usecols, names=columns)\nlocal = all_data.loc[all_data['Origin'] == 'ORD'].dropna()\n", "intent": "For simplicity, we limit our analysis to flights that departed from O'Hare. We will try to predict `DepDelay` from `CRSDepTime`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), test_size=0.3)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "x_standard = StandardScaler().fit_transform(x)\n", "intent": "First, let's standardize the data\n"}
{"snippet": "scaler = StandardScaler()\nscalled_set = scaler.fit_transform(airports)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "train = pd.DataFrame(columns=['filename','label'])\ntest = pd.DataFrame(columns=['filename','label'])\nfor label in list(files_df['label'].unique()):\n    threshold = files_df.loc[files_df['label'] == label].shape[0] * 0.7\n    threshold = int(np.floor(threshold))\n    train = train.append(files_df.loc[files_df['label'] == label].iloc[:threshold,:],ignore_index=True)\n    test = test.append(files_df.loc[files_df['label'] == label].iloc[threshold:,:],ignore_index=True)\nprint(train.shape[0],test.shape[0])\n", "intent": "For each class, splitting the documents to training and test based on a 70-30 rule.\n"}
{"snippet": "Top_100_tokens = pd.DataFrame(speech_tokens_top100.toarray(), columns=count_vectorizer_100.get_feature_names())\nTop_100_tokens.head()\n", "intent": "Now let's push all of that into a dataframe with nicely named columns.\n"}
{"snippet": "f = z.open('ml-1m/users.dat')\ntmp = f.read()\ntmp = tmp.decode()\nusers = np.array([np.array(t.split('::'))[: -1] for t in tmp.splitlines()]) \nusers[:, 1] = (users[:, 1] == 'M').astype(np.int)\nusers = users.astype(np.int)\nf.close()\n", "intent": "Users' information\n* UserID, Gender (1 is M), Age, Occupation\n"}
{"snippet": "cancer = load_breast_cancer()\ndigits = load_digits()\ndata = cancer\n", "intent": "We will use the breast cancer dataset in which the target variable has 1 if the person has cancer and 0 otherwise. Let's load the data.\n"}
{"snippet": "from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scaled = min_max_scaler.fit_transform(X)\nx_train2, x_test2, y_train2, y_test2 = train_test_split(X_scaled,Y, test_size = 0.15, random_state=100)\n", "intent": "refer:http://scikit-learn.org/stable/modules/preprocessing.html\nmore at: https://en.wikipedia.org/wiki/Feature_scaling\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(lowercase=True,stop_words='english')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "df = pd.read_csv(\n    '/home/data_scientist/data/2001.csv',\n    encoding='latin-1',\n    usecols=(1, 2, 21)\n    )\n", "intent": "We use the following columns:\n- Column 1: Month, 1-12\n- Column 2: DayofMonth, 1-31\n- Column 21: Cancelled, was the flight cancelled?\n"}
{"snippet": "df1 = pd.read_csv('Data/Auto.csv', na_values='?').dropna()\ndf1.info()\n", "intent": "Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ntest = pd.read_csv('test.csv', encoding = \"ISO-8859-1\", header=None)\ntrain = pd.read_csv('train.csv', encoding = \"ISO-8859-1\", header=None)\ntrain.columns = ['id', 'sentiment', 'review']\ntest.columns = ['id', 'sentiment', 'review']\ntrain.head()\n", "intent": "Load the train and test files into a dataframe. Name the columns to help you in the rest of the problem.\n"}
{"snippet": "cement_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\",index_col=0)\ncement_data.columns = cement_data.columns.str.lower().str.replace(\" \",\"_\").str.replace(\".\",\"\") \n", "intent": "Lets get the data directly off the web this time:\n"}
{"snippet": "df = pd.read_csv(\"data/smsspamcollection/SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"target\", \"text\"])\n", "intent": "* UCI Dataset - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n"}
{"snippet": "train_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\nstops = set(stopwords.words('english'))\ndef text_to_word_list(text):\n", "intent": "Create embedding matrix\n"}
{"snippet": "import seaborn as sb\nsb.pairplot(pd.DataFrame(iris.data, columns = iris.feature_names))\n", "intent": "**Using `pairplot` from `seaborn` is a quick way to see which features separate out our data**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nimport pylab as pl\npl.gray();\n", "intent": "Import the dataset and use `pylab` to explore.\n"}
{"snippet": "path = 'https://ibm.box.com/shared/static/q6iiqb1pd7wo8r3q28jvgsrprzezjqk3.csv'\ndf = pd.read_csv(path)\ndf.head()\n", "intent": "load data and store in dataframe df:\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/submission_xgb_new2.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/submission_xgb_new2.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Now we create the submission file.\n"}
{"snippet": "df = pd.read_csv(\n    '/home/data_scientist/data/2001.csv',\n    encoding='latin-1',\n    usecols=(1, 2, 3, 5, 7, 13, 15, 16, 18)\n    )\n", "intent": "You can find the descriptions of the columns [here](http://stat-computing.org/dataexpo/2009/the-data.html).\n"}
{"snippet": "authorship['random'] = [random.random() for i in range(841)]\nx, y = authorship[features], authorship.Author_num.values\nx_train, x_test, y_train, y_test = train_test_split(authorship[features],\n                                                    authorship.Author_num.values,\n                                                    test_size=0.4,\n                                                    random_state=123)\n", "intent": "- Create a random variable column \n- Create test/train sets using test_train_split\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\n", "intent": "** Leia o arquivo csv 'KNN_Project_Data' em um DataFrame **\n"}
{"snippet": "pca_pivot = df_pca.pivot_table(index = ['cluster_id_3'], values = list(range(1,33)), aggfunc = np.sum).transpose()\n", "intent": "Based on the PCA plots, k = 3 seems to fit the data best visually, since there is much less overlap.\n"}
{"snippet": "xout=pd.DataFrame()\nxout['tag']=y\nxout['PCA1'] = X_2D[:, 0]\nxout['PCA2'] = X_2D[:, 1]\nsns.lmplot(\"PCA1\", \"PCA2\", hue='tag', data=xout, fit_reg=False, markers='.');\n", "intent": "**now reduced 64 to 2 dimensions  \n&rarr; visualize it**\n"}
{"snippet": "X = cleaned[['Pclass', 'Age', 'female']]\nX = X.values\ny = cleaned[['Survived']]\ny = y.values.ravel()\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\nm = LogisticRegression()\nm.fit(Xtrain, ytrain)\n", "intent": "Re-run the prediction above using the additional feature. How does the accuracy change?\n"}
{"snippet": "rich_features_final = rich_features_no_male.fillna(rich_features_no_male.dropna().median())\nrich_features_final.head(5)\n", "intent": "Let us not forget to imput the median age for passengers without age information:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ncount_vectorizer = CountVectorizer()\ncorpus = data['mda']\ncorpus_tfidf = tfidf_vectorizer.fit_transform(corpus)\nprint(corpus_tfidf[0,0:100])\n", "intent": "Implementing TF-IDF in code is fairly simple.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(wine_df, y, test_size= 0.25)\n", "intent": "Let's create a test that simulates fresh data that model might be predicting on when it is put into production.\n"}
{"snippet": "count_vectorizer = CountVectorizer()\ncounts = count_vectorizer.fit_transform(df['text'].values)\ntfidf_vectorizer = TfidfVectorizer()\ntfidfs = tfidf_vectorizer.fit_transform(df['text'].values)\n", "intent": "[Count Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"}
{"snippet": "def train_test_split(pos_emails, neg_emails, pos_labels, neg_labels, split_value):\n    return X_train, X_test, y_train, y_test\n", "intent": "- Create four new arrays from our email data and labels.\n"}
{"snippet": "rets = prices.pct_change()[1:].fillna(0)\n", "intent": "Calculate the returns of these price data.\n"}
{"snippet": "votes = pd.read_csv('../assets/datasets/')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('data_lcurve.csv')\nimport numpy as np\nimport util\nX = np.array(data[['x1', 'x2']])\ny = np.array(data['y'])\nnp.random.seed(55)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n", "intent": "**Model Selection - Learning Curves**\n"}
{"snippet": "from dopy.doplot.selection import add_log_to_dataframe, add_max_to_dataframe, add_min_to_dataframe\nadd_min_to_dataframe(real_dataframe_2015, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(real_dataframe_2016, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(real_dataframe_2015, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(real_dataframe_2016, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(real_dataframe_2015, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nadd_min_to_dataframe(real_dataframe_2016, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nreal_dataframe_2015['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe_2015['B0_FitPVConst_KS0_tau']/real_dataframe_2015['B0_FitPVConst_KS0_tauErr']\nreal_dataframe_2016['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe_2016['B0_FitPVConst_KS0_tau']/real_dataframe_2016['B0_FitPVConst_KS0_tauErr']\n", "intent": "Check fractions of the DD/LL type events\n-----------------------------------------------------------\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "**Step 5:** Create an instance of `StandardScaler` called `scaler`.\n"}
{"snippet": "ngram_vectorizer = CountVectorizer(ngram_range=(1,3))\nX_ngram = ngram_vectorizer.fit_transform(critics.quote)\n", "intent": "The random forest classifier underperforms naive bayes using both td-idf and non td-idf input data.\n"}
{"snippet": "imputer = Imputer(missing_values = 'NaN',strategy = 'mean',axis = 0)\nimputer = imputer.fit(X[:,2:])\nX[:,2:] = imputer.transform(X[:,2:])\n", "intent": "Now replace NaN or missing value with mean value of column\n"}
{"snippet": "college_data = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_train = StandardScaler().fit_transform(X_train)\nX_train_expanded = StandardScaler().fit_transform(X_train_expanded)\n", "intent": "The only thing to do is to normalize the variables.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data, targets, test_size=0.2, random_state=check_random_state(0)\n    )\n", "intent": "We train on 80% of the data, and test the performance on the remaining 20%.\n"}
{"snippet": "model = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=1000, ngram_range=(1,3))\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "test_ids = pd.read_csv(\"processed_dataset/test.csv\", index_col=0)[\"match_id_hash\"]\n", "intent": "AUC: 0.8216585820565936\nACC: 0.740679012345679\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Great now that we've imported the data we can load it. \n"}
{"snippet": "label_df.pivot_table(index='Id', aggfunc=len).sort_values('Image', ascending=False).head(10)\n", "intent": "There are 4251 unique classes represented in the training set.\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\n", "intent": "We use the same `MinMaxScaler` we used earlier to scale the data.\n"}
{"snippet": "scores = spca.fit_transform(boroughs)\n", "intent": "Scores (projection of 'boroughs' on the PCs):\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(varts['counts'], varts['workingday'], test_size=0.33, random_state=42)\n", "intent": "In the next cell, we re-create our X and y matrices. This time, they contain time series of varying lengths.\n"}
{"snippet": "act_vec = CountVectorizer(ngram_range=(2,3), max_features = 22, strip_accents = 'ascii')\nact_vec.fit(movies['Actors'])\nactor_bigrams = pd.DataFrame(act_vec.transform(movies['Actors']).todense(),\n                      columns = act_vec.get_feature_names())\n", "intent": "These are the top 20 2-words plot themes to appear in the NYT top 1000 movies. Wars, youth, love, marriage, and cities seem to be common themes.\n"}
{"snippet": "d = pd.DataFrame({\n        'prediction': [0.1, 0.5, 0.95, 0.99, 0.8, 0.4, 0.03, 0.44, 0.2],\n        'y': [1, 0, 1, 1, 1, 1, 0, 0, 0]})\nd\n", "intent": "Compute AUC score by hand with the formula explained in class for the following dataset.\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n", "intent": "We will read the train and test datasets using pandas\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\npipeline = make_pipeline(PCA(), LogisticRegression())\nparams = {'logisticregression__C': [.1, 1, 10, 100],\n          \"pca__n_components\": [5, 10, 20, 40]}\ngrid = GridSearchCV(pipeline, param_grid=params, cv=5)\ngrid.fit(X_train, y_train)\nprint(grid.best_params_)\ngrid.score(X_test, y_test)\n", "intent": "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:\n"}
{"snippet": "data['Cabin'].fillna('U0', inplace=True)\ndata['CabinSection'] = LabelEncoder().fit_transform(data['Cabin'].map(lambda x: x[0]))\ndata['CabinDistance'] = data['Cabin'].map(lambda x: x[1:])\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: x.split(' ')[0])\ndata['CabinDistance'].where(data['CabinDistance'] != '', '0', inplace=True)\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: int(x))\ndata.head()\n", "intent": "In this attemp we include all the cleaning inside one function and included comment to explain what we do in each step\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(os.path.join(basepath, csv_filename), encoding='utf-8')\n", "intent": "Read back the data-frame from file, local or remote.\n"}
{"snippet": "test_df = pd.read_csv(\"data/example1_test.csv\")\ny_test, test_design_mat = build_model1_design_mat(test_df)\nfitted_sk_ols.score(test_design_mat, y_test)\n", "intent": "**Answers**:\n1. See code below.\n"}
{"snippet": "all_data = pd.merge(all_data,\n                    id_df.loc[:, ['item_id', 'shop_id', 'ID']].drop_duplicates(),\n                    how='left',\n                    on=['item_id', 'shop_id'])\nall_data.loc[:, 'ID'].fillna(-1, inplace=True)\nall_data.loc[:, 'ID'] = all_data.loc[:, 'ID'].astype('int32')\n", "intent": "Item ID is a combination of `item_id` and `shop_id`. As before, if an ID doesn't exist for the combination, it will be assigned `-1`\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nquadratic = PolynomialFeatures(degree=2)\ncubic = PolynomialFeatures(degree=3)\nquartic = PolynomialFeatures(degree=4)\nXtrain2 = quadratic.fit_transform(Xtrain)\nXtest2 = quadratic.fit_transform(Xtest)\nXtrain3 = cubic.fit_transform(Xtrain)\nXtest3 = cubic.fit_transform(Xtest)\nXtrain4 = quartic.fit_transform(Xtrain)\nXtest4 = quartic.fit_transform(Xtest)\n", "intent": "6 - Fit a polynomial regression model to the data and print the error.  Comment on your results.\n"}
{"snippet": "pca = PCA(n_components=20)\nXtrain_pca = pca.fit_transform(Xtrain)\nXtest_pca = pca.transform(Xtest)\n", "intent": "OK, in the solution he tries to find a good number of components, and it's 20:\n"}
{"snippet": "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n", "intent": "Agora vamos fazer nosso split para avaliar nossos modelos:\n"}
{"snippet": "from ggplot import *\ndf = pd.DataFrame({'click_mean' : train[\"click\"].groupby(train[\"hour\"]).mean()}).reset_index()\nggplot(df, aes(x = 'hour', y = 'click_mean'))+ geom_line() + geom_point()\n", "intent": "plot and value_counts\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class['text'], \n                                                    yelp_class['stars'], \n                                                    test_size=0.3, \n                                                    random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "env = pd.read_csv('ces3results_environment.csv')\ndemog = pd.read_csv('ces3results_demographics.csv')\nprint('Enviro cols are ', env.columns)\nprint('Demographics cols are ',demog.columns)\n", "intent": "Let's import the environmental and demographic datasets from CES:\n"}
{"snippet": "posts_frame = pd.DataFrame(converted_posts)\nposts_frame.head()\n", "intent": "Creating dataframe from parsed data\n"}
{"snippet": "loans = pd.read_csv(\"./loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "pd.DataFrame(probs_dnn).to_hdf('data/probs_dnn.hf5', 'probs', mode='w')\n", "intent": "Save the DNN classification probabilities to compare with the visual scanning exercise results:\n"}
{"snippet": "reviews = pd.read_csv('./imdb_data/reviews.txt', header=None)\nlabels = pd.read_csv('./imdb_data/labels_ohe.csv', header=None)\n", "intent": "Read IMDB movie reviews dataset\n"}
{"snippet": "df = pd.DataFrame(data=np.c_[data, target], columns=iris['feature_names'] + ['target'])\ndf.head()\n", "intent": "Plot the data using scatterplots - take a look at all the combinations of variables to get a feel for how the data is distributed. \n"}
{"snippet": "wine = pd.read_csv('../CSV/wine_v.csv')\n", "intent": "And read in our data:\n"}
{"snippet": "from sys import path\npath.append('../2 - Text data preprocessing')\nimport load_spam\nspam_data = load_spam.spam_data_loader()\nspam_data.load_data()\nXtrain, ytrain, Xtest, ytest = spam_data.split(2000)\n", "intent": "If you have done the previous notebooks, you are used to these two examples now.\n"}
{"snippet": "f_scaled = pd.DataFrame(preprocessing.StandardScaler().fit_transform(f), columns=f.columns)\n", "intent": "We have explored feature scaling in [Problem 5.1](../Week5/Assignments/w5p1.ipynb), so I'll do this for you.\n"}
{"snippet": "tfidfvec = TfidfVectorizer(stop_words = 'english', min_df = 1, binary=True)\ncountvec = CountVectorizer(stop_words = 'english', min_df = 1, binary=True)\ntraining_dtm_tf = countvec.fit_transform(df_train.text)\ntest_dtm_tf = countvec.transform(df_test.text)\ntraining_labels = df_train.stars\ntest_labels = df_test.stars\ntest_labels.value_counts()\n", "intent": "Next we need to create a dtm for each review, and an array containing the classification label for each review (for us, this is called 'label')\n"}
{"snippet": "media_satisfaccion = df['satisfaction_level'].mean()\ndf['satisfaction_level'] = df['satisfaction_level'].fillna(media_satisfaccion)\n", "intent": "Vamos a imputar el valor nulo de satisfaction_level con la media de la columna.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nrand_state = np.random.randint(0, 100)\nX_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.1, random_state=rand_state)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\n", "intent": "Split the data in training and test data. This is useful to check if the model is over/underfitting:\n"}
{"snippet": "sentance = [\"abc def pqr\", \"abc pqr cdf jkl cdf\"]\nexample = CountVectorizer()\nexample.fit(sentance)\nprint(example.vocabulary_) \nprint('='*50)\nprint(example.get_feature_names())\nprint(example.transform(sentance).toarray())\n", "intent": "<h2> <font color='red'>WHAT ARE THESE FUNCTIONS: FIT, TRANSFORM, FIT_TRANSFORM</font></h2>\n"}
{"snippet": "cols_to_use = ['LotArea', 'BsmtFinSF1','GrLivArea', 'OpenPorchSF', 'YearBuilt', 'SalePrice']\ndata = pd.read_csv('houseprice.csv', usecols=cols_to_use)\ndata.head()\n", "intent": "As expected, Random Forests did not see a benefit from transforming the variables to a more Gaussian like distribution.\n"}
{"snippet": "for df in [X_train, X_test, submission]:\n    df['Embarked'].fillna(X_train['Embarked'].mode()[0], inplace=True)\n    df['Cabin_categorical'].fillna('Missing', inplace=True)\n", "intent": "- Embarked NA imputed by most frequent category, because NA is low\n- Cabin_categorical imputed by 'Missing', because NA is high\n"}
{"snippet": "data_text =pd.read_csv(\"training/training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\ndata_text.head()\n", "intent": "<h3>3.1.2. Reading Text Data</h3>\n"}
{"snippet": "train = pd.read_csv('train.csv')\n", "intent": "Load `train.csv` from Kaggle into a pandas DataFrame.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33)\n", "intent": "We notice that the scores are more or less the aame \n"}
{"snippet": "def fit_pca(df, n_components):\n    return pca, reduced\n", "intent": "Write a function named fit_pca() that takes a pandas.DataFrame and uses sklearn.decomposition.PCA to fit a PCA model on all values of df.\n"}
{"snippet": "import seaborn as sns\ncsv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/chorSub.csv\"\ndf = pd.read_csv(csv_file).iloc[:, 1:]\n", "intent": "<a id='chem'></a>\n---\nLet's load in a dataset on chemical composition and plot the silhouette scores for different numbers of clusters.\n"}
{"snippet": "donald_df = pd.DataFrame(donald_tweets)\nbernie_df = pd.DataFrame(bernie_tweets)\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "cv = CountVectorizer(preprocessor=cleaner)\ncv.fit(space_messages)\ncustom_preprocess = pd.DataFrame(cv.transform(space_messages).todense(),\n                              columns=cv.get_feature_names())\nprint(custom_preprocess.sum().sort_values(ascending=False).head(10))\n", "intent": "We can pass this function into `CountVectorizer` as a way to preprocess the text as a part of the fitting.\n"}
{"snippet": "strategy = '' \nage_imputer = Imputer(strategy=strategy)\nage_imputer.fit(df['age'].values.reshape(-1, 1))\nages = age_imputer.transform(\n    df['age'].values.reshape(-1, 1))\nprint(ages[0:5], ages.mean())\n", "intent": "Our results are as follows:\n- Mean: 16.65\n- Median: 17\n- Mode: 16.0\nWhat is most appropriate? Check in on Slack\n"}
{"snippet": "crime_df = pd.read_csv('https://raw.githubusercontent.com/albertw1/data/master/Crime.csv').drop(['Date', 'Year'], axis=1)\ncrime_df.head()\n", "intent": "We will first load in our dataset below and look at the first few rows. Then, we use the describe function to get a sense of the data.\n"}
{"snippet": "spam_train, spam_test = train_test_split(spam, test_size=0.5, random_state=1000)\n", "intent": "Let us split the dataset into a 50-50 split by using the following:\n"}
{"snippet": "df_3 = DataFrame(data_1, columns=['year', 'state', 'pop', 'unempl'])\ndf_3\n", "intent": "Like Series, columns that are not present in the data are NaN:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "t_raw, t_genres = select_genres(27)\nt_X_train, t_X_test, t_y_train, t_y_test = train_test_split(t_raw, \n                                                            t_genres, \n                                                            random_state=check_random_state(0), \n                                                            test_size=0.3)\n", "intent": "Run the cell below to split selected data (We'll use `n`=27) into training and testing sets with a test size of 0.3.\n"}
{"snippet": "random_states=[5,10,20]\nfor random_state in random_states:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n    clf = LogisticRegression(penalty='l1', C=alphas[opt_index])\n    clf.fit(X_train, y_train)\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n    print \"The ROC curve for random state is:\", random_state\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(lfw_people.data, lfw_people.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll do a typical train-test split on the images before performing unsupervised learning:\n"}
{"snippet": "x_train, x_test,y_train,y_test=train_test_split(\nx,y,test_size=0.2,random_state=45)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X, Y_binary, test_size=0.3, random_state=123)\n", "intent": "Above result confirms the four classes in the dataset. \n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nfor col in data.columns:\n    data[col] = mms.fit_transform(data[[col]]).squeeze()\n", "intent": "Scale the data again. Let's use `MinMaxScaler` this time just to mix things up.\n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = faces.data, faces.target\n", "intent": "We'll use this code to load the data:\n"}
{"snippet": "from sklearn.decomposition import PCA\ndef extract_eigenvectors(k, X_train):\n    pca = PCA(n_components=k)\n    pca.fit(X_train)\n    eigen_vectors = pca.components_\n    return eigen_vectors, pca\ndef make_pca_features(eigen_vectors, X):\n    return np.transpose(np.dot(eigen_vectors, np.transpose(X)))\n", "intent": "Use PCA to reduce features high dimensionality features into low dimansionality features\n"}
{"snippet": "data1 = pd.read_csv('hw4data1.csv')\ndata1.head()\n", "intent": "Let's explore clustering models on an artificial data set with five features.\n"}
{"snippet": "mush = pd.read_csv('../data/mushrooms.data',header=None,names=['edible?']+ordered_attributes)\nmush.head()\n", "intent": "So we have a dictionary of more verbose categories. We'll now want to apply them to the dataset.\n"}
{"snippet": "df_test = pd.DataFrame({\n    'AirTime': X_test['AirTime'].values,\n    'Distance': y_test['Distance'].values\n    })\ndf_pred = pd.DataFrame({\n    'AirTime': X_pred['AirTime'].values,\n    'Distance': y_pred\n    })\nax2 = plot_statsmodels_reg(df_test, df_pred)\n", "intent": "Here's an example. The blue points are points in the test data, and the red line is the regression model predicted by statsmodels.\n"}
{"snippet": "cereals_df = pd.read_csv('Cereals.csv')\npcs = PCA(n_components=2)\npcs.fit(cereals_df[['calories', 'rating']])\n", "intent": "Compute principal components on two dimensions\n"}
{"snippet": "elect_train = pd.DataFrame(data=normalized_electricity, index=np.arange('2012-01', '2014-01', dtype='datetime64[D]')).dropna()\nelect_test = pd.DataFrame(data=normalized_electricity, index=np.arange('2014-01', '2014-11', dtype='datetime64[D]')).dropna()\nXX_elect_train = elect_train.drop('electricity-kWh', axis = 1).reset_index().drop('index', axis = 1)\nXX_elect_test = elect_test.drop('electricity-kWh', axis = 1).reset_index().drop('index', axis = 1)\nYY_elect_train = elect_train['electricity-kWh']\nYY_elect_test = elect_test['electricity-kWh']\nprint XX_elect_train.shape, XX_elect_test.shape\n", "intent": "Analysis of electricity data.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX= df[range(1,17)]\nY= df[0]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=4444)\n", "intent": "Split the data into a test and training set. But this time, use this function: from sklearn.cross_validation import train_test_split\n"}
{"snippet": "df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])\n", "intent": "Compenents represent groups of features (e.g. amplitude features vs freq features)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)\n", "intent": "We hold out 20% of all words to be used for validation.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns= df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('../datasets/housesaleprediction/kc_house_data.csv')\n", "intent": "Vi leser inn filer med `pd.read_csv(fil)`. Denne funksjonen returnerer en ny Dataframe, som er et objekt vi kan behandle som en tabell.\n"}
{"snippet": "df = pd.read_csv(\"../../../datasets/movie_weekend/movie_weekend.csv\")\ndf.info()\n", "intent": "This is a brief intro / review to timeseries\n"}
{"snippet": "def docm(y_true, y_pred, labels=None):\n    cm = confusion_matrix(y_true, y_pred)\n    if labels is not None:\n        cols = ['p_'+c for c in labels]\n        df = pd.DataFrame(cm, index=labels, columns=cols)\n    else:\n        cols = ['p_'+str(i) for i in xrange(len(cm))]\n        df = pd.DataFrame(cm, columns=cols)\n    return df\n", "intent": "Print a confusion matrix and investigate what gets mixed\n"}
{"snippet": "def read_data():\n    data=pd.read_csv('multiclassdata.csv')\n    return np.array(data)  \n", "intent": "Read in the data from the file `multiclassdata.csv` using pandas\n"}
{"snippet": "st = pd.pivot_table(df_15, index = ['Store Number', 'County'],\\\n                       values = ['Bottles Sold',\\\n                                 'Sale (Dollars)',\\\n                                 'State Bottle Retail' \\\n                                ],\\\n                       aggfunc = [np.sum, np.mean]\\\n                      ).sort_values([('sum', 'Sale (Dollars)')], ascending = False)\nst.reset_index(inplace=True)\nst.head()                 \n", "intent": "In 2015, Polk had, by far, the highest sales in all counties in IOWA, over 2 times the next highest county, Linn.\n"}
{"snippet": "train = train.fillna({\"Embarked\": \"S\"})\n", "intent": "It's clear that the majority of people embarked in Southampton (S). Let's go ahead and fill in the missing values with S.\n"}
{"snippet": "results = [standard_classification_knn(madelon_feature_df,\n                                       madelon_target_df,\n                                       n_neighbors,\n                                       random_state=42) \n           for n_neighbors in range(2,20)]\nresults_df = pd.DataFrame(results)\nresults_df.head(2)\n", "intent": "In order to get some context into what this means, we should look at multiple values of train and testing score.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntext_train, text_test, y_train, y_test = train_test_split(text, y, \n                                                          random_state=42,\n                                                          test_size=0.25,\n                                                          stratify=y)\n", "intent": "Next, we split our dataset into 2 parts, the test and training dataset:\n"}
{"snippet": "df = pd.read_csv(\"data/tips.csv\")\ndf.head()\n", "intent": "For our example we'll work with a simple dataset on tips:\n"}
{"snippet": "train_df = pd.read_csv('../input/flight_delays_train.csv')\ntest_df = pd.read_csv('../input/flight_delays_test.csv')\n", "intent": "Download data from the [competition page](https://www.kaggle.com/c/flight-delays-fall-2018/data) and change paths if needed.\n"}
{"snippet": "from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_scaled = pd.DataFrame(data = X)\nX_scaled = scaler.fit_transform(X)\n", "intent": "<b>Eliminating Scaling issues using RobustScaler</b>\n"}
{"snippet": "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\ndf = pd.read_csv('//home//anshul//anaconda3//anshul//scikit-learn//scikit-learn-application//Mastering-Machine-Learning-with-scikit-learn-Second-Edition-master//chapter08//ad.data', header=None, low_memory=False)\n", "intent": "- Decision Trees\nTree-like graphs that model a decision.\n- Algorithm to create a Decision Tree:\nIterative Dichotomiser3 (ID3)\n"}
{"snippet": "train = pd.read_csv(\"datasets/titanic/train.csv\")\ntest = pd.read_csv(\"datasets/titanic/test.csv\")\n", "intent": "```\ntrain = pd.read_csv(\"../datasets/titanic/train.csv\")\ntest = pd.read_csv(\"../datasets/titanic/test.csv\")```\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "Use train_test_split to create training and testing data\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nCV = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "from keras import preprocessing\npad_seq = preprocessing.sequence.pad_sequences(token_seq, maxlen=120, value=0)\n", "intent": "3\\. Use `pad_sequences` from `keras.preprocessing.sequence` to pad each sequence with zeros to **make the sequence length 120**. [2 pts]\n"}
{"snippet": "data = pd.read_csv('data/houses.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "df = pd.read_csv(\"data/beer_reviews.tar.gz\", compression='gzip')\n", "intent": "Import data in pandas dataframe\n"}
{"snippet": "iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  \ny = (iris[\"target\"] == 2).astype(np.float64)  \n", "intent": "* conventions: b = bias term; w = feature weights vector.\n"}
{"snippet": "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n               'capital_gain', 'capital_loss', 'hours_per_week',\n               'native_country', 'income_bracket']\npd.read_csv(TRAIN_DATA_FILE, names=HEADER).head()\n", "intent": "The **training** data includes **32,561** records, while the **evaluation** data includes **16,278** records. \n"}
{"snippet": "X = np.vstack([logRe_kpc, mu_e, logsigma])\ndf = pd.DataFrame(X.T, columns=('logRe', 'mu_e', 'logsigma'))\n", "intent": "For the data to fit we use physical size for the radius.\n"}
{"snippet": "def load_mnist(dataset_folder, filename):\n    filepath = os.path.join(dataset_folder, filename)\n    with gzip.open(filepath, 'rb') as f:\n        file_content = f.read()\n    print(\"Loaded %s to memory\" % filepath)\n    return file_content\n", "intent": "----------\n[Back to Implementation Overview](\n"}
{"snippet": "df = pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "from scipy.misc import imread\npath = datafolder + \"indoor/1.jpg\"\nimage1 = imread(path)\n", "intent": "Two subfolders exist: one containing a range of indoor images and another one with outdoor images. We can load an image into python by making use of:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n", "intent": "In this lab we will use the perceptron to \n"}
{"snippet": "test = pd.read_table(\"test.tsv\")\nX_actual_test = test['Phrase']\n", "intent": "Now load the actual test data from Kaggle and test the model\n"}
{"snippet": "tdf = pd.read_csv('titanic_train.csv')\n", "intent": "Import Titanic data\n"}
{"snippet": "X = cv.fit_transform(X)\n", "intent": "** Use the fit_transform() on the CountVectorizer object and pass in X (the 'text' column), and save this result by overwriting X.**\n"}
{"snippet": "linear_dep = pd.DataFrame()\n", "intent": "On the training set, we compute the Pearson correlation, $F$-statistic, and $p$ value of each predictor with the response variable `charged_off`.\n"}
{"snippet": "yelp_review_votes=pd.read_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_review_votes_master_data.csv\", encoding='utf-8-sig')\nyelp_data_final=pd.read_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data.csv\", encoding='utf-8-sig')\n", "intent": "Exploratory analysis on the final analytics dataset: yelp_data_final_update\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "We've successfully loaded `MNIST` into our environment, so let's take a look at its structure.\n"}
{"snippet": "target_var = 'W_Pct'\nrs = 0\ndef get_train_test():\n    return train_test_split(X, y[target_var],\n                            test_size=0.3, random_state=rs)\n", "intent": "Predicting win percentage (W_Pct)\n"}
{"snippet": "UserMovieMatrixz = UserMovieMatrix.fillna(2.5)\n", "intent": "Initialize an `SVD` instance called `svd`\n"}
{"snippet": "x_test, y_test = sine_test_data()\nprint('The R2 score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), y_test))\n", "intent": "Now let's see what this results to in the test set.\n"}
{"snippet": "df3 = pd.read_csv('name_freq.csv')\ndf3 = df3.drop('Unnamed: 0', 1)\n", "intent": "Add first name frequency\n"}
{"snippet": "wine_mag_train, wine_mag_test, wine_abv_train, wine_abv_test = train_test_split(wine_data_mag, wine_data_abv)\n", "intent": "And, as always, let's split up the data. Our target values are going to be the continuous abv values.\n"}
{"snippet": "from sklearn.linear_model import Lasso, LinearRegression\npipe_lr = make_pipeline(PolynomialFeatures(include_bias=False), \n                        StandardScaler(),\n                        LinearRegression())\npipe_lr.fit(X_dev, y_dev)\n", "intent": "Let's train a linear regression model\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/aprilypchen/depy2016/master/adult.csv', na_values=['\n", "intent": "Task: Given attributes about a person, predict whether their income is <=50K or >50K\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\nprint(data.head(10))\ndata.isnull().sum()\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "from sklearn import cross_validation\ndef shuffle_split_data(X, y):\n    X, y =housing_prices, housing_features\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(housing_features, housing_prices, test_size=0.3, random_state=0)\n    return X_train, y_train, X_test, y_test\ntry:\n    X_train, y_train, X_test, y_test = shuffle_split_data(housing_features, housing_prices)\n    print \"Successfully shuffled and split the data!\"\nexcept RuntimeError:\n    print \"Something went wrong with shuffling and splitting the data.\", RuntimeError\n", "intent": "*Why do we split the data into training and testing subsets for our model?*\n"}
{"snippet": "iris = datasets.load_iris()\nx = iris.data[ :, [ 2, 3 ] ]\ny = iris.target\nnp.unique(y)\n", "intent": "load the iris datset, the target is already stored as integers\n"}
{"snippet": "file_path = os.path.join('data', 'model_likes_anon.psv')\ndf = pd.read_csv(file_path, sep = '|', quotechar = '\\\\')\nprint( 'Drop duplicated rows: ', df.duplicated().sum() )\ndf = df.drop_duplicates()\ndf = df[['mid', 'uid']]\nprint('dimension: ', df.shape)\ndf.head()\n", "intent": "http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/\n"}
{"snippet": "KNN_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "x_test, y_test = sine_test_data()\nprint 'The R2 score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), y_test)\n", "intent": "Now let's see what this results to in the test set.\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\ntrain.head()\n", "intent": "Let us load in the training data provided using Pandas:\n"}
{"snippet": "from sklearn.decomposition import PCA\nwine_pca = PCA().fit(wine_cont_n)\n", "intent": "---\nCreate a new dataframe with the principal components and the `red_wine` column added back in from the original data.\n"}
{"snippet": "test_data_encoded['AMT_ANNUITY'].fillna(test_data_encoded['AMT_ANNUITY'].mean(), inplace=True)\n", "intent": "Those two values will be filled with mean value for whole feature\n"}
{"snippet": "OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\nOR\n", "intent": "Thus, it has learned the function perfectly. Now for OR:\n"}
{"snippet": "pca_featurized = PCA(n_components = 2).fit(featurized)\n", "intent": "Run PCA with 2 components on the featurizations\n"}
{"snippet": "df_tram = pd.DataFrame(tram, columns=df.columns[:-1])\ndf_tram.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.datasets import load_boston\nX = load_boston().data\nprint X.shape\n", "intent": "http://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-analysis\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\nprint dtm.shape\n", "intent": "---\nReddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "band_similarities = pd.DataFrame(similarity_matrix, index=data.columns[1:],columns=data.columns[1:])\nband_similarities\n", "intent": "To make a nice print of the data we will use the pandas library as follows.\n"}
{"snippet": "X, y = yelp_class['text'], yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['TailNum'].unique().tolist()[0:10])\nle.transform(df['TailNum'].unique().tolist()[0:10]) \n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "tracks = pd.read_csv('../data/fma_tracks.csv', index_col=0)\nprint('You got {} track IDs.'.format(len(tracks)))\n", "intent": "The `fma_tracks.csv` file contains a list of 2'000 track IDs that we will use through the assignment.\n"}
{"snippet": "DiffSparsBudgW = pd.read_csv('Saved_Datasets/DiffNormSparsBudgW.csv')\nDiffSparsBudgW = DiffSparsBudgW.as_matrix()\n", "intent": "It can be seen that on the plot above the movies cannot be separated. Therefore it was tried to sparsify the matrix.\n"}
{"snippet": "feat_train, feat_test, crime_train, crime_test = train_test_split(features, crime_rate, test_size = 0.2, random_state=11)\n", "intent": "Split 80-20 train and test data, to use for model.\n"}
{"snippet": "results = pd.DataFrame(columns=[\"Accuracy\"])\n", "intent": "It is important to track results of different experiments for the sake of further comparison.\n"}
{"snippet": "sentences_indexed = pad_sequences(sentences_indexed, maxlen=MAX_SEQUENCE_LENGTH)\ntags_indexed = pad_sequences(tags_indexed, maxlen=MAX_SEQUENCE_LENGTH)\n", "intent": "And then pad these sequences to the same length (equal to the maximum length of the sentence)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(df.drop('Outcome', axis=1))\nX.shape\n", "intent": "- X feature engineering by Standard Scaler (every feature get mean = 0)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itrain]= True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "n_best = 10\ntop_n_similar_bands = pd.DataFrame(index=band_similarities.columns,columns=range(1, n_best + 1))\nfor i in range(0, len(band_similarities.columns)):\n    top_n_similar_bands.iloc[i,:] = band_similarities.iloc[:,i].sort_values(ascending=False)[:n_best].index\ntop_n_similar_bands\n", "intent": "Now let's do this for all bands.\n"}
{"snippet": "stats = pd.read_csv('~/Documents/Data Science/Capstone 1/data/CLEAN/ncaa_d1_stats.csv', index_col=0)\nstats = reset_order(stats)\nstats.info()\nstats.head()\n", "intent": "- describe\n- deal with missing values\n- fix outliers\n- drop unneeded columns\n- save for later\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "authorship = pd.read_csv(\"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\")\nprint(authorship.columns)\n", "intent": "- \"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\"\n- Print the columns, print df.head()\n"}
{"snippet": "dataset = pd.read_csv('Churn_Modelling.csv')\n", "intent": "Importamos el dataset\n"}
{"snippet": "with open('reviews.txt', 'r') as f:\n    reviews = f.read()\nwith open('labels.txt', 'r') as f:\n    labels = f.read()\n", "intent": "Load data and labels.\n"}
{"snippet": "tfidf_vectorizer_title = TfidfVectorizer(ngram_range=(1, 3),\n                                         max_features=100000)\nwith open('../data/train_title.txt', encoding='utf-8') as input_train_file:\n    X_train_title_sparse = tfidf_vectorizer_title.fit_transform(input_train_file)\nwith open('../data/test_title.txt', encoding='utf-8') as input_test_file:\n    X_test_title_sparse = tfidf_vectorizer_title.transform(input_test_file)\n", "intent": "**Tf-Idf with titles.**\n"}
{"snippet": "def normalize(array):\n    scaler = StandardScaler()\n    array = scaler.fit_transform(array)\n    return scaler, array\n", "intent": "I defined a normalizer to be able to normalize the data using a simple function rather than a complex one that I would constantly have to google.\n"}
{"snippet": "dfMCats.fillna(0, inplace = True)\n", "intent": "- Random Forest Don't Like\n"}
{"snippet": "reload(preproc);\nX_tr, Y_tr = preproc.load_data(TRAIN_FILE)\nprint (bilstm.prepare_sequence(X_tr[5],word_to_ix).data.numpy())\nprint (bilstm.prepare_sequence(Y_tr[5],tag_to_ix).data.numpy())\n", "intent": "- Loading Train data for english:\n"}
{"snippet": "print('For user with id', data.iloc[user_index, 0], 'we advice:')\npd.DataFrame(scores, index=band_similarities.columns).sort_values(0, ascending=False).iloc[:5]\n", "intent": "Now let's make a nice print of the top 5 bands to advice to this user:\n"}
{"snippet": "X = pd.DataFrame(house.data, columns=house.feature_names)\ny = pd.Series(house.target)\n", "intent": "For both datasets, define the feature and target variables\n"}
{"snippet": "votes = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=462)\n", "intent": "You're each going to be a decision tree on some data based on a bootstrap sample, and then we'll all ensemble the results.\n"}
{"snippet": "data = Series ([1., NA, 3.5, NA, 7])\ndata.fillna(data.mean())\n", "intent": "But, sometimes is important to evaluate first other methods to fill our data, for example through the application of the basic statistics\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nbc_feature_cols = over_sample.drop(['Class','Sample_code_number'], axis=1)\nbc_features = scaler.fit_transform(bc_feature_cols)\n", "intent": "**2) Are the features normalized? If not, use the scikit-learn standard scaler to normalize them.**\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == ['ORG'] or ['PERSON'] for word in parsed])\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(centers=2, random_state=0)\n", "intent": "First, we will look at a **classification problem in two dimensions**. We use the synthetic data generated by the **`make_blobs`** function.\n"}
{"snippet": "importance_dt = pd.DataFrame(dt.feature_importances_, index=credit_clean.columns[:-1], \n                          columns=[\"D Tree importance\"])\nimportance_rfdt = pd.DataFrame(rfdt.feature_importances_, index=credit_clean.columns[:-1], \n                               columns=[\"Random f Importance\"])\npd.concat([importance_dt,importance_rfdt],axis=1)\n", "intent": "Compare the feature importances as estimated with the decision tree and random forest classifiers.\n"}
{"snippet": "data_path1 = os.path.join(os.getcwd(),'datasets','splice_train.csv')\ndata_path2 = os.path.join(os.getcwd(),'datasets','splice_test.csv')\nsplice_train = pd.read_csv(data_path1)\nsplice_test = pd.read_csv(data_path2)\nprint('Shape of splice_train: {}'.format(splice_train.shape))\nprint('Shape of splice_test: {}'.format(splice_test.shape))\nsplice_train.head(10)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "bank_attributes_train, bank_attributes_test, bank_labels_train, bank_labels_test = train_test_split(\n    bank_attributes, bank_labels, train_size = 0.7, stratify = bank_labels)\n", "intent": "Use the standard 70% / 30% split. Since this is a classification problem, be sure to stratify the split according to the `bank_labels`.\n"}
{"snippet": "wine = pd.read_table(\"../2. Python Data Handling - Pandas and Networkx/wine.dat\", sep=\"\\s+\")\nattributes = ['Grape','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols',\n            'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue',\n            'OD280/OD315 of diluted wines','Proline']\nwine.columns = attributes\n", "intent": "For scatterplots, the x and y axis must be specified as columns:\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "* Load the data set, reshape and assign to variables\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)\n", "intent": "Preprocessing and Pipelines\n=============================\n"}
{"snippet": "scaler=StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "url = './data/bikeshare.csv'\nbikes_cols=['season_num','is_holiday','is_workingday','weather','temp_celsius','atemp','humidity_percent',\n                'windspeed_knots','num_casual_users','num_registered_users','num_total_users']\nbikes = pd.read_csv(url, names=bikes_cols, skiprows=1)\nbikes.columns = bikes_cols\nbikes.head()\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "conn = pg2.connect(host=\"michaelgfrantz.com\", user=\"postgres\", password=\"dsism4\")\ncurs = conn.cursor(cursor_factory=pgex.RealDictCursor)\ncurs.execute(\"select channel from customer where region=3\")\nresults = curs.fetchall()\nconn.close()\ntarget_reg_3_df = pd.DataFrame(results)\ntarget_reg_3 = target_reg_3_df['channel']\n", "intent": "1. Use a `RealDictCursor`\n1. assign the results to `target_reg_3_df`\n1. assign the `channel` column to a series `target_reg_3`\n"}
{"snippet": "pca = PCA()\nboston_train_pca = pca.fit_transform(X_train_scaled)\nboston_test_pca = pca.transform(X_train_scaled)\n", "intent": "Just so we can visualize our data, let's do a PCA and plot our components 1 and 2, and make the color the target for our train set. \n"}
{"snippet": "pd.DataFrame(clf.cv_results_).T[2:6]\n", "intent": "Define the target and feature set for the test data\n"}
{"snippet": "data = pd.read_csv('../data/zillow_realtor_homes_201804.csv')\ndata.head()\n", "intent": "Keywords: linear regression, nonparametric regression\nThe number of bedrooms is a predictor for the list price of the house\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../DS-SF-32/dataset/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "df.fillna(0, inplace=True) \n", "intent": "mostly categorical noise and features that we do not want our model exposed to, also fill Nulls (Nans) with zeroes, but most of all save save save!\n"}
{"snippet": "binary_vectorizer = CountVectorizer(binary=True)\nbinary_vectorizer.fit(X_text)\nX = binary_vectorizer.transform(X_text)\n", "intent": "Next, we will turn `X_text` into just `X` -- a numeric representation!\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\nprint(f'Shape of the training data {X_train.shape}')\nprint(f'Shape of the validation dat {X_val.shape}')\n", "intent": "Split training data into training and validation data\n"}
{"snippet": "data_path = 'c:/Users/E411208/Documents/Python Scripts/data/Bluetooth/'\ndata = pd.DataFrame()\nfor id,file in enumerate(os.listdir(data_path)):\n    if file.endswith(\".csv\"):\n        print('Loading file ... ' + file)\n        measurement = pd.read_csv(data_path + file)\n        measurement['measurement']=id \n        data = data.append(measurement,ignore_index = True)\n", "intent": "Here we load all CSV files\n"}
{"snippet": "pd.pivot_table(prediction_of_actve_df,index=[\"CHURN_BAND\"],values=[\"EXTRELNO\"],aggfunc='count')\n", "intent": "    *******************************  Summarizing the probability of churn ****************************************\n"}
{"snippet": "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()\n", "intent": "What is the relationship between _Fare_ and survival\n"}
{"snippet": "data = loadmat('data/ex7data2.mat')\nX = pd.DataFrame(data['X'], columns=['X1','X2'])\nX.head()\n", "intent": "Let's load our data\n"}
{"snippet": "X=pd.DataFrame(iris.data,columns=iris.feature_names)\ny=pd.DataFrame(iris.target)\nX.head()\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "MPI_data = pd.read_csv(\"MPI_data_poverty.csv\", encoding=\"ISO-8859-1\")\nloan_data = pd.read_csv(\"loans_midterm.csv\")\ncountry_mapper = pd.read_csv(\"country_mapper.csv\")\n", "intent": "Import the MPI, country mapper, and loans data.\n"}
{"snippet": "tsne_df = pd.DataFrame(coordinates, \n                       columns=['tsne_y', 'tsne_x'])\n", "intent": "These coordinates can be stored in a dataframe.  \n"}
{"snippet": "path = Path('.','data','titanic.csv')\ntitanic = pd.read_csv(path)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "df = df.fillna(999)\n", "intent": "replacing the missing values with 999.\n"}
{"snippet": "documents = []\nfor ind, talk in talks.fillna('').iterrows():\n    documents.append( ' '.join([talk['title'],\n                                talk['desc'],\n                                talk['abstract'] \n                                ]) )\nprint (\"Read %d corpus of documents\" % len(documents))\n", "intent": "Build a bag of words for each document\n"}
{"snippet": "pd.DataFrame(lm.coef_, X.columns, columns = [\"coeff\"])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "train_features = pd.read_csv('data/training_features.csv')\ntest_features = pd.read_csv('data/testing_features.csv')\ntrain_labels = pd.read_csv('data/training_labels.csv')\ntest_labels = pd.read_csv('data/testing_labels.csv')\nprint('Training Feature Size: ', train_features.shape)\nprint('Testing Feature Size:  ', test_features.shape)\nprint('Training Labels Size:  ', train_labels.shape)\nprint('Testing Labels Size:   ', test_labels.shape)\n", "intent": "First let's read in the formatted data from the previous notebook. \n"}
{"snippet": "vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(raw_documents)\nprint(\"Number of terms in model is %d\" % len(vectorizer.vocabulary_) )\n", "intent": "We can remove low frequency terms that appear in fewer than a specified number of documents:\n"}
{"snippet": "import pandas as pd\ntracks = pd.read_csv(\"fma-rock-vs-hiphop.csv\")\nechonest_metrics = pd.read_json(\"echonest-metrics.json\")\n", "intent": "First import `pandas` and use `read_csv` and `read_json` to read in the files `fma-rock-vs-hiphop.csv` and `echonest-metrics.json`.\n"}
{"snippet": "X = subframe\nscaler = Normalizer().fit(X)\nnormalized = scaler.transform(X)\nnp.set_printoptions(precision=3)\nprint(normalized[0:5,:])\n", "intent": "- Use sklearn to Normalize the subframe based on a length of 1 (experiment)\n- Display the changes in a summary\n"}
{"snippet": "pca = PCA(n_components=361)\npca.fit(faces)\nv = np.concatenate([pca.mean_[None], pca.components_])\n", "intent": "Run the provided function `PCA` on the images (rows) in `faces`:\n"}
{"snippet": "prediction = probabilities > 0.05\nconfusion_matrix_small = pd.DataFrame(metrics.confusion_matrix(Y_test, prediction, labels=[1, 0]).T,\n                                columns=['p', 'n'], index=['Y', 'N'])\nprint confusion_matrix_small\n", "intent": "What if we lower the threshold to 5%?\n"}
{"snippet": "from sklearn.cross_validation import KFold, cross_val_score\ny, X = pima_predict_df['Class variable'], pima_predict_df.drop(['Class variable'], axis=1)\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, random_state=7)\n", "intent": "**5. What's the best performance you can get with kNN? Is kNN a good choice for this dataset?**\n==\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)\n", "intent": "Standardizing the data.\n"}
{"snippet": "lasso_model = Lasso(alpha=100.0)\nlasso_model.fit(X_train, y_train)\nbest_feature_selector = SelectFromModel(lasso_model, prefit=True)\nX_best = best_feature_selector.transform(X_train)\nX_best.shape\n", "intent": "Best features determined using the lasso (a linear model with L1 regularisation).\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, y)\nprint len(x_train), len(x_test)\nprint len(y_train), len(y_test)\n", "intent": "- Create test/train splits of the original data set\nx_train, x_test, y_train, y_test = train_test_split(...)\n"}
{"snippet": "def csv_to_df(filepath):\n    df = pd.read_csv(filepath, header = 0)\n    return df\ntrain = csv_to_df('/home/nikita/Documents/DataScienceTaskExzeo/exzeo_training_data.csv')\ntest = csv_to_df('/home/nikita/Documents/DataScienceTaskExzeo/exzeo_test_data.csv')\n", "intent": "Now first of all, we need to load the data using pandas read_csv function which will load the csv file and convert it into dataframe.\n"}
{"snippet": " ad_data = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "df = pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv', index_col=0)\ndf.head()\n", "intent": "1) Load in the dataset https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv into a pandas dataframe\n"}
{"snippet": "df =pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv', index_col = 0)\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "df_data = pd.read_csv(open('vehicle_stops_2016_datasd.csv')); \ndf_data2 = pd.read_csv(open('vehicle_stops_2017_datasd.csv'));\n", "intent": "Here we import the data sets. The data is held in local files which we downloaded from https://data.sandiego.gov\n"}
{"snippet": "df = pd.read_csv(\"./datasets/iris.csv\")\n", "intent": "Load in the iris data set.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())\n", "intent": "* Detect the species type based on petal width\n* Import logistic regression from sklearn linear model\n"}
{"snippet": "from sklearn.decomposition import IncrementalPCA\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_mnist, n_batches):\n    print(\".\", end=\"\") \n    inc_pca.partial_fit(X_batch)\nX_mnist_reduced_inc = inc_pca.transform(X_mnist)\nprint(np.sum(inc_pca.explained_variance_ratio_))\n", "intent": "* PCA normally requires entire dataset in memory for SVD algorithm.\n* **Incremental PCA (IPCA)** splits dataset into batches.\n"}
{"snippet": "for dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset.drop(['Cabin', 'Ticket', 'Fare', 'Name'], axis=1, inplace = True)\n", "intent": "In the case of the port of Embarkation, we see that only 2 values are null. We will use the mode of this column to fill in these values.\n"}
{"snippet": "rmse_lambda_df = pd.DataFrame(rmse_CV_lambda,columns=['lambda', 'RMSE']).set_index('lambda')\nrmse_lambda_df.plot()\n", "intent": "We search the $\\lambda$ grid : 0.0001-0.01 to find the optimal $\\lambda$ parameter\n"}
{"snippet": "numeric_cols = pd.read_csv(DataPath+\"train_numeric.csv\", nrows = 10000).columns.values\nimp_idxs = [np.argwhere(feature_name == numeric_cols)[0][0] for feature_name in feature_names]\ntrain = pd.read_csv(DataPath+\"train_numeric.csv\", \n                index_col = 0, header = 0, usecols = [0, len(numeric_cols) - 1] + imp_idxs)\ntrain = train[feature_names + ['Response']]\n", "intent": "We determine the indices of the most important features. After that the training data is loaded\n"}
{"snippet": "pca_mod = skde.PCA()\npca_comps = pca_mod.fit(X_train)\npca_comps\n", "intent": "The code in the cell below computes the principle components for the training feature subset. Execute this code:\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n", "intent": "The dataset we'll use for this lesson is included as part of Scikit-Learn, namely the breast cancer dataset.\n"}
{"snippet": "df = pd.read_csv('RWA_DHS6_2010_2011_HH_ASSETS.CSV', index_col=0)\ndf\n", "intent": "Load, clean, and prepare DHS asset ownership data:\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "The handwritten digits recognition problem we will face is already included as a testbed in keras. Loading it only requires invoking\n"}
{"snippet": "from sklearn import decomposition\nnmf = decomposition.LatentDirichletAllocation(n_topics=6)\nnmf.fit(tfidf)\nW = nmf.transform(tfidf)\nH = nmf.components_\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_ents_dist.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_ents\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom IPython.display import display\nX = wells.drop(\"status_group\", axis=1)\ny = wells[\"status_group\"]\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.25,\n                                                    random_state=666)\ndisplay(y_train.value_counts() / len(y_train))\ndisplay(y_test.value_counts() / len(y_test))\n", "intent": "Why is this a problem when doing a train-test split?\n"}
{"snippet": "try_new_vectoriser(CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2\n                                  ),\n                   X_train,\n                   y_train)\n", "intent": "Instead of binary let's use actual counts\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nX = df[[\"fixed acidity\", \"chlorides\", \"total sulfur dioxide\"]]\ny = df[\"colour\"]\ny_binary = y.map({\"red\": 1, \"white\": 0})\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, stratify=y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\n", "intent": "Using the make-up of classes investigated above as an indication - should you or shouldn't you stratify your samples in the train-test split?\n"}
{"snippet": "X = np.array(data.loc[:, features])\nle = LabelEncoder()\ny = le.fit_transform(np.array(data.types))\nprint X.shape, y.shape\ntrees = ExtraTreesClassifier(n_estimators=200, max_features=5, bootstrap=True, random_state=0)\ntrees.fit(X, y)\nfeature_importances = pd.Series(trees.feature_importances_, index = features)\nfeature_importances.plot(kind = 'bar')\n", "intent": "- kernel_groove_len + kernel_length\n- kernel_length\n- perimeter\n- area - very related to perimeter\n"}
{"snippet": "pca = PCA(0.50).fit(noisy)\npca.n_components_\n", "intent": "* Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\nX_train, X_test, y_train, y_test = train_test_split(train[['O3', 'PM10', 'PM25', 'NO2', 'T2M']],\n                                                    train['mortality_rate'], \n                                                    test_size = 0.3,\n                                                    random_state=22)\n", "intent": "Our test set is like the training set above, except it does not have `mortality_rate` (that's what we're after). Ignore region and date for now.\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\nX_tr_scaled = scaler.fit_transform(X_tr)\nX_te_scaled = scaler.transform(X_te)\nX_tr_scaled.mean(axis=0), X_tr_scaled.std(axis=0)\n", "intent": "Tranformation of all features to zero mean and unit variance.\n"}
{"snippet": "from scipy.misc import imread\npath = datafolder + \"indoor/1.jpg\"\nimage = imread(path)\n", "intent": "Two subfolders exist: one containing a range of indoor images and another one with outdoor images. We can load an image into python by making use of:\n"}
{"snippet": "from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nx_scaled = min_max_scaler.fit_transform(geo_df_EU['pop_density'].values.reshape(-1,1))\ngeo_df_EU['inf_rate'] = pd.Series(x_scaled.flatten())\n", "intent": "Normalization of pop density into a contact rate between 0 and 1 \n"}
{"snippet": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:, ::2, ::2].copy()\nx_test = x_test[:, ::2, ::2].copy()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_shape = x_train.shape[1:]\nprint('train set shape:', x_train.shape)\nprint('test set shape:', x_test.shape)\n", "intent": "*Please execute the cell bellow in order to prepare the MNIST dataset*\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in ccpi_country['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\n", "intent": "New dataframe which contains only the data from countries being mentioned in CCPI document is created.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Feature Interaction'] == 'average') & (df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(ccpi_country)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)    \n", "intent": "New engineered dataframe is generated. First the dataframe with the averaged features is appended. \n"}
{"snippet": "df_ccpi_clean = pd.DataFrame()\narray_flag = np.ones((len(df_ccpi)), dtype=bool)\nseries_flag = pd.Series(data = array_flag, index=range(len(df_ccpi)))\nfor indi in sorted(set(list_indi)):\n    series_flag = (series_flag & (df_ccpi['Indicator Name'] != indi))\ndf_ccpi_clean = df_ccpi[series_flag]\ndf_ccpi_clean_reset = df_ccpi_clean.reset_index(drop=True)\ndf_ccpi_clean_reset = df_ccpi_clean_reset.fillna(0)\ndf_ccpi_clean_reset.head(8)\n", "intent": "Filter the data frame so it only consists of the indicators which has complete data for the entire year (1960-2016).\n"}
{"snippet": "scaled_values = scaler.transform(X)\nX_scaled = pd.DataFrame(data=scaled_values, columns=X.columns)\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "train_df.fillna(-99, inplace=True)\ntest_df.fillna(-99, inplace=True)\n", "intent": "Now let us impute the missing values with some value which is outside the range of values of the column, say -99.\n"}
{"snippet": "df = pd.read_csv('../../assets/datasets/cars.csv')\ndf.isnull().sum()\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "pop_gen = pd.DataFrame(gen_df['genre'].value_counts()).reset_index()\npop_gen.columns = ['genre', 'movies']\npop_gen.head(10)\n", "intent": "TMDB defines 32 different genres for our set of 45,000 movies. Let us now have a look at the most commonly occuring genres in movies.\n"}
{"snippet": "dataset1,dataset2 = boston_housing.load_data()\nx_train, y_train = dataset1\nx_test, y_test = dataset2\ntrain_frame = pd.DataFrame(y_train)\ntrain_frame\n", "intent": "- Define a class `MultiLayerPerceptron` which passes the Multi-Layer Perceptron tests\n"}
{"snippet": "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_start, y, train_size=0.8)\n", "intent": "Split data into training and testing sets.\n"}
{"snippet": "df_feat_scaled = pd.DataFrame(\n    scaler.transform(df.drop('TARGET CLASS', axis=1)), columns=df.columns[:-1])\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    count_vect = CountVectorizer(stop_words=stoplist_combined,\n                                 tokenizer=word_tokenize)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_\n", "intent": "We can **override the tokenizer and stop_words**:\n"}
{"snippet": "df = pd.read_csv('Iowa_Liquor_Sales.csv')\ndf.shape\n", "intent": "Perform some exploratory statistical analysis and make some plots, such as histograms of transaction totals, bottles sold, etc.\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='median', axis=0)\ntrain = imp.fit_transform(train)\nxtrain = train[fold1,1:]\nytrain = train[fold1,0]\nxtest = train[K[0],1:]\nytest = train[K[0],0]\n", "intent": "The following code replace missing data with median in the column.\n"}
{"snippet": "cv = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "pd.DataFrame(rfe.ranking_.T, index = X.columns.values).sort_values(by = 0).head(20)\n", "intent": "Review the top ranked features from our RFE selection:\n"}
{"snippet": "nhl = pd.read_csv('https://raw.githubusercontent.com/josephnelson93/GA-DSI/master/NHL_Data_GA.csv')\nnhl.head()\n", "intent": "Feel free to also do basic EDA. At least check the head()!\n"}
{"snippet": "data.index = pd.to_datetime(data.index)\nstore1 = data[data.Store==1]\nstore1 = pd.DataFrame(store1['Weekly_Sales'].resample('1D').sum(),columns=['Weekly_Sales'])\nstore1.dropna(inplace=True)\n", "intent": "- Filter the dataframe to Store 1 sales and aggregate over departments to compute the total sales per store.\n"}
{"snippet": "n_best = 10\ntop_n_similar_bands = pd.DataFrame(index=band_similarities.columns,columns=range(1, n_best + 1))\nfor i in range(0, len(band_similarities.columns)):\nraise NotImplementedError()\ntop_n_similar_bands\n", "intent": "Now let's do this for all bands.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "But in reality our model has to form predictions on *unseen* data. Let's model this situation.\n"}
{"snippet": "y_scaler.inverse_transform([[0.024229725354342153]])\n", "intent": "transfer to degrees\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "train_df = pd.read_csv(\"./input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"./input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n", "intent": "Reading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/michaelkunkel/WORK/GIT_HUB/DeepLearning/Introduction2/DataScience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "pca  =  PCA(n_components =1)\npca.fit(images)\nimages_pca = pca.transform(images)\nprint(\"original shape:   \", images.shape)\nprint(\"transformed shape:\", images_pca.shape)\n", "intent": "Here is an example of using PCA as a dimensionality reduction transform:\n"}
{"snippet": "import s3fs\nfs = s3fs.S3FileSystem(anon=True)\ntestimage = fs.ls('esipfed/cdi-workshop/semseg_data/ontario/test/')[0]\nwith fs.open(testimage, 'rb') as f:\n    img = imread(f)\n", "intent": "The ontario dataset sits on S3. Let's load an image in\n"}
{"snippet": "bos_plot = pd.DataFrame(columns=['PRICE','fittedvalues'])\nbos_plot = bos_plot.fillna(0)\nbos_plot[\"fittedvalues\"] = m.fittedvalues\nbos_plot[\"PRICE\"] = bos.PRICE\nbos_plot.head()\n", "intent": "**Your turn:** Create a scatterpot between the predicted prices, available in `m.fittedvalues` and the original prices. How does the plot look?\n"}
{"snippet": "df = pd.DataFrame(scaled_data, columns=data.columns)\ndf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = open('./utils/kafka.txt', 'r', encoding=\"utf-8\").read()\nchars = list(set(data)) \ndata_size, vocab_size = len(data), len(chars)\nprint('data has %d chars, %d unique' % (data_size, vocab_size))\n", "intent": "The text corpus to train your RNN on is going to be the book The metamorphosis by Franz Kafka. Run the cell below to load the text.\n"}
{"snippet": "spotify = pd.read_csv(\"../../data/spotify_data.csv\", index_col=[0])\nspotify.head()\n", "intent": "Link to my article about the project: https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/\n"}
{"snippet": "ss = StandardScaler()\nmm = MinMaxScaler()\nX_ss = ss.fit_transform(X)\nX_mm = mm.fit_transform(X)\n", "intent": "We can fit and transform at the same time\n"}
{"snippet": "path = \"../../data/fraud.csv\"\nfraud = pd.read_csv(path, index_col=[0])\nfraud.drop(\"Time\", axis = 1)\nfraud.head()\n", "intent": "Let's move onto the real thing by modeling credit card fraud data\nhttps://www.kaggle.com/dalpozz/creditcardfraud\n"}
{"snippet": "path = \"../../data/NLP_data/ds_articles.csv\"\narticles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\narticles.dropna(inplace=True)\narticles.reset_index(inplace=True, drop=True)\narticles.head()\n", "intent": "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles\n"}
{"snippet": "path = \"../../data/fraud.csv\"\nfraud = pd.read_csv(path, index_col=[0])\nfraud.drop(\"Time\", axis = 1, inplace = True)\nfraud.head()\n", "intent": "Let's move onto the real thing by modeling credit card fraud data\nhttps://www.kaggle.com/dalpozz/creditcardfraud\n"}
{"snippet": "from pyensae.datasource import download_data\nfile = download_data(\"Divvy_Trips_2016_Q3Q4.zip\", url=\"https://s3.amazonaws.com/divvy-data/tripdata/\")\n", "intent": "[Divvy Data](https://www.divvybikes.com/system-data) publishes a sample of the data. \n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame({'predictions':y_knn_pred, 'actual label':y_test})\nsgd_acc = round((df[df.predictions == df['actual label']] .shape[0]/df.shape[0])*100, 2)\nsgd_acc\n", "intent": "<h1><a name=\"acc knn\">Knn + GridSearchCV accuracy</a></h1>\n"}
{"snippet": "talk_webpage = request.urlopen(urls[0]).read()\ntalk_webpage\n", "intent": "Let's scrape each individual link for the abstract.\n"}
{"snippet": "pca = PCA(n_components=5)\npca.fit(X_std)\nX_5d = pca.transform(X_std)\n", "intent": "<h1>PCA using inbuilt functions</h1>\n"}
{"snippet": "test = imresize(data[idx],0.5)\n", "intent": "We can use the 'imresize' function from scipy.misc to downsample the image. Let's run a quick test.\n"}
{"snippet": "fnames = os.path.join(DATA_DIR, 'amazon', '*.csv')\nfnames = glob.glob(fnames)\nreviews = []\ncolumn_names = ['id', 'product_id', 'user_id', 'profile_name', 'helpfulness_num', 'helpfulness_denom',\n               'score', 'time', 'summary', 'text']\nfor fname in fnames[:2]:\n    df = pd.read_csv(fname, names=column_names)\n    text = list(df['text'])\n    reviews.extend(text)\n", "intent": "Read in all the `.csv` files in the folder `amazon`. Extract out only the text column from each file and store them all in a list called `reviews`.\n"}
{"snippet": "Xprime = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "estimator2 = LinearRegression()\nselector2 = RFECV(estimator2, step=3, cv=5)\nselector2 = selector2.fit(X[include], y)\n", "intent": "Estimation with Linear Regression\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "The Random Forest Regressor Does very well on both Training and Validation - a promising model!\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X[features_one], np.log(target), test_size=.30, random_state=1)\nX_train_two, X_val, y_train_two, y_val = train_test_split(X_train, y_train, test_size=.30, random_state=1)\n", "intent": " In this iteration, we try predicting the log of the target\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n", "intent": "Creating Training and Test datasets\n"}
{"snippet": "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, \n    stratify=train_df.coverage_class, \n    random_state=1234)\n", "intent": "stratified by salt coverage\n"}
{"snippet": "dist_mat = pd.DataFrame(np.zeros(shape=(len(final),len(final))))\nfor i in range(len(final)):\n    for j in range(len(final)):\n        dist_mat.iloc[i][j] = euclid_dist(final.iloc[i],final.iloc[j])\n", "intent": "This is another clustering algorithm I used. \nI think K-means performs better thanks DBScan Cluster.\n"}
{"snippet": "data311=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Cluster_lecture/cluster.csv\")\n", "intent": "Please carefully read 2.3.9:\nhttp://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "imagedata_list = []\nfor image in data:\n    ar = imresize(image,0.5)\n    imagedata_list.append(ar)\n", "intent": "The image looks reasonable, despite having 1/4 the number of pixels. Let's go ahead and downsample the entire set.\n"}
{"snippet": "df = pd.read_csv('datasets/dataset_2.txt', delimiter=',')\ndf.head()\n", "intent": "**Solution:**\nLet's start with some data exploration for ourselves, behind the scenes:\n"}
{"snippet": "df = pd.read_csv(\"Classified Data\",index_col=0)\n", "intent": "Setzen wir die erste Spalte (index_col) = 0, um sie als Index zu verwenden.\n"}
{"snippet": "from keras.datasets import mnist\n(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n", "intent": "- https://datascienceschool.net/view-notebook/51e147088d474fe1bf32e394394eaea7/\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\niris.keys()\n", "intent": "Utilize the starter code to practice hierarchical clustering on the iris dataset\n"}
{"snippet": "class CategoricalImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, cols):\n        self.cols = cols\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        transformed_df = X\n        for col in self.cols:\n            transformed_df.loc[:, col] = transformed_df.loc[:, col].astype(str)\n        return transformed_df\n", "intent": "For example, we could write the feature processing step above 'the sklearn way':\n"}
{"snippet": "pca = PCA()\npca.fit(X_train)\n", "intent": "Fit PCA to the training data\n"}
{"snippet": "comb_data['Electrical'] = comb_data['Electrical'].fillna(comb_data['Electrical'].mode()[0])\n", "intent": "This columns has maximum number as 'SBrk' and 'KitchenQual' resp, So we can replace value which only 1 using Mode.\n"}
{"snippet": "standardScaler = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=4444)\n", "intent": "Draw the learning curve for logistic regression in this case.\n"}
{"snippet": "def train_test_split(pos_emails, neg_emails, pos_labels, neg_labels, split_value):\n    X_train = np.concatenate((pos_emails[:split_value], \n                              neg_emails[:split_value]), axis = 0)\n    X_test = np.concatenate((pos_emails[split_value:],\n                             neg_emails[split_value:]), axis = 0)\n    y_train = np.concatenate((pos_labels[:split_value], \n                              neg_labels[:split_value]), axis = 0)\n    y_test = np.concatenate((pos_labels[split_value:],\n                             neg_labels[split_value:]), axis = 0)\n    return X_train, X_test, y_train, y_test\n", "intent": "- Create four new arrays from our email data and labels.\n"}
{"snippet": "test = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/vehicles_test.csv')\ntest['type'] = test.type.map({'car':0, 'truck':1})\ntest\n", "intent": "How good is scikit-learn's regression tree at predicting the price for test observations?\n"}
{"snippet": "Range = pd.date_range('05/28/2010', periods=194, freq='W')\nRange = pd.DataFrame(Range)\nprint ('The Date of the Significant Drop: ', Range.loc[126])\n", "intent": "- Guessing the time series start date - 05/28/2012 instead of 05/21/2012.\n- The range can also be created in such a way so as to reveal the data.\n"}
{"snippet": "power_series = pd.DataFrame()\nfor year in range(2013,2018):\n    power_data = pd.read_excel('Downloads/WindForecast_{0}-01-01_{0}-12-31.xls'.format(str(year)), header=3)\n    power_data = power_data.drop(columns=['Week-ahead Forecast [MW]',\n                          'Day-ahead forecast (11h00) [MW]',\n                          'Most recent forecast [MW]',\n                          'Active Decremental Bids [yes/no]'])\n    power_data['DateTime'] = pd.to_datetime(power_data['DateTime'].apply(lambda x: x[3:6]+x[0:3]+x[6:]))\n    power_data = power_data.set_index(['DateTime'])\n    power_series = power_series.append(power_data)\n", "intent": "Next step is to try an time-series forecasting approach. First algo is ARIMA\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('./some datasets/SMSSpamCollection',delimiter='\\t',header=None)\ndf.head()\n", "intent": "http://archive.ics.uci.edu/ml/machine-learning-databases/00228/\n"}
{"snippet": "def split_train_test_linear_regression_basic(df):\n    adj_close = df['Close'].tolist()\n    date      = df['Item'].tolist()\n    date = np.reshape(date, (len(date), 1))\n    adj_close = np.reshape(adj_close, (len(adj_close), 1))\n    X_train, X_test, y_train, y_test = train_test_split(date, adj_close, test_size=0.20, random_state=42)\n    return X_train, X_test, y_train, y_test\n", "intent": "   **Split the data**\n"}
{"snippet": "df = pd.read_csv(\"monthly-milk-production.csv\",index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "X = pd.DataFrame(iris.data)\nX.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny = pd.DataFrame(iris.target)\ny.columns = ['Targets']\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nprint(digits.keys())\n", "intent": "The datasets are loaded into a dictionary.\n"}
{"snippet": "dataset_wine = pd.read_csv(\"wine_dataset.csv\")\ndataset_red = pd.read_csv(\"red_wine_dataset.csv\")\ndataset_white = pd.read_csv(\"white_wine_dataset.csv\")\n", "intent": "wine_dataset is divided into red_wine_dataset and white_wine_dataset for analysis\n"}
{"snippet": "bank_attributes_train, bank_attributes_test, bank_labels_train, bank_labels_test = train_test_split(bank_attributes, bank_labels, stratify=bank_labels, train_size=0.7, test_size=0.3)\n", "intent": "Use the standard 70% / 30% split. Since this is a classification problem, be sure to stratify the split according to the `bank_labels`.\n"}
{"snippet": "esquerda = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                     'key2': ['K0', 'K1', 'K0', 'K1'],\n                        'A': ['A0', 'A1', 'A2', 'A3'],\n                        'B': ['B0', 'B1', 'B2', 'B3']})\ndireita = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                               'key2': ['K0', 'K0', 'K0', 'K0'],\n                                  'C': ['C0', 'C1', 'C2', 'C3'],\n                                  'D': ['D0', 'D1', 'D2', 'D3']})\n", "intent": "**Diferentes tipos de merge (semelhantes aos do SQL JOIN)**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nparams={'C':[0.001,0.1,10.0,1.0,0.01]}\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\ndigits=load_digits()\nX_train,X_test,Y_train,Y_test=train_test_split(digits.data,digits.target)\ngrid=GridSearchCV(LogisticRegression(),param_grid=params)\ngrid.fit(X_train,Y_train)\ngrid.score(X_test,Y_test)\n", "intent": "Use LogisticRegression to classify digits, and grid-search the C parameter.\n"}
{"snippet": "import pandas as pd\ntrain_neg_df = pd.DataFrame({'data_train': data['train']['neg'], 'labels_train': labels['train']['neg']})\ntrain_pos_df = pd.DataFrame({'data_train': data['train']['pos'], 'labels_train': labels['train']['pos']})\ntrain_df = pd.concat([train_neg_df, train_pos_df], axis=0)\ntest_neg_df = pd.DataFrame({'data_test': data['test']['neg'], 'labels_test': labels['test']['neg']})\ntest_pos_df = pd.DataFrame({'data_test': data['test']['pos'], 'labels_test': labels['test']['pos']})\ntest_df = pd.concat([test_neg_df, test_pos_df], axis=0)\ntrain_df.head()\n", "intent": "Save loaded data into a Pandas DataFrame for easy read/write\n"}
{"snippet": "yelp_class = yelp[ (yelp['stars'] == 5) | (yelp['stars'] == 1) ]\nX = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7) \nmask = np.zeros(critics.shape[0], dtype=np.bool) \nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                    iris.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Here we'll use 75% of the data for training, and test on the remaining 25%.\n"}
{"snippet": "votedf=pd.read_csv(votes_file)\nairportdf=pd.read_csv(airport_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "df = pd.read_csv('TS.csv')\nts = pd.Series(list(df['Sales']), index=pd.to_datetime(df['Month'],format='%Y-%m'))\n", "intent": "Let's predict sales data using ARIMA\n"}
{"snippet": "X_partno_onehot = enc_onehot.fit_transform(X_partno_labelencoded.reshape(-1, 1))\n", "intent": "scikit-learn api requires that our data is a 2-D array, so need to also perform a .reshape(-1, 1)\n"}
{"snippet": "df = pd.read_csv(\n    '~/2001.csv',\n    encoding='latin-1',\n    usecols=(13, 16)\n    )\n", "intent": "We use the `AirTime` column at the Willard airport.\n"}
{"snippet": "import pandas as pd\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\ncol_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\nglass = pd.read_csv(url, names=col_names, index_col='id')\nglass.sort('al', inplace=True)\nglass.head()\n", "intent": "- Using linear regression model\n"}
{"snippet": "from sklearn.utils import shuffle\nX_train, y_train = shuffle(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.02, random_state=0)\nprint(\"X_train shape\", X_train.shape)\nprint(\"X_test shape\", X_test.shape)\nprint(\"Y_valid shape\", X_validation.shape)\n", "intent": "Describe how you set up the training, validation and testing data for your model.\nOnce I have my data preprocess, next splitting the data into 80,20\n"}
{"snippet": "factor_exposures = pd.DataFrame(index=[\"factor 1\", \"factor 2\"], \n                                columns=portfolio_returns.columns,\n                                data = pca.components_).T\n", "intent": "The factor returns here are an analogue to the principal component matrix $\\mathbf{V}$ in the image processing example. \n"}
{"snippet": "X, y = (yelp_class['text'], yelp_class['stars'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "data = pd.read_csv('AirQualityUCI.csv', sep=';')\ndata.head()\n", "intent": "Try other data sets for regression\nYou can use pd.Categorical(df.variable name).codes if needed\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\nprint(y_train.shape)\nprint(y_test.shape)\n", "intent": "You will be asked to create a train/test split. You will be told the size to use for the test set. \n"}
{"snippet": "data = pd.read_csv('data/adult.csv', header=0, sep=', ', engine='python')\ndata.head()\n", "intent": "Adult dataset: https://archive.ics.uci.edu/ml/datasets/Adult\n"}
{"snippet": "print(bdata.keys())\nprint(bdata.feature_names)\nprint(bdata.DESCR)\nprint(bdata.feature_names)\nboston  = pd.DataFrame(data=bdata.data,columns=bdata.feature_names)\n", "intent": "The following commands will provide some basic information about the shape of the data:\n"}
{"snippet": "df3 = pd.DataFrame({'AAA' : [4,5,6,7], 'BBB' : [10,20,30,40],'CCC' : [100,50,-30,-50]})\ndf3\n", "intent": "Another silly example:\n"}
{"snippet": "def fit_pca(df, n_components):\n    pca = PCA(n_components=n_components)\n    reduced = pca.fit_transform(df)\n    return pca, reduced\n", "intent": "Write a function named fit_pca() that takes a pandas.DataFrame and uses sklearn.decomposition.PCA to fit a PCA model on all values of df.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(X0, y0, test_size=0.25, random_state=30)\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n", "intent": "<h1>Build the model</h1>\n<p>Create train test splits for your data.  Use the training data to .</p>\n"}
{"snippet": "PATH_TO_DATA = ('../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntrain_df['time1'] = pd.to_datetime(train_df['time1'])\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\ntest_df['time1'] = pd.to_datetime(test_df['time1'])\n", "intent": "Reading original data\n"}
{"snippet": "df_train = pd.read_csv('../input/dog-breed-identification/labels.csv')\ndf_test = pd.read_csv('../input/dog-breed-identification/sample_submission.csv')\ndf_train.head(10)\n", "intent": "*Step 2: Describe Data*\n"}
{"snippet": "df_raw = pd.read_csv(f'{PATH}TrainAndValid.csv', low_memory=False, \n                     parse_dates=[\"saledate\"])\n", "intent": "*Question*\nWhat stands out to you from the above description?  What needs to be true of our training and validation sets?\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndataHitters = pd.read_csv('../../data/Hitters.csv', index_col = 0)\ndataHitters = dataHitters.dropna()\ndataHitters['Salary'] = np.log(dataHitters['Salary'])\ndataHitters.dtypes\n", "intent": "a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.\n"}
{"snippet": "def read_file(filename):\n    data = []\n    with open(filename, 'r') as f:\n        text = f.read().split('\\n')\n        for parag in text:\n            if len(parag) >2:\n                data = data + [a for a in parag.split('. ') if len(a)>0]\n    return data\n", "intent": "num: 547\nNote: There're 5013 texts within articles folder, so this dataframe only contains articles that are labeled as one of the 58 topics\n"}
{"snippet": "df = pd.read_csv('glass.data', names=['id','ri','Na','Mg','Al','Si','K','Ca','Ba','Fe','Type'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(random_state=17)\nx_tsne = tsne.fit_transform(data_x_scaled)\n", "intent": "Visualize the data using t-SNE. See if tuning the perplexity parameter helps obtaining a\nbetter visualization.\n"}
{"snippet": "from sklearn import preprocessing\ndataset = oml.datasets.get_dataset(10)\nX, y, categorical = dataset.get_data(\n    target=dataset.default_target_attribute,\n    return_categorical_indicator=True)\nprint(\"Categorical features: %s\" % categorical)\nenc = preprocessing.OneHotEncoder(categorical_features=categorical)\nX = enc.fit_transform(X)\nclf.fit(X, y)\n", "intent": "You can also ask for meta-data to automatically preprocess the data\n- e.g. categorical features -> do feature encoding\n"}
{"snippet": "df=pd.read_csv(\"COGS108_IntroQuestionnaireData.csv\")\n", "intent": "Import datafile 'COGS108_IntroQuestionnaireData.csv' into a DataFrame called 'df'.\n"}
{"snippet": "datasets = datasets.load_iris()\n", "intent": "Now, let's create an object called *datasets* and load Iris dataset.\n"}
{"snippet": "titanic = pd.read_csv(\"../input/train.csv\")\ntitanic.head()\n", "intent": "Load train & test data\n======================\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\nX_new.head()\nX_new2 = pd.DataFrame({'Radio': [data.Radio.min(), data.Radio.max()]})\nX_new2.head()\n", "intent": "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:\n"}
{"snippet": "X, y = sklearn.datasets.make_classification(\n    n_samples=10000, n_features=4, n_redundant=0, n_informative=2, \n    n_clusters_per_class=2, hypercube=False, random_state=0\n)\nX, Xt, y, yt = sklearn.cross_validation.train_test_split(X, y)\nind = np.random.permutation(X.shape[0])[:1000]\ndf = pd.DataFrame(X[ind])\n_ = pd.scatter_matrix(df, figsize=(9, 9), diagonal='kde', marker='o', s=40, alpha=.4, c=y[ind])\n", "intent": "Synthesize a dataset of 10,000 4-vectors for binary classification with 2 informative features and 2 noise features.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.decomposition import PCA\niris = datasets.load_iris()\npca = PCA(n_components=2)\npca.fit(iris.data)\nX = pca.transform(iris.data)\n", "intent": "Can you run (and plot) a PCA on the Iris dataset?\n"}
{"snippet": "ultimate[\"avg_rating_by_driver\"]=ultimate[\"avg_rating_by_driver\"].fillna(-1) \nultimate[\"avg_rating_of_driver\"]=ultimate[\"avg_rating_of_driver\"].fillna(-1) \nbins = [-2, 0, 1.1, 2.1,  3.1, 4.1 ,5.1]\ngroup_names = ['None', '0-1', '1-2', '2-3', '3-4', '4-5']\nultimate['avg_rating_by_driver_grouped'] = pd.cut(ultimate[\"avg_rating_by_driver\"], bins, labels=group_names)\nultimate['avg_rating_of_driver_grouped'] = pd.cut(ultimate[\"avg_rating_of_driver\"], bins, labels=group_names)\n", "intent": "For both the ratings we will bin them into categories None ,0-1, 1-2, 2-3, 3-4, 4-5, where 0 would be the category with missing ratings.\n"}
{"snippet": "adult_train_standardize_all = (StandardScaler()\n                               .fit_transform(adult_train_standardize_all_df))\nadult_train_standardize_all_df = pd.DataFrame(adult_train_standardize_all,\n                                              columns=adult_train_standardize_all_df.columns)\n", "intent": "    StandardScaler().fit_transform()\n"}
{"snippet": "def write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('lr', LogisticRegression())\n]) \npipeline.fit(X_train, Y_train)\npipeline.score(x_test, y_test)\n", "intent": "Remove Tfidf also. Good / bad?\n"}
{"snippet": "def read_data():\n    return pd.read_csv('multiclassdata.csv').values\n", "intent": "Read in the data from the file `multiclassdata.csv` using pandas\n"}
{"snippet": "ames = pd.read_csv(\"https://www.openintro.org/stat/data/ames.csv\")\names.columns = [c.replace(\".\",\"\") for c in ames.columns]\n", "intent": "You can find a link to variable explanations [here](https://www.openintro.org/stat/data/?data=ames).\n"}
{"snippet": "countries = countries.fillna(countries.mean())\n", "intent": "Fill `NaN`s with the mean of that column.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)\n", "intent": "Split our data into training and test sets so we could visualise the accuracy of the model by testing it on our test data\n"}
{"snippet": "train_no_dummy_workingday_piv = all_no_dummy_interpolate[all_no_dummy_interpolate['train'] == 1].pivot_table(values='count',index='hour',columns=['workingday'])\n", "intent": "** Let's cluster hour and workingday impact on rentals. **\n"}
{"snippet": "c = pd.DataFrame({'VarA':['aa','bb'], 'VarB':[20,30]}, index = ['Case1','Case2'])\nc = c.drop(['Case1'])\nc\n", "intent": "We can delete entire rows and columns:\n"}
{"snippet": "df['Apartment and Flat avg'] = df['Apartment and Flat avg'].fillna(df['Apartment and Flat avg'].mean())\ndf['Individual House avg'] = df['Individual House avg'].fillna(df['Individual House avg'].mean())\ndf['Plot avg'] = df['Plot avg'].fillna(df['Plot avg'].mean())\ndf['Avg rate per sq ft'] = df['Avg rate per sq ft'].fillna(df['Avg rate per sq ft'].mean())\n", "intent": "**Data cleaning on featured variables :**\n"}
{"snippet": "tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n", "intent": "Faster, do both at once! \n"}
{"snippet": "diabetes_train, diabetes_test = train_test_split(diabetes_full, test_size=1/3)\ndiabetes_train = diabetes_train.copy()\ndiabetes_test = diabetes_test.copy()\n", "intent": "Before we begin any EDA, we better hold out our final test set.  Otherwise we will be implicitly learning about our test set through EDA.\n"}
{"snippet": "lr_biz_titles_pr = LogisticRegression(penalty='l1', C=lr_biz_titles_pr_gs.best_params_['C'],\n                                      solver='liblinear')\nlr_biz_titles_pr.fit(title_X, business)\ncoefs = pd.DataFrame({'coefs':lr_biz_titles_pr.coef_[0],\n                      'variable':title_cols})\n", "intent": "Print out the top 20 or 25 word features as ranked by their coefficients.\n---\n"}
{"snippet": "test_df = pd.read_csv(\"./proj2_test_data.csv\")\ntest_df['tpep_pickup_datetime'] = pd.to_datetime(test_df['tpep_pickup_datetime'])\ntest_df.head()\n", "intent": "Here we load our testing data on which we will evaluate your model.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                            test_size = 0.2, random_state=0)\n", "intent": " Split the features and the target into a Train and a Test subsets.\n Ratio should be 80/20\n"}
{"snippet": "triplet_dataset = pd.read_csv(filepath_or_buffer=data_home+'train_triplets.txt', \n                              sep='\\t', header=None, \n                              names=['user','song','play_count'])\n", "intent": "reading user, song, play_count data\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom patsy import dmatrix\nfeatures_df = dfrttf[['CA','MDW','NE','PNW','SE','SW','TX','Junior','Senior']]\ntarget_df = dfrttf['high_salary']\nX_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, \n                                                    test_size=0.33, random_state=5)\nscaler = StandardScaler() \n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "K-nearest neighbors (KNN) classification\n"}
{"snippet": "masked_epi = masker.inverse_transform(samples)\n", "intent": "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:\n"}
{"snippet": "feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n", "intent": "The model correctly identifies 62% of all expired loans as \"not funded\"\n"}
{"snippet": "import pandas as pd\nreviews = pd.read_csv('../data/en_reviews.csv', sep='\\t', header=None, names =['rating', 'text'])\nreviews = reviews['text'].tolist()\nprint(reviews[:2])\n", "intent": "Implement a text generator, which imitates reviewers of a travel agency using the LSTM language model.\n"}
{"snippet": "(train_data, train_label),(test_data, test_label) = reuters.load_data(num_words=10000)   \n", "intent": "**Load the data into variable**\n"}
{"snippet": "corpus_set = set(training_curpos.word)\ncount_vect = CountVectorizer(vocabulary = corpus_set, tokenizer=nltk.word_tokenize)\nbow_counts = count_vect.fit_transform(df.pureTextTweet)\nbow_counts.shape\n", "intent": "Bag-of-words where with occurences count:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=0.70, random_state=0)\n", "intent": "Do a train/test split, with 70% of the data used for training and a `random_state=0`:\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/same_filled_nan.csv\", index=False)\n", "intent": "---\n<a id=\"save\"></a>\n"}
{"snippet": "DATA_PATH = \"../data/home_default/\"\nbureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of bureau:\",    bureau.shape)\nprint(\"Shape of prev_app:\",  prev_app.shape)\n", "intent": "---\n<a id=\"load\"></a>\n"}
{"snippet": "DATA_PATH = \"../../data/home_default/\"\ntrain = pd.read_csv(DATA_PATH + \"train.csv\")\ntest  = pd.read_csv(DATA_PATH + \"test.csv\")\ntest['is_train'] = 0\ntrain['is_train'] = 1\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test:\",  test.shape)\n", "intent": "---\n<a id=\"load\"></a>\n"}
{"snippet": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "- This rubs me the wrong way though, I think another column should definitely be added\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=30)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "sacramento = pandas.read_csv('../../data/Sacramento-real-estate-transactions.csv')\n", "intent": "We will predict prices based on:\n* Bedrooms\n* Bathrooms\n* Other stuff\n"}
{"snippet": "df = pd.read_csv('./data/auto-mpg.csv')\n", "intent": "Categorical features\n"}
{"snippet": "data = datasets.load_boston()\nprint(data.DESCR) \n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\ndata.corr()\n", "intent": "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!\n"}
{"snippet": "df = pd.read_csv('./dow_jones_index.data')\ndf.head()\n", "intent": "The problems will use data from the down jones index.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1], [np.nan, np.nan, np.nan, 5]], columns=list('ABCD'))\ndf.head()\n", "intent": "from https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nbaseline_submission = pd.read_csv(\"baseline_submission.tsv\")\nmy_submission = pd.read_csv(\"./my_submission1.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "iris = pd.read_csv(\"./assets/datasets/iris.csv\")\niris.head(n=5)\n", "intent": "We're going to be using **Scikit-Learn** for our analysis; let's load in the [Iris](./assets/datasets/iris.csv) dataset using Pandas\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123) \n", "intent": "> Splitting the data set into a training and a test set\n"}
{"snippet": "import pandas as pd\ndata311=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Cluster_lecture/cluster.csv\")\n", "intent": "Please carefully read 2.3.9:\nhttp://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "longitudes = [point[0] for point in centermost_points]\nlatitudes = [point[1] for point in centermost_points]\nrep_points = pd.DataFrame({'lon':longitudes, 'lat':latitudes})\n", "intent": "Assemble all center-most points into a new series\n"}
{"snippet": "from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nrfe = rfe.fit(tf_train, target)\nprint(rfe.support_)wejtiaet\nprint(rfe.ranking_)\n", "intent": "To get a better idea of what our features are doing, we use scikit's feature importance library\n"}
{"snippet": "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n", "intent": "Notice the `\\t` in the string shown above. This means we have tab-separated values.\n"}
{"snippet": "X = pd.DataFrame(scaled_features, columns=X.columns)\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "def fit_pca(df, n_components):\n    pca = PCA(n_components = n_components)\n    pca.fit(df)\n    reduced = pca.fit_transform(df)\n    return pca, reduced\n", "intent": "Write a function named fit_pca() that takes a pandas.DataFrame and uses sklearn.decomposition.PCA to fit a PCA model on all values of df.\n"}
{"snippet": "def analyze_wav(filename):\n    s, fs, enc = ad.wavread(filename)  \n    s = np.array(s)\n    s = s / abs(s).max() \n    left = s[:,0]\n    right = s[:,1]\n    return left, right, fs\n", "intent": "**1. Looking at WAV File Composition**\nFirst, we explore a wav file to see what the data looks like\n"}
{"snippet": "df = pd.read_csv('/Users/rulanxiao/Documents/GitHub/APMAE4990-/data/hw2data.csv')\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "ccblc = pd.read_csv('./data/input/credit_card_balance.csv')\nccblc = ccblc.drop(['SK_ID_PREV', 'MONTHS_BALANCE'], axis=1)\nccblc = summary_extra_data(ccblc, 'ccblc_')\nfull_df = full_df.join(ccblc, on='SK_ID_CURR')\nadd_features(ccblc.columns.tolist())\n", "intent": "**credit_card_balance.csv**\n"}
{"snippet": "scaler = StandardScaler()\ncustomer_sc = scaler.fit_transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_sc_df.describe().T\nsc_stats['skew'] = st.skew(customer_sc_df)\nsc_stats['kurt'] = st.kurtosis(customer_sc_df)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv('../datasets/train.csv')\ndata.set_index('Date', inplace=True)\ndata.head()\n", "intent": "- Assemble observations and graphs as well as timeseries models in a notebook.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(df, test_size=0.25, random_state= 42)\n", "intent": "Now that the data is loaded, we proceed to splitting it into a training set and a testing set.\n"}
{"snippet": "path = \"../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "Let's analyze the sentiment of yelp reviews\n"}
{"snippet": "reviews = pd.read_csv(music_fname, sep='\\t')\nreviews.head()\n", "intent": "Our first attempt at reading in the csv file failed. Why?\n"}
{"snippet": "video = io.open('run4.mp4', 'r+b').read()\nencoded = base64.b64encode(video)\n", "intent": "The model without maxpooling is seen to perform as in the video (run-4) below.\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nreviews  = pd.read_csv('./data/reviews.csv')\ntrain, test = train_test_split(reviews, test_size=0.2, random_state=4622)\nX_train = train['reviews'].values\nX_test = test['reviews'].values\ny_train = train['sentiment']\ny_test = test['sentiment']\n", "intent": "Following is a dataset containing reviews and sentiments associated with it.\nCreate a SVM Classifier to predict positive or negative sentiments\n"}
{"snippet": "iris = load_iris()\nX_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, Y_train)\nclf_poly = svm.SVC(kernel='poly', C=1, degree=2).fit(X_train, Y_train)\nclf.score(X_test, Y_test)\nclf_poly.score(X_test, Y_test)\n", "intent": "We can use the `train_test_split` for testing the SVM Algorithm\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n", "intent": "* http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html\n"}
{"snippet": "stores_data = pd.read_csv('store.csv')  \nstores_data.head()\n", "intent": "the week aggregation is defined by the aggregation of observations starting from the week before the displayed week until the displayed week\n"}
{"snippet": "w=98\ncols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\ny_pred_test_mix=(w/100)*y_pred_test_nw2+y_pred_test_nw*((100-w)/100)\npd.DataFrame(y_pred_test_mix, index=dataTesting.index, columns=cols).to_csv('Final_pred_NW_MIX2.csv', index_label='ID')\n", "intent": "Export results Neural Network model for mixed model Images Transfer Learning VGG16+Text. Text model weight=0.84\n"}
{"snippet": "from sklearn.datasets import load_boston\ndataset = load_boston()\nprint(dataset.DESCR)\n", "intent": "Today, we'll apply linear regression to real-life data!\n"}
{"snippet": "features = X.columns\nfeature_importances = rf.feature_importances_\nfeatures_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\nfeatures_df.sort_values('Importance Score', inplace=True, ascending=False)\nfeatures_df.head(15)\n", "intent": "1. satisfaction_level \n2. time_spend_company\n3. number_project\n4. average_monthly_hours\n5. last_evaluation\n"}
{"snippet": "College_Data = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "data  = pd.read_csv('./dataset/dataset_1_full.txt')\n", "intent": "*** 1-b-1: Load the contents of dataset_1_full.txt into a pandas dataframe, or numpy array. ***\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "def tokenize(text): \n    tknzr = TweetTokenizer()\n    return tknzr.tokenize(text)\nen_stopwords = set(stopwords.words(\"english\")) \ncount_vect = CountVectorizer(tokenizer=tokenize, stop_words=en_stopwords, min_df=5)\nskf = StratifiedKFold(n_splits=5)\ncv = skf.get_n_splits(X_train, y_train)\n", "intent": "**PART A**\nUse CountVectorizer to vectorize reviews as dictionary of term frequencies.\nDefine the crossvalidation split using StratifiedKFold.\n"}
{"snippet": "with open('ultimate_data_challenge.json') as f:\n    ultimate = json.load(f)\nultimate = pd.DataFrame(ultimate)\nultimate.signup_date = pd.to_datetime(ultimate.signup_date)\nultimate.last_trip_date = pd.to_datetime(ultimate.last_trip_date)\nultimate.head()\n", "intent": "We start of course by loading and then briefly exploring the data. \n"}
{"snippet": "x_pca = pd.DataFrame(x_pca, columns=['PC1', 'PC2', 'PC3', 'PC4'])\nx_pca.head()\n", "intent": "Calculate the correlation matrix between the principal components and the original features:\n"}
{"snippet": "y = pd.DataFrame(data.target)\nX = pd.DataFrame(data.data, columns = data.feature_names)\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "X,y=make_blobs(n_samples=200, centers=2, n_features=2,\n               random_state=0,cluster_std=.5)\nsv.fit(X,y)\n", "intent": "We can create some synthetic data using `make_blobs` and then\nfit it to the SVM,\n"}
{"snippet": "users = pd.read_csv('takehome_users.csv')\nusers.info()\n", "intent": "We can read in the users file.\n"}
{"snippet": "colspecs = [(0, 11), (11, 21), (21, 31), (31, 38),(39,41),(41,72),(72,76),(76,80),(80,86)]\nstations = pd.read_fwf('/home/ubuntu/UCSD_BigData/data/weather/ghcnd-stations_buffered.txt', colspecs=colspecs, header=None,\n                       names=['station','latitude','longitude','elevation','state','name','GSNFLAG','HCNFLAG','WMOID'])\nstations = stations.ix[:,['station','longitude','latitude']]\nweights = pd.read_fwf('station_weights2',index_col=0, names=['weight'])\nstationJoin=stations.join(weights,on='station')\nstationJoin.dropna(axis=0)\nprint shape(stationJoin)\nstationJoin.head()\n", "intent": "This step prepare the source data for geographic partitiong in the next step. \n"}
{"snippet": "f = '/home/szong/bin/GISTIC_2_0_22/124_patients_20180323/hg19_cytoband_ucsc.txt'\ndf = pd.read_csv(f, header=None, sep='\\t')\ndf.columns = ['chr', 'start', 'end', 'cytoband', 'comments']\ndf.head()\n", "intent": "clean up copy number calls\n"}
{"snippet": "from sklearn.datasets import load_iris\nevaluate_features(*load_iris(True))\n", "intent": "Let's do a quick test of evaluate_features\n"}
{"snippet": "of = '/projects/trans_scratch/validations/workspace/szong/Cervical/hotspots/123_patients/mut_rate.txt'\nmut_rate = pd.read_csv(of, sep='\\t')\nmut_rate['mut_rate_per_base'] = mut_rate.mut_rate/1e6\nmut_rate.head(2)\n", "intent": "figure out mutation rate for the whole genome mu=expected mutation rate at each position\n"}
{"snippet": "df = pd.read_csv('./breast-cancer-wisconsin.data')\ndf.drop('id', axis=1, inplace=True)\ndf.head()\n", "intent": "Now let's switch gears and load in the familiar breast cancer data\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.4, random_state=1734)\n", "intent": "**Part A**: Construct a K-Nearest Neighbors classifier to make predictions on the data. \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=0)\n", "intent": "<font color='red'>TASK MARK: 1</font>\n<br>\n<font color='green'>COMMENT:  - </font>\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardizer = StandardScaler()\nstandardizer.fit(X)\nXst = standardizer.transform(X)\n", "intent": "In the following we standardize this dataset for numerical stability.  Note that due to the exponents extreme values can be an issue.\n"}
{"snippet": "mnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n", "intent": "Load and prepare the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. Convert the samples from integers to floating-point numbers:\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.cross_validation import train_test_split\nX, y, true_coefficient = make_regression(n_samples=80, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\n", "intent": "```\ny_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n```\n"}
{"snippet": "from sklearn.kernel_approximation import RBFSampler\nsgd = SGDClassifier()\nkernel_approximation = RBFSampler(gamma=.001, n_components=400)\nfor i in range(9):\n    X_batch, y_batch = cPickle.load(open(\"data/batch_%02d.pickle\" % i))\n    if i == 0:\n        kernel_approximation.fit(X_batch)\n    X_transformed = kernel_approximation.transform(X_batch)\n    sgd.partial_fit(X_transformed, y_batch, classes=range(10))\n", "intent": "Kernel Approximations\n=======================\n"}
{"snippet": "url = '../assets/dataset/bikeshare/bikeshare.csv'\nbikes = pd.read_csv(url, parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "X1train, X1test, y1train, y1test = train_test_split(X1,y1, test_size=0.3, random_state = 7)\nX2train, X2test, y2train, y2test = train_test_split(X2,y2, test_size=0.3, random_state = 7)\n", "intent": "Generate train test variables for both word sets\n"}
{"snippet": "def movie_parser(m):\n    return [m['num_votes'], m['rating'], m['tconst'], m['title'], m['year']]\nmovies = pd.DataFrame(np.array([movie_parser(m) for m in top]),\n                      columns=['num_votes','rating','tconst','title','year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "data = pd.read_csv(\"http://www.ds100.org/sp18/assets/datasets/hw5_data.csv\", index_col=0)\ndata.head()\n", "intent": "Load the data.csv file into a pandas dataframe.  \nNote that we are reading the data directly from the URL address.\n"}
{"snippet": "print X.columns\npd.DataFrame(model.feature_importances_,index=X.columns)\n", "intent": "The precision of acc is 105/sum(105,1,3,3)\nthe recall of acc is 105/sum(105,4,6,3)\n"}
{"snippet": "project_unit = pd.read_csv('project_unit.csv', delimiter = ';')\nproject_unit.head(3)\n", "intent": "Let's import data into a dataframe with pandas.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\nbreast_cancer = load_breast_cancer()\nX = breast_cancer['data']\ny = breast_cancer['target']\n", "intent": "The dataset can be loaded with *sklearn.datasets.load_breast_cancer*.\n"}
{"snippet": "yelp = pd.read_csv('yelp.csv')\nyelp.head(3)\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params, cv=5, n_jobs=-1, verbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now, let's tune the number of neighbors $k$ for k-NN:\n"}
{"snippet": "df = pd.read_csv('health_data.txt', delimiter='\\t')\nprint(len(df), df.columns)\ndf.head()\n", "intent": "As usual start by importing our dataset:\n"}
{"snippet": "sac = pd.read_csv('/Users/smoot/Downloads/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "test_file = data_path + r\"\\test.json\"\nX_test2 = pd.read_json(test_file)\nprint(\"Test dataset loaded\")\ntype(X_test2)\n", "intent": "By using best fit parameter we improved the model by 1%\n"}
{"snippet": "df_fea = pd.DataFrame(scaler_fea,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "features = pd.concat([categorical_data, data[['size']]], axis=1)\nlabels = data.review\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=10)\n", "intent": "Let us create the training and testing set to be used with our model.\n"}
{"snippet": "enc = OneHotEncoder()\ntransformed = enc.fit_transform(selected[key_attributes].apply(lambda x: x.cat.codes))\nN, D = transformed.shape\nN, D\n", "intent": "Same as before, we have three columns of categorical data. We now convert them into one-hot encoding.\n"}
{"snippet": "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\nX += np.random.random(X.shape)\nsets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]\n", "intent": "<h1 align=\"center\">Warm Up: 3 datasets</h1> \n"}
{"snippet": "from sklearn import datasets\npoints, labels = datasets.make_moons(n_samples=100, noise=.05)\n", "intent": "Aside from issues with choosing the number of groups *a priori*, K-means has trouble identifying groups that are not spherical and convex.\n"}
{"snippet": "print(pd.DataFrame({'effect': params.round(0),\n                    'error': err.round(0)}))\n", "intent": "- With these errors estimated, let's again look at the results:\n"}
{"snippet": "scalerSVM = StandardScaler()\n", "intent": "* Scale Data for SVM:\n"}
{"snippet": "X_train, X_test, y_train, y_test = \\\n    train_test_split(yelp_class['text'],\n                     yelp_class['stars'],\n                     test_size=0.3,\n                     random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "tweets = pd.read_csv('/Users/macbook/GA-DSI/projects/projects-capstone/captured-tweets.txt', sep='\\n', header = None, error_bad_lines=False)\n", "intent": "```It also took forever to get the tweets to load without errors```\n"}
{"snippet": "imputer = preprocessing.Imputer(missing_values='NaN', strategy = 'mean', axis = 0)\nimputer.fit(raw_data.loc[:, idx_cols_missing])\nraw_data.loc[:, idx_cols_missing] = imputer.transform(raw_data.loc[:, idx_cols_missing])\n", "intent": "As there are only 8 missing values in InteractionTime, Therefore, the way of dealing with missing values won't affect prediction results too much \n"}
{"snippet": "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\ntrain_features, valid_features, train_labels, valid_labels = train_test_split(\n    train_features,\n    train_labels,\n    test_size=0.05,\n    random_state=832289)\nprint('Training features and labels randomized and split.')\n", "intent": "** Split Training and Validation Sets:**\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.data')\ndf.head()\n", "intent": "The cell below reads in a simulated dataset where y is an unknown function of a, b, and c.\n"}
{"snippet": "data = pd.read_csv(\"Advertising.csv\")\n", "intent": "Load Advertising data set\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news')\nnews_A = pd.read_csv(data_path + '_partA.csv', delimiter = ',')\nnews_B = pd.read_csv(data_path + '_partB.csv', delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "url = './assets/dataset/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n", "intent": "* Convertimos los valores en un objeto dataframe y graficamos la curva ROC\n"}
{"snippet": "import random\nimport numpy as np\nimport pandas as pd\nX = pd.DataFrame([x for x in range(0,15)])\nY = pd.Series([2*(1 + random.random()) + 5*x*(1+random.random()) for x in X])\nX\n", "intent": "queremos hacer una regresion lineal\ngenero una variable X de 0 a 15, y una variable Y de la forma Y = 2 + 5x con ruido\n"}
{"snippet": "ica = repres.FastICA(n_components=3)\nres = ica.fit_transform(df[feat_cols].values)\ndf['ICA-one'] = res[:,0]\ndf['ICA-two'] = res[:,1] \ndf['ICA-three'] = res[:,2]\nsns.pairplot(df, hue=\"label\", x_vars=[\"ICA-one\",\"ICA-two\"], y_vars=[\"ICA-two\",\"ICA-three\"], plot_kws={\"alpha\": 0.2},  size=4,)\n", "intent": "<h2>What would the representation of Independent Component Analysis look like?</h3>\n"}
{"snippet": "allbeers = pd.read_csv('beer_reviews.tar.gz', compression='gzip')\n", "intent": "Import data in a pandas dataframe called \"allbeers\". Use the compression keyword\n"}
{"snippet": "needs_pivot_risk = pd.pivot_table(needs.join(profile['AT-RISK']), index='AT-RISK')\nneeds_pivot_risk.head()\n", "intent": "Let's start by visually inspecting needs for those designated at-risk versus their well counterparts.\n"}
{"snippet": "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\nnsamples, width, height = X_train.shape\nnfeatures = width * height\nX_train = X_train/255\nX_test = X_test/255\n", "intent": "We use Keras' datasets module to load the data:\n"}
{"snippet": "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\nX_train = train_data.drop('y', axis=1)\nX_test = test_data.drop('y', axis=1)\ny_train = train_data['y']\ny_test = test_data['y']\n", "intent": "Creating a 80/20 split for training and testing respectively. These variables will be used for Problem 4.2 & 4.3.\n"}
{"snippet": "size = 1000\nX_moons, y_moons = make_moons(size, noise=0.05, random_state=42)\n", "intent": "As input, moons data with 1000 points, a noise of 0.05 and a fixed random state to have a repetable model\n"}
{"snippet": "scaled_features = scaler.fit_transform(bank_note.drop('Class', axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    paribas_numeric.drop(labels=['target', 'ID'], axis=1),\n    paribas_numeric['target'],\n    test_size=0.3,\n    random_state=0\n)\nX_train.shape, X_test.shape\n", "intent": "Before doing anything, let's first split the dataset and examine only the training subset.\n"}
{"snippet": "results = pd.DataFrame(index=['k='+ str(i) for i in range(1, 11)], columns=['mean_accuracy'])\n", "intent": "Using the univariate selection method SelectKBest, which is the value of k that maximizes the accuracy of the model\n"}
{"snippet": "data = pd.read_csv('training.csv')\n", "intent": "- First step is to load the csv data into a data frame\n"}
{"snippet": "custdata14 = pd.read_csv('C:\\LoanStats14.csv',low_memory=False)\n", "intent": "We are going to be combining 2014 and 2015 dataset in one dataframe and call it \"custdata\"\n"}
{"snippet": "from sklearn.datasets import load_boston\ndataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h3>  Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nUse a code below to download the  dataset: \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['is_benign', 'is_malignant'],columns=['predicted_benign', 'predicted_malignant'])\nconfusion\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "cvec = CountVectorizer(binary=True, ngram_range=(1,3), stop_words='english', max_features=5000)\nX = cvec.fit_transform(rt[\"quote\"])\nX.shape\n", "intent": "It is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "sim_data = pd.read_csv('/home/data_scientist/data/misc/sim.bNA.data')\nsim_data.head()\n", "intent": "In the cell below a simulated dataset is read in, where y (labels) is a unknown function of a, b, and c (your features).\n"}
{"snippet": "data_clus=pd.concat([data_geog.reset_index().drop('index',axis=1),pd.DataFrame(final_kmeans.labels_,columns=['Cluster'])],axis=1)\n", "intent": "Now to make the new dataset with the cluster data\n"}
{"snippet": "sales.to_csv('sales.csv', index=False)\n", "intent": "Note - this submission (submission17) scored 0.95281 on the public leaderboard (3/31/2018)\n"}
{"snippet": "month_wise_delay=flights_df.groupby(['month'])['dep_delay'].mean()\nmonth_wise_delay = pd.DataFrame(month_wise_delay)\nmonth_wise_delay.sort_values(ascending=False,by='dep_delay')\n", "intent": "(c) Are there any seasonal patterns in departure delays for flights from NYC?\n"}
{"snippet": "df = pd.read_csv(\"seeds_dataset.csv\",names=[\"area\", \"perimeter\", \"compactness\",\"length of kernel\",\"width of kernel\",\"asymmetry coefficient\",\"length of kernel groove\",\"target\"] ) \ndf\n", "intent": "For context of the data, see the documentation here: https://archive.ics.uci.edu/ml/datasets/seeds\n"}
{"snippet": "def make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n", "intent": "Lets put this alltogether. Below we write a function to create multiple datasets, one for each polynomial degree:\n"}
{"snippet": "bonds['market_issue'] = bonds['market_issue'].fillna('Global')\ncounts = bonds['market_issue'].value_counts()\nuncommon = counts[counts < 60]\nbonds['market_issue'] = bonds['market_issue'].apply(lambda x: 'Rare' if x in uncommon.index else x)\n", "intent": "Clean market issue.\n"}
{"snippet": "from sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\nX = housing[\"data\"]\ny = housing[\"target\"]\n", "intent": "Let's load the dataset using Scikit-Learn's `fetch_california_housing()` function:\n"}
